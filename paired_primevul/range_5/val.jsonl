{"project": "Chrome", "commit_id": "327585cb0eab0859518643a2d00917081f7e7645", "target": 1, "func": "WebGraphicsContext3DDefaultImpl::WebGraphicsContext3DDefaultImpl()\n    : m_initialized(false)\n    , m_renderDirectlyToWebView(false)\n     , m_texture(0)\n     , m_fbo(0)\n     , m_depthStencilBuffer(0)\n     , m_multisampleFBO(0)\n     , m_multisampleDepthStencilBuffer(0)\n     , m_multisampleColorBuffer(0)\n    , m_boundFBO(0)\n#ifdef FLIP_FRAMEBUFFER_VERTICALLY", "cwe": "", "big_vul_idx": 183491, "idx": 4575, "hash": 78432407338258014397826864332755560960}
{"project": "Chrome", "commit_id": "327585cb0eab0859518643a2d00917081f7e7645", "target": 0, "func": "    : m_initialized(false)\n    , m_renderDirectlyToWebView(false)\n     , m_texture(0)\n     , m_fbo(0)\n     , m_depthStencilBuffer(0)\n    , m_cachedWidth(0)\n    , m_cachedHeight(0)\n     , m_multisampleFBO(0)\n     , m_multisampleDepthStencilBuffer(0)\n     , m_multisampleColorBuffer(0)\n    , m_boundFBO(0)\n#ifdef FLIP_FRAMEBUFFER_VERTICALLY", "cwe": "", "big_vul_idx": 183491, "idx": 161720, "hash": 170893231026076192607745221635147409815}
{"project": "Chrome", "commit_id": "123e68f88fd0ed4f7447ba81148f9b619b947c47", "target": 1, "func": "    SkBitmap bitmap = SystemClipboard::GetInstance().ReadImage(\n        mojom::ClipboardBuffer::kStandard);\n\n     SkPixmap pixmap;\n     bitmap.peekPixels(&pixmap);\n \n    Vector<uint8_t> png_data;\n     SkPngEncoder::Options options;\n    options.fZLibLevel = 1;  // Fastest compression.\n     if (!ImageEncoder::Encode(&png_data, pixmap, options))\n       return nullptr;\n \n    auto data = std::make_unique<BlobData>();\n    data->SetContentType(kMimeTypeImagePng);", "cwe": "", "big_vul_idx": 183613, "idx": 4678, "hash": 302659875351380439224615645999140556334}
{"project": "Chrome", "commit_id": "123e68f88fd0ed4f7447ba81148f9b619b947c47", "target": 0, "func": "        mojom::ClipboardBuffer::kStandard);\n\n     SkPixmap pixmap;\n     bitmap.peekPixels(&pixmap);\n \n    // Set encoding options to favor speed over size.\n     SkPngEncoder::Options options;\n    options.fZLibLevel = 1;\n    options.fFilterFlags = SkPngEncoder::FilterFlag::kNone;\n\n    Vector<uint8_t> png_data;\n     if (!ImageEncoder::Encode(&png_data, pixmap, options))\n       return nullptr;\n \n    auto data = std::make_unique<BlobData>();\n    data->SetContentType(kMimeTypeImagePng);", "cwe": "", "big_vul_idx": 183613, "idx": 161816, "hash": 99565885850033950966117426207956082917}
{"project": "Chrome", "commit_id": "1da0daecc540238cb473f0d6322da51d3a544244", "target": 1, "func": " void VideoRendererBase::FrameReady(VideoDecoder::DecoderStatus status,\n                                   scoped_refptr<VideoFrame> frame) {\n   base::AutoLock auto_lock(lock_);\n   DCHECK_NE(state_, kUninitialized);\n \n  CHECK(pending_read_);\n  pending_read_ = false;", "cwe": "", "big_vul_idx": 184481, "idx": 5439, "hash": 58467890702351424416835303863183847864}
{"project": "Chrome", "commit_id": "1da0daecc540238cb473f0d6322da51d3a544244", "target": 0, "func": " void VideoRendererBase::FrameReady(VideoDecoder::DecoderStatus status,\n                                   const scoped_refptr<VideoFrame>& frame) {\n   base::AutoLock auto_lock(lock_);\n   DCHECK_NE(state_, kUninitialized);\n \n  CHECK(pending_read_);\n  pending_read_ = false;", "cwe": "", "big_vul_idx": 184481, "idx": 162555, "hash": 27375883205547855814872471618190506441}
{"project": "Chrome", "commit_id": "1a113d35a19c0ed6500fb5c0acdc35730617fb3f", "target": 1, "func": "    ExtensionPrefsScope scope) {\n  bool notify = false;\n   {\n     base::AutoLock lock(lock_);\n     OriginIdentifierValueMap* map = GetValueMap(ext_id, scope);\n      char ext_id_buffer[33];\n      base::strlcpy(ext_id_buffer, ext_id.c_str(), sizeof(ext_id_buffer));\n      base::debug::Alias(ext_id_buffer);\n      CHECK(false);\n     }\n     notify = !map->empty();\n     map->clear();\n  }\n  if (notify) {", "cwe": "", "big_vul_idx": 184504, "idx": 5458, "hash": 265797374762752738816496693872000415093}
{"project": "Chrome", "commit_id": "1a113d35a19c0ed6500fb5c0acdc35730617fb3f", "target": 0, "func": "    ExtensionPrefsScope scope) {\n  bool notify = false;\n   {\n     base::AutoLock lock(lock_);\n     OriginIdentifierValueMap* map = GetValueMap(ext_id, scope);\n      // Fail gracefully in Release builds.\n      NOTREACHED();\n      return;\n     }\n     notify = !map->empty();\n     map->clear();\n  }\n  if (notify) {", "cwe": "", "big_vul_idx": 184504, "idx": 162573, "hash": 12967184921243506354797539177835054040}
{"project": "Chrome", "commit_id": "87190165c55bcf3eecd8824dd8d083f5e3236552", "target": 1, "func": "  scoped_refptr<AudioOutputDispatcher>& dispatcher =\n      output_dispatchers_[params];\n   if (!dispatcher) {\n     base::TimeDelta close_delay =\n         base::TimeDelta::FromSeconds(kStreamCloseDelaySeconds);\n#if defined(OS_WIN) || defined(OS_MACOSX)\n     const CommandLine* cmd_line = CommandLine::ForCurrentProcess();\n    if (!cmd_line->HasSwitch(switches::kDisableAudioMixer)) {\n       dispatcher = new AudioOutputMixer(this, params, close_delay);\n    } else\n#endif\n    {\n       dispatcher = new AudioOutputDispatcherImpl(this, params, close_delay);\n     }\n   }\n  return new AudioOutputProxy(dispatcher);\n}", "cwe": "", "big_vul_idx": 184505, "idx": 5459, "hash": 22383497495526524382736548714282288020}
{"project": "Chrome", "commit_id": "87190165c55bcf3eecd8824dd8d083f5e3236552", "target": 0, "func": "\n  scoped_refptr<AudioOutputDispatcher>& dispatcher =\n      output_dispatchers_[params];\n   if (!dispatcher) {\n     base::TimeDelta close_delay =\n         base::TimeDelta::FromSeconds(kStreamCloseDelaySeconds);\n     const CommandLine* cmd_line = CommandLine::ForCurrentProcess();\n    // TODO(dalecurtis): Browser side mixing has a couple issues that must be\n    // fixed before it can be turned on by default: http://crbug.com/138098 and\n    // http://crbug.com/140247\n    if (cmd_line->HasSwitch(switches::kEnableAudioMixer)) {\n       dispatcher = new AudioOutputMixer(this, params, close_delay);\n    } else {\n       dispatcher = new AudioOutputDispatcherImpl(this, params, close_delay);\n     }\n   }\n  return new AudioOutputProxy(dispatcher);\n}", "cwe": "", "big_vul_idx": 184505, "idx": 162574, "hash": 163384496782536430146812673574056017632}
{"project": "Chrome", "commit_id": "a5333583f14284a411abac2fef7caed889a8bba3", "target": 1, "func": " void ServiceWorkerScriptContext::OnInstallEvent(\n     int active_version_embedded_worker_id) {\n  DidHandleInstallEvent(current_request_id_);\n }", "cwe": "", "big_vul_idx": 185011, "idx": 5909, "hash": 199087988793021330181873049420535142766}
{"project": "Chrome", "commit_id": "a5333583f14284a411abac2fef7caed889a8bba3", "target": 0, "func": " void ServiceWorkerScriptContext::OnInstallEvent(\n     int active_version_embedded_worker_id) {\n  proxy_->dispatchInstallEvent(current_request_id_);\n }", "cwe": "", "big_vul_idx": 185011, "idx": 162999, "hash": 294367026305307966881882708988400529093}
{"project": "Chrome", "commit_id": "9a3dbf43f97aa7cb6b4399f9b11ce1de20f0680f", "target": 1, "func": "void SpeechSynthesis::handleSpeakingCompleted(SpeechSynthesisUtterance* utterance, bool errorOccurred)\n {\n     ASSERT(utterance);\n \n     bool didJustFinishCurrentUtterance = false;\n    if (utterance == currentSpeechUtterance()) {\n        m_utteranceQueue.removeFirst();\n        didJustFinishCurrentUtterance = true;\n    }", "cwe": "", "big_vul_idx": 185386, "idx": 6244, "hash": 51064366369637881630682008748783014232}
{"project": "Chrome", "commit_id": "9a3dbf43f97aa7cb6b4399f9b11ce1de20f0680f", "target": 0, "func": "void SpeechSynthesis::handleSpeakingCompleted(SpeechSynthesisUtterance* utterance, bool errorOccurred)\n {\n     ASSERT(utterance);\n \n    // Keep the utterance around long enough to fire an event on it in case m_utteranceQueue\n    // is holding the last reference to it.\n    RefPtrWillBeRawPtr<SpeechSynthesisUtterance> protect(utterance);\n\n     bool didJustFinishCurrentUtterance = false;\n    if (utterance == currentSpeechUtterance()) {\n        m_utteranceQueue.removeFirst();\n        didJustFinishCurrentUtterance = true;\n    }", "cwe": "", "big_vul_idx": 185386, "idx": 163331, "hash": 23380105532145118333362184433256570897}
{"project": "Chrome", "commit_id": "ea994548ed483e234a6fadd0cbdfa10d58b75cef", "target": 1, "func": "  scoped_ptr<base::SharedMemory> shared_memory(\n      new base::SharedMemory(frame_data->handle, true));\n#endif\n \n   if (base::SharedMemory::IsHandleValid(shared_memory->handle())) {\n    const size_t size_in_bytes = 4 * frame_data->size.GetArea();\n #ifdef OS_WIN\n     if (!shared_memory->Map(0)) {\n       DLOG(ERROR) << \"Unable to map renderer memory.\";\n      RecordAction(\n          base::UserMetricsAction(\"BadMessageTerminate_SharedMemoryManager1\"));", "cwe": "", "big_vul_idx": 185417, "idx": 6270, "hash": 116003027451590305936545199542467169705}
{"project": "Chrome", "commit_id": "ea994548ed483e234a6fadd0cbdfa10d58b75cef", "target": 0, "func": "  scoped_ptr<base::SharedMemory> shared_memory(\n      new base::SharedMemory(frame_data->handle, true));\n#endif\n \n   if (base::SharedMemory::IsHandleValid(shared_memory->handle())) {\n    base::CheckedNumeric<size_t> size_in_bytes_checked =\n        base::CheckedNumeric<size_t>(4) *\n        base::CheckedNumeric<size_t>(frame_data->size.width()) *\n        base::CheckedNumeric<size_t>(frame_data->size.height());\n    if (!size_in_bytes_checked.IsValid()) {\n      DLOG(ERROR) << \"Integer overflow when computing bytes to map.\";\n      return false;\n    }\n    size_t size_in_bytes = size_in_bytes_checked.ValueOrDie();\n #ifdef OS_WIN\n     if (!shared_memory->Map(0)) {\n       DLOG(ERROR) << \"Unable to map renderer memory.\";\n      RecordAction(\n          base::UserMetricsAction(\"BadMessageTerminate_SharedMemoryManager1\"));", "cwe": "", "big_vul_idx": 185417, "idx": 163357, "hash": 38030128573621593790463547507154903710}
{"project": "Chrome", "commit_id": "3454ed7b88318dcd4539c6e1a50d27b0ca535686", "target": 1, "func": "    url = entry->GetURL();\n    if (!url.is_valid())\n       return base::string16();\n   }\n \n  if (!url.SchemeIs(url::kHttpScheme) || (url.path().length() > 1))\n     return base::string16();\n \n   return TemplateURL::GenerateKeyword(url, accept_languages);\n }", "cwe": "", "big_vul_idx": 185451, "idx": 6302, "hash": 186236423855654680462739638936218281309}
{"project": "Chrome", "commit_id": "3454ed7b88318dcd4539c6e1a50d27b0ca535686", "target": 0, "func": "    url = entry->GetURL();\n    if (!url.is_valid())\n       return base::string16();\n   }\n \n  // Don't autogenerate keywords for referrers that\n  // a) are anything other than HTTP/HTTPS or\n  // b) have a path.\n  if (!(url.SchemeIs(url::kHttpScheme) || url.SchemeIs(url::kHttpsScheme)) ||\n      (url.path().length() > 1)) {\n     return base::string16();\n  }\n \n   return TemplateURL::GenerateKeyword(url, accept_languages);\n }", "cwe": "", "big_vul_idx": 185451, "idx": 163389, "hash": 215967538991632573956354911036881851299}
{"project": "Chrome", "commit_id": "6834289784ed45b5524de0fb7ef43ae283b0d6d3", "target": 1, "func": "        return;\n    }\n\n     MutexTryLocker tryLocker(m_processLock);\n     if (tryLocker.locked()) {\n         if (AudioSourceProvider* provider = mediaElement()->audioSourceProvider()) {\n             if (m_multiChannelResampler.get()) {\n                 ASSERT(m_sourceSampleRate != sampleRate());\n                 m_multiChannelResampler->process(provider, outputBus, numberOfFrames);\n            } else {\n                 ASSERT(m_sourceSampleRate == sampleRate());\n                 provider->provideInput(outputBus, numberOfFrames);\n             }\n         } else {\n            outputBus->zero();\n        }\n    } else {\n        outputBus->zero();", "cwe": "", "big_vul_idx": 185901, "idx": 6705, "hash": 214516605251072031911924399596715967965}
{"project": "Chrome", "commit_id": "6834289784ed45b5524de0fb7ef43ae283b0d6d3", "target": 0, "func": "    }\n\n     MutexTryLocker tryLocker(m_processLock);\n     if (tryLocker.locked()) {\n         if (AudioSourceProvider* provider = mediaElement()->audioSourceProvider()) {\n            // Grab data from the provider so that the element continues to make progress, even if\n            // we're going to output silence anyway.\n             if (m_multiChannelResampler.get()) {\n                 ASSERT(m_sourceSampleRate != sampleRate());\n                 m_multiChannelResampler->process(provider, outputBus, numberOfFrames);\n            } else {\n                 ASSERT(m_sourceSampleRate == sampleRate());\n                 provider->provideInput(outputBus, numberOfFrames);\n             }\n            // Output silence if we don't have access to the element.\n            if (!(mediaElement()->webMediaPlayer()->didPassCORSAccessCheck()\n                || context()->securityOrigin()->canRequest(mediaElement()->currentSrc()))) {\n                outputBus->zero();\n            }\n         } else {\n            outputBus->zero();\n        }\n    } else {\n        outputBus->zero();", "cwe": "", "big_vul_idx": 185901, "idx": 163787, "hash": 144288665457382459246238377147852566025}
{"func": "    delim_char = delimiter.charAt(0);\n    quote_delim = true;\n  }\n\n  /* Allocate enough memory so that even if each character\n     is quoted, we won't run out of room */\n  String ret(4 * str.size() + 1, ReserveString);\n  char* out_str = ret.mutableData();\n\n  /* Go through the string and quote necessary characters */\n  const char* p;", "project": "hhvm", "hash": 189278878836233230295539538309071035452, "size": 55, "commit_id": "08193b7f0cd3910256e00d599f0f3eb2519c44ca", "message": "security fixes\n\nhttps://hhvm.com/blog/2021/02/25/security-update.html", "target": 1, "dataset": "other", "idx": 194999}
{"func": "    quote_delim = true;\n  }\n\n  /* Allocate enough memory so that even if each character\n     is quoted, we won't run out of room */\n  static_assert(\n    (StringData::MaxSize * 4 + 1) < std::numeric_limits<int64_t>::max()\n  );\n  String ret(4 * str.size() + 1, ReserveString);\n  char* out_str = ret.mutableData();\n\n  /* Go through the string and quote necessary characters */\n  const char* p;", "project": "hhvm", "hash": 220981898952866903098305403168592775003, "size": 58, "commit_id": "08193b7f0cd3910256e00d599f0f3eb2519c44ca", "message": "security fixes\n\nhttps://hhvm.com/blog/2021/02/25/security-update.html", "target": 0, "dataset": "other", "idx": 219542}
{"func": "  }\n  /*\n   * Add the terminating null here since it wasn't added incrementally above\n   * once the whole string has been composed.\n   */\n  result[outpos] = NUL;\n  *outbuf = result;\n  return outpos;\n}", "project": "hhvm", "hash": 262008690145899457289472739326596420497, "size": 592, "commit_id": "08193b7f0cd3910256e00d599f0f3eb2519c44ca", "message": "security fixes\n\nhttps://hhvm.com/blog/2021/02/25/security-update.html", "target": 1, "dataset": "other", "idx": 195007}
{"func": "  }\n  /*\n   * Add the terminating null here since it wasn't added incrementally above\n   * once the whole string has been composed.\n   */\n  appendchar(&result, &outpos, &size, NUL);\n  *outbuf = result;\n  return outpos - 1;\n}", "project": "hhvm", "hash": 85852174078948997591369954532322878743, "size": 592, "commit_id": "08193b7f0cd3910256e00d599f0f3eb2519c44ca", "message": "security fixes\n\nhttps://hhvm.com/blog/2021/02/25/security-update.html", "target": 0, "dataset": "other", "idx": 219451}
{"func": "  if(stream.peek() == '[')\n  {\n    return LoadAsxIniInfo(stream);\n  }\n  else\n  {\n    CXBMCTinyXML xmlDoc;\n    stream >> xmlDoc;\n\n    if (xmlDoc.Error())\n    {\n      CLog::Log(LOGERROR, \"Unable to parse ASX info Error: {}\", xmlDoc.ErrorDesc());\n      return false;\n    }\n\n    TiXmlElement *pRootElement = xmlDoc.RootElement();\n\n    // lowercase every element\n    TiXmlNode *pNode = pRootElement;\n    TiXmlNode *pChild = NULL;\n    std::string value;", "project": "xbmc", "hash": 328153551345565129880768466980342348853, "size": 110, "commit_id": "80c8138c09598e88b4ddb6dbb279fa193bbb3237", "message": "[Playlist] dont use istream directly to a tinyxml structure\n\nTurn istream into a std::string to handle large buffers (#20305)", "target": 1, "dataset": "other", "idx": 195020}
{"func": "  {\n    return LoadAsxIniInfo(stream);\n  }\n  else\n  {\n    std::string asxstream(std::istreambuf_iterator<char>(stream), {});\n    CXBMCTinyXML xmlDoc;\n    xmlDoc.Parse(asxstream, TIXML_DEFAULT_ENCODING);\n\n    if (xmlDoc.Error())\n    {\n      CLog::Log(LOGERROR, \"Unable to parse ASX info Error: {}\", xmlDoc.ErrorDesc());\n      return false;\n    }\n\n    TiXmlElement *pRootElement = xmlDoc.RootElement();\n\n    if (!pRootElement)\n      return false;\n\n    // lowercase every element\n    TiXmlNode *pNode = pRootElement;\n    TiXmlNode *pChild = NULL;\n    std::string value;", "project": "xbmc", "hash": 48091596250511939083384668842890615241, "size": 114, "commit_id": "80c8138c09598e88b4ddb6dbb279fa193bbb3237", "message": "[Playlist] dont use istream directly to a tinyxml structure\n\nTurn istream into a std::string to handle large buffers (#20305)", "target": 0, "dataset": "other", "idx": 219934}
{"func": "      const auto& sparsity = *filter->sparsity;\n      if (!SupportedSparsityFormat(sparsity)) {\n        TF_LITE_KERNEL_LOG(context,\n                           \"Unsupported sparse fully-connected weight format.\");\n        return kTfLiteError;\n      }\n\n      if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\n        // Random sparse.\n        optimized_ops::FullyConnectedSparseWeight(\n            sparsity, op_params, GetTensorShape(input),\n            GetTensorData<float>(input), GetTensorShape(filter),\n            GetTensorData<float>(filter), GetTensorShape(bias),\n            GetTensorData<float>(bias), GetTensorShape(output),\n            GetTensorData<float>(output));\n      } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\n                 sparsity.dim_metadata[2].dense_size == 4) {\n        // Block sparse with block size of 1x4.\n        optimized_ops::FullyConnectedSparseWeight1x4(\n            sparsity, op_params, GetTensorShape(input),\n            GetTensorData<float>(input), GetTensorShape(filter),\n            GetTensorData<float>(filter), GetTensorShape(bias),\n            GetTensorData<float>(bias), GetTensorShape(output),\n            GetTensorData<float>(output),\n            CpuBackendContext::GetFromContext(context));\n      } else {\n        TF_LITE_KERNEL_LOG(context,\n                           \"Unsupported sparse fully-connected weight format.\");\n        return kTfLiteError;", "project": "tensorflow", "hash": 44351393402196759229206049448766479746, "size": 78, "commit_id": "6c0b2b70eeee588591680f5b7d5d38175fd7cdf6", "message": "[lite] add validation check for sparse fully connected\n\nPiperOrigin-RevId: 417629354\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7", "target": 1, "dataset": "other", "idx": 195042}
{"func": "      if (!SupportedSparsityFormat(sparsity)) {\n        TF_LITE_KERNEL_LOG(context,\n                           \"Unsupported sparse fully-connected weight format.\");\n        return kTfLiteError;\n      }\n      const auto& input_shape = GetTensorShape(input);\n      const auto& filter_shape = GetTensorShape(filter);\n      const auto& output_shape = GetTensorShape(output);\n      const auto& bias_shape = GetTensorShape(bias);\n      if (!VerifySparsity(filter_shape, input_shape, output_shape, &sparsity)) {\n        TF_LITE_KERNEL_LOG(context, \"Invalid sparse fully-connected format.\");\n        return kTfLiteError;\n      }\n\n      if (sparsity.dim_metadata_size == kDimMetadataSizeRandomSparse) {\n        // Random sparse.\n        optimized_ops::FullyConnectedSparseWeight(\n            sparsity, op_params,                         // Disable formatting\n            input_shape, GetTensorData<float>(input),    // Disable formatting\n            filter_shape, GetTensorData<float>(filter),  // Disable formatting\n            bias_shape, GetTensorData<float>(bias),      // Disable formatting\n            output_shape, GetTensorData<float>(output));\n      } else if (sparsity.dim_metadata_size == kDimMetadataSizeBlockSparse &&\n                 sparsity.dim_metadata[2].dense_size == 4) {\n        // Block sparse with block size of 1x4.\n        optimized_ops::FullyConnectedSparseWeight1x4(\n            sparsity, op_params,                         // Disable formatting\n            input_shape, GetTensorData<float>(input),    // Disable formatting\n            filter_shape, GetTensorData<float>(filter),  // Disable formatting\n            bias_shape, GetTensorData<float>(bias),      // Disable formatting\n            output_shape, GetTensorData<float>(output),\n            CpuBackendContext::GetFromContext(context));\n      } else {\n        TF_LITE_KERNEL_LOG(context,\n                           \"Unsupported sparse fully-connected weight format.\");\n        return kTfLiteError;", "project": "tensorflow", "hash": 296480280703228258350511532286684008579, "size": 86, "commit_id": "6c0b2b70eeee588591680f5b7d5d38175fd7cdf6", "message": "[lite] add validation check for sparse fully connected\n\nPiperOrigin-RevId: 417629354\nChange-Id: If96171c4bd4f5fdb01d6368d6deab19d1c9beca7", "target": 0, "dataset": "other", "idx": 220473}
{"func": "                                          char *buffer) {\n  // Swap data begins 164 chars into data buffer:\n  // offset = deposit function hash + address + address + uint256\n  uint16_t offset = 4 + (5 * 32);\n  int16_t len = msg->data_length - offset;\n  if (msg->has_data_length && len > 0) {\n    memcpy(buffer, msg->data_initial_chunk.bytes + offset, len);\n    // String length must be < 255 characters\n    return len < 256 ? (uint8_t)len : 0;\n  }\n  return 0;\n}", "project": "keepkey-firmware", "hash": 30009078677497671388353724048438003192, "size": 13, "commit_id": "e49d45594002d4d3fbc1f03488e6dfc0a0a65836", "message": "710 merge", "target": 1, "dataset": "other", "idx": 195057}
{"func": "                                          char *buffer) {\n  // Swap data begins 164 chars into data buffer:\n  // offset = deposit function hash + address + address + uint256\n  uint16_t offset = 4 + (5 * 32);\n  int16_t len = msg->data_length - offset;\n  if (msg->has_data_length && len > 0 && len < 256) {\n    memcpy(buffer, msg->data_initial_chunk.bytes + offset, len);\n    // String length must be < 255 characters\n    return (uint8_t)len;\n  }\n  return 0;\n}", "project": "keepkey-firmware", "hash": 31658134530975392085648913239261058010, "size": 13, "commit_id": "e49d45594002d4d3fbc1f03488e6dfc0a0a65836", "message": "710 merge", "target": 0, "dataset": "other", "idx": 220894}
{"func": "    u32in();\n    // Sample size\n    u32in();\n    // Number of entries\n    mp4config.frame.ents = u32in();\n    // fixme: check atom size\n    mp4config.frame.data = malloc(sizeof(*mp4config.frame.data)\n                                  * (mp4config.frame.ents + 1));\n\n    if (!mp4config.frame.data)\n        return ERR_FAIL;", "project": "faad2", "hash": 162931728092633650703368658690852714538, "size": 36, "commit_id": "1b71a6ba963d131375f5e489b3b25e36f19f3f24", "message": "fix heap-buffer-overflow in mp4read.c\n\nThis originated from an integer overflow: If mp4config.frame.ents\nwould be read-in with a value of (uint32t)(-1), it would overflow to 0\nin the size calculation for the allocation in the next line. The\nmalloc() function would then successfully return a pointer to a memory\nregion of size 0, which will cause a segfault when written to.\n\nFixes #57.", "target": 1, "dataset": "other", "idx": 195084}
{"func": "    u32in();\n    // Sample size\n    u32in();\n    // Number of entries\n    mp4config.frame.ents = u32in();\n\n    if (!(mp4config.frame.ents + 1))\n        return ERR_FAIL;\n\n    mp4config.frame.data = malloc(sizeof(*mp4config.frame.data)\n                                  * (mp4config.frame.ents + 1));\n\n    if (!mp4config.frame.data)\n        return ERR_FAIL;", "project": "faad2", "hash": 128822819783274424928244029461541937327, "size": 39, "commit_id": "1b71a6ba963d131375f5e489b3b25e36f19f3f24", "message": "fix heap-buffer-overflow in mp4read.c\n\nThis originated from an integer overflow: If mp4config.frame.ents\nwould be read-in with a value of (uint32t)(-1), it would overflow to 0\nin the size calculation for the allocation in the next line. The\nmalloc() function would then successfully return a pointer to a memory\nregion of size 0, which will cause a segfault when written to.\n\nFixes #57.", "target": 0, "dataset": "other", "idx": 221432}
{"func": "mrb_proc_copy(mrb_state *mrb, struct RProc *a, struct RProc *b)\n{\n  if (a->body.irep) {\n    /* already initialized proc */\n    return;\n  }\n  a->flags = b->flags;\n  a->body = b->body;\n  a->upper = b->upper;\n  if (!MRB_PROC_CFUNC_P(a) && a->body.irep) {\n    mrb_irep_incref(mrb, (mrb_irep*)a->body.irep);\n  }\n  a->e.env = b->e.env;\n  /* a->e.target_class = a->e.target_class; */\n}", "project": "mruby", "hash": 244789115000433749131120833427639242199, "size": 15, "commit_id": "28ccc664e5dcd3f9d55173e9afde77c4705a9ab6", "message": "proc.c: should not reference `irep` when copying failed.\n\nIt may cause broken reference count numbers.", "target": 1, "dataset": "other", "idx": 195217}
{"func": "{\n  if (a->body.irep) {\n    /* already initialized proc */\n    return;\n  }\n  if (!MRB_PROC_CFUNC_P(b) && b->body.irep) {\n    mrb_irep_incref(mrb, (mrb_irep*)b->body.irep);\n  }\n  a->flags = b->flags;\n  a->body = b->body;\n  a->upper = b->upper;\n  a->e.env = b->e.env;\n  /* a->e.target_class = a->e.target_class; */\n}", "project": "mruby", "hash": 178869132473237542289873957189130108512, "size": 15, "commit_id": "28ccc664e5dcd3f9d55173e9afde77c4705a9ab6", "message": "proc.c: should not reference `irep` when copying failed.\n\nIt may cause broken reference count numbers.", "target": 0, "dataset": "other", "idx": 222586}
{"func": "int64_t TensorByteSize(const TensorProto& t) {\n  // num_elements returns -1 if shape is not fully defined.\n  int64_t num_elems = TensorShape(t.tensor_shape()).num_elements();\n  return num_elems < 0 ? -1 : num_elems * DataTypeSize(t.dtype());\n}", "project": "tensorflow", "hash": 12987503072502364739383952943830392043, "size": 5, "commit_id": "c2426bba00a01de6913738df8fa78e0215fcce02", "message": "Use `PartialTensorShape` instead of `TensorShape`.\n\n`TensorShape` constructor throws a CHECK-fail if shape is partial/overflows which the other doesn't. We are only determining the number of elements in the shape and partial shape should be used as it returns negative number when needed.\n\nPiperOrigin-RevId: 409205384\nChange-Id: Ia56542ff9ec758f2c9ffc7e4dcc9fa7eecd86e7b", "target": 1, "dataset": "other", "idx": 195244}
{"func": "int64_t TensorByteSize(const TensorProto& t) {\n  // num_elements returns -1 if shape is not fully defined.\n  int64_t num_elems = PartialTensorShape(t.tensor_shape()).num_elements();\n  return num_elems < 0 ? -1 : num_elems * DataTypeSize(t.dtype());\n}", "project": "tensorflow", "hash": 57172409333461341216253559449273971, "size": 5, "commit_id": "c2426bba00a01de6913738df8fa78e0215fcce02", "message": "Use `PartialTensorShape` instead of `TensorShape`.\n\n`TensorShape` constructor throws a CHECK-fail if shape is partial/overflows which the other doesn't. We are only determining the number of elements in the shape and partial shape should be used as it returns negative number when needed.\n\nPiperOrigin-RevId: 409205384\nChange-Id: Ia56542ff9ec758f2c9ffc7e4dcc9fa7eecd86e7b", "target": 0, "dataset": "other", "idx": 223149}
{"func": "  TfLiteTensor* output;\n  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n\n  const TfLiteType data_type = input->type;\n\n  const TfLiteType filter_type = filter->type;\n  const bool is_hybrid =", "project": "tensorflow", "hash": 195314963762430554293071564484057477345, "size": 187, "commit_id": "e5b0eec199c2d03de54fd6a7fd9275692218e2bc", "message": "[lite] Add validation check for dilation height/width to be positive integers.\n\nPiperOrigin-RevId: 416429178\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25", "target": 1, "dataset": "other", "idx": 195247}
{"func": "  TF_LITE_ENSURE_OK(context,\n                    GetOutputSafe(context, node, kOutputTensor, &output));\n\n  TF_LITE_ENSURE_EQ(context, NumDimensions(input), 4);\n  TF_LITE_ENSURE_EQ(context, NumDimensions(filter), 4);\n  TF_LITE_ENSURE(context, params->dilation_height_factor > 0);\n  TF_LITE_ENSURE(context, params->dilation_width_factor > 0);\n\n  const TfLiteType data_type = input->type;\n\n  const TfLiteType filter_type = filter->type;\n  const bool is_hybrid =", "project": "tensorflow", "hash": 67839353802325440041115603135108378593, "size": 189, "commit_id": "e5b0eec199c2d03de54fd6a7fd9275692218e2bc", "message": "[lite] Add validation check for dilation height/width to be positive integers.\n\nPiperOrigin-RevId: 416429178\nChange-Id: If7cdcddca54486434d9b2f06e7e2b401d7c3ee25", "target": 0, "dataset": "other", "idx": 223174}
{"func": "\tnewpn->rx_fc = newpn->tx_fc = PN_LEGACY_FLOW_CONTROL;\n\tnewpn->init_enable = enabled;\n\tnewpn->aligned = aligned;\n\n\terr = pep_accept_conn(newsk, skb);\n\tif (err) {\n\t\tsock_put(newsk);\n\t\tnewsk = NULL;\n\t\tgoto drop;\n\t}\n\tsk_add_node(newsk, &pn->hlist);", "project": "linux", "hash": 179235263739189663934709119365050162181, "size": 120, "commit_id": "bcd0f93353326954817a4f9fa55ec57fb38acbb0", "message": "phonet: refcount leak in pep_sock_accep\n\nsock_hold(sk) is invoked in pep_sock_accept(), but __sock_put(sk) is not\ninvoked in subsequent failure branches(pep_accept_conn() != 0).\n\nSigned-off-by: Hangyu Hua <hbh25y@gmail.com>\nLink: https://lore.kernel.org/r/20211209082839.33985-1-hbh25y@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>", "target": 1, "dataset": "other", "idx": 195337}
{"func": "\tnewpn->init_enable = enabled;\n\tnewpn->aligned = aligned;\n\n\terr = pep_accept_conn(newsk, skb);\n\tif (err) {\n\t\t__sock_put(sk);\n\t\tsock_put(newsk);\n\t\tnewsk = NULL;\n\t\tgoto drop;\n\t}\n\tsk_add_node(newsk, &pn->hlist);", "project": "linux", "hash": 89449369873513891205913064355633457403, "size": 121, "commit_id": "bcd0f93353326954817a4f9fa55ec57fb38acbb0", "message": "phonet: refcount leak in pep_sock_accep\n\nsock_hold(sk) is invoked in pep_sock_accept(), but __sock_put(sk) is not\ninvoked in subsequent failure branches(pep_accept_conn() != 0).\n\nSigned-off-by: Hangyu Hua <hbh25y@gmail.com>\nLink: https://lore.kernel.org/r/20211209082839.33985-1-hbh25y@gmail.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>", "target": 0, "dataset": "other", "idx": 224826}
{"func": "\t\treturn GF_ISOM_INCOMPLETE_FILE;\n\t}\n\n\tnewBox->size = size - hdr_size;\n\n\tif (newBox->size) {\n\t\te = gf_isom_full_box_read(newBox, bs);\n\t\tif (!e) e = gf_isom_box_read(newBox, bs);\n\t\tnewBox->size = size;\n\t\tend = gf_bs_get_position(bs);\n\t} else {\n\t\tnewBox->size = size;\n\t\t//empty box\n\t\te = GF_OK;\n\t\tend = gf_bs_get_position(bs);\n\t}\n\n\tif (e && (e != GF_ISOM_INCOMPLETE_FILE)) {\n\t\tgf_isom_box_del(newBox);\n\t\t*outBox = NULL;\n", "project": "gpac", "hash": 63019188358498749414838156609255855956, "size": 165, "commit_id": "8e585e623b1d666b4ef736ed609264639cb27701", "message": "fixed potential crash - cf #1406", "target": 1, "dataset": "other", "idx": 195345}
{"func": "\t\treturn GF_ISOM_INCOMPLETE_FILE;\n\t}\n\n\tnewBox->size = size - hdr_size;\n\n\t//parse even if size is 0 - this makes sure that we perform box parsing (usually in box->read)\n\te = gf_isom_full_box_read(newBox, bs);\n\tif (!e) e = gf_isom_box_read(newBox, bs);\n\tnewBox->size = size;\n\tend = gf_bs_get_position(bs);\n\n\tif (e && (e != GF_ISOM_INCOMPLETE_FILE)) {\n\t\tgf_isom_box_del(newBox);\n\t\t*outBox = NULL;\n", "project": "gpac", "hash": 267073801759882457635850212143910860248, "size": 159, "commit_id": "8e585e623b1d666b4ef736ed609264639cb27701", "message": "fixed potential crash - cf #1406", "target": 0, "dataset": "other", "idx": 224922}
{"func": "snmp_ber_decode_string_len_buffer(unsigned char *buf, uint32_t *buff_len, const char **str, uint32_t *length)\n{\n  uint8_t type, i, length_bytes;\n\n  buf = snmp_ber_decode_type(buf, buff_len, &type);\n\n  if(buf == NULL || type != BER_DATA_TYPE_OCTET_STRING) {\n    /*\n     * Sanity check\n     * Invalid type in buffer\n     */\n    return NULL;\n  }\n\n  if((*buf & 0x80) == 0) {\n    *length = (uint32_t)*buf++;\n    (*buff_len)--;\n  } else {\n\n    length_bytes = (uint8_t)(*buf++ & 0x7F);\n    (*buff_len)--;\n    if(length_bytes > 4) {\n      /*\n       * Sanity check\n       * It will not fit in the uint32_t\n       */\n      return NULL;\n    }\n\n    *length = (uint32_t)*buf++;\n    (*buff_len)--;\n    for(i = 1; i < length_bytes; ++i) {\n      *length <<= 8;\n      *length |= *buf++;\n      (*buff_len)--;\n    }\n  }\n\n  *str = (const char *)buf;\n  *buff_len -= *length;\n\n  return buf + *length;\n}", "project": "contiki-ng", "hash": 245878712235324239686057305671224454457, "size": 43, "commit_id": "12c824386ab60de757de5001974d73b32e19ad71", "message": "Refactored SNMP engine after vulnerabilities", "target": 1, "dataset": "other", "idx": 195362}
{"func": "snmp_ber_decode_string_len_buffer(snmp_packet_t *snmp_packet, const char **str, uint32_t *length)\n{\n  uint8_t type, i, length_bytes;\n\n  if(!snmp_ber_decode_type(snmp_packet, &type)) {\n    return 0;\n  }\n\n  if(type != BER_DATA_TYPE_OCTET_STRING) {\n    /*\n     * Sanity check\n     * Invalid type in buffer\n     */\n    return 0;\n  }\n\n  if((*snmp_packet->in & 0x80) == 0) {\n\n    if(snmp_packet->used == 0) {\n      return 0;\n    }\n\n    *length = (uint32_t)*snmp_packet->in++;\n    snmp_packet->used--;\n  } else {\n\n    if(snmp_packet->used == 0) {\n      return 0;\n    }\n\n    length_bytes = (uint8_t)(*snmp_packet->in++ & 0x7F);\n    snmp_packet->used--;\n\n    if(length_bytes > 4) {\n      /*\n       * Sanity check\n       * It will not fit in the uint32_t\n       */\n      return 0;\n    }\n\n    if(snmp_packet->used == 0) {\n      return 0;\n    }\n\n    *length = (uint32_t)*snmp_packet->in++;\n    snmp_packet->used--;\n\n    for(i = 1; i < length_bytes; ++i) {\n      *length <<= 8;\n\n      if(snmp_packet->used == 0) {\n        return 0;\n      }\n\n      *length |= *snmp_packet->in++;\n      snmp_packet->used--;\n    }\n  }\n\n  *str = (const char *)snmp_packet->in;\n\n  if(snmp_packet->used == 0 || snmp_packet->used - *length <= 0) {\n    return 0;\n  }\n\n  snmp_packet->used -= *length;\n  snmp_packet->in += *length;\n\n  return 1;\n}", "project": "contiki-ng", "hash": 41319319420355179801852466083422626714, "size": 71, "commit_id": "12c824386ab60de757de5001974d73b32e19ad71", "message": "Refactored SNMP engine after vulnerabilities", "target": 0, "dataset": "other", "idx": 224950}
{"func": "      case 'n': out = '\\n'; return true;\n      case 'r': out = '\\r'; return true;\n      case 't': out = '\\t'; return true;\n      case 'u': {\n        if (UNLIKELY(is_tsimplejson)) {\n          auto const ch1 = *p++;\n          auto const ch2 = *p++;\n          auto const dch3 = dehexchar(*p++);\n          auto const dch4 = dehexchar(*p++);\n          if (UNLIKELY(ch1 != '0' || ch2 != '0' || dch3 < 0 || dch4 < 0)) {\n            return false;\n          }\n          out = (dch3 << 4) | dch4;\n          return true;\n        } else {\n          uint16_t u16cp = 0;\n          for (int i = 0; i < 4; i++) {", "project": "hhvm", "hash": 222661936735492214340109696360679833540, "size": 42, "commit_id": "b3679121bb3c7017ff04b4c08402ffff5cf59b13", "message": "Fix buffer overrun in SimpleParser::handleBackslash\n\nSummary:\nIt read 4 chars, then checked for validity, but any of them could have\nbeen the end of the string, so check after each one instead.\n\nReviewed By: oulgen\n\nDifferential Revision: D19611163\n\nfbshipit-source-id: 3da0a39555cb85a93f4fd98048368f17cf37e2e4", "target": 1, "dataset": "other", "idx": 195486}
{"func": "      case 'r': out = '\\r'; return true;\n      case 't': out = '\\t'; return true;\n      case 'u': {\n        if (UNLIKELY(is_tsimplejson)) {\n          auto const ch1 = *p++;\n          if (UNLIKELY(ch1 != '0')) return false;\n          auto const ch2 = *p++;\n          if (UNLIKELY(ch2 != '0')) return false;\n          auto const dch3 = dehexchar(*p++);\n          if (UNLIKELY(dch3 < 0)) return false;\n          auto const dch4 = dehexchar(*p++);\n          if (UNLIKELY(dch4 < 0)) return false;\n          out = (dch3 << 4) | dch4;\n          return true;\n        } else {\n          uint16_t u16cp = 0;\n          for (int i = 0; i < 4; i++) {", "project": "hhvm", "hash": 144019738520182441880919167101542852402, "size": 43, "commit_id": "b3679121bb3c7017ff04b4c08402ffff5cf59b13", "message": "Fix buffer overrun in SimpleParser::handleBackslash\n\nSummary:\nIt read 4 chars, then checked for validity, but any of them could have\nbeen the end of the string, so check after each one instead.\n\nReviewed By: oulgen\n\nDifferential Revision: D19611163\n\nfbshipit-source-id: 3da0a39555cb85a93f4fd98048368f17cf37e2e4", "target": 0, "dataset": "other", "idx": 227282}
{"func": "    if (linkIterator != m_links.end()) {\n        //qCDebug(KDECONNECT_CORE) << \"Reusing link to\" << deviceId;\n        deviceLink = linkIterator.value();\n        deviceLink->reset(socket, connectionOrigin);\n    } else {\n        deviceLink = new LanDeviceLink(deviceId, this, socket, connectionOrigin);\n        connect(deviceLink, &QObject::destroyed, this, &LanLinkProvider::deviceLinkDestroyed);\n        m_links[deviceId] = deviceLink;\n        if (m_pairingHandlers.contains(deviceId)) {\n            //We shouldn't have a pairinghandler if we didn't have a link.\n            //Crash if debug, recover if release (by setting the new devicelink to the old pairinghandler)", "project": "kdeconnect-kde", "hash": 173422384228019084279107414716208706323, "size": 25, "commit_id": "542d94a70c56aa386c8d4d793481ce181b0422e8", "message": "Limit number of connected sockets from unpaired devices\n\nThanks Matthias Gerstner <mgerstner@suse.de> for reporting this.", "target": 1, "dataset": "other", "idx": 195488}
{"func": "        //qCDebug(KDECONNECT_CORE) << \"Reusing link to\" << deviceId;\n        deviceLink = linkIterator.value();\n        deviceLink->reset(socket, connectionOrigin);\n    } else {\n        deviceLink = new LanDeviceLink(deviceId, this, socket, connectionOrigin);\n        // Socket disconnection will now be handled by LanDeviceLink\n        disconnect(socket, &QAbstractSocket::disconnected, socket, &QObject::deleteLater);\n        bool isDeviceTrusted = KdeConnectConfig::instance().trustedDevices().contains(deviceId);\n        if (!isDeviceTrusted && m_links.size() > MAX_UNPAIRED_CONNECTIONS) {\n            qCWarning(KDECONNECT_CORE) << \"Too many unpaired devices to remember them all. Ignoring \" << deviceId;\n            socket->disconnectFromHost();\n            socket->deleteLater();\n            return;\n        }\n        connect(deviceLink, &QObject::destroyed, this, &LanLinkProvider::deviceLinkDestroyed);\n        m_links[deviceId] = deviceLink;\n        if (m_pairingHandlers.contains(deviceId)) {\n            //We shouldn't have a pairinghandler if we didn't have a link.\n            //Crash if debug, recover if release (by setting the new devicelink to the old pairinghandler)", "project": "kdeconnect-kde", "hash": 240980444615899412189553804947186003078, "size": 34, "commit_id": "542d94a70c56aa386c8d4d793481ce181b0422e8", "message": "Limit number of connected sockets from unpaired devices\n\nThanks Matthias Gerstner <mgerstner@suse.de> for reporting this.", "target": 0, "dataset": "other", "idx": 227355}
{"func": "_libssh2_packet_add(LIBSSH2_SESSION * session, unsigned char *data,\n                    size_t datalen, int macstate)\n{\n    int rc = 0;\n    char *message = NULL;\n    char *language = NULL;\n    size_t message_len = 0;\n    size_t language_len = 0;\n    LIBSSH2_CHANNEL *channelp = NULL;\n    size_t data_head = 0;\n    unsigned char msg = data[0];\n              string    language tag [RFC3066]\n            */\n\n        case SSH_MSG_DISCONNECT:\n            if(datalen >= 5) {\n                size_t reason = _libssh2_ntohu32(data + 1);\n\n                if(datalen >= 9) {\n                    message_len = _libssh2_ntohu32(data + 5);\n\n                    if(message_len < datalen-13) {\n                        /* 9 = packet_type(1) + reason(4) + message_len(4) */\n                        message = (char *) data + 9;\n\n                        language_len =\n                            _libssh2_ntohu32(data + 9 + message_len);\n                        language = (char *) data + 9 + message_len + 4;\n\n                        if(language_len > (datalen-13-message_len)) {\n                            /* bad input, clear info */\n                            language = message = NULL;\n                            language_len = message_len = 0;\n                        }\n                    }\n                    else\n                        /* bad size, clear it */\n                        message_len = 0;\n                }\n                if(session->ssh_msg_disconnect) {\n                    LIBSSH2_DISCONNECT(session, reason, message,\n                                       message_len, language, language_len);\n                }\n                _libssh2_debug(session, LIBSSH2_TRACE_TRANS,\n                               \"Disconnect(%d): %s(%s)\", reason,\n                               message, language);\n            }\n\n        case SSH_MSG_DEBUG:\n            if(datalen >= 2) {\n                int always_display = data[1];\n\n                if(datalen >= 6) {\n                    message_len = _libssh2_ntohu32(data + 2);\n\n                    if(message_len <= (datalen - 10)) {\n                        /* 6 = packet_type(1) + display(1) + message_len(4) */\n                        message = (char *) data + 6;\n                        language_len = _libssh2_ntohu32(data + 6 +\n                                                        message_len);\n\n                        if(language_len <= (datalen - 10 - message_len))\n                            language = (char *) data + 10 + message_len;\n                    }\n                }\n\n                if(session->ssh_msg_debug) {\n                    LIBSSH2_DEBUG(session, always_display, message,\n                                  message_len, language, language_len);\n                }\n            }\n            /*\n             * _libssh2_debug will actually truncate this for us so\n             * that it's not an inordinate about of data\n             */\n            _libssh2_debug(session, LIBSSH2_TRACE_TRANS,\n        case SSH_MSG_GLOBAL_REQUEST:\n            if(datalen >= 5) {\n                uint32_t len = 0;\n                unsigned char want_reply = 0;\n                len = _libssh2_ntohu32(data + 1);\n                if(datalen >= (6 + len)) {\n                    want_reply = data[5 + len];\n                    _libssh2_debug(session,\n                                   LIBSSH2_TRACE_CONN,\n                                   \"Received global request type %.*s (wr %X)\",\n                                   len, data + 5, want_reply);", "project": "libssh2", "hash": 1170263160591633863198263904436599036, "size": 621, "commit_id": "dedcbd106f8e52d5586b0205bc7677e4c9868f9c", "message": "packet.c: improve message parsing (#402)\n\n* packet.c: improve parsing of packets\r\n\r\nfile: packet.c\r\n\r\nnotes:\r\nUse _libssh2_get_string API in SSH_MSG_DEBUG/SSH_MSG_DISCONNECT. Additional uint32 bounds check in SSH_MSG_GLOBAL_REQUEST.", "target": 1, "dataset": "other", "idx": 195648}
{"func": "_libssh2_packet_add(LIBSSH2_SESSION * session, unsigned char *data,\n                    size_t datalen, int macstate)\n{\n    int rc = 0;\n    unsigned char *message = NULL;\n    unsigned char *language = NULL;\n    size_t message_len = 0;\n    size_t language_len = 0;\n    LIBSSH2_CHANNEL *channelp = NULL;\n    size_t data_head = 0;\n    unsigned char msg = data[0];\n              string    language tag [RFC3066]\n            */\n\n        case SSH_MSG_DISCONNECT:\n            if(datalen >= 5) {\n                uint32_t reason = 0;\n                struct string_buf buf;\n                buf.data = (unsigned char *)data;\n                buf.dataptr = buf.data;\n                buf.len = datalen;\n                buf.dataptr++; /* advance past type */\n\n                _libssh2_get_u32(&buf, &reason);\n                _libssh2_get_string(&buf, &message, &message_len);\n                _libssh2_get_string(&buf, &language, &language_len);\n\n                if(session->ssh_msg_disconnect) {\n                    LIBSSH2_DISCONNECT(session, reason, (const char *)message,\n                                       message_len, (const char *)language,\n                                       language_len);\n                }\n\n                _libssh2_debug(session, LIBSSH2_TRACE_TRANS,\n                               \"Disconnect(%d): %s(%s)\", reason,\n                               message, language);\n            }\n\n        case SSH_MSG_DEBUG:\n            if(datalen >= 2) {\n                int always_display = data[1];\n\n                if(datalen >= 6) {\n                    struct string_buf buf;\n                    buf.data = (unsigned char *)data;\n                    buf.dataptr = buf.data;\n                    buf.len = datalen;\n                    buf.dataptr += 2; /* advance past type & always display */\n\n                    _libssh2_get_string(&buf, &message, &message_len);\n                    _libssh2_get_string(&buf, &language, &language_len);\n                }\n\n                if(session->ssh_msg_debug) {\n                    LIBSSH2_DEBUG(session, always_display,\n                                  (const char *)message,\n                                  message_len, (const char *)language,\n                                  language_len);\n                }\n            }\n\n            /*\n             * _libssh2_debug will actually truncate this for us so\n             * that it's not an inordinate about of data\n             */\n            _libssh2_debug(session, LIBSSH2_TRACE_TRANS,\n        case SSH_MSG_GLOBAL_REQUEST:\n            if(datalen >= 5) {\n                uint32_t len = 0;\n                unsigned char want_reply = 0;\n                len = _libssh2_ntohu32(data + 1);\n                if((len <= (UINT_MAX - 6)) && (datalen >= (6 + len))) {\n                    want_reply = data[5 + len];\n                    _libssh2_debug(session,\n                                   LIBSSH2_TRACE_CONN,\n                                   \"Received global request type %.*s (wr %X)\",\n                                   len, data + 5, want_reply);", "project": "libssh2", "hash": 132242532710662252043866474126087535393, "size": 611, "commit_id": "dedcbd106f8e52d5586b0205bc7677e4c9868f9c", "message": "packet.c: improve message parsing (#402)\n\n* packet.c: improve parsing of packets\r\n\r\nfile: packet.c\r\n\r\nnotes:\r\nUse _libssh2_get_string API in SSH_MSG_DEBUG/SSH_MSG_DISCONNECT. Additional uint32 bounds check in SSH_MSG_GLOBAL_REQUEST.", "target": 0, "dataset": "other", "idx": 229853}
{"func": "    const Tensor& labels_in = context->input(1);\n\n    TensorShape shape_in = logits_in.shape();\n\n    BCast bcast(BCast::FromShape(logits_in.shape()),\n                BCast::FromShape(labels_in.shape()));\n    if (!logits_in.IsSameSize(labels_in)) {\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\n                      \"logits and labels must be broadcastable: logits_size=\",\n                      logits_in.shape().DebugString(),\n    // Try to reuse the logits_in buffer for the backprop output.\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 1, shape_in, &back_out));\n    if (shape_in.dim_size(0) > 0) {\n      functor::XentFunctor<Device, T> functor;\n      if (logits_in.IsSameSize(labels_in)) {\n        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n                Eigen::array<Eigen::DenseIndex, 2>{1, 1},\n                Eigen::array<Eigen::DenseIndex, 2>{1, 1}, logits_in.matrix<T>(),\n                labels_in.matrix<T>(), scratch.matrix<T>(), loss_out->vec<T>(),\n                back_out->matrix<T>());\n      } else {\n        functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n                BCast::ToIndexArray<2>(bcast.x_bcast()),\n                BCast::ToIndexArray<2>(bcast.y_bcast()),\n                logits_in.template shaped<T, 2>(bcast.x_reshape()),\n                labels_in.template shaped<T, 2>(bcast.y_reshape()),\n                scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n      }\n    }\n  }", "project": "tensorflow", "hash": 231911183952942139031599044274210867918, "size": 65, "commit_id": "4d74d8a00b07441cba090a02e0dd9ed385145bf4", "message": "Fix crash in softmax-xent when some input dimensions are 1.\n\nBefore, tf.nn.softmax_cross_entropy_with_logits would fail a CHECK if one input tensor had shape (1, 1) and the other did not.\n\nIn particular, the call to ToIndexArray<2> here https://github.com/tensorflow/tensorflow/blob/1f3da84a89702d3b4f234ee83762d738caffe098/tensorflow/core/kernels/xent_op.cc#L99 would fail, since the call assumed the array had two dimensions. If both dimensions were 1, BCast would merge the two dimensions into a single dimension. Passing fewer_dims_optimization=false stops this optimization\n\nPiperOrigin-RevId: 384844496\nChange-Id: Ifb02dc74964132c3ed3f3bc98b0858dbe4e258b7", "target": 1, "dataset": "other", "idx": 195649}
{"func": "    const Tensor& labels_in = context->input(1);\n\n    TensorShape shape_in = logits_in.shape();\n\n    BCast bcast(BCast::FromShape(logits_in.shape()),\n                BCast::FromShape(labels_in.shape()),\n                /*fewer_dims_optimization=*/false);\n    if (!logits_in.IsSameSize(labels_in)) {\n      OP_REQUIRES(context, bcast.IsValid(),\n                  errors::InvalidArgument(\n                      \"logits and labels must be broadcastable: logits_size=\",\n                      logits_in.shape().DebugString(),\n    // Try to reuse the logits_in buffer for the backprop output.\n    OP_REQUIRES_OK(context, context->forward_input_or_allocate_output(\n                                {0}, 1, shape_in, &back_out));\n    if (shape_in.dim_size(0) > 0) {\n      functor::XentFunctor<Device, T> functor;\n      functor(context->eigen_device<Device>(), shape_in.AsEigenDSizes<2>(),\n              BCast::ToIndexArray<2>(bcast.x_bcast()),\n              BCast::ToIndexArray<2>(bcast.y_bcast()),\n              logits_in.template shaped<T, 2>(bcast.x_reshape()),\n              labels_in.template shaped<T, 2>(bcast.y_reshape()),\n              scratch.matrix<T>(), loss_out->vec<T>(), back_out->matrix<T>());\n    }\n  }", "project": "tensorflow", "hash": 24795655124731851084034518111067821112, "size": 58, "commit_id": "4d74d8a00b07441cba090a02e0dd9ed385145bf4", "message": "Fix crash in softmax-xent when some input dimensions are 1.\n\nBefore, tf.nn.softmax_cross_entropy_with_logits would fail a CHECK if one input tensor had shape (1, 1) and the other did not.\n\nIn particular, the call to ToIndexArray<2> here https://github.com/tensorflow/tensorflow/blob/1f3da84a89702d3b4f234ee83762d738caffe098/tensorflow/core/kernels/xent_op.cc#L99 would fail, since the call assumed the array had two dimensions. If both dimensions were 1, BCast would merge the two dimensions into a single dimension. Passing fewer_dims_optimization=false stops this optimization\n\nPiperOrigin-RevId: 384844496\nChange-Id: Ifb02dc74964132c3ed3f3bc98b0858dbe4e258b7", "target": 0, "dataset": "other", "idx": 229858}
{"func": "\n    int next_ragged = 0;\n    int next_sparse = 0;\n    int next_dense = 0;\n    for (char c : input_order_) {\n      if (c == 'R') {\n        TF_RETURN_IF_ERROR(BuildRaggedFeatureReader(\n            ragged_values_list[next_ragged], ragged_splits_list[next_ragged],\n            features));\n        next_ragged++;\n      } else if (c == 'S') {\n        TF_RETURN_IF_ERROR(BuildSparseFeatureReader(\n            sparse_indices_list[next_sparse], sparse_values_list[next_sparse],\n            batch_size, features));\n        next_sparse++;\n      } else if (c == 'D') {\n        TF_RETURN_IF_ERROR(\n            BuildDenseFeatureReader(dense_list[next_dense++], features));\n      } else {\n        return errors::InvalidArgument(\"Unexpected input_order value.\");\n      }", "project": "tensorflow", "hash": 119852077215802301992803812414131651636, "size": 32, "commit_id": "44b7f486c0143f68b56c34e2d01e146ee445134a", "message": "Fix out of bounds read in `ragged_cross_op.cc`.\n\nPiperOrigin-RevId: 369757702\nChange-Id: Ie6e5d2c21513a8d56bf41fcf35960caf76e890f9", "target": 1, "dataset": "other", "idx": 195659}
{"func": "    int next_ragged = 0;\n    int next_sparse = 0;\n    int next_dense = 0;\n    for (char c : input_order_) {\n      if (c == 'R') {\n        if (next_ragged >= ragged_values_list.size())\n          return errors::InvalidArgument(\n              \"input_order \\\"\", input_order_,\n              \"\\\" specifies reading a ragged tensor value at index \",\n              next_ragged, \" from a list of \", ragged_values_list.size(),\n              \" values.\");\n        if (next_ragged >= ragged_splits_list.size())\n          return errors::InvalidArgument(\n              \"input_order \\\"\", input_order_,\n              \"\\\" specifies reading a ragged tensor split at index \",\n              next_ragged, \" from a list of \", ragged_splits_list.size(),\n              \" splits.\");\n        TF_RETURN_IF_ERROR(BuildRaggedFeatureReader(\n            ragged_values_list[next_ragged], ragged_splits_list[next_ragged],\n            features));\n        next_ragged++;\n      } else if (c == 'S') {\n        if (next_sparse >= sparse_values_list.size())\n          return errors::InvalidArgument(\n              \"input_order \\\"\", input_order_,\n              \"\\\" specifies reading a sparse tensor value at index \",\n              next_sparse, \" from a list of \", sparse_values_list.size(),\n              \" values.\");\n        if (next_sparse >= sparse_indices_list.size())\n          return errors::InvalidArgument(\n              \"input_order \\\"\", input_order_,\n              \"\\\" specifies reading a sparse tensor index at index \",\n              next_sparse, \" from a list of \", sparse_indices_list.size(),\n              \" indices.\");\n        TF_RETURN_IF_ERROR(BuildSparseFeatureReader(\n            sparse_indices_list[next_sparse], sparse_values_list[next_sparse],\n            batch_size, features));\n        next_sparse++;\n      } else if (c == 'D') {\n        if (next_dense >= dense_list.size())\n          return errors::InvalidArgument(\n              \"input_order \\\"\", input_order_,\n              \"\\\" specifies reading a dense tensor at index \", next_dense,\n              \" from a list of \", dense_list.size(), \" tensors.\");\n        TF_RETURN_IF_ERROR(\n            BuildDenseFeatureReader(dense_list[next_dense++], features));\n      } else {\n        return errors::InvalidArgument(\"Unexpected input_order value.\");\n      }", "project": "tensorflow", "hash": 17914142839785054126542444128922850158, "size": 61, "commit_id": "44b7f486c0143f68b56c34e2d01e146ee445134a", "message": "Fix out of bounds read in `ragged_cross_op.cc`.\n\nPiperOrigin-RevId: 369757702\nChange-Id: Ie6e5d2c21513a8d56bf41fcf35960caf76e890f9", "target": 0, "dataset": "other", "idx": 230086}
{"func": "\n\tsc_log(ctx, \"read oberthur file result %i\", rv);\n\tif (verify_pin && rv == SC_ERROR_SECURITY_STATUS_NOT_SATISFIED)   {\n\t\tstruct sc_pkcs15_object *objs[0x10], *pin_obj = NULL;\n\t\tconst struct sc_acl_entry *acl = sc_file_get_acl_entry(file, SC_AC_OP_READ);\n\t\tint ii;\n\n\t\tif (acl == NULL) {\n\t\t\tsc_file_free(file);\n\t\t\tfree(*out);\n\t\t\t*out = NULL;\n\t\t\tLOG_FUNC_RETURN(ctx, SC_ERROR_INVALID_DATA);\n\t\t}\n\n\t\trv = sc_pkcs15_get_objects(p15card, SC_PKCS15_TYPE_AUTH_PIN, objs, 0x10);\n\t\tif (rv != SC_SUCCESS) {\n\t\t\tsc_file_free(file);\n\t\t\tfree(*out);\n\t\t\t*out = NULL;\n\t\t\tLOG_TEST_RET(ctx, rv, \"Cannot read oberthur file: get AUTH objects error\");\n\t\t}\n\n\t\tfor (ii=0; ii<rv; ii++)   {\n\t\t\tstruct sc_pkcs15_auth_info *auth_info = (struct sc_pkcs15_auth_info *) objs[ii]->data;\n\t\t\tsc_log(ctx, \"compare PIN/ACL refs:%i/%i, method:%i/%i\",\n\t\t\t\t\tauth_info->attrs.pin.reference, acl->key_ref, auth_info->auth_method, acl->method);\n\t\t\tif (auth_info->attrs.pin.reference == (int)acl->key_ref && auth_info->auth_method == (unsigned)acl->method)   {\n\t\t\t\tpin_obj = objs[ii];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}", "project": "OpenSC", "hash": 327143307223640707157004214003277795771, "size": 124, "commit_id": "1db88374bb7706a115d5c3617c6f16115c33bf27", "message": "oberthur: Correctly check for return values\n\nThanks oss-fuzz\n\nhttps://bugs.chromium.org/p/oss-fuzz/issues/detail?id=28843", "target": 1, "dataset": "other", "idx": 195660}
{"func": "\n\tsc_log(ctx, \"read oberthur file result %i\", rv);\n\tif (verify_pin && rv == SC_ERROR_SECURITY_STATUS_NOT_SATISFIED)   {\n\t\tstruct sc_pkcs15_object *objs[0x10], *pin_obj = NULL;\n\t\tconst struct sc_acl_entry *acl = sc_file_get_acl_entry(file, SC_AC_OP_READ);\n\t\tint ii, nobjs;\n\n\t\tif (acl == NULL) {\n\t\t\tsc_file_free(file);\n\t\t\tfree(*out);\n\t\t\t*out = NULL;\n\t\t\tLOG_FUNC_RETURN(ctx, SC_ERROR_INVALID_DATA);\n\t\t}\n\n\t\tnobjs = sc_pkcs15_get_objects(p15card, SC_PKCS15_TYPE_AUTH_PIN, objs, 0x10);\n\t\tif (nobjs < 1) {\n\t\t\tsc_file_free(file);\n\t\t\tfree(*out);\n\t\t\t*out = NULL;\n\t\t\tLOG_TEST_RET(ctx, SC_ERROR_DATA_OBJECT_NOT_FOUND,\n\t\t\t\t\"Cannot read oberthur file: get AUTH objects error\");\n\t\t}\n\n\t\tfor (ii = 0; ii < nobjs; ii++) {\n\t\t\tstruct sc_pkcs15_auth_info *auth_info = (struct sc_pkcs15_auth_info *) objs[ii]->data;\n\t\t\tsc_log(ctx, \"compare PIN/ACL refs:%i/%i, method:%i/%i\",\n\t\t\t\tauth_info->attrs.pin.reference, acl->key_ref, auth_info->auth_method, acl->method);\n\t\t\tif (auth_info->attrs.pin.reference == (int)acl->key_ref && auth_info->auth_method == (unsigned)acl->method)   {\n\t\t\t\tpin_obj = objs[ii];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}", "project": "OpenSC", "hash": 219491834048613279001620385704271437400, "size": 125, "commit_id": "1db88374bb7706a115d5c3617c6f16115c33bf27", "message": "oberthur: Correctly check for return values\n\nThanks oss-fuzz\n\nhttps://bugs.chromium.org/p/oss-fuzz/issues/detail?id=28843", "target": 0, "dataset": "other", "idx": 230105}
{"func": "    const Tensor& input_max_tensor = ctx->input(2);\n\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;", "project": "tensorflow", "hash": 301693154738415813623979332331002049816, "size": 50, "commit_id": "5899741d0421391ca878da47907b1452f06aaf1b", "message": "Fix heap OOB read in dequantize op.\n\nAlso fixes SEGV in same op\n\nPiperOrigin-RevId: 372437896\nChange-Id: I135e94d360c2a1ce374c10f7e0fed1af603dbc02", "target": 1, "dataset": "other", "idx": 195663}
{"func": "\n    int num_slices = 1;\n    if (axis_ > -1) {\n      num_slices = input.dim_size(axis_);\n    }\n    OP_REQUIRES(ctx, input_min_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_min_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_min_tensor.NumElements(),\n                    \", expected \", num_slices));\n    OP_REQUIRES(ctx, input_max_tensor.NumElements() == num_slices,\n                errors::InvalidArgument(\n                    \"input_max_tensor must have as many elements as input on \"\n                    \"the dequantization axis (\",\n                    axis_, \"), got \", input_max_tensor.NumElements(),\n                    \", expected \", num_slices));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    Tensor float_output =\n        need_cast_ ? tensorflow::Tensor(DT_FLOAT, input.shape()) : *output;", "project": "tensorflow", "hash": 327725359159328114053466591557219239474, "size": 62, "commit_id": "5899741d0421391ca878da47907b1452f06aaf1b", "message": "Fix heap OOB read in dequantize op.\n\nAlso fixes SEGV in same op\n\nPiperOrigin-RevId: 372437896\nChange-Id: I135e94d360c2a1ce374c10f7e0fed1af603dbc02", "target": 0, "dataset": "other", "idx": 230153}
{"func": "    return accessor->getter.get(runtime)->executeCall0(\n        runtime->makeHandle(accessor->getter), runtime, receiver);\n  } else if (desc.flags.hostObject) {\n    SymbolID id{};\n    LAZY_TO_IDENTIFIER(runtime, nameValPrimitiveHandle, id);\n    auto propRes = vmcast<HostObject>(selfHandle.get())->get(id);\n    if (propRes == ExecutionStatus::EXCEPTION)\n      return ExecutionStatus::EXCEPTION;\n    return createPseudoHandle(*propRes);\n  } else {\n    assert(desc.flags.proxyObject && \"descriptor flags are impossible\");", "project": "hermes", "hash": 177426946888629742787453579694850240993, "size": 71, "commit_id": "fe52854cdf6725c2eaa9e125995da76e6ceb27da", "message": "[CVE-2020-1911] Look up HostObject computed properties on the right object in the prototype chain.\n\nSummary:\nThe change in the hermes repository fixes the security vulnerability\nCVE-2020-1911.  This vulnerability only affects applications which\nallow evaluation of uncontrolled, untrusted JavaScript code not\nshipped with the app, so React Native apps will generally not be affected.\n\nThis revision includes a test for the bug.  The test is generic JSI\ncode, so it is included in the hermes and react-native repositories.\n\nChangelog: [Internal]\n\nReviewed By: tmikov\n\nDifferential Revision: D23322992\n\nfbshipit-source-id: 4e88c974afe1ad33a263f9cac03e9dc98d33649a", "target": 1, "dataset": "other", "idx": 195664}
{"func": "    return accessor->getter.get(runtime)->executeCall0(\n        runtime->makeHandle(accessor->getter), runtime, receiver);\n  } else if (desc.flags.hostObject) {\n    SymbolID id{};\n    LAZY_TO_IDENTIFIER(runtime, nameValPrimitiveHandle, id);\n    auto propRes = vmcast<HostObject>(propObj.get())->get(id);\n    if (propRes == ExecutionStatus::EXCEPTION)\n      return ExecutionStatus::EXCEPTION;\n    return createPseudoHandle(*propRes);\n  } else {\n    assert(desc.flags.proxyObject && \"descriptor flags are impossible\");", "project": "hermes", "hash": 144306568268444659213632345202957132495, "size": 71, "commit_id": "fe52854cdf6725c2eaa9e125995da76e6ceb27da", "message": "[CVE-2020-1911] Look up HostObject computed properties on the right object in the prototype chain.\n\nSummary:\nThe change in the hermes repository fixes the security vulnerability\nCVE-2020-1911.  This vulnerability only affects applications which\nallow evaluation of uncontrolled, untrusted JavaScript code not\nshipped with the app, so React Native apps will generally not be affected.\n\nThis revision includes a test for the bug.  The test is generic JSI\ncode, so it is included in the hermes and react-native repositories.\n\nChangelog: [Internal]\n\nReviewed By: tmikov\n\nDifferential Revision: D23322992\n\nfbshipit-source-id: 4e88c974afe1ad33a263f9cac03e9dc98d33649a", "target": 0, "dataset": "other", "idx": 230182}
{"func": "\tGF_AdobeBootstrapInfoBox *ptr = (GF_AdobeBootstrapInfoBox *)s;\n\tint i;\n\tu32 tmp_strsize;\n\tchar *tmp_str;\n\tBool zfound=GF_FALSE;\n\tGF_Err e;\n\n\tISOM_DECREASE_SIZE(ptr, 25)\n\tptr->bootstrapinfo_version = gf_bs_read_u32(bs);\n\tptr->profile = gf_bs_read_int(bs, 2);\n\tptr->live = gf_bs_read_int(bs, 1);\n\ttmp_str = gf_malloc(sizeof(char)*tmp_strsize);\n\tif (!tmp_str) return GF_OUT_OF_MEM;\n\tmemset(tmp_str, 0, sizeof(char)*tmp_strsize);\n\n\twhile (tmp_strsize) {\n\t\tISOM_DECREASE_SIZE(ptr, 1)\n\t\ttmp_str[i] = gf_bs_read_u8(bs);\n\t\ttmp_strsize--;\n\t\tif (!tmp_str[i]) {\n\t\t\tzfound = GF_TRUE;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\tif (!zfound)\n\t\treturn GF_ISOM_INVALID_FILE;\n\tif (i) {\n\t\tptr->movie_identifier = gf_strdup(tmp_str);\n\t}\n\n\tISOM_DECREASE_SIZE(ptr, 1)\n\tptr->server_entry_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->server_entry_count; i++) {\n\t\tint j=0;\n\t\tzfound = GF_FALSE;\n\t\ttmp_strsize=(u32)ptr->size;\n\t\twhile (tmp_strsize) {\n\t\t\tISOM_DECREASE_SIZE(ptr, 1)\n\t\t\ttmp_str[j] = gf_bs_read_u8(bs);\n\t\t\ttmp_strsize--;\n\t\t\tif (!tmp_str[j]) {\n\t\t\t\tzfound = GF_TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tj++;\n\t\t}\n\t\tif (!zfound)\n\t\t\treturn GF_ISOM_INVALID_FILE;\n\t\tif (j) {\n\t\t\tgf_list_insert(ptr->server_entry_table, gf_strdup(tmp_str), i);\n\t\t}\n\t}\n\n\tISOM_DECREASE_SIZE(ptr, 1)\n\tptr->quality_entry_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->quality_entry_count; i++) {\n\t\tint j=0;\n\t\tzfound = GF_FALSE;\n\t\ttmp_strsize=(u32)ptr->size;\n\t\twhile (tmp_strsize) {\n\t\t\tISOM_DECREASE_SIZE(ptr, 1)\n\t\t\ttmp_str[j] = gf_bs_read_u8(bs);\n\t\t\ttmp_strsize--;\n\t\t\tif (!tmp_str[j]) {\n\t\t\t\tzfound = GF_TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tj++;\n\t\t}\n\n\t\tif (!zfound)\n\t\t\treturn GF_ISOM_INVALID_FILE;\n\t\tif (j) {\n\t\t\tgf_list_insert(ptr->quality_entry_table, gf_strdup(tmp_str), i);\n\t\t}\n\t}\n\n\ti=0;\n\ttmp_strsize=(u32)ptr->size;\n\tzfound = GF_FALSE;\n\twhile (tmp_strsize) {\n\t\tISOM_DECREASE_SIZE(ptr, 1)\n\t\ttmp_str[i] = gf_bs_read_u8(bs);\n\t\ttmp_strsize--;\n\t\tif (!tmp_str[i]) {\n\t\t\tzfound = GF_TRUE;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\tif (!zfound)\n\t\treturn GF_ISOM_INVALID_FILE;\n\tif (i) {\n\t\tptr->drm_data = gf_strdup(tmp_str);\n\t}\n\n\ti=0;\n\ttmp_strsize=(u32)ptr->size;\n\tzfound = GF_FALSE;\n\twhile (tmp_strsize) {\n\t\tISOM_DECREASE_SIZE(ptr, 1)\n\t\ttmp_str[i] = gf_bs_read_u8(bs);\n\t\ttmp_strsize--;\n\t\tif (!tmp_str[i]) {\n\t\t\tzfound = GF_TRUE;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\tif (!zfound)\n\t\treturn GF_ISOM_INVALID_FILE;\n\tif (i) {\n\t\tptr->meta_data = gf_strdup(tmp_str);\n\t}\n\n\tISOM_DECREASE_SIZE(ptr, 1)\n\tptr->segment_run_table_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->segment_run_table_count; i++) {\n\t\tGF_AdobeSegmentRunTableBox *asrt = NULL;\n\t\te = gf_isom_box_parse((GF_Box **)&asrt, bs);\n\t\tif (e) {\n\t\t\tif (asrt) gf_isom_box_del((GF_Box*)asrt);\n\t\t\tgf_free(tmp_str);\n\t\t\treturn e;\n\t\t}\n\t\tgf_list_add(ptr->segment_run_table_entries, asrt);\n\t}\n\n\tISOM_DECREASE_SIZE(ptr, 1)\n\tptr->fragment_run_table_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->fragment_run_table_count; i++) {\n\t\tGF_AdobeFragmentRunTableBox *afrt = NULL;\n\t\te = gf_isom_box_parse((GF_Box **)&afrt, bs);\n\t\tif (e) {\n\t\t\tif (afrt) gf_isom_box_del((GF_Box*)afrt);\n\t\t\tgf_free(tmp_str);\n\t\t\treturn e;\n\t\t}\n\t\tgf_list_add(ptr->fragment_run_table_entries, afrt);\n\t}\n\n\tgf_free(tmp_str);\n\n\treturn GF_OK;\n}", "project": "gpac", "hash": 65800177890771330818160234491326522750, "size": 157, "commit_id": "e74be5976a6fee059c638050a237893f7e9a3b23", "message": "fixed #1753", "target": 1, "dataset": "other", "idx": 195679}
{"func": "\tGF_AdobeBootstrapInfoBox *ptr = (GF_AdobeBootstrapInfoBox *)s;\n\tint i;\n\tu32 tmp_strsize;\n\tchar *tmp_str;\n\tBool zfound=GF_FALSE;\n\tGF_Err e = GF_OK;\n\n\tISOM_DECREASE_SIZE(ptr, 25)\n\tptr->bootstrapinfo_version = gf_bs_read_u32(bs);\n\tptr->profile = gf_bs_read_int(bs, 2);\n\tptr->live = gf_bs_read_int(bs, 1);\n\ttmp_str = gf_malloc(sizeof(char)*tmp_strsize);\n\tif (!tmp_str) return GF_OUT_OF_MEM;\n\tmemset(tmp_str, 0, sizeof(char)*tmp_strsize);\n\n\twhile (tmp_strsize) {\n\t\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\t\ttmp_str[i] = gf_bs_read_u8(bs);\n\t\ttmp_strsize--;\n\t\tif (!tmp_str[i]) {\n\t\t\tzfound = GF_TRUE;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\tif (!zfound) {\n\t\te = GF_ISOM_INVALID_FILE;\n\t\tgoto exit;\n\t}\n\tif (i) {\n\t\tptr->movie_identifier = gf_strdup(tmp_str);\n\t}\n\n\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\tptr->server_entry_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->server_entry_count; i++) {\n\t\tint j=0;\n\t\tzfound = GF_FALSE;\n\t\ttmp_strsize=(u32)ptr->size;\n\t\twhile (tmp_strsize) {\n\t\t\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\t\t\ttmp_str[j] = gf_bs_read_u8(bs);\n\t\t\ttmp_strsize--;\n\t\t\tif (!tmp_str[j]) {\n\t\t\t\tzfound = GF_TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tj++;\n\t\t}\n\t\tif (!zfound) {\n\t\t\te = GF_ISOM_INVALID_FILE;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (j) {\n\t\t\tgf_list_insert(ptr->server_entry_table, gf_strdup(tmp_str), i);\n\t\t}\n\t}\n\tif (ptr->server_entry_count != gf_list_count(ptr->server_entry_table)) {\n\t\te = GF_ISOM_INVALID_FILE;\n\t\tgoto exit;\n\t}\n\n\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\tptr->quality_entry_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->quality_entry_count; i++) {\n\t\tint j=0;\n\t\tzfound = GF_FALSE;\n\t\ttmp_strsize=(u32)ptr->size;\n\t\twhile (tmp_strsize) {\n\t\t\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\t\t\ttmp_str[j] = gf_bs_read_u8(bs);\n\t\t\ttmp_strsize--;\n\t\t\tif (!tmp_str[j]) {\n\t\t\t\tzfound = GF_TRUE;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tj++;\n\t\t}\n\n\t\tif (!zfound) {\n\t\t\te = GF_ISOM_INVALID_FILE;\n\t\t\tgoto exit;\n\t\t}\n\t\tif (j) {\n\t\t\tgf_list_insert(ptr->quality_entry_table, gf_strdup(tmp_str), i);\n\t\t}\n\t}\n\tif (ptr->quality_entry_count != gf_list_count(ptr->quality_entry_table)) {\n\t\te = GF_ISOM_INVALID_FILE;\n\t\tgoto exit;\n\t}\n\n\ti=0;\n\ttmp_strsize=(u32)ptr->size;\n\tzfound = GF_FALSE;\n\twhile (tmp_strsize) {\n\t\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\t\ttmp_str[i] = gf_bs_read_u8(bs);\n\t\ttmp_strsize--;\n\t\tif (!tmp_str[i]) {\n\t\t\tzfound = GF_TRUE;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\tif (!zfound) {\n\t\te = GF_ISOM_INVALID_FILE;\n\t\tgoto exit;\n\t}\n\n\tif (i) {\n\t\tptr->drm_data = gf_strdup(tmp_str);\n\t}\n\n\ti=0;\n\ttmp_strsize=(u32)ptr->size;\n\tzfound = GF_FALSE;\n\twhile (tmp_strsize) {\n\t\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\t\ttmp_str[i] = gf_bs_read_u8(bs);\n\t\ttmp_strsize--;\n\t\tif (!tmp_str[i]) {\n\t\t\tzfound = GF_TRUE;\n\t\t\tbreak;\n\t\t}\n\t\ti++;\n\t}\n\tif (!zfound) {\n\t\te = GF_ISOM_INVALID_FILE;\n\t\tgoto exit;\n\t}\n\n\tif (i) {\n\t\tptr->meta_data = gf_strdup(tmp_str);\n\t}\n\n\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\tptr->segment_run_table_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->segment_run_table_count; i++) {\n\t\tGF_AdobeSegmentRunTableBox *asrt = NULL;\n\t\te = gf_isom_box_parse((GF_Box **)&asrt, bs);\n\t\tif (e) {\n\t\t\tif (asrt) gf_isom_box_del((GF_Box*)asrt);\n\t\t\tgoto exit;\n\t\t}\n\t\tgf_list_add(ptr->segment_run_table_entries, asrt);\n\t}\n\tif (ptr->segment_run_table_count != gf_list_count(ptr->segment_run_table_entries)) {\n\t\te = GF_ISOM_INVALID_FILE;\n\t\tgoto exit;\n\t}\n\n\tISOM_DECREASE_SIZE_GOTO_EXIT(ptr, 1)\n\tptr->fragment_run_table_count = gf_bs_read_u8(bs);\n\tfor (i=0; i<ptr->fragment_run_table_count; i++) {\n\t\tGF_AdobeFragmentRunTableBox *afrt = NULL;\n\t\te = gf_isom_box_parse((GF_Box **)&afrt, bs);\n\t\tif (e) {\n\t\t\tif (afrt) gf_isom_box_del((GF_Box*)afrt);\n\t\t\tgoto exit;\n\t\t}\n\t\tgf_list_add(ptr->fragment_run_table_entries, afrt);\n\t}\n\tif (ptr->fragment_run_table_count != gf_list_count(ptr->fragment_run_table_entries)) {\n\t\te = GF_ISOM_INVALID_FILE;\n\t\tgoto exit;\n\t}\n\nexit:\n\tgf_free(tmp_str);\n\treturn e;\n}", "project": "gpac", "hash": 263041082295436052908187100475540340486, "size": 183, "commit_id": "e74be5976a6fee059c638050a237893f7e9a3b23", "message": "fixed #1753", "target": 0, "dataset": "other", "idx": 230580}
{"func": "    int n = (aFts5UnicodeData[iTbl] >> 5) + i;\n    for(; i<128 && i<n; i++){\n      aAscii[i] = (u8)bToken;\n    }\n    iTbl++;\n  }\n}", "project": "sqlite", "hash": 214840049614538452414476078085917423347, "size": 12, "commit_id": "d1d43efa4fb0f2098c0e2c5bf2e807c58d5ec05b", "message": "Prevent fts5 tokenizer unicode61 from considering '\\0' to be a token characters, even if other characters of class \"Cc\" are.\n\nFossilOrigin-Name: b7b7bde9b7a03665e3691c6d51118965f216d2dfb1617f138b9f9e60e418ed2f", "target": 1, "dataset": "other", "idx": 195680}
{"func": "    for(; i<128 && i<n; i++){\n      aAscii[i] = (u8)bToken;\n    }\n    iTbl++;\n  }\n  aAscii[0] = 0;                  /* 0x00 is never a token character */\n}", "project": "sqlite", "hash": 138808873422941292787822650142783113699, "size": 13, "commit_id": "d1d43efa4fb0f2098c0e2c5bf2e807c58d5ec05b", "message": "Prevent fts5 tokenizer unicode61 from considering '\\0' to be a token characters, even if other characters of class \"Cc\" are.\n\nFossilOrigin-Name: b7b7bde9b7a03665e3691c6d51118965f216d2dfb1617f138b9f9e60e418ed2f", "target": 0, "dataset": "other", "idx": 230603}
{"func": "  input.Push<uint64_t>(sizeof(int32_t));\n  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kLocalLifetimeAllocHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_create_wait_queue\",\n                           2);\n  int32_t *queue = reinterpret_cast<int32_t *>(output.next<uintptr_t>());\n  int klinux_errno = output.next<int>();\n  if (queue == nullptr) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n  }\n  enc_untrusted_disable_waiting(queue);", "project": "asylo", "hash": 37573905407616895373101266094510917833, "size": 16, "commit_id": "a37fb6a0e7daf30134dbbf357c9a518a1026aa02", "message": "Check untrusted queue is in outside enclave\n\nPiperOrigin-RevId: 333370935\nChange-Id: Ic3f15d5db1302d95c7cb199b44172474fecb81ca", "target": 1, "dataset": "other", "idx": 195697}
{"func": "  const auto status = NonSystemCallDispatcher(\n      ::asylo::host_call::kLocalLifetimeAllocHandler, &input, &output);\n  CheckStatusAndParamCount(status, output, \"enc_untrusted_create_wait_queue\",\n                           2);\n  int32_t *queue = reinterpret_cast<int32_t *>(output.next<uintptr_t>());\n  if (!TrustedPrimitives::IsOutsideEnclave(queue, sizeof(int32_t))) {\n    TrustedPrimitives::BestEffortAbort(\n        \"enc_untrusted_create_wait_queue: queue should be in untrusted memory\");\n  }\n  int klinux_errno = output.next<int>();\n  if (queue == nullptr) {\n    errno = FromkLinuxErrorNumber(klinux_errno);\n  }\n  enc_untrusted_disable_waiting(queue);", "project": "asylo", "hash": 48703534045389445580807437477821613621, "size": 20, "commit_id": "a37fb6a0e7daf30134dbbf357c9a518a1026aa02", "message": "Check untrusted queue is in outside enclave\n\nPiperOrigin-RevId: 333370935\nChange-Id: Ic3f15d5db1302d95c7cb199b44172474fecb81ca", "target": 0, "dataset": "other", "idx": 231074}
{"func": "        if (m_pNetwork) {\n            // May be nullptr.\n            Message.SetChan(m_pNetwork->FindChan(sTarget));\n        }\n\n        if (sTarget.TrimPrefix(m_pUser->GetStatusPrefix())) {\n            if (sTarget.Equals(\"status\")) {\n                CString sMsg = Message.GetText();\n                UserCommand(sMsg);\n            } else {\n                CALLMOD(sTarget, this, m_pUser, m_pNetwork,", "project": "znc", "hash": 89511507100186789300014022416773480646, "size": 48, "commit_id": "d229761821da38d984a9e4098ad96842490dc001", "message": "Fix echo-message for *status\n\nClose #1705", "target": 1, "dataset": "other", "idx": 195717}
{"func": "            // May be nullptr.\n            Message.SetChan(m_pNetwork->FindChan(sTarget));\n        }\n\n        if (sTarget.TrimPrefix(m_pUser->GetStatusPrefix())) {\n            EchoMessage(Message);\n\n            if (sTarget.Equals(\"status\")) {\n                CString sMsg = Message.GetText();\n                UserCommand(sMsg);\n            } else {\n                CALLMOD(sTarget, this, m_pUser, m_pNetwork,", "project": "znc", "hash": 224787576817106767054259735051627673489, "size": 50, "commit_id": "d229761821da38d984a9e4098ad96842490dc001", "message": "Fix echo-message for *status\n\nClose #1705", "target": 0, "dataset": "other", "idx": 231582}
{"func": "GF_Err Media_CheckDataEntry(GF_MediaBox *mdia, u32 dataEntryIndex)\n{\n\n\tGF_DataEntryURLBox *entry;\n\tGF_DataMap *map;\n\tGF_Err e;\n\tif (!mdia || !dataEntryIndex || dataEntryIndex > gf_list_count(mdia->information->dataInformation->dref->child_boxes)) return GF_BAD_PARAM;\n\n\tentry = (GF_DataEntryURLBox*)gf_list_get(mdia->information->dataInformation->dref->child_boxes, dataEntryIndex - 1);\n\tif (!entry) return GF_ISOM_INVALID_FILE;\n\tif (entry->flags == 1) return GF_OK;\n\n\t//ok, not self contained, let's go for it...\n\t//we don't know what's a URN yet\n\tif (entry->type == GF_ISOM_BOX_TYPE_URN) return GF_NOT_SUPPORTED;\n\tif (mdia->mediaTrack->moov->mov->openMode == GF_ISOM_OPEN_WRITE) {\n\t\te = gf_isom_datamap_new(entry->location, NULL, GF_ISOM_DATA_MAP_READ, &map);\n\t} else {\n\t\te = gf_isom_datamap_new(entry->location, mdia->mediaTrack->moov->mov->fileName, GF_ISOM_DATA_MAP_READ, &map);\n\t}", "project": "gpac", "hash": 163064601320028078105851138396564236942, "size": 24, "commit_id": "328def7d3b93847d64ecb6e9e0399684e57c3eca", "message": "fixed #1766 (fuzz)", "target": 1, "dataset": "other", "idx": 195719}
{"func": "GF_Err Media_CheckDataEntry(GF_MediaBox *mdia, u32 dataEntryIndex)\n{\n\tGF_DataEntryURLBox *entry;\n\tGF_DataMap *map;\n\tGF_Err e;\n\tif (!mdia || !dataEntryIndex || dataEntryIndex > gf_list_count(mdia->information->dataInformation->dref->child_boxes)) return GF_BAD_PARAM;\n\n\tentry = (GF_DataEntryURLBox*)gf_list_get(mdia->information->dataInformation->dref->child_boxes, dataEntryIndex - 1);\n\tif (!entry) return GF_ISOM_INVALID_FILE;\n\tif (entry->flags == 1) return GF_OK;\n\n\t//ok, not self contained, let's go for it...\n\t//we only support alias and URL boxes\n\tif ((entry->type != GF_ISOM_BOX_TYPE_URL) && (entry->type != GF_QT_BOX_TYPE_ALIS) )\n\t\treturn GF_NOT_SUPPORTED;\n\n\tif (mdia->mediaTrack->moov->mov->openMode == GF_ISOM_OPEN_WRITE) {\n\t\te = gf_isom_datamap_new(entry->location, NULL, GF_ISOM_DATA_MAP_READ, &map);\n\t} else {\n\t\te = gf_isom_datamap_new(entry->location, mdia->mediaTrack->moov->mov->fileName, GF_ISOM_DATA_MAP_READ, &map);\n\t}", "project": "gpac", "hash": 334934742632849937568706482655465507791, "size": 25, "commit_id": "328def7d3b93847d64ecb6e9e0399684e57c3eca", "message": "fixed #1766 (fuzz)", "target": 0, "dataset": "other", "idx": 231612}
{"func": "  CHECK_OCALL(\n      ocall_dispatch_untrusted_call(&ret, untrusted_selector, sgx_params));\n  if (sgx_params->input) {\n    untrusted_cache->Free(const_cast<void *>(sgx_params->input));\n  }\n  if (!TrustedPrimitives::IsOutsideEnclave(sgx_params->output,\n                                           sgx_params->output_size)) {\n    TrustedPrimitives::BestEffortAbort(\n        \"UntrustedCall: sgx_param output should be in untrusted memory\");\n  }\n  if (sgx_params->output) {\n    // For the results obtained in |output_buffer|, copy them to |output|\n    // before freeing the buffer.\n    output->Deserialize(sgx_params->output, sgx_params->output_size);\n    TrustedPrimitives::UntrustedLocalFree(sgx_params->output);\n  }\n  return PrimitiveStatus::OkStatus();\n}", "project": "asylo", "hash": 257682981429665497503990072998187396956, "size": 51, "commit_id": "53ed5d8fd8118ced1466e509606dd2f473707a5c", "message": "Store untrusted output pointer in enclave\n\nValidate the pointer after it's stored in enclave to avoid unexpected\nmodifications after it's validated.\n\nPiperOrigin-RevId: 365648810\nChange-Id: I3079128040c142e86bab8255b07d03562a6fcb61", "target": 1, "dataset": "other", "idx": 195725}
{"func": "  CHECK_OCALL(\n      ocall_dispatch_untrusted_call(&ret, untrusted_selector, sgx_params));\n  if (sgx_params->input) {\n    untrusted_cache->Free(const_cast<void *>(sgx_params->input));\n  }\n  const void *output_pointer = sgx_params->output;\n  uint64_t output_size = sgx_params->output_size;\n  if (!TrustedPrimitives::IsOutsideEnclave(output_pointer, output_size)) {\n    TrustedPrimitives::BestEffortAbort(\n        \"UntrustedCall: sgx_param output should be in untrusted memory\");\n  }\n  if (sgx_params->output) {\n    // For the results obtained in |output_buffer|, copy them to |output|\n    // before freeing the buffer.\n    output->Deserialize(output_pointer, output_size);\n    TrustedPrimitives::UntrustedLocalFree(sgx_params->output);\n  }\n  return PrimitiveStatus::OkStatus();\n}", "project": "asylo", "hash": 20214115828053448204164555554097192268, "size": 52, "commit_id": "53ed5d8fd8118ced1466e509606dd2f473707a5c", "message": "Store untrusted output pointer in enclave\n\nValidate the pointer after it's stored in enclave to avoid unexpected\nmodifications after it's validated.\n\nPiperOrigin-RevId: 365648810\nChange-Id: I3079128040c142e86bab8255b07d03562a6fcb61", "target": 0, "dataset": "other", "idx": 232064}
{"func": "\tr_num_free (c->num);\n\t// TODO: sync or not? sdb_sync (c->sdb);\n\t// TODO: sync all dbs?\n\t//r_core_file_free (c->file);\n\t//c->file = NULL;\n\tfree (c->table_query);\n\tr_list_free (c->files);\n\tr_list_free (c->watchers);\n\tr_list_free (c->scriptstack);\n\tr_core_task_scheduler_fini (&c->tasks);\n\tc->rcmd = r_cmd_free (c->rcmd);", "project": "radare2", "hash": 307897061826018138760096537879143991047, "size": 63, "commit_id": "cb8b683758edddae2d2f62e8e63a738c39f92683", "message": "Fix #16303 - c->table_query double free (#16318)", "target": 1, "dataset": "other", "idx": 195727}
{"func": "\tr_num_free (c->num);\n\t// TODO: sync or not? sdb_sync (c->sdb);\n\t// TODO: sync all dbs?\n\t//r_core_file_free (c->file);\n\t//c->file = NULL;\n\tR_FREE (c->table_query);\n\tr_list_free (c->files);\n\tr_list_free (c->watchers);\n\tr_list_free (c->scriptstack);\n\tr_core_task_scheduler_fini (&c->tasks);\n\tc->rcmd = r_cmd_free (c->rcmd);", "project": "radare2", "hash": 120305734667673744603788082192514109384, "size": 63, "commit_id": "cb8b683758edddae2d2f62e8e63a738c39f92683", "message": "Fix #16303 - c->table_query double free (#16318)", "target": 0, "dataset": "other", "idx": 232153}
{"func": "    const float in = scaler(i, scale);\n    const float in_f = std::floor(in);\n    interpolation->lower[i] =\n        std::max(static_cast<int64>(in_f), static_cast<int64>(0));\n    interpolation->upper[i] =\n        std::min(static_cast<int64>(std::ceil(in)), in_size - 1);\n    interpolation->lerp[i] = in - in_f;\n    interpolation->ilerp[i] =\n        static_cast<T_SCALE>((in - in_f) * (1 << resolution));\n  }\n}", "project": "tensorflow", "hash": 270585959924700897209636611295822247365, "size": 23, "commit_id": "f851613f8f0fb0c838d160ced13c134f778e3ce7", "message": "Fix heap buffer overflow caused by rounding.\n\nThis was hard to fix. Due to the way we compute the pixels that influence an output pixel in resized images, for certain input configuration we might have issued a read to a pixel that is outside of boundary of the original image. This is because of floating errors that affected truncation results.\n\nPiperOrigin-RevId: 369757871\nChange-Id: If89425fff930983829a2168203c11858883eebc9", "target": 1, "dataset": "other", "idx": 195747}
{"func": "    const float in_f = std::floor(in);\n    interpolation->lower[i] =\n        std::max(static_cast<int64>(in_f), static_cast<int64>(0));\n    interpolation->upper[i] =\n        std::min(static_cast<int64>(std::ceil(in)), in_size - 1);\n    interpolation->lower[i] =\n        std::min(interpolation->lower[i], interpolation->upper[i]);\n    interpolation->lerp[i] = in - in_f;\n    interpolation->ilerp[i] =\n        static_cast<T_SCALE>((in - in_f) * (1 << resolution));\n  }\n}", "project": "tensorflow", "hash": 247234673289123575753752195039643028677, "size": 25, "commit_id": "f851613f8f0fb0c838d160ced13c134f778e3ce7", "message": "Fix heap buffer overflow caused by rounding.\n\nThis was hard to fix. Due to the way we compute the pixels that influence an output pixel in resized images, for certain input configuration we might have issued a read to a pixel that is outside of boundary of the original image. This is because of floating errors that affected truncation results.\n\nPiperOrigin-RevId: 369757871\nChange-Id: If89425fff930983829a2168203c11858883eebc9", "target": 0, "dataset": "other", "idx": 232376}
{"func": "\n\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}", "project": "linux", "hash": 256853574586537399963383949255567066385, "size": 125, "commit_id": "294f2fc6da27620a506e6c050241655459ccd6bd", "message": "bpf: Verifer, adjust_scalar_min_max_vals to always call update_reg_bounds()\n\nCurrently, for all op verification we call __red_deduce_bounds() and\n__red_bound_offset() but we only call __update_reg_bounds() in bitwise\nops. However, we could benefit from calling __update_reg_bounds() in\nBPF_ADD, BPF_SUB, and BPF_MUL cases as well.\n\nFor example, a register with state 'R1_w=invP0' when we subtract from\nit,\n\n w1 -= 2\n\nBefore coerce we will now have an smin_value=S64_MIN, smax_value=U64_MAX\nand unsigned bounds umin_value=0, umax_value=U64_MAX. These will then\nbe clamped to S32_MIN, U32_MAX values by coerce in the case of alu32 op\nas done in above example. However tnum will be a constant because the\nALU op is done on a constant.\n\nWithout update_reg_bounds() we have a scenario where tnum is a const\nbut our unsigned bounds do not reflect this. By calling update_reg_bounds\nafter coerce to 32bit we further refine the umin_value to U64_MAX in the\nalu64 case or U32_MAX in the alu32 case above.\n\nSigned-off-by: John Fastabend <john.fastabend@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/158507151689.15666.566796274289413203.stgit@john-Precision-5820-Tower", "target": 1, "dataset": "other", "idx": 195753}
{"func": "\tif (BPF_CLASS(insn->code) != BPF_ALU64) {\n\t\t/* 32-bit ALU ops are (32,32)->32 */\n\t\tcoerce_reg_to_size(dst_reg, 4);\n\t}\n\n\t__update_reg_bounds(dst_reg);\n\t__reg_deduce_bounds(dst_reg);\n\t__reg_bound_offset(dst_reg);\n\treturn 0;\n}", "project": "linux", "hash": 201029413509759936437749590618667202456, "size": 126, "commit_id": "294f2fc6da27620a506e6c050241655459ccd6bd", "message": "bpf: Verifer, adjust_scalar_min_max_vals to always call update_reg_bounds()\n\nCurrently, for all op verification we call __red_deduce_bounds() and\n__red_bound_offset() but we only call __update_reg_bounds() in bitwise\nops. However, we could benefit from calling __update_reg_bounds() in\nBPF_ADD, BPF_SUB, and BPF_MUL cases as well.\n\nFor example, a register with state 'R1_w=invP0' when we subtract from\nit,\n\n w1 -= 2\n\nBefore coerce we will now have an smin_value=S64_MIN, smax_value=U64_MAX\nand unsigned bounds umin_value=0, umax_value=U64_MAX. These will then\nbe clamped to S32_MIN, U32_MAX values by coerce in the case of alu32 op\nas done in above example. However tnum will be a constant because the\nALU op is done on a constant.\n\nWithout update_reg_bounds() we have a scenario where tnum is a const\nbut our unsigned bounds do not reflect this. By calling update_reg_bounds\nafter coerce to 32bit we further refine the umin_value to U64_MAX in the\nalu64 case or U32_MAX in the alu32 case above.\n\nSigned-off-by: John Fastabend <john.fastabend@gmail.com>\nSigned-off-by: Alexei Starovoitov <ast@kernel.org>\nLink: https://lore.kernel.org/bpf/158507151689.15666.566796274289413203.stgit@john-Precision-5820-Tower", "target": 0, "dataset": "other", "idx": 232639}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(", "project": "tensorflow", "hash": 129370171757238809505094793708920834743, "size": 52, "commit_id": "0ab290774f91a23bebe30a358fde4e53ab4876a0", "message": "Ensure validation sticks in banded_triangular_solve_op\n\nPiperOrigin-RevId: 373275480\nChange-Id: Id7717cf275b2d6fdb9441fbbe166d555182d2e79", "target": 1, "dataset": "other", "idx": 195754}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& in0 = ctx->input(0);\n    const Tensor& in1 = ctx->input(1);\n\n    ValidateInputTensors(ctx, in0, in1);\n    if (!ctx->status().ok()) return;\n\n    MatMulBCast bcast(in0.shape().dim_sizes(), in1.shape().dim_sizes());\n    OP_REQUIRES(\n        ctx, bcast.IsValid(),\n        errors::InvalidArgument(", "project": "tensorflow", "hash": 126900769494079489680653992675879726552, "size": 53, "commit_id": "0ab290774f91a23bebe30a358fde4e53ab4876a0", "message": "Ensure validation sticks in banded_triangular_solve_op\n\nPiperOrigin-RevId: 373275480\nChange-Id: Id7717cf275b2d6fdb9441fbbe166d555182d2e79", "target": 0, "dataset": "other", "idx": 232648}
{"func": "Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n                                   TF_DataType* out_tf_datatype) {\n  PyObject* key;\n  PyObject* value;\n  Py_ssize_t pos = 0;\n  if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n    // In Python 3, the keys of numpy custom struct types are unicode, unlike\n    // Python 2, where the keys are bytes.\n    const char* key_string =\n        PyBytes_Check(key) ? PyBytes_AsString(key)", "project": "tensorflow", "hash": 29762935772843750891990207013052403837, "size": 38, "commit_id": "030af767d357d1b4088c4a25c72cb3906abac489", "message": "Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.\n\nPiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8", "target": 1, "dataset": "other", "idx": 195755}
{"func": "Status PyArrayDescr_to_TF_DataType(PyArray_Descr* descr,\n                                   TF_DataType* out_tf_datatype) {\n  PyObject* key;\n  PyObject* value;\n  Py_ssize_t pos = 0;\n\n  // Return an error if the fields attribute is null.\n  // Occurs with an improper conversion attempt to resource.\n  if (descr->fields == nullptr) {\n    return errors::Internal(\"Unexpected numpy data type\");\n  }\n\n  if (PyDict_Next(descr->fields, &pos, &key, &value)) {\n    // In Python 3, the keys of numpy custom struct types are unicode, unlike\n    // Python 2, where the keys are bytes.\n    const char* key_string =\n        PyBytes_Check(key) ? PyBytes_AsString(key)", "project": "tensorflow", "hash": 118618212277768394037584817676898970795, "size": 45, "commit_id": "030af767d357d1b4088c4a25c72cb3906abac489", "message": "Fix `tf.raw_ops.ResourceCountUpTo` null pointer dereference.\n\nPiperOrigin-RevId: 368294347\nChange-Id: I2c16fbfc9b4966c402c3d8e311f0d665a9c852d8", "target": 0, "dataset": "other", "idx": 232661}
{"func": "vq_endchains(struct virtio_vq_info *vq, int used_all_avail)\n{\n\tstruct virtio_base *base;\n\tuint16_t event_idx, new_idx, old_idx;\n\tint intr;\n\n\t/*\n\t * Interrupt generation: if we're using EVENT_IDX,\n\t * interrupt if we've crossed the event threshold.\n\t * Otherwise interrupt is generated if we added \"used\" entries,", "project": "acrn-hypervisor", "hash": 93450239856945602125965929759509786033, "size": 39, "commit_id": "154fe59531c12b82e26d1b24b5531f5066d224f5", "message": "dm: validate inputs in vq_endchains\n\n inputs shall be validated to avoid NULL pointer access.\n\nTracked-On: #6129\nSigned-off-by: Yonghua Huang <yonghua.huang@intel.com>", "target": 1, "dataset": "other", "idx": 195776}
{"func": "vq_endchains(struct virtio_vq_info *vq, int used_all_avail)\n{\n\tstruct virtio_base *base;\n\tuint16_t event_idx, new_idx, old_idx;\n\tint intr;\n\n\tif (!vq || !vq->used)\n\t\treturn;\n\n\t/*\n\t * Interrupt generation: if we're using EVENT_IDX,\n\t * interrupt if we've crossed the event threshold.\n\t * Otherwise interrupt is generated if we added \"used\" entries,", "project": "acrn-hypervisor", "hash": 116982534787850067763114019938336435522, "size": 42, "commit_id": "154fe59531c12b82e26d1b24b5531f5066d224f5", "message": "dm: validate inputs in vq_endchains\n\n inputs shall be validated to avoid NULL pointer access.\n\nTracked-On: #6129\nSigned-off-by: Yonghua Huang <yonghua.huang@intel.com>", "target": 0, "dataset": "other", "idx": 232979}
{"func": "                                determined_size));\n\n    if (neg_one_dim >= 0) {\n      (*split_sizes_vec)[neg_one_dim] = input_size_split_dim - determined_size;\n    }\n\n    // Special case 2: split along the 1st dimension. The requirements are that\n    // either we are splitting the outer dimension of two or more such that\n    // every outer subpart is aligned or that the split sizes mean that they are\n    // always aligned. In these cases, we can share the underlying buffer.\n    //", "project": "tensorflow", "hash": 126245865552496072215885042297203605082, "size": 110, "commit_id": "25d622ffc432acc736b14ca3904177579e733cc6", "message": "A negative size in one of the split sizes allowed the computed size of another\nto exceed the total dimension, leading to a segfault and security vulnerability.\nAdding a check for negative sizes prevents this.\n\nPiperOrigin-RevId: 401035665\nChange-Id: I79bbe329787dac82aa4bf60397a9129b716aedab", "target": 1, "dataset": "other", "idx": 195778}
{"func": "\n    if (neg_one_dim >= 0) {\n      (*split_sizes_vec)[neg_one_dim] = input_size_split_dim - determined_size;\n    }\n\n    for (int i = 0; i < split_sizes_vec->size(); ++i) {\n      const Tlen& split_size = (*split_sizes_vec)[i];\n      OP_REQUIRES(context, split_size >= Tlen(0),\n                  errors::InvalidArgument(\"Split size at index \", i,\n                                          \" must be >= 0. Got: \", split_size));\n    }\n\n    // Special case 2: split along the 1st dimension. The requirements are that\n    // either we are splitting the outer dimension of two or more such that\n    // every outer subpart is aligned or that the split sizes mean that they are\n    // always aligned. In these cases, we can share the underlying buffer.\n    //", "project": "tensorflow", "hash": 246328078065637501416321248235339183827, "size": 117, "commit_id": "25d622ffc432acc736b14ca3904177579e733cc6", "message": "A negative size in one of the split sizes allowed the computed size of another\nto exceed the total dimension, leading to a segfault and security vulnerability.\nAdding a check for negative sizes prevents this.\n\nPiperOrigin-RevId: 401035665\nChange-Id: I79bbe329787dac82aa4bf60397a9129b716aedab", "target": 0, "dataset": "other", "idx": 233116}
{"func": "\t}\n\n\ti += 4 + extension_len, offset += 4 + extension_len;\n      } /* for */\n\n      ja3_str_len = snprintf(ja3_str, sizeof(ja3_str), \"%u,\", ja3.server.tls_handshake_version);\n\n      for(i=0; i<ja3.server.num_cipher; i++) {\n\trc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \"%s%u\", (i > 0) ? \"-\" : \"\", ja3.server.cipher[i]);\n\n\tif(rc <= 0) break; else ja3_str_len += rc;\n      }\n\n      rc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \",\");\n      if(rc > 0 && ja3_str_len + rc < JA3_STR_LEN) ja3_str_len += rc;\n\n      /* ********** */\n\n      for(i=0; i<ja3.server.num_tls_extension; i++) {\n\tint rc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \"%s%u\", (i > 0) ? \"-\" : \"\", ja3.server.tls_extension[i]);\n\n\tif(rc <= 0) break; else ja3_str_len += rc;\n      }\n\n      if(ndpi_struct->enable_ja3_plus) {\n\tfor(i=0; i<ja3.server.num_elliptic_curve_point_format; i++) {\n\t  rc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \"%s%u\",\n\t\t\t(i > 0) ? \"-\" : \"\", ja3.server.elliptic_curve_point_format[i]);\n\t  if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t}\n\n\tif(ja3.server.alpn[0] != '\\0') {\n\t  rc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \",%s\", ja3.server.alpn);\n\t  if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\t}\n\n#ifdef DEBUG_TLS\n\tprintf(\"[JA3+] Server: %s \\n\", ja3_str);\n\n\t    if(!invalid_ja3) {\n\t      int rc;\n\n\t    compute_ja3c:\n\t      ja3_str_len = snprintf(ja3_str, sizeof(ja3_str), \"%u,\", ja3.client.tls_handshake_version);\n\n\t      for(i=0; i<ja3.client.num_cipher; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.cipher[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      rc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \",\");\n\t      if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\n\t      /* ********** */\n\n\t      for(i=0; i<ja3.client.num_tls_extension; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.tls_extension[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      rc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \",\");\n\t      if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\n\t      /* ********** */\n\n\t      for(i=0; i<ja3.client.num_elliptic_curve; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.elliptic_curve[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      rc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \",\");\n\t      if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\n\t      for(i=0; i<ja3.client.num_elliptic_curve_point_format; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.elliptic_curve_point_format[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      if(ndpi_struct->enable_ja3_plus) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], sizeof(ja3_str)-ja3_str_len,\n\t\t\t      \",%s,%s,%s\", ja3.client.signature_algorithms, ja3.client.supported_versions, ja3.client.alpn);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\t      }\n\n#ifdef DEBUG_JA3C", "project": "nDPI", "hash": 257200396347775652568989609363155151045, "size": 941, "commit_id": "1ec621c85b9411cc611652fd57a892cfef478af3", "message": "Added further checks", "target": 1, "dataset": "other", "idx": 195820}
{"func": "\t}\n\n\ti += 4 + extension_len, offset += 4 + extension_len;\n      } /* for */\n\n      ja3_str_len = snprintf(ja3_str, JA3_STR_LEN, \"%u,\", ja3.server.tls_handshake_version);\n\n      for(i=0; (i<ja3.server.num_cipher) && (JA3_STR_LEN > ja3_str_len); i++) {\n\trc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \"%s%u\", (i > 0) ? \"-\" : \"\", ja3.server.cipher[i]);\n\n\tif(rc <= 0) break; else ja3_str_len += rc;\n      }\n\n      if(JA3_STR_LEN > ja3_str_len) {\n\trc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \",\");\n\tif(rc > 0 && ja3_str_len + rc < JA3_STR_LEN) ja3_str_len += rc;\n      }\n      \n      /* ********** */\n\n      for(i=0; (i<ja3.server.num_tls_extension) && (JA3_STR_LEN > ja3_str_len); i++) {\n\tint rc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \"%s%u\", (i > 0) ? \"-\" : \"\", ja3.server.tls_extension[i]);\n\n\tif(rc <= 0) break; else ja3_str_len += rc;\n      }\n\n      if(ndpi_struct->enable_ja3_plus) {\n\tfor(i=0; (i<ja3.server.num_elliptic_curve_point_format) && (JA3_STR_LEN > ja3_str_len); i++) {\n\t  rc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \"%s%u\",\n\t\t\t(i > 0) ? \"-\" : \"\", ja3.server.elliptic_curve_point_format[i]);\n\t  if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t}\n\n\tif((ja3.server.alpn[0] != '\\0') && (JA3_STR_LEN > ja3_str_len)) {\n\t  rc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \",%s\", ja3.server.alpn);\n\t  if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\t}\n\n#ifdef DEBUG_TLS\n\tprintf(\"[JA3+] Server: %s \\n\", ja3_str);\n\n\t    if(!invalid_ja3) {\n\t      int rc;\n\n\t    compute_ja3c:\n\t      ja3_str_len = snprintf(ja3_str, JA3_STR_LEN, \"%u,\", ja3.client.tls_handshake_version);\n\n\t      for(i=0; i<ja3.client.num_cipher; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.cipher[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      rc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \",\");\n\t      if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\n\t      /* ********** */\n\n\t      for(i=0; i<ja3.client.num_tls_extension; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.tls_extension[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      rc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \",\");\n\t      if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\n\t      /* ********** */\n\n\t      for(i=0; i<ja3.client.num_elliptic_curve; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.elliptic_curve[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      rc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \",\");\n\t      if((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\n\t      for(i=0; i<ja3.client.num_elliptic_curve_point_format; i++) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len, \"%s%u\",\n\t\t\t      (i > 0) ? \"-\" : \"\", ja3.client.elliptic_curve_point_format[i]);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc; else break;\n\t      }\n\n\t      if(ndpi_struct->enable_ja3_plus) {\n\t\trc = snprintf(&ja3_str[ja3_str_len], JA3_STR_LEN-ja3_str_len,\n\t\t\t      \",%s,%s,%s\", ja3.client.signature_algorithms, ja3.client.supported_versions, ja3.client.alpn);\n\t\tif((rc > 0) && (ja3_str_len + rc < JA3_STR_LEN)) ja3_str_len += rc;\n\t      }\n\n#ifdef DEBUG_JA3C", "project": "nDPI", "hash": 178466311908364350569618120682794749914, "size": 943, "commit_id": "1ec621c85b9411cc611652fd57a892cfef478af3", "message": "Added further checks", "target": 0, "dataset": "other", "idx": 234082}
{"func": "\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\treturn -ENOBUFS;\n\tfh->size = be32_to_cpup(p++);\n\tif (fh->size > sizeof(struct nfs_fh)) {\n\t\tprintk(KERN_ERR \"NFS flexfiles: Too big fh received %d\\n\",\n\t\t       fh->size);\n\t\treturn -EOVERFLOW;\n\t}\n\t/* fh.data */", "project": "linux", "hash": 234844035562921129628265308957208614973, "size": 22, "commit_id": "ed34695e15aba74f45247f1ee2cf7e09d449f925", "message": "pNFS/flexfiles: fix incorrect size check in decode_nfs_fh()\n\nWe (adam zabrocki, alexander matrosov, alexander tereshkin, maksym\nbazalii) observed the check:\n\n\tif (fh->size > sizeof(struct nfs_fh))\n\nshould not use the size of the nfs_fh struct which includes an extra two\nbytes from the size field.\n\nstruct nfs_fh {\n\tunsigned short         size;\n\tunsigned char          data[NFS_MAXFHSIZE];\n}\n\nbut should determine the size from data[NFS_MAXFHSIZE] so the memcpy\nwill not write 2 bytes beyond destination.  The proposed fix is to\ncompare against the NFS_MAXFHSIZE directly, as is done elsewhere in fs\ncode base.\n\nFixes: d67ae825a59d (\"pnfs/flexfiles: Add the FlexFile Layout Driver\")\nSigned-off-by: Nikola Livic <nlivic@gmail.com>\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>", "target": 1, "dataset": "other", "idx": 195843}
{"func": "\n\tp = xdr_inline_decode(xdr, 4);\n\tif (unlikely(!p))\n\t\treturn -ENOBUFS;\n\tfh->size = be32_to_cpup(p++);\n\tif (fh->size > NFS_MAXFHSIZE) {\n\t\tprintk(KERN_ERR \"NFS flexfiles: Too big fh received %d\\n\",\n\t\t       fh->size);\n\t\treturn -EOVERFLOW;\n\t}\n\t/* fh.data */", "project": "linux", "hash": 53704026130374252810142557328449284293, "size": 22, "commit_id": "ed34695e15aba74f45247f1ee2cf7e09d449f925", "message": "pNFS/flexfiles: fix incorrect size check in decode_nfs_fh()\n\nWe (adam zabrocki, alexander matrosov, alexander tereshkin, maksym\nbazalii) observed the check:\n\n\tif (fh->size > sizeof(struct nfs_fh))\n\nshould not use the size of the nfs_fh struct which includes an extra two\nbytes from the size field.\n\nstruct nfs_fh {\n\tunsigned short         size;\n\tunsigned char          data[NFS_MAXFHSIZE];\n}\n\nbut should determine the size from data[NFS_MAXFHSIZE] so the memcpy\nwill not write 2 bytes beyond destination.  The proposed fix is to\ncompare against the NFS_MAXFHSIZE directly, as is done elsewhere in fs\ncode base.\n\nFixes: d67ae825a59d (\"pnfs/flexfiles: Add the FlexFile Layout Driver\")\nSigned-off-by: Nikola Livic <nlivic@gmail.com>\nSigned-off-by: Dan Carpenter <dan.carpenter@oracle.com>\nSigned-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>", "target": 0, "dataset": "other", "idx": 234482}
{"func": "    input_slice_sizes[0] = input_dims[0];\n    TensorShape temp_shape{input_dims[0]};\n    for (int i = 1; i <= FFTRank; ++i) {\n      input_slice_sizes[i] = fft_shape[i - 1];\n      temp_shape.AddDim(fft_shape[i - 1]);\n    }\n\n    auto output = out->flat_inner_dims<ComplexT, FFTRank + 1>();\n    const Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> zero_start_indices;\n\n    // Compute the full FFT using a temporary tensor.", "project": "tensorflow", "hash": 280992516761239439019736638647860473416, "size": 33, "commit_id": "31bd5026304677faa8a0b77602c6154171b9aec1", "message": "Prevent check fail in FFT\n\nPiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299", "target": 1, "dataset": "other", "idx": 195958}
{"func": "    TensorShape temp_shape{input_dims[0]};\n    for (int i = 1; i <= FFTRank; ++i) {\n      input_slice_sizes[i] = fft_shape[i - 1];\n      temp_shape.AddDim(fft_shape[i - 1]);\n    }\n    OP_REQUIRES(ctx, temp_shape.num_elements() > 0,\n                errors::InvalidArgument(\"Obtained a FFT shape of 0 elements: \",\n                                        temp_shape.DebugString()));\n\n    auto output = out->flat_inner_dims<ComplexT, FFTRank + 1>();\n    const Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> zero_start_indices;\n\n    // Compute the full FFT using a temporary tensor.", "project": "tensorflow", "hash": 309164948673975832942418971794723232104, "size": 36, "commit_id": "31bd5026304677faa8a0b77602c6154171b9aec1", "message": "Prevent check fail in FFT\n\nPiperOrigin-RevId: 372031044\nChange-Id: I50994e3e8a5d1342d01bde80256f6bf2730ca299", "target": 0, "dataset": "other", "idx": 235720}
{"func": "\t\tapplog(LOG_INFO, \"Failed to get nonce1 in initiate_stratum\");\n\t\tfree(sessionid);\n\t\tgoto out;\n\t}\n\tn2size = json_integer_value(json_array_get(res_val, 2));\n\tif (!n2size) {\n\t\tapplog(LOG_INFO, \"Failed to get n2size in initiate_stratum\");\n\t\tfree(sessionid);\n\t\tfree(nonce1);\n\t\tgoto out;\n\t}", "project": "bfgminer", "hash": 53047281314977669876157597332818027856, "size": 124, "commit_id": "ff7f30129f15f7a2213f8ced0cd65c9a331493d9", "message": "Bugfix: initiate_stratum: Ensure extranonce2 size is not negative (which could lead to exploits later as too little memory gets allocated)\n\nThanks to Mick Ayzenberg <mick@dejavusecurity.com> for finding this!", "target": 1, "dataset": "other", "idx": 195966}
{"func": "\t\tapplog(LOG_INFO, \"Failed to get nonce1 in initiate_stratum\");\n\t\tfree(sessionid);\n\t\tgoto out;\n\t}\n\tn2size = json_integer_value(json_array_get(res_val, 2));\n\tif (n2size < 1)\n\t{\n\t\tapplog(LOG_INFO, \"Failed to get n2size in initiate_stratum\");\n\t\tfree(sessionid);\n\t\tfree(nonce1);\n\t\tgoto out;\n\t}", "project": "bfgminer", "hash": 38082979272544137077318747389004597030, "size": 125, "commit_id": "ff7f30129f15f7a2213f8ced0cd65c9a331493d9", "message": "Bugfix: initiate_stratum: Ensure extranonce2 size is not negative (which could lead to exploits later as too little memory gets allocated)\n\nThanks to Mick Ayzenberg <mick@dejavusecurity.com> for finding this!", "target": 0, "dataset": "other", "idx": 235766}
{"func": "    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({nnz}), &output_values));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, TensorShape({nnz}),\n                                &dense_gathered));\n\n    // Pulls relevant entries from the dense side, with reshape and broadcasting\n    // *of the dense side* taken into account.  Use a TensorRef to avoid blowing\n    // up memory.\n    //\n    // We can directly use the sparse indices to look up dense side, because\n      }                                                                        \\\n      OP_REQUIRES(                                                             \\\n          ctx, indices_valid,                                                  \\\n          errors::InvalidArgument(\"Provided indices are out-of-bounds w.r.t. \" \\\n                                  \"dense side with broadcasted shape\"));       \\\n      dense_gathered_flat(i) = rhs_ref.coeff(idx);                             \\\n    }                                                                          \\\n    break;                                                                     \\\n  }\n\n      CASE(1);", "project": "tensorflow", "hash": 157920771574414128903170642379842966624, "size": 107, "commit_id": "d9204be9f49520cdaaeb2541d1dc5187b23f31d9", "message": "Disallow division by zero FPE in tf.raw_ops.SparseDenseCwiseDiv\n\nPiperOrigin-RevId: 383959809\nChange-Id: Ibe88458bdf66a686c93e354b8255dec94285c560", "target": 1, "dataset": "other", "idx": 195972}
{"func": "    OP_REQUIRES_OK(ctx,\n                   ctx->allocate_output(0, TensorShape({nnz}), &output_values));\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_temp(DataTypeToEnum<T>::value, TensorShape({nnz}),\n                                &dense_gathered));\n    bool op_is_div = false;\n    if (absl::StrContains(ctx->op_kernel().type_string_view(), \"Div\")) {\n      op_is_div = true;\n    }\n    // Pulls relevant entries from the dense side, with reshape and broadcasting\n    // *of the dense side* taken into account.  Use a TensorRef to avoid blowing\n    // up memory.\n    //\n    // We can directly use the sparse indices to look up dense side, because\n      OP_REQUIRES(                                                             \\\n          ctx, indices_valid,                                                  \\\n          errors::InvalidArgument(\"Provided indices are out-of-bounds w.r.t. \" \\\n                                  \"dense side with broadcasted shape\"));       \\\n      dense_gathered_flat(i) = rhs_ref.coeff(idx);                             \\\n      if (op_is_div) {                                                         \\\n        OP_REQUIRES(ctx, dense_gathered_flat(i) != 0,                          \\\n                    errors::InvalidArgument(                                   \\\n                        \"SparseDenseCwiseDiv cannot divide by zero,\"           \\\n                        \"but input dense tensor contains zero \"));             \\\n      }                                                                        \\\n    }                                                                          \\\n    break;                                                                     \\\n  }\n\n      CASE(1);", "project": "tensorflow", "hash": 76360371434450887584761667916814072129, "size": 116, "commit_id": "d9204be9f49520cdaaeb2541d1dc5187b23f31d9", "message": "Disallow division by zero FPE in tf.raw_ops.SparseDenseCwiseDiv\n\nPiperOrigin-RevId: 383959809\nChange-Id: Ibe88458bdf66a686c93e354b8255dec94285c560", "target": 0, "dataset": "other", "idx": 235840}
{"func": "static GF_Err av1dmx_parse_flush_sample(GF_Filter *filter, GF_AV1DmxCtx *ctx)\n{\n\tu32 pck_size;\n\tGF_FilterPacket *pck;\n\tu8 *output;\n\n\tgf_bs_get_content_no_truncate(ctx->state.bs, &ctx->state.frame_obus, &pck_size, &ctx->state.frame_obus_alloc);\n\n\tif (!pck_size) {\n\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_CONTAINER, (\"[AV1Dmx] no frame OBU, skipping OBU\\n\"));\n\t\treturn GF_OK;", "project": "gpac", "hash": 124123704759614734447704073729222685997, "size": 42, "commit_id": "13dad7d5ef74ca2e6fe4010f5b03eb12e9bbe0ec", "message": "fixed #1719", "target": 1, "dataset": "other", "idx": 195985}
{"func": "{\n\tu32 pck_size;\n\tGF_FilterPacket *pck;\n\tu8 *output;\n\n\tif (!ctx->opid)\n\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\n\tgf_bs_get_content_no_truncate(ctx->state.bs, &ctx->state.frame_obus, &pck_size, &ctx->state.frame_obus_alloc);\n\n\tif (!pck_size) {\n\t\tGF_LOG(GF_LOG_DEBUG, GF_LOG_CONTAINER, (\"[AV1Dmx] no frame OBU, skipping OBU\\n\"));\n\t\treturn GF_OK;", "project": "gpac", "hash": 155851759706352627613263901786806712657, "size": 45, "commit_id": "13dad7d5ef74ca2e6fe4010f5b03eb12e9bbe0ec", "message": "fixed #1719", "target": 0, "dataset": "other", "idx": 236225}
{"func": "NativeModule::NativeModule(const std::string& filename) : init(nullptr) {\n\tif (uv_dlopen(filename.c_str(), &lib) != 0) {\n\t\tthrow RuntimeGenericError(\"Failed to load module\");\n\t}\n\tif (uv_dlsym(&lib, \"InitForContext\", reinterpret_cast<void**>(&init)) != 0 || init == nullptr) {\n\t\tuv_dlclose(&lib);", "project": "isolated-vm", "hash": 137961370509523293475357475023661605207, "size": 9, "commit_id": "27151bfecc260e96714443613880e3b2e6596704", "message": "Disallow NativeModule creation unless main isolate", "target": 1, "dataset": "other", "idx": 195986}
{"func": "NativeModule::NativeModule(const std::string& filename) : init(nullptr) {\n\tif (!IsolateEnvironment::GetCurrent()->IsDefault()) {\n\t\tthrow RuntimeGenericError(\"NativeModule may only be instantiated from default nodejs isolate\");\n\t}\n\tif (uv_dlopen(filename.c_str(), &lib) != 0) {\n\t\tthrow RuntimeGenericError(\"Failed to load module\");\n\t}\n\tif (uv_dlsym(&lib, \"InitForContext\", reinterpret_cast<void**>(&init)) != 0 || init == nullptr) {\n\t\tuv_dlclose(&lib);", "project": "isolated-vm", "hash": 331411135769016276128654023219291021075, "size": 12, "commit_id": "27151bfecc260e96714443613880e3b2e6596704", "message": "Disallow NativeModule creation unless main isolate", "target": 0, "dataset": "other", "idx": 236238}
{"func": "        }\n        /* block arguments */\n        if (tree->cdr->cdr) {\n          codegen(s, tree->cdr->cdr, VAL);\n        }\n        else if (!s2) {/* super at top-level */\n          push();      /* no need to push block */\n        }\n        else {\n          gen_blkmove(s, s2->ainfo, lv);\n        }\n        st++;\n      }\n      else {\n        if (!s2) push();\n        else gen_blkmove(s, s2->ainfo, lv);\n        st++;\n      }\n      pop_n(st+1);\n      genop_2(s, OP_SUPER, cursp(), n);\n      if (val) push();\n    }\n    break;\n      int idx = lv_idx(s, MRB_OPSYM_2(s->mrb, and));\n\n      if (idx == 0) {\n        codegen_error(s, \"no anonymous block argument\");\n      }\n      gen_move(s, cursp(), idx, val);\n    }\n    else {\n      codegen(s, tree, val);\n    }\n    break;", "project": "mruby", "hash": 230573661446267941821122200387226170788, "size": 1535, "commit_id": "44f591aa8f7091e6ca6cb418e428ae6d4ceaf77d", "message": "codegen.c: adjust stack position for `OP_SUPER` instruction.", "target": 1, "dataset": "other", "idx": 196318}
{"func": "        }\n        /* block arguments */\n        if (tree->cdr->cdr) {\n          codegen(s, tree->cdr->cdr, VAL);\n        }\n        else if (s2) gen_blkmove(s, s2->ainfo, lv);\n        else {\n          genop_1(s, OP_LOADNIL, cursp());\n          push();\n        }\n      }\n      else {\n        if (s2) gen_blkmove(s, s2->ainfo, lv);\n        else {\n          genop_1(s, OP_LOADNIL, cursp());\n          push();\n        }\n      }\n      st++;\n      pop_n(st+1);\n      genop_2(s, OP_SUPER, cursp(), n);\n      if (val) push();\n    }\n    break;\n\n      if (idx == 0) {\n        codegen_error(s, \"no anonymous block argument\");\n      }\n      gen_move(s, cursp(), idx, val);\n      if (val) push();\n    }\n    else {\n      codegen(s, tree, val);\n    }\n    break;", "project": "mruby", "hash": 331098739854123957172781686688963498599, "size": 1537, "commit_id": "44f591aa8f7091e6ca6cb418e428ae6d4ceaf77d", "message": "codegen.c: adjust stack position for `OP_SUPER` instruction.", "target": 0, "dataset": "other", "idx": 238366}
{"func": "                                ecma_value_t value_arg, /**< value argument */\n                                lit_magic_string_id_t lit_id) /**< class id */\n{\n  JERRY_ASSERT (container_p != NULL);\n\n  ecma_collection_push_back (container_p, ecma_copy_value_if_not_object (key_arg));\n\n  if (lit_id == LIT_MAGIC_STRING_WEAKMAP_UL || lit_id == LIT_MAGIC_STRING_MAP_UL)\n  {\n    ecma_collection_push_back (container_p, ecma_copy_value_if_not_object (value_arg));\n  }\n\n  ECMA_CONTAINER_SET_SIZE (container_p, ECMA_CONTAINER_GET_SIZE (container_p) + 1);\n} /* ecma_op_internal_buffer_append */", "project": "jerryscript", "hash": 289394230641505307974784830249750953781, "size": 16, "commit_id": "c2b662170245a16f46ce02eae68815c325d99821", "message": "Fix adding entries to the internal buffer of a Map object (#3805)\n\nWhen appending the key/value pair separately, garbage collection could be\r\ntriggered before the value is added, which could cause problems during\r\nmarking. This patch changes insertion to add both values at the same\r\ntime, which prevents partial entries from being present in the internal\r\nbuffer.\r\n\r\nFixes #3804.\r\n\r\nJerryScript-DCO-1.0-Signed-off-by: D\u00e1niel B\u00e1tyai dbatyai@inf.u-szeged.hu", "target": 1, "dataset": "other", "idx": 196327}
{"func": "                                ecma_value_t key_arg, /**< key argument */\n                                ecma_value_t value_arg, /**< value argument */\n                                lit_magic_string_id_t lit_id) /**< class id */\n{\n  JERRY_ASSERT (container_p != NULL);\n\n  if (lit_id == LIT_MAGIC_STRING_WEAKMAP_UL || lit_id == LIT_MAGIC_STRING_MAP_UL)\n  {\n    ecma_value_t values[] = { ecma_copy_value_if_not_object (key_arg), ecma_copy_value_if_not_object (value_arg) };\n    ecma_collection_append (container_p, values, 2);\n  }\n  else\n  {\n    ecma_collection_push_back (container_p, ecma_copy_value_if_not_object (key_arg));\n  }\n\n  ECMA_CONTAINER_SET_SIZE (container_p, ECMA_CONTAINER_GET_SIZE (container_p) + 1);\n} /* ecma_op_internal_buffer_append */", "project": "jerryscript", "hash": 269950026591073206896646719908346441717, "size": 19, "commit_id": "c2b662170245a16f46ce02eae68815c325d99821", "message": "Fix adding entries to the internal buffer of a Map object (#3805)\n\nWhen appending the key/value pair separately, garbage collection could be\r\ntriggered before the value is added, which could cause problems during\r\nmarking. This patch changes insertion to add both values at the same\r\ntime, which prevents partial entries from being present in the internal\r\nbuffer.\r\n\r\nFixes #3804.\r\n\r\nJerryScript-DCO-1.0-Signed-off-by: D\u00e1niel B\u00e1tyai dbatyai@inf.u-szeged.hu", "target": 0, "dataset": "other", "idx": 238757}
{"func": "    // Get inputs\n    const Tensor& input_tensor = context->input(0);\n    const auto input_tensor_flat = input_tensor.flat<int32>();\n    const Tensor& input_splits = context->input(1);\n    const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n\n    // Operation will treat first argument in input_splits as if it were zero\n    // regardless of its actual value since splits should begin with zero and\n    // end with the length of the input values vector.\n    OP_REQUIRES(\n        context, input_splits_flat(0) == 0,", "project": "tensorflow", "hash": 95725917459110570359817499250275418833, "size": 61, "commit_id": "2e0ee46f1a47675152d3d865797a18358881d7a6", "message": "Ensure non-empty input_splits in tf.raw_ops.UnicodeEncode\n\nPiperOrigin-RevId: 387170080\nChange-Id: I3b489acc51c5cb4124c535b9df7cc6e62ef21766", "target": 1, "dataset": "other", "idx": 196329}
{"func": "    const Tensor& input_tensor = context->input(0);\n    const auto input_tensor_flat = input_tensor.flat<int32>();\n    const Tensor& input_splits = context->input(1);\n    const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n\n    OP_REQUIRES(\n        context, input_splits.NumElements() > 0,\n        errors::InvalidArgument(\"Input_splits should contain elements, but \"\n                                \"given input_values has 0 elements\"));\n    // Operation will treat first argument in input_splits as if it were zero\n    // regardless of its actual value since splits should begin with zero and\n    // end with the length of the input values vector.\n    OP_REQUIRES(\n        context, input_splits_flat(0) == 0,", "project": "tensorflow", "hash": 191582078444848979487063653458796885790, "size": 65, "commit_id": "2e0ee46f1a47675152d3d865797a18358881d7a6", "message": "Ensure non-empty input_splits in tf.raw_ops.UnicodeEncode\n\nPiperOrigin-RevId: 387170080\nChange-Id: I3b489acc51c5cb4124c535b9df7cc6e62ef21766", "target": 0, "dataset": "other", "idx": 238838}
{"func": "                                   boxes.shape().DebugString());\n  }\n  *num_boxes = boxes.dim_size(0);\n  if (boxes.dim_size(1) != 4) {\n    return errors::InvalidArgument(\"boxes must have 4 columns\");\n  }\n  // The shape of 'box_index' is [num_boxes].\n  if (box_index.dims() != 1) {\n    return errors::InvalidArgument(\"box_index must be 1-D\",\n                                   box_index.shape().DebugString());\n  }", "project": "tensorflow", "hash": 327348859222179558551343679127178889587, "size": 26, "commit_id": "3ade2efec2e90c6237de32a19680caaa3ebc2845", "message": "Fix segmentation fault in tf.image.crop_and_resize when boxes is inf or nan\n\nThis fix tries to address the issue raised in 42129 where segmentation fault\nhappened in tf.image.crop_and_resize when boxes is inf or nan.\n\nThis fix adds the check to make sure boxes is not inf or nan (isfinite)\n\nThis fix fixes 42129.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "target": 1, "dataset": "other", "idx": 196330}
{"func": "  }\n  *num_boxes = boxes.dim_size(0);\n  if (boxes.dim_size(1) != 4) {\n    return errors::InvalidArgument(\"boxes must have 4 columns\");\n  }\n  for (int64 i = 0; i < *num_boxes; i++) {\n    for (int64 j = 0; j < 4; j++) {\n      if (!isfinite(boxes.tensor<float, 2>()(i, j))) {\n        return errors::InvalidArgument(\n            \"boxes values must be finite, received boxes[\", i, \"]: \",\n            boxes.tensor<float, 2>()(i, 0), \", \",\n            boxes.tensor<float, 2>()(i, 1), \", \",\n            boxes.tensor<float, 2>()(i, 2), \", \",\n            boxes.tensor<float, 2>()(i, 3));\n      }\n    }\n  }\n  // The shape of 'box_index' is [num_boxes].\n  if (box_index.dims() != 1) {\n    return errors::InvalidArgument(\"box_index must be 1-D\",\n                                   box_index.shape().DebugString());\n  }", "project": "tensorflow", "hash": 162824839659560449491498365320039591607, "size": 38, "commit_id": "3ade2efec2e90c6237de32a19680caaa3ebc2845", "message": "Fix segmentation fault in tf.image.crop_and_resize when boxes is inf or nan\n\nThis fix tries to address the issue raised in 42129 where segmentation fault\nhappened in tf.image.crop_and_resize when boxes is inf or nan.\n\nThis fix adds the check to make sure boxes is not inf or nan (isfinite)\n\nThis fix fixes 42129.\n\nSigned-off-by: Yong Tang <yong.tang.github@outlook.com>", "target": 0, "dataset": "other", "idx": 238855}
{"func": "\n    /* Set the possible outgoing buffer */\n    *out_buf = tmp_out_buf;\n    *out_size = tmp_out_size;\n    if (mp_buf != tmp_out_buf) {\n        flb_free(mp_buf);\n    }\n\n    /* Do time resolution ? */\n    if (!parser->time_fmt) {\n        msgpack_unpacked_destroy(&result);\n            continue;\n        }\n\n        /* Ensure the pointer we are about to read is not NULL */\n        if (k->via.str.ptr == NULL) {\n            flb_free(mp_buf);\n            *out_buf = NULL;\n            msgpack_unpacked_destroy(&result);\n            return -1;\n        }\n", "project": "fluent-bit", "hash": 88227932456379194164638143524160893177, "size": 197, "commit_id": "22346a74c07ceb90296be872be2d53eb92252a54", "message": "parser: json: fix double-free (#3453)\n\nSigned-off-by: davkor <david@adalogics.com>", "target": 1, "dataset": "other", "idx": 196589}
{"func": "    /* Set the possible outgoing buffer */\n    *out_buf = tmp_out_buf;\n    *out_size = tmp_out_size;\n    if (mp_buf != tmp_out_buf) {\n        flb_free(mp_buf);\n        mp_buf = NULL;\n    }\n\n    /* Do time resolution ? */\n    if (!parser->time_fmt) {\n        msgpack_unpacked_destroy(&result);\n        }\n\n        /* Ensure the pointer we are about to read is not NULL */\n        if (k->via.str.ptr == NULL) {\n            flb_free(mp_buf);\n            flb_free(tmp_out_buf);\n            *out_buf = NULL;\n            msgpack_unpacked_destroy(&result);\n            return -1;\n        }\n", "project": "fluent-bit", "hash": 271145738730261777712614129952005894298, "size": 199, "commit_id": "22346a74c07ceb90296be872be2d53eb92252a54", "message": "parser: json: fix double-free (#3453)\n\nSigned-off-by: davkor <david@adalogics.com>", "target": 0, "dataset": "other", "idx": 240791}
{"func": "\tpamauth(targpw->pw_name, mypw->pw_name, !nflag, rule->options & NOPASS,\n\t    rule->options & PERSIST);\n#endif\n\n#ifdef HAVE_LOGIN_CAP_H\n\tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n\t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n\t    LOGIN_SETUSER) != 0)\n\t\terrx(1, \"failed to set user context for target\");\n#else\n\tif (setresgid(targpw->pw_gid, targpw->pw_gid, targpw->pw_gid) != 0)\n\t\terr(1, \"setresgid\");\n\tif (initgroups(targpw->pw_name, targpw->pw_gid) != 0)\n\t\terr(1, \"initgroups\");\n\tif (setresuid(target, target, target) != 0)\n\t\terr(1, \"setresuid\");\n#endif\n\n\tif (getcwd(cwdpath, sizeof(cwdpath)) == NULL)\n\t\tcwd = \"(failed)\";\n\telse", "project": "OpenDoas", "hash": 321353994930714283258779977752854978365, "size": 188, "commit_id": "d5acd52e2a15c36a8e06f9103d35622933aa422d", "message": "correctly reset path for rules without specific command\n\nThis is a fixup for commit 01c658f8c45cb92a343be5f32aa6da70b2032168\nwhere the behaviour was changed to not inherit the PATH variable\nby default.", "target": 1, "dataset": "other", "idx": 196601}
{"func": "\t    rule->options & PERSIST);\n#endif\n\n#ifdef HAVE_LOGIN_CAP_H\n\tif (setusercontext(NULL, targpw, target, LOGIN_SETGROUP |\n\t    LOGIN_SETPATH |\n\t    LOGIN_SETPRIORITY | LOGIN_SETRESOURCES | LOGIN_SETUMASK |\n\t    LOGIN_SETUSER) != 0)\n\t\terrx(1, \"failed to set user context for target\");\n#else\n\tif (setresgid(targpw->pw_gid, targpw->pw_gid, targpw->pw_gid) != 0)\n\t\terr(1, \"setresgid\");\n\tif (initgroups(targpw->pw_name, targpw->pw_gid) != 0)\n\t\terr(1, \"initgroups\");\n\tif (setresuid(target, target, target) != 0)\n\t\terr(1, \"setresuid\");\n\tif (setenv(\"PATH\", safepath, 1) == -1)\n\t\terr(1, \"failed to set PATH '%s'\", safepath);\n#endif\n\n\tif (getcwd(cwdpath, sizeof(cwdpath)) == NULL)\n\t\tcwd = \"(failed)\";\n\telse", "project": "OpenDoas", "hash": 292077285564712260600228001977188221108, "size": 191, "commit_id": "d5acd52e2a15c36a8e06f9103d35622933aa422d", "message": "correctly reset path for rules without specific command\n\nThis is a fixup for commit 01c658f8c45cb92a343be5f32aa6da70b2032168\nwhere the behaviour was changed to not inherit the PATH variable\nby default.", "target": 0, "dataset": "other", "idx": 240829}
{"func": "bool WindowsServiceControl::install( const QString& filePath, const QString& displayName  )\n{\n\tm_serviceHandle = CreateService(\n\t\t\t\tm_serviceManager,\t\t// SCManager database\n\t\t\t\tWindowsCoreFunctions::toConstWCharArray( m_name ),\t// name of service\n\t\t\t\tWindowsCoreFunctions::toConstWCharArray( displayName ),// name to display\n\t\t\t\tSERVICE_ALL_ACCESS,\t// desired access\n\t\t\t\tSERVICE_WIN32_OWN_PROCESS,\n\t\t\t\t// service type\n\t\t\t\tSERVICE_AUTO_START,\t// start type\n\t\t\t\tSERVICE_ERROR_NORMAL,\t// error control type\n\t\t\t\tWindowsCoreFunctions::toConstWCharArray( filePath ),\t\t// service's binary\n\t\t\t\tnullptr,\t\t\t// no load ordering group\n\t\t\t\tnullptr,\t\t\t// no tag identifier\n\t\t\t\tL\"Tcpip\\0RpcSs\\0\\0\",\t\t// dependencies\n\t\t\t\tnullptr,\t\t\t// LocalSystem account\n\t\t\t\tnullptr );\t\t\t// no password", "project": "veyon", "hash": 215268809291038026293397703676241296444, "size": 50, "commit_id": "f231ec511b9a09f43f49b2c7bb7c60b8046276b1", "message": "WindowsServiceControl: quote service binary path\n\nFix unquoted service path vulnerability.\n\nCloses #657.", "target": 1, "dataset": "other", "idx": 196610}
{"func": "bool WindowsServiceControl::install( const QString& filePath, const QString& displayName  )\n{\n\tconst auto binaryPath = QStringLiteral(\"\\\"%1\\\"\").arg( QString( filePath ).replace( QLatin1Char('\"'), QString() ) );\n\n\tm_serviceHandle = CreateService(\n\t\t\t\tm_serviceManager,\t\t// SCManager database\n\t\t\t\tWindowsCoreFunctions::toConstWCharArray( m_name ),\t// name of service\n\t\t\t\tWindowsCoreFunctions::toConstWCharArray( displayName ),// name to display\n\t\t\t\tSERVICE_ALL_ACCESS,\t// desired access\n\t\t\t\tSERVICE_WIN32_OWN_PROCESS,\n\t\t\t\t// service type\n\t\t\t\tSERVICE_AUTO_START,\t// start type\n\t\t\t\tSERVICE_ERROR_NORMAL,\t// error control type\n\t\t\t\tWindowsCoreFunctions::toConstWCharArray( binaryPath ),\t\t// service's binary\n\t\t\t\tnullptr,\t\t\t// no load ordering group\n\t\t\t\tnullptr,\t\t\t// no tag identifier\n\t\t\t\tL\"Tcpip\\0RpcSs\\0\\0\",\t\t// dependencies\n\t\t\t\tnullptr,\t\t\t// LocalSystem account\n\t\t\t\tnullptr );\t\t\t// no password", "project": "veyon", "hash": 54175189195648828986613730583176708145, "size": 52, "commit_id": "f231ec511b9a09f43f49b2c7bb7c60b8046276b1", "message": "WindowsServiceControl: quote service binary path\n\nFix unquoted service path vulnerability.\n\nCloses #657.", "target": 0, "dataset": "other", "idx": 241036}
{"func": "  const u_int8_t * session_remote;\n  u_int8_t opcode;\n  u_int8_t alen;\n  int8_t hmac_size;\n  int8_t failed = 0;\n\n  if(packet->payload_packet_len >= 40) {\n    // skip openvpn TCP transport packet size\n    if(packet->tcp != NULL)\n      ovpn_payload += 2;\n\n    opcode = ovpn_payload[0] & P_OPCODE_MASK;\n\n    if(packet->udp) {\n#ifdef DEBUG\n      printf(\"[packet_id: %u][opcode: %u][Packet ID: %d][%u <-> %u][len: %u]\\n\",\n\t     flow->num_processed_pkts,\n\t     opcode, check_pkid_and_detect_hmac_size(ovpn_payload),\n\t     htons(packet->udp->source), htons(packet->udp->dest), packet->payload_packet_len);\t   \n#endif\n      \n      if(\n\t (flow->num_processed_pkts == 1)\n\t && (\n\t     ((packet->payload_packet_len == 112)\n\t      && ((opcode == 168) || (opcode == 192))\n\t      )\n\t     || ((packet->payload_packet_len == 80)\n\t\t && ((opcode == 184) || (opcode == 88) || (opcode == 160) || (opcode == 168) || (opcode == 200)))\n\t     )) {\n\tNDPI_LOG_INFO(ndpi_struct,\"found openvpn\\n\");\n\tndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_OPENVPN, NDPI_PROTOCOL_UNKNOWN);\n\treturn;\n            (opcode == P_CONTROL_HARD_RESET_SERVER_V1 || opcode == P_CONTROL_HARD_RESET_SERVER_V2)) {\n\n      hmac_size = check_pkid_and_detect_hmac_size(ovpn_payload);\n\n      if(hmac_size > 0) {\n        alen = ovpn_payload[P_PACKET_ID_ARRAY_LEN_OFFSET(hmac_size)];\n        if (alen > 0) {\n\t  session_remote = ovpn_payload + P_PACKET_ID_ARRAY_LEN_OFFSET(hmac_size) + 1 + alen * 4;\n\n          if(memcmp(flow->ovpn_session_id, session_remote, 8) == 0) {\n\t    NDPI_LOG_INFO(ndpi_struct,\"found openvpn\\n\");\n\t    ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_OPENVPN, NDPI_PROTOCOL_UNKNOWN);\n\t    return;\n\t  } else {\n            NDPI_LOG_DBG2(ndpi_struct,\n\t\t   \"key mismatch: %02x%02x%02x%02x%02x%02x%02x%02x\\n\",\n\t\t   session_remote[0], session_remote[1], session_remote[2], session_remote[3],\n\t\t   session_remote[4], session_remote[5], session_remote[6], session_remote[7]);\n            failed = 1;\n          }\n        } else\n          failed = 1;\n      } else\n        failed = 1;\n    } else\n      failed = 1;", "project": "nDPI", "hash": 263464745339090965084831362920390201622, "size": 85, "commit_id": "8e7b1ea7a136cc4e4aa9880072ec2d69900a825e", "message": "Fix for potential heap-buffer-overflow in ndpi_search_openvpn", "target": 1, "dataset": "other", "idx": 196624}
{"func": "  const u_int8_t * session_remote;\n  u_int8_t opcode;\n  u_int8_t alen;\n  int8_t hmac_size;\n  int8_t failed = 0;\n  /* No u_ */int16_t ovpn_payload_len = packet->payload_packet_len;\n  \n  if(ovpn_payload_len >= 40) {\n    // skip openvpn TCP transport packet size\n    if(packet->tcp != NULL)\n      ovpn_payload += 2, ovpn_payload_len -= 2;;\n\n    opcode = ovpn_payload[0] & P_OPCODE_MASK;\n\n    if(packet->udp) {\n#ifdef DEBUG\n      printf(\"[packet_id: %u][opcode: %u][Packet ID: %d][%u <-> %u][len: %u]\\n\",\n\t     flow->num_processed_pkts,\n\t     opcode, check_pkid_and_detect_hmac_size(ovpn_payload),\n\t     htons(packet->udp->source), htons(packet->udp->dest), ovpn_payload_len);\t   \n#endif\n      \n      if(\n\t (flow->num_processed_pkts == 1)\n\t && (\n\t     ((ovpn_payload_len == 112)\n\t      && ((opcode == 168) || (opcode == 192))\n\t      )\n\t     || ((ovpn_payload_len == 80)\n\t\t && ((opcode == 184) || (opcode == 88) || (opcode == 160) || (opcode == 168) || (opcode == 200)))\n\t     )) {\n\tNDPI_LOG_INFO(ndpi_struct,\"found openvpn\\n\");\n\tndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_OPENVPN, NDPI_PROTOCOL_UNKNOWN);\n\treturn;\n            (opcode == P_CONTROL_HARD_RESET_SERVER_V1 || opcode == P_CONTROL_HARD_RESET_SERVER_V2)) {\n\n      hmac_size = check_pkid_and_detect_hmac_size(ovpn_payload);\n\n      if(hmac_size > 0) {\n\tu_int16_t offset = P_PACKET_ID_ARRAY_LEN_OFFSET(hmac_size);\n\t  \n        alen = ovpn_payload[offset];\n\t\n        if (alen > 0) {\n\t  offset += 1 + alen * 4;\n\n\t  if((offset+8) <= ovpn_payload_len) {\n\t    session_remote = &ovpn_payload[offset];\n\t    \n\t    if(memcmp(flow->ovpn_session_id, session_remote, 8) == 0) {\n\t      NDPI_LOG_INFO(ndpi_struct,\"found openvpn\\n\");\n\t      ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_OPENVPN, NDPI_PROTOCOL_UNKNOWN);\n\t      return;\n\t    } else {\n\t      NDPI_LOG_DBG2(ndpi_struct,\n\t\t\t    \"key mismatch: %02x%02x%02x%02x%02x%02x%02x%02x\\n\",\n\t\t\t    session_remote[0], session_remote[1], session_remote[2], session_remote[3],\n\t\t\t    session_remote[4], session_remote[5], session_remote[6], session_remote[7]);\n\t      failed = 1;\n\t    }\n\t  } else\n\t    failed = 1;\n\t} else\n          failed = 1;\n      } else\n        failed = 1;\n    } else\n      failed = 1;", "project": "nDPI", "hash": 292660118622334727722308632088083734812, "size": 94, "commit_id": "8e7b1ea7a136cc4e4aa9880072ec2d69900a825e", "message": "Fix for potential heap-buffer-overflow in ndpi_search_openvpn", "target": 0, "dataset": "other", "idx": 241321}
{"func": "    // TODO(andydavis) Explore alternatives to branching the code in this way\n    // (i.e. run multiple, parallel tensor contractions in another thread pool).\n    const bool use_parallel_contraction =\n        dims.batch_size == 1 ||\n        thread_work_unit_size >= min_thread_work_unit_size;\n\n    const size_t shard_size =\n        use_parallel_contraction\n            ? 1\n            : (target_working_set_size + work_unit_size - 1) / work_unit_size;\n", "project": "tensorflow", "hash": 221795662216558573434423513228112279102, "size": 236, "commit_id": "2be2cdf3a123e231b16f766aa0e27d56b4606535", "message": "Prevent yet another division by zero\n\nPiperOrigin-RevId: 369343977\nChange-Id: I1a60da4cf512e60fd91e069c16e026544632fe7f", "target": 1, "dataset": "other", "idx": 196632}
{"func": "    // (i.e. run multiple, parallel tensor contractions in another thread pool).\n    const bool use_parallel_contraction =\n        dims.batch_size == 1 ||\n        thread_work_unit_size >= min_thread_work_unit_size;\n\n    OP_REQUIRES(\n        context, work_unit_size > 0,\n        errors::InvalidArgument(\"input, filter_sizes and out_backprop tensors \"\n                                \"must all have at least 1 element\"));\n\n    const size_t shard_size =\n        use_parallel_contraction\n            ? 1\n            : (target_working_set_size + work_unit_size - 1) / work_unit_size;\n", "project": "tensorflow", "hash": 165188925159723349541537214047020092751, "size": 241, "commit_id": "2be2cdf3a123e231b16f766aa0e27d56b4606535", "message": "Prevent yet another division by zero\n\nPiperOrigin-RevId: 369343977\nChange-Id: I1a60da4cf512e60fd91e069c16e026544632fe7f", "target": 0, "dataset": "other", "idx": 241532}
{"func": "\tstatic char *escaped;\n\tstatic size_t escaped_size;\n\tchar *out;\n\tsize_t len;\n\n\tif (!strlen(text)) return \"empty string\";\n\n\tfor (out=escaped, len=0; *text; ++len, ++out, ++text) {\n\t\t/* Make sure there's plenty of room for a quoted character */\n\t\tif ((len + 8) > escaped_size) {\n\t\t\tchar *bigger_escaped;", "project": "exif", "hash": 130225628845924531529156533690677997225, "size": 49, "commit_id": "eb84b0e3c5f2a86013b6fcfb800d187896a648fa", "message": "actually return empty stringand not 'em,pty string' as expected", "target": 1, "dataset": "other", "idx": 196672}
{"func": "\tstatic char *escaped;\n\tstatic size_t escaped_size;\n\tchar *out;\n\tsize_t len;\n\n\tif (!strlen(text)) return \"\";\n\n\tfor (out=escaped, len=0; *text; ++len, ++out, ++text) {\n\t\t/* Make sure there's plenty of room for a quoted character */\n\t\tif ((len + 8) > escaped_size) {\n\t\t\tchar *bigger_escaped;", "project": "exif", "hash": 264716872538103587635258225337770794121, "size": 49, "commit_id": "eb84b0e3c5f2a86013b6fcfb800d187896a648fa", "message": "actually return empty stringand not 'em,pty string' as expected", "target": 0, "dataset": "other", "idx": 242168}
{"func": "  void Compute(OpKernelContext* context) override {\n    const float in_min = context->input(2).flat<float>()(0);\n    const float in_max = context->input(3).flat<float>()(0);\n\n    ImageResizerState st(align_corners_, false);\n    st.ValidateAndCreateOutput(context);\n\n    if (!context->status().ok()) return;", "project": "tensorflow", "hash": 327227610424235160739799447249420118357, "size": 26, "commit_id": "f6c40f0c6cbf00d46c7717a26419f2062f2f8694", "message": "Validate min and max arguments to `QuantizedResizeBilinear`.\n\nPiperOrigin-RevId: 369765091\nChange-Id: I33be8b78273ab7d08b97541692fe05cb7f94963a", "target": 1, "dataset": "other", "idx": 196673}
{"func": "  void Compute(OpKernelContext* context) override {\n    const auto& in_min_tensor = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_min_tensor.shape()),\n                errors::InvalidArgument(\"min must be a scalar\"));\n    const float in_min = in_min_tensor.flat<float>()(0);\n    const auto& in_max_tensor = context->input(3);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(in_max_tensor.shape()),\n                errors::InvalidArgument(\"max must be a scalar\"));\n    const float in_max = in_max_tensor.flat<float>()(0);\n\n    ImageResizerState st(align_corners_, false);\n    st.ValidateAndCreateOutput(context);\n\n    if (!context->status().ok()) return;", "project": "tensorflow", "hash": 230635596964007868338231930811957784418, "size": 32, "commit_id": "f6c40f0c6cbf00d46c7717a26419f2062f2f8694", "message": "Validate min and max arguments to `QuantizedResizeBilinear`.\n\nPiperOrigin-RevId: 369765091\nChange-Id: I33be8b78273ab7d08b97541692fe05cb7f94963a", "target": 0, "dataset": "other", "idx": 242186}
{"func": "\t\tu32 w, h;\n\t\tgf_isom_get_visual_info(file, trackNum, 1, &w, &h);\n\t\tfprintf(stderr, \"\\tAOM AV1 stream - Resolution %d x %d\\n\", w, h);\n\n\t\tav1c = gf_isom_av1_config_get(file, trackNum, 1);\n\t\tfprintf(stderr, \"\\tversion=%u, profile=%u, level_idx0=%u, tier=%u\\n\", (u32)av1c->version, (u32)av1c->seq_profile, (u32)av1c->seq_level_idx_0, (u32)av1c->seq_tier_0);\n\t\tfprintf(stderr, \"\\thigh_bitdepth=%u, twelve_bit=%u, monochrome=%u\\n\", (u32)av1c->high_bitdepth, (u32)av1c->twelve_bit, (u32)av1c->monochrome);\n\t\tfprintf(stderr, \"\\tchroma: subsampling_x=%u, subsampling_y=%u, sample_position=%u\\n\", (u32)av1c->chroma_subsampling_x, (u32)av1c->chroma_subsampling_y, (u32)av1c->chroma_sample_position);\n\n\t\tif (av1c->initial_presentation_delay_present)\n\t\t\tfprintf(stderr, \"\\tInitial presentation delay %u\\n\", (u32) av1c->initial_presentation_delay_minus_one+1);\n\n\t\tcount = gf_list_count(av1c->obu_array);\n\t\tfor (i=0; i<count; i++) {\n\t\t\tu8 hash[20];\n\t\t\tGF_AV1_OBUArrayEntry *obu = gf_list_get(av1c->obu_array, i);\n\t\t\tgf_sha1_csum((u8*)obu->obu, (u32)obu->obu_length, hash);\n\t\t\tfprintf(stderr, \"\\tOBU#%d %s hash: \", i+1, gf_av1_get_obu_name(obu->obu_type) );\n\t\t\tfor (j=0; j<20; j++) fprintf(stderr, \"%02X\", hash[j]);\n\t\t\tfprintf(stderr, \"\\n\");\n\t\t}\n\t\tgf_odf_av1_cfg_del(av1c);\n\t} else if (msub_type == GF_ISOM_SUBTYPE_3GP_H263) {\n\t\tu32 w, h;\n\t\tgf_isom_get_visual_info(file, trackNum, 1, &w, &h);\n\t\tfprintf(stderr, \"\\t3GPP H263 stream - Resolution %d x %d\\n\", w, h);\n\t} else if (msub_type == GF_ISOM_SUBTYPE_MJP2) {", "project": "gpac", "hash": 106085528695165772265170925572687749931, "size": 1036, "commit_id": "289ffce3e0d224d314f5f92a744d5fe35999f20b", "message": "fixed #1767 (fuzz)", "target": 1, "dataset": "other", "idx": 196719}
{"func": "\t\tu32 w, h;\n\t\tgf_isom_get_visual_info(file, trackNum, 1, &w, &h);\n\t\tfprintf(stderr, \"\\tAOM AV1 stream - Resolution %d x %d\\n\", w, h);\n\n\t\tav1c = gf_isom_av1_config_get(file, trackNum, 1);\n\t\tif (!av1c) {\n\t\t\tfprintf(stderr, \"\\tCorrupted av1 config\\n\");\n\t\t} else {\n\t\t\tfprintf(stderr, \"\\tversion=%u, profile=%u, level_idx0=%u, tier=%u\\n\", (u32)av1c->version, (u32)av1c->seq_profile, (u32)av1c->seq_level_idx_0, (u32)av1c->seq_tier_0);\n\t\t\tfprintf(stderr, \"\\thigh_bitdepth=%u, twelve_bit=%u, monochrome=%u\\n\", (u32)av1c->high_bitdepth, (u32)av1c->twelve_bit, (u32)av1c->monochrome);\n\t\t\tfprintf(stderr, \"\\tchroma: subsampling_x=%u, subsampling_y=%u, sample_position=%u\\n\", (u32)av1c->chroma_subsampling_x, (u32)av1c->chroma_subsampling_y, (u32)av1c->chroma_sample_position);\n\n\t\t\tif (av1c->initial_presentation_delay_present)\n\t\t\t\tfprintf(stderr, \"\\tInitial presentation delay %u\\n\", (u32) av1c->initial_presentation_delay_minus_one+1);\n\n\t\t\tcount = gf_list_count(av1c->obu_array);\n\t\t\tfor (i=0; i<count; i++) {\n\t\t\t\tu8 hash[20];\n\t\t\t\tGF_AV1_OBUArrayEntry *obu = gf_list_get(av1c->obu_array, i);\n\t\t\t\tgf_sha1_csum((u8*)obu->obu, (u32)obu->obu_length, hash);\n\t\t\t\tfprintf(stderr, \"\\tOBU#%d %s hash: \", i+1, gf_av1_get_obu_name(obu->obu_type) );\n\t\t\t\tfor (j=0; j<20; j++) fprintf(stderr, \"%02X\", hash[j]);\n\t\t\t\tfprintf(stderr, \"\\n\");\n\t\t\t}\n\t\t\tgf_odf_av1_cfg_del(av1c);\n\t\t}\n\t} else if (msub_type == GF_ISOM_SUBTYPE_3GP_H263) {\n\t\tu32 w, h;\n\t\tgf_isom_get_visual_info(file, trackNum, 1, &w, &h);\n\t\tfprintf(stderr, \"\\t3GPP H263 stream - Resolution %d x %d\\n\", w, h);\n\t} else if (msub_type == GF_ISOM_SUBTYPE_MJP2) {", "project": "gpac", "hash": 4539415702861642474065400031381230619, "size": 1040, "commit_id": "289ffce3e0d224d314f5f92a744d5fe35999f20b", "message": "fixed #1767 (fuzz)", "target": 0, "dataset": "other", "idx": 243213}
{"func": "    // If the data is already in the host's byte order, or if the width of the\n    // output type is a single byte (meaning the ordering doesn't matter), we\n    // can copy the memory directly.\n    if (!convert_data_endianness_ || sizeof(T) == 1) {\n      for (int64 i = 0; i < flat_in.size(); ++i) {\n        const T* in_data = reinterpret_cast<const T*>(flat_in(i).data());\n\n        if (flat_in(i).size() > fixed_length) {\n          memcpy(out_data, in_data, fixed_length);\n        } else {\n          memcpy(out_data, in_data, flat_in(i).size());\n        }\n        out_data += fixed_length;\n      }\n    } else {\n      // Otherwise, the data is not in the host's byte order, and rather than a\n      // direct copy, we need to reverse the byte ordering of each element.\n      for (int64 i = 0; i < flat_in.size(); ++i) {\n        char* p_out = out_data_bytes;\n        for (; p_in < in_data_bytes + fixed_length;\n             p_in += sizeof(T), p_out += sizeof(T)) {\n          std::reverse_copy(p_in, p_in + sizeof(T), p_out);\n        }\n        out_data += fixed_length;\n      }\n    }\n  }", "project": "tensorflow", "hash": 146552813611946796478012198860441040266, "size": 72, "commit_id": "698e01511f62a3c185754db78ebce0eee1f0184d", "message": "Fix `tf.io.decode_raw` bugs and update documentation.\n\nFixes cases where specifying `fixed_length` resulted in data loss and even segfault and corruption of the Python interpreter. The fix is subtle but needed due to pointer arithmetic rules.\n\nMakes sure that `fixed_length` does not change the output when present but not needed.\n\nEliminates needless copy and cast in the main codepath.\n\nPiperOrigin-RevId: 371322725\nChange-Id: I514ef67a2961c86422f69d05122d31615e87896c", "target": 1, "dataset": "other", "idx": 196739}
{"func": "    // If the data is already in the host's byte order, or if the width of the\n    // output type is a single byte (meaning the ordering doesn't matter), we\n    // can copy the memory directly.\n    if (!convert_data_endianness_ || sizeof(T) == 1) {\n      for (int64 i = 0; i < flat_in.size(); ++i) {\n        const auto to_copy =\n            std::min(flat_in(i).size(), static_cast<size_t>(fixed_length));\n        memcpy(out_data, flat_in(i).data(), to_copy);\n        // Note: increase out_data by width since it's already of type T* so\n        // each shift amount is implicitly multiplied by sizeof(T) according to\n        // pointer arithmetic rules.\n        out_data += width;\n      }\n    } else {\n      // Otherwise, the data is not in the host's byte order, and rather than a\n      // direct copy, we need to reverse the byte ordering of each element.\n      for (int64 i = 0; i < flat_in.size(); ++i) {\n        char* p_out = out_data_bytes;\n        for (; p_in < in_data_bytes + fixed_length;\n             p_in += sizeof(T), p_out += sizeof(T)) {\n          std::reverse_copy(p_in, p_in + sizeof(T), p_out);\n        }\n        // Note: increase out_data by width since it's already of type T* so\n        // each shift amount is implicitly multiplied by sizeof(T) according to\n        // pointer arithmetic rules.\n        out_data += width;\n      }\n    }\n  }", "project": "tensorflow", "hash": 126932864429790550515712824205640725368, "size": 74, "commit_id": "698e01511f62a3c185754db78ebce0eee1f0184d", "message": "Fix `tf.io.decode_raw` bugs and update documentation.\n\nFixes cases where specifying `fixed_length` resulted in data loss and even segfault and corruption of the Python interpreter. The fix is subtle but needed due to pointer arithmetic rules.\n\nMakes sure that `fixed_length` does not change the output when present but not needed.\n\nEliminates needless copy and cast in the main codepath.\n\nPiperOrigin-RevId: 371322725\nChange-Id: I514ef67a2961c86422f69d05122d31615e87896c", "target": 0, "dataset": "other", "idx": 243619}
{"func": "\t\t}\n\t\tsc_log(ctx, \n\t\t\t\"Searching for PIN-Ref %02X\\n\", pin_reference);\n\t\twhile ((r = sc_read_record(card, ++rec_no, buf, sizeof(buf), SC_RECORD_BY_REC_NR)) > 0) {\n\t\t\tint found = 0, fbz = -1;\n\t\t\tif (buf[0] != 0xA0)\n\t\t\t\tcontinue;\n\t\t\tfor (i = 2; i < buf[1] + 2; i += 2 + buf[i + 1]) {\n\t\t\t\tif (buf[i] == 0x83 && buf[i + 1] == 1 && buf[i + 2] == pin_reference) {\n\t\t\t\t\t++found;\n\t\t\t\t}\n\t\t\t\tif (buf[i] == 0x90) {\n\t\t\t\t\tfbz = buf[i + 1 + buf[i + 1]];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (found) {\n\t\t\t\tpin_info.tries_left = fbz;", "project": "OpenSC", "hash": 106837843557221778698667602114311306154, "size": 92, "commit_id": "5df913b7f57ad89b9832555d24c08d23a534311e", "message": "tcos: Check bounds in insert_pin()\n\nThanks oss-fuzz\n\nhttps://bugs.chromium.org/p/oss-fuzz/issues/detail?id=28383", "target": 1, "dataset": "other", "idx": 196754}
{"func": "\t\t}\n\t\tsc_log(ctx, \n\t\t\t\"Searching for PIN-Ref %02X\\n\", pin_reference);\n\t\twhile ((r = sc_read_record(card, ++rec_no, buf, sizeof(buf), SC_RECORD_BY_REC_NR)) > 0) {\n\t\t\tint found = 0, fbz = -1;\n\t\t\tif (r < 2 || buf[0] != 0xA0)\n\t\t\t\tcontinue;\n\t\t\tfor (i = 2; i < buf[1] + 2 && (i + 2) < r; i += 2 + buf[i + 1]) {\n\t\t\t\tif (buf[i] == 0x83 && buf[i + 1] == 1 && buf[i + 2] == pin_reference) {\n\t\t\t\t\t++found;\n\t\t\t\t}\n\t\t\t\tif (buf[i] == 0x90 && (i + 1 + buf[i + 1]) < r) {\n\t\t\t\t\tfbz = buf[i + 1 + buf[i + 1]];\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (found) {\n\t\t\t\tpin_info.tries_left = fbz;", "project": "OpenSC", "hash": 53450176173982244017827403871002445488, "size": 92, "commit_id": "5df913b7f57ad89b9832555d24c08d23a534311e", "message": "tcos: Check bounds in insert_pin()\n\nThanks oss-fuzz\n\nhttps://bugs.chromium.org/p/oss-fuzz/issues/detail?id=28383", "target": 0, "dataset": "other", "idx": 243958}
{"func": "    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(dims_tensor.shape()),\n        errors::InvalidArgument(\"The indices can only be 1-D, got \\\"\",\n                                dims_tensor.shape().DebugString(), \"\\\"\"));\n\n    auto dims = dims_tensor.vec<Tidx>();\n\n    // Chek to make sure indices is not out of boundary\n    Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n    Tidx dims_prod = dims_prod_eigen();\n    const Tidx* indices = indices_tensor.flat<Tidx>().data();", "project": "tensorflow", "hash": 264158770569740569963092041338700970996, "size": 87, "commit_id": "a776040a5e7ebf76eeb7eb923bf1ae417dd4d233", "message": "Disallow dims input of 0 in tf.raw_ops.UnravelIndex\n\nPiperOrigin-RevId: 384284198\nChange-Id: Ia1804ef1aec57b4d857ea507e6891bcccde18e9b", "target": 1, "dataset": "other", "idx": 196763}
{"func": "        ctx, TensorShapeUtils::IsVector(dims_tensor.shape()),\n        errors::InvalidArgument(\"The indices can only be 1-D, got \\\"\",\n                                dims_tensor.shape().DebugString(), \"\\\"\"));\n\n    auto dims = dims_tensor.vec<Tidx>();\n    // Make sure dims does not contain a zero\n    for (int i = 0; i < dims.size(); i++) {\n      OP_REQUIRES(\n          ctx, dims(i) != 0,\n          errors::InvalidArgument(\"Input dims cannot contain a dim of zero, \"\n                                  \"but dims contains zero at index \",\n                                  i));\n    }\n\n    // Chek to make sure indices is not out of boundary\n    Eigen::Tensor<Tidx, 0, Eigen::RowMajor> dims_prod_eigen = dims.prod();\n    Tidx dims_prod = dims_prod_eigen();\n    const Tidx* indices = indices_tensor.flat<Tidx>().data();", "project": "tensorflow", "hash": 190979480133558515936234630943195362228, "size": 95, "commit_id": "a776040a5e7ebf76eeb7eb923bf1ae417dd4d233", "message": "Disallow dims input of 0 in tf.raw_ops.UnravelIndex\n\nPiperOrigin-RevId: 384284198\nChange-Id: Ia1804ef1aec57b4d857ea507e6891bcccde18e9b", "target": 0, "dataset": "other", "idx": 243980}
{"func": "\t\tsprintf(nhml, \"sampleRate=\\\"%d\\\" numChannels=\\\"%d\\\" \", ctx->sr, ctx->chan);\n\t\tgf_bs_write_data(ctx->bs_w, nhml, (u32) strlen(nhml));\n\t\tsprintf(nhml, \"sampleRate=\\\"%d\\\" numChannels=\\\"%d\\\" \", ctx->sr, ctx->chan);\n\t\tgf_bs_write_data(ctx->bs_w, nhml, (u32) strlen(nhml));\n\t\tp = gf_filter_pid_get_property(ctx->ipid, GF_PROP_PID_AUDIO_FORMAT);\n\t\tsprintf(nhml, \"bitsPerSample=\\\"%d\\\" \", gf_audio_fmt_bit_depth(p->value.uint));\n\t\tgf_bs_write_data(ctx->bs_w, nhml, (u32) strlen(nhml));\n\t}\n\n\tNHML_PRINT_4CC(0, \"codec_vendor\", \"codecVendor\")\n\tNHML_PRINT_UINT(0, \"codec_version\", \"codecVersion\")", "project": "gpac", "hash": 171851966591827362626279278095562125748, "size": 154, "commit_id": "9eeac00b38348c664dfeae2525bba0cf1bc32349", "message": "fixed #1565", "target": 1, "dataset": "other", "idx": 196766}
{"func": "\t\tsprintf(nhml, \"sampleRate=\\\"%d\\\" numChannels=\\\"%d\\\" \", ctx->sr, ctx->chan);\n\t\tgf_bs_write_data(ctx->bs_w, nhml, (u32) strlen(nhml));\n\t\tsprintf(nhml, \"sampleRate=\\\"%d\\\" numChannels=\\\"%d\\\" \", ctx->sr, ctx->chan);\n\t\tgf_bs_write_data(ctx->bs_w, nhml, (u32) strlen(nhml));\n\t\tp = gf_filter_pid_get_property(ctx->ipid, GF_PROP_PID_AUDIO_FORMAT);\n\t\tif (p)\n\t\t\tsprintf(nhml, \"bitsPerSample=\\\"%d\\\" \", gf_audio_fmt_bit_depth(p->value.uint));\n\t\tgf_bs_write_data(ctx->bs_w, nhml, (u32) strlen(nhml));\n\t}\n\n\tNHML_PRINT_4CC(0, \"codec_vendor\", \"codecVendor\")\n\tNHML_PRINT_UINT(0, \"codec_version\", \"codecVersion\")", "project": "gpac", "hash": 192488898754254972476380860269472215022, "size": 155, "commit_id": "9eeac00b38348c664dfeae2525bba0cf1bc32349", "message": "fixed #1565", "target": 0, "dataset": "other", "idx": 244385}
{"func": "        avio_seek(pb, q, SEEK_CUR); // data_2\n        avio_r8(pb); // zeropad\n\n        if (avio_tell(pb) < off) {\n            int num_data;\n            int xd_size = 0;\n            int data_len[256];\n            int offset = 1;\n            uint8_t *p;\n            ffio_read_varlen(pb); // val_13\n            avio_r8(pb); // '19'\n                uint64_t len = ffio_read_varlen(pb);\n                if (len > INT_MAX/2 - xd_size) {\n                    return AVERROR_INVALIDDATA;\n                }\n                data_len[j] = len;\n                xd_size += len;\n            }\n\n            ret = ff_alloc_extradata(st->codecpar, 64 + xd_size + xd_size / 255);\n            if (ret < 0)\n                return ret;\n\n            p = st->codecpar->extradata;\n            p[0] = 2;\n\n            for (j = 0; j < num_data - 1; j++) {\n                unsigned delta = av_xiphlacing(&p[offset], data_len[j]);\n                if (delta > data_len[j]) {\n                    return AVERROR_INVALIDDATA;\n                }\n                offset += delta;\n            }\n\n            for (j = 0; j < num_data; j++) {\n                int ret = avio_read(pb, &p[offset], data_len[j]);\n                if (ret < data_len[j]) {\n                    st->codecpar->extradata_size = 0;\n                    av_freep(&st->codecpar->extradata);\n                    break;\n                }\n                offset += data_len[j];\n            }\n\n            if (offset < st->codecpar->extradata_size)\n                st->codecpar->extradata_size = offset;", "project": "FFmpeg", "hash": 206145715012219860741644177051309694139, "size": 152, "commit_id": "27a99e2c7d450fef15594671eef4465c8a166bd7", "message": "avformat/vividas: improve extradata packing checks in track_header()\n\nFixes: out of array accesses\nFixes: 26622/clusterfuzz-testcase-minimized-ffmpeg_dem_VIVIDAS_fuzzer-6581200338288640\n\nFound-by: continuous fuzzing process https://github.com/google/oss-fuzz/tree/master/projects/ffmpeg\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>", "target": 1, "dataset": "other", "idx": 196768}
{"func": "        avio_seek(pb, q, SEEK_CUR); // data_2\n        avio_r8(pb); // zeropad\n\n        if (avio_tell(pb) < off) {\n            int num_data;\n            int xd_size = 1;\n            int data_len[256];\n            int offset = 1;\n            uint8_t *p;\n            ffio_read_varlen(pb); // val_13\n            avio_r8(pb); // '19'\n                uint64_t len = ffio_read_varlen(pb);\n                if (len > INT_MAX/2 - xd_size) {\n                    return AVERROR_INVALIDDATA;\n                }\n                data_len[j] = len;\n                xd_size += len + 1 + len/255;\n            }\n\n            ret = ff_alloc_extradata(st->codecpar, xd_size);\n            if (ret < 0)\n                return ret;\n\n            p = st->codecpar->extradata;\n            p[0] = 2;\n\n            for (j = 0; j < num_data - 1; j++) {\n                unsigned delta = av_xiphlacing(&p[offset], data_len[j]);\n                av_assert0(delta <= xd_size - offset);\n                offset += delta;\n            }\n\n            for (j = 0; j < num_data; j++) {\n                int ret = avio_read(pb, &p[offset], data_len[j]);\n                if (ret < data_len[j]) {\n                    st->codecpar->extradata_size = 0;\n                    av_freep(&st->codecpar->extradata);\n                    break;\n                }\n                av_assert0(data_len[j] <= xd_size - offset);\n                offset += data_len[j];\n            }\n\n            if (offset < st->codecpar->extradata_size)\n                st->codecpar->extradata_size = offset;", "project": "FFmpeg", "hash": 221246914153953907748437805686154036081, "size": 151, "commit_id": "27a99e2c7d450fef15594671eef4465c8a166bd7", "message": "avformat/vividas: improve extradata packing checks in track_header()\n\nFixes: out of array accesses\nFixes: 26622/clusterfuzz-testcase-minimized-ffmpeg_dem_VIVIDAS_fuzzer-6581200338288640\n\nFound-by: continuous fuzzing process https://github.com/google/oss-fuzz/tree/master/projects/ffmpeg\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>", "target": 0, "dataset": "other", "idx": 244426}
{"func": "static int process_base_block(struct archive_read* a,\n    struct archive_entry* entry)\n{\n\tstruct rar5* rar = get_context(a);\n\tuint32_t hdr_crc, computed_crc;\n\tsize_t raw_hdr_size = 0, hdr_size_len, hdr_size;\n\tsize_t header_id = 0;\n\tsize_t header_flags = 0;\n\n\t/* Read header size. */\n\tif(!read_var_sized(a, &raw_hdr_size, &hdr_size_len)) {\n\t\treturn ARCHIVE_EOF;\n\t}\n\n\t/* Sanity check, maximum header size for RAR5 is 2MB. */\n\tif(raw_hdr_size > (2 * 1024 * 1024)) {\n\t\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_FILE_FORMAT,\n\t\t    \"Base block header is too large\");\n\n\t\treturn ARCHIVE_FATAL;\n\t}\n\n\thdr_size = raw_hdr_size + hdr_size_len;\n\n\t/* Read the whole header data into memory, maximum memory use here is\n\t * 2MB. */\n\tif(!read_ahead(a, hdr_size, &p)) {\n\t\treturn ARCHIVE_EOF;", "project": "libarchive", "hash": 100291370177792498743783572399624603579, "size": 150, "commit_id": "94821008d6eea81e315c5881cdf739202961040a", "message": "RAR5 reader: reject files that declare invalid header flags\n\nOne of the fields in RAR5's base block structure is the size of the\nheader. Some invalid files declare a 0 header size setting, which can\nconfuse the unpacker. Minimum header size for RAR5 base blocks is 7\nbytes (4 bytes for CRC, and 3 bytes for the rest), so block size of 0\nbytes should be rejected at header parsing stage.\n\nThe fix adds an error condition if header size of 0 bytes is detected.\nIn this case, the unpacker will not attempt to unpack the file, as the\nheader is corrupted.\n\nThe commit also adds OSSFuzz #20459 sample to test further regressions\nin this area.", "target": 1, "dataset": "other", "idx": 196781}
{"func": "static int process_base_block(struct archive_read* a,\n    struct archive_entry* entry)\n{\n\tconst size_t SMALLEST_RAR5_BLOCK_SIZE = 3;\n\n\tstruct rar5* rar = get_context(a);\n\tuint32_t hdr_crc, computed_crc;\n\tsize_t raw_hdr_size = 0, hdr_size_len, hdr_size;\n\tsize_t header_id = 0;\n\tsize_t header_flags = 0;\n\t/* Read header size. */\n\tif(!read_var_sized(a, &raw_hdr_size, &hdr_size_len)) {\n\t\treturn ARCHIVE_EOF;\n\t}\n\n\thdr_size = raw_hdr_size + hdr_size_len;\n\n\t/* Sanity check, maximum header size for RAR5 is 2MB. */\n\tif(hdr_size > (2 * 1024 * 1024)) {\n\t\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_FILE_FORMAT,\n\t\t    \"Base block header is too large\");\n\n\t\treturn ARCHIVE_FATAL;\n\t}\n\n\t/* Additional sanity checks to weed out invalid files. */\n\tif(raw_hdr_size == 0 || hdr_size_len == 0 ||\n\t\thdr_size < SMALLEST_RAR5_BLOCK_SIZE)\n\t{\n\t\tarchive_set_error(&a->archive, ARCHIVE_ERRNO_FILE_FORMAT,\n\t\t    \"Too small block encountered (%ld bytes)\",\n\t\t    raw_hdr_size);\n\n\t\treturn ARCHIVE_FATAL;\n\t}\n\n\t/* Read the whole header data into memory, maximum memory use here is\n\t * 2MB. */\n\tif(!read_ahead(a, hdr_size, &p)) {\n\t\treturn ARCHIVE_EOF;", "project": "libarchive", "hash": 7503224727997862368510784011625765002, "size": 163, "commit_id": "94821008d6eea81e315c5881cdf739202961040a", "message": "RAR5 reader: reject files that declare invalid header flags\n\nOne of the fields in RAR5's base block structure is the size of the\nheader. Some invalid files declare a 0 header size setting, which can\nconfuse the unpacker. Minimum header size for RAR5 base blocks is 7\nbytes (4 bytes for CRC, and 3 bytes for the rest), so block size of 0\nbytes should be rejected at header parsing stage.\n\nThe fix adds an error condition if header size of 0 bytes is detected.\nIn this case, the unpacker will not attempt to unpack the file, as the\nheader is corrupted.\n\nThe commit also adds OSSFuzz #20459 sample to test further regressions\nin this area.", "target": 0, "dataset": "other", "idx": 244730}
{"func": "    // Get inputs\n    const Tensor& input_tensor = context->input(0);\n    const auto input_tensor_flat = input_tensor.flat<int32>();\n    const Tensor& input_splits = context->input(1);\n    const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n\n    // Since we limit to a 2-D input (flat_values of rank 1 and a single splits\n    // tensor), our output dimension will be 1 with it's size equal to the\n    // number of splits (outer dimension or ragged tensor).\n    TensorShape output_shape({input_splits.dim_size(0) - 1});\n    Tensor* output_tensor;\n    // Use a single index over the flattened input values tensor.\n    int idx = 0;\n    // Loop through our split dimension to create a new string at each split.\n    for (int i = 1; i < input_splits_flat.size(); ++i) {\n      icu::UnicodeString unicode_string;\n      icu::UnicodeStringAppendable appendable_unicode_string(unicode_string);\n      for (; idx < input_splits_flat(i); ++idx) {\n        int32 code_point = input_tensor_flat(idx);\n        // Check for invalid code point\n        if (!U_IS_UNICODE_CHAR(code_point)) {\n          if (error_options_.error_on_malformatting) {", "project": "tensorflow", "hash": 446394916603269042647737762256325608, "size": 42, "commit_id": "51300ba1cc2f487aefec6e6631fef03b0e08b298", "message": "Fix heap buffer overflow in tf.raw_ops.UnicodeEncode.\n\nPiperOrigin-RevId: 371717714\nChange-Id: If33443b28f158e58078f1268f6b92f2728d219e0", "target": 1, "dataset": "other", "idx": 196791}
{"func": "    const Tensor& input_tensor = context->input(0);\n    const auto input_tensor_flat = input_tensor.flat<int32>();\n    const Tensor& input_splits = context->input(1);\n    const auto input_splits_flat = input_splits.flat<SPLITS_TYPE>();\n\n    // Operation will treat first argument in input_splits as if it were zero\n    // regardless of its actual value since splits should begin with zero and\n    // end with the length of the input values vector.\n    OP_REQUIRES(\n        context, input_splits_flat(0) == 0,\n        errors::InvalidArgument(\"First value in input_splits must be zero.\"));\n    OP_REQUIRES(context,\n                input_splits_flat(input_splits_flat.size() - 1) ==\n                    input_tensor_flat.size(),\n                errors::InvalidArgument(\"Last value in input_splits must be \"\n                                        \"equal to length of input_tensor.\"));\n    // Since we limit to a 2-D input (flat_values of rank 1 and a single splits\n    // tensor), our output dimension will be 1 with it's size equal to the\n    // number of splits (outer dimension or ragged tensor).\n    TensorShape output_shape({input_splits.dim_size(0) - 1});\n    Tensor* output_tensor;\n    int idx = 0;\n    // Loop through our split dimension to create a new string at each split.\n    for (int i = 1; i < input_splits_flat.size(); ++i) {\n      icu::UnicodeString unicode_string;\n      icu::UnicodeStringAppendable appendable_unicode_string(unicode_string);\n      OP_REQUIRES(\n          context, input_splits_flat(i - 1) <= input_splits_flat(i),\n          errors::InvalidArgument(\n              \"Values in input_splits must be equal or in ascending order.\"));\n      OP_REQUIRES(\n          context, input_splits_flat(i) <= input_tensor_flat.size(),\n          errors::InvalidArgument(\"Values in input_splits must be less than or \"\n                                  \"equal to input_tensor length.\"));\n      for (; idx < input_splits_flat(i); ++idx) {\n        int32 code_point = input_tensor_flat(idx);\n        // Check for invalid code point\n        if (!U_IS_UNICODE_CHAR(code_point)) {\n          if (error_options_.error_on_malformatting) {", "project": "tensorflow", "hash": 337772719006525180924147964884184973162, "size": 61, "commit_id": "51300ba1cc2f487aefec6e6631fef03b0e08b298", "message": "Fix heap buffer overflow in tf.raw_ops.UnicodeEncode.\n\nPiperOrigin-RevId: 371717714\nChange-Id: If33443b28f158e58078f1268f6b92f2728d219e0", "target": 0, "dataset": "other", "idx": 245150}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const int depth = (axis_ == -1) ? 1 : input.dim_size(axis_);\n    Tensor input_min_tensor;\n    Tensor input_max_tensor;\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n    if (range_given_) {\n      input_min_tensor = ctx->input(1);\n      input_max_tensor = ctx->input(2);\n      if (axis_ == -1) {\n        auto min_val = input_min_tensor.scalar<T>()();\n        auto max_val = input_max_tensor.scalar<T>()();\n        OP_REQUIRES(ctx, min_val <= max_val,\n                    errors::InvalidArgument(\"Invalid range: input_min \",\n                                            min_val, \" > input_max \", max_val));\n      } else {\n        OP_REQUIRES(ctx, input_min_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_min_tensor has incorrect size, was \",\n                        input_min_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_min_tensor.shape()));\n        OP_REQUIRES(ctx, input_max_tensor.dim_size(0) == depth,\n                    errors::InvalidArgument(\n                        \"input_max_tensor has incorrect size, was \",\n                        input_max_tensor.dim_size(0), \" expected \", depth,\n                        \" to match dim \", axis_, \" of the input \",\n                        input_max_tensor.shape()));\n      }\n    } else {\n      auto range_shape = (axis_ == -1) ? TensorShape({}) : TensorShape({depth});\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_min_tensor));\n      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n                                             range_shape, &input_max_tensor));\n    }\n\n    if (axis_ == -1) {\n      functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(), input.flat<T>(), signed_input_, num_bits_,\n        range_given_, &input_min_tensor, &input_max_tensor, round_mode_,\n        narrow_range_, output->flat<T>());\n    } else {\n      functor::QuantizeAndDequantizePerChannelFunctor<Device, T> f;\n      f(ctx->eigen_device<Device>(),\n        input.template flat_inner_outer_dims<T, 3>(axis_ - 1), signed_input_,\n        num_bits_, range_given_, &input_min_tensor, &input_max_tensor,\n        round_mode_, narrow_range_,\n        output->template flat_inner_outer_dims<T, 3>(axis_ - 1));\n    }\n  }", "project": "tensorflow", "hash": 293324771546003032170952415326365543811, "size": 52, "commit_id": "eccb7ec454e6617738554a255d77f08e60ee0808", "message": "Prevent segfault in `quantize_and_dequantize`\n\nFixes #42105.\n\nIf `tf.quantization.quantize_and_dequantize` is called with `axis` argument pointing to outside of the input tensor, we obtain a `CHECK` fail which then aborts the application/interpreter. This change adds a condition check and returns a `Status` instead of crashing.\n\nPiperOrigin-RevId: 337972243\nChange-Id: I71ec32c00a87266e364fb017f0ad5dfd3e23542f", "target": 1, "dataset": "other", "idx": 196800}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));\n\n    // One global scale.\n    Tensor input_min_tensor(DataTypeToEnum<T>::value, TensorShape());\n    Tensor input_max_tensor(DataTypeToEnum<T>::value, TensorShape());\n    // Initialize the tensors with the values in the Attrs.\n    input_min_tensor.template scalar<T>()() = static_cast<T>(input_min_);\n    input_max_tensor.template scalar<T>()() = static_cast<T>(input_max_);\n\n    functor::QuantizeAndDequantizeOneScaleFunctor<Device, T> functor;\n    functor(ctx->eigen_device<Device>(), input.flat<T>(), signed_input_,\n            num_bits_, range_given_, &input_min_tensor, &input_max_tensor,\n            ROUND_HALF_TO_EVEN, /*narrow_range=*/false, output->flat<T>());\n  }", "project": "tensorflow", "hash": 34025787901370453152773493086761626349, "size": 18, "commit_id": "eccb7ec454e6617738554a255d77f08e60ee0808", "message": "Prevent segfault in `quantize_and_dequantize`\n\nFixes #42105.\n\nIf `tf.quantization.quantize_and_dequantize` is called with `axis` argument pointing to outside of the input tensor, we obtain a `CHECK` fail which then aborts the application/interpreter. This change adds a condition check and returns a `Status` instead of crashing.\n\nPiperOrigin-RevId: 337972243\nChange-Id: I71ec32c00a87266e364fb017f0ad5dfd3e23542f", "target": 0, "dataset": "other", "idx": 245427}
{"func": "                                  output_end - output_start);\n        inputShard.setConstant(T(0));\n\n        const int input_start = start * input_size_per_batch;\n        const int input_end = limit * input_size_per_batch;\n        for (int64 index = input_start; index < input_end; index++) {\n          int64 grad_out_index = argmax_flat(index);\n          if (!include_batch_in_index) {\n            const int64 cur_batch = index / input_size_per_batch;\n            grad_out_index += cur_batch * output_size_per_batch;\n          }", "project": "tensorflow", "hash": 251209108610120812809202532236756676128, "size": 45, "commit_id": "dcd7867de0fea4b72a2b34bd41eb74548dc23886", "message": "Fix heap buffer overflow\n\nPiperOrigin-RevId: 372132844\nChange-Id: Idef9895efaf145f2b1c23d31983601ec980cd5e4", "target": 1, "dataset": "other", "idx": 196802}
{"func": "        inputShard.setConstant(T(0));\n\n        const int input_start = start * input_size_per_batch;\n        const int input_end = limit * input_size_per_batch;\n        for (int64 index = input_start; index < input_end; index++) {\n          if (index >= argmax.NumElements()) {\n            break;\n          }\n          int64 grad_out_index = argmax_flat(index);\n          if (!include_batch_in_index) {\n            const int64 cur_batch = index / input_size_per_batch;\n            grad_out_index += cur_batch * output_size_per_batch;\n          }", "project": "tensorflow", "hash": 118757592964186460200476445367138932686, "size": 48, "commit_id": "dcd7867de0fea4b72a2b34bd41eb74548dc23886", "message": "Fix heap buffer overflow\n\nPiperOrigin-RevId: 372132844\nChange-Id: Idef9895efaf145f2b1c23d31983601ec980cd5e4", "target": 0, "dataset": "other", "idx": 245438}
{"func": "void UncompressElementOp::Compute(OpKernelContext* ctx) {\n  Tensor tensor = ctx->input(0);\n  const Variant& variant = tensor.scalar<Variant>()();\n  const CompressedElement* compressed = variant.get<CompressedElement>();\n\n  std::vector<Tensor> components;\n  OP_REQUIRES_OK(ctx, UncompressElement(*compressed, &components));\n  OP_REQUIRES(ctx, components.size() == output_types_.size(),\n              errors::FailedPrecondition(\"Expected \", output_types_.size(),", "project": "tensorflow", "hash": 123137112005180251715591720813780418586, "size": 21, "commit_id": "7bdf50bb4f5c54a4997c379092888546c97c3ebd", "message": "Ensure non-empty compressed input in tf.raw_ops.UncompressElement\n\nPiperOrigin-RevId: 383955815\nChange-Id: I072a84fd02738dd2f51b3f42836ed80067dba4a8", "target": 1, "dataset": "other", "idx": 196811}
{"func": "void UncompressElementOp::Compute(OpKernelContext* ctx) {\n  Tensor tensor = ctx->input(0);\n  const Variant& variant = tensor.scalar<Variant>()();\n  const CompressedElement* compressed = variant.get<CompressedElement>();\n  OP_REQUIRES(\n      ctx, compressed != nullptr,\n      errors::InvalidArgument(\n          \"Input does not contain a compressed element. Instead got tensor \",\n          tensor.DebugString()));\n\n  std::vector<Tensor> components;\n  OP_REQUIRES_OK(ctx, UncompressElement(*compressed, &components));\n  OP_REQUIRES(ctx, components.size() == output_types_.size(),\n              errors::FailedPrecondition(\"Expected \", output_types_.size(),", "project": "tensorflow", "hash": 63230142722208908504743697445986676990, "size": 26, "commit_id": "7bdf50bb4f5c54a4997c379092888546c97c3ebd", "message": "Ensure non-empty compressed input in tf.raw_ops.UncompressElement\n\nPiperOrigin-RevId: 383955815\nChange-Id: I072a84fd02738dd2f51b3f42836ed80067dba4a8", "target": 0, "dataset": "other", "idx": 245642}
{"func": "  sgx_params->output = nullptr;\n  CHECK_OCALL(\n      ocall_dispatch_untrusted_call(&ret, untrusted_selector, sgx_params));\n  if (sgx_params->input) {\n    untrusted_cache->Free(const_cast<void *>(sgx_params->input));\n  }\n  if (sgx_params->output) {\n    // For the results obtained in |output_buffer|, copy them to |output|\n    // before freeing the buffer.\n    output->Deserialize(sgx_params->output, sgx_params->output_size);\n    TrustedPrimitives::UntrustedLocalFree(sgx_params->output);", "project": "asylo", "hash": 261964073185143736765939860190443192174, "size": 36, "commit_id": "83036fd841d33baa7e039f842d131aa7881fdcc2", "message": "Verify UntrustedCall output is outside enclave\n\nPiperOrigin-RevId: 333781703\nChange-Id: I9df55c04dc8b04f4bf0bda8e68cc32bca81b933a", "target": 1, "dataset": "other", "idx": 196831}
{"func": "  CHECK_OCALL(\n      ocall_dispatch_untrusted_call(&ret, untrusted_selector, sgx_params));\n  if (sgx_params->input) {\n    untrusted_cache->Free(const_cast<void *>(sgx_params->input));\n  }\n  if (!TrustedPrimitives::IsOutsideEnclave(sgx_params->output,\n                                           sgx_params->output_size)) {\n    TrustedPrimitives::BestEffortAbort(\n        \"UntrustedCall: sgx_param output should be in untrusted memory\");\n  }\n  if (sgx_params->output) {\n    // For the results obtained in |output_buffer|, copy them to |output|\n    // before freeing the buffer.\n    output->Deserialize(sgx_params->output, sgx_params->output_size);\n    TrustedPrimitives::UntrustedLocalFree(sgx_params->output);", "project": "asylo", "hash": 97744345373278828596253695008510711317, "size": 41, "commit_id": "83036fd841d33baa7e039f842d131aa7881fdcc2", "message": "Verify UntrustedCall output is outside enclave\n\nPiperOrigin-RevId: 333781703\nChange-Id: I9df55c04dc8b04f4bf0bda8e68cc32bca81b933a", "target": 0, "dataset": "other", "idx": 246050}
{"func": "        sqlite3ExprDelete(db, p->pLimit);\n        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);", "project": "sqlite", "hash": 200791527881523564389362957209573506757, "size": 343, "commit_id": "5f69512404cd2e5153ddf90ea277fbba6dd58ab7", "message": "Early-out on the INTERSECT query processing following an error.\n\nFossilOrigin-Name: a67cf5b7d37d5b1484be32092635faafd8f76e5881898cd9435517c4b287d663", "target": 1, "dataset": "other", "idx": 196833}
{"func": "        p->pLimit = pLimit;\n  \n        /* Generate code to take the intersection of the two temporary\n        ** tables.\n        */\n        if( rc ) break;\n        assert( p->pEList );\n        iBreak = sqlite3VdbeMakeLabel(pParse);\n        iCont = sqlite3VdbeMakeLabel(pParse);\n        computeLimitRegisters(pParse, p, iBreak);\n        sqlite3VdbeAddOp2(v, OP_Rewind, tab1, iBreak); VdbeCoverage(v);", "project": "sqlite", "hash": 222304138699688756475020408719005629756, "size": 344, "commit_id": "5f69512404cd2e5153ddf90ea277fbba6dd58ab7", "message": "Early-out on the INTERSECT query processing following an error.\n\nFossilOrigin-Name: a67cf5b7d37d5b1484be32092635faafd8f76e5881898cd9435517c4b287d663", "target": 0, "dataset": "other", "idx": 246169}
{"func": "         * and the buffer follows immediately after.  The requested size is\n         * incremented so the free space is returned as the user would expect -\n         * this is a quirk of the implementation that means otherwise the free\n         * space would be reported as one byte smaller than would be logically\n         * expected. */\n        xBufferSizeBytes++;\n        pucAllocatedMemory = ( uint8_t * ) pvPortMalloc( xBufferSizeBytes + sizeof( StreamBuffer_t ) ); /*lint !e9079 malloc() only returns void*. */\n\n        if( pucAllocatedMemory != NULL )\n        {\n            prvInitialiseNewStreamBuffer( ( StreamBuffer_t * ) pucAllocatedMemory,       /* Structure at the start of the allocated memory. */ /*lint !e9087 Safe cast as allocated memory is aligned. */ /*lint !e826 Area is not too small and alignment is guaranteed provided malloc() behaves as expected and returns aligned buffer. */\n                                          pucAllocatedMemory + sizeof( StreamBuffer_t ), /* Storage area follows. */ /*lint !e9016 Indexing past structure valid for uint8_t pointer, also storage area has no alignment requirement. */", "project": "FreeRTOS-Kernel", "hash": 218343301167279592040435651791165811, "size": 61, "commit_id": "d05b9c123f2bf9090bce386a244fc934ae44db5b", "message": "Add addition overflow check for stream buffer (#226)", "target": 1, "dataset": "other", "idx": 196843}
{"func": "         * and the buffer follows immediately after.  The requested size is\n         * incremented so the free space is returned as the user would expect -\n         * this is a quirk of the implementation that means otherwise the free\n         * space would be reported as one byte smaller than would be logically\n         * expected. */\n        if( xBufferSizeBytes < ( xBufferSizeBytes + 1 + sizeof( StreamBuffer_t ) ) )\n        {\n            xBufferSizeBytes++;\n            pucAllocatedMemory = ( uint8_t * ) pvPortMalloc( xBufferSizeBytes + sizeof( StreamBuffer_t ) ); /*lint !e9079 malloc() only returns void*. */\n        }\n        else\n        {\n            pucAllocatedMemory = NULL;\n        }\n        \n\n        if( pucAllocatedMemory != NULL )\n        {\n            prvInitialiseNewStreamBuffer( ( StreamBuffer_t * ) pucAllocatedMemory,       /* Structure at the start of the allocated memory. */ /*lint !e9087 Safe cast as allocated memory is aligned. */ /*lint !e826 Area is not too small and alignment is guaranteed provided malloc() behaves as expected and returns aligned buffer. */\n                                          pucAllocatedMemory + sizeof( StreamBuffer_t ), /* Storage area follows. */ /*lint !e9016 Indexing past structure valid for uint8_t pointer, also storage area has no alignment requirement. */", "project": "FreeRTOS-Kernel", "hash": 8256987615210769350257310115874004097, "size": 69, "commit_id": "d05b9c123f2bf9090bce386a244fc934ae44db5b", "message": "Add addition overflow check for stream buffer (#226)", "target": 0, "dataset": "other", "idx": 246422}
{"func": "  // non-memcopyable tensors, which we save to use again later.\n  std::vector<TensorProto> non_memcpy_components;\n  int64 total_size = 0;\n  for (auto& component : element) {\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      // Some datatypes can be memcopied, allowing us to save two copies\n      // (AsProtoTensorContent and SerializeToArray).\n      total_size += DMAHelper::buffer(&component)->size();\n    } else {\n      non_memcpy_components.emplace_back();\n      component.AsProtoTensorContent(&non_memcpy_components.back());\n      total_size += non_memcpy_components.back().ByteSizeLong();\n    }\n        out->mutable_component_metadata()->Add();\n    metadata->set_dtype(component.dtype());\n    component.shape().AsProto(metadata->mutable_tensor_shape());\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      const TensorBuffer* buffer = DMAHelper::buffer(&component);\n      memcpy(position, buffer->data(), buffer->size());\n      metadata->set_tensor_size_bytes(buffer->size());\n    } else {\n      TensorProto& proto = non_memcpy_components[non_memcpy_component_index++];\n      proto.SerializeToArray(position, proto.ByteSizeLong());\n      metadata->set_tensor_size_bytes(proto.ByteSizeLong());\n    }", "project": "tensorflow", "hash": 221515392574513944494740423419101303962, "size": 51, "commit_id": "5dc7f6981fdaf74c8c5be41f393df705841fb7c5", "message": "Fix accessing possible nullptr in tensorflow::data::CompressElement and UncompressElement which are used in tf.data.service.\n\nPiperOrigin-RevId: 373920841\nChange-Id: Ia88d78aee09fa19bb53a0f163fd19620d0c68743", "target": 1, "dataset": "other", "idx": 196856}
{"func": "  // non-memcopyable tensors, which we save to use again later.\n  std::vector<TensorProto> non_memcpy_components;\n  int64 total_size = 0;\n  for (auto& component : element) {\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      const TensorBuffer* buffer = DMAHelper::buffer(&component);\n      if (buffer) {\n        total_size += buffer->size();\n      }\n    } else {\n      non_memcpy_components.emplace_back();\n      component.AsProtoTensorContent(&non_memcpy_components.back());\n      total_size += non_memcpy_components.back().ByteSizeLong();\n    }\n        out->mutable_component_metadata()->Add();\n    metadata->set_dtype(component.dtype());\n    component.shape().AsProto(metadata->mutable_tensor_shape());\n    if (DataTypeCanUseMemcpy(component.dtype())) {\n      const TensorBuffer* buffer = DMAHelper::buffer(&component);\n      if (buffer) {\n        memcpy(position, buffer->data(), buffer->size());\n        metadata->set_tensor_size_bytes(buffer->size());\n      }\n    } else {\n      TensorProto& proto = non_memcpy_components[non_memcpy_component_index++];\n      proto.SerializeToArray(position, proto.ByteSizeLong());\n      metadata->set_tensor_size_bytes(proto.ByteSizeLong());\n    }", "project": "tensorflow", "hash": 20244254151554116869938450542850593726, "size": 54, "commit_id": "5dc7f6981fdaf74c8c5be41f393df705841fb7c5", "message": "Fix accessing possible nullptr in tensorflow::data::CompressElement and UncompressElement which are used in tf.data.service.\n\nPiperOrigin-RevId: 373920841\nChange-Id: Ia88d78aee09fa19bb53a0f163fd19620d0c68743", "target": 0, "dataset": "other", "idx": 246589}
{"func": "                    \"but received shapes: \",\n                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64 a_nnz = a_indices_t->dim_size(0);\n    const int64 b_nnz = b_indices_t->dim_size(0);\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n                TensorShapeUtils::IsVector(a_shape_t->shape()) &&\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    OP_REQUIRES(\n        ctx, a_indices_t->dim_size(1) == b_indices_t->dim_size(1),\n        errors::InvalidArgument(\n            \"Indices' dimensions do not match: got \", a_indices_t->dim_size(1),\n            \" and \", b_indices_t->dim_size(1), \" for the second dimension.\"));\n    const int num_dims = a_indices_t->dim_size(1);\n    const auto a_indices_mat = a_indices_t->matrix<int64>();\n    const auto b_indices_mat = b_indices_t->matrix<int64>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,", "project": "tensorflow", "hash": 294501462862477461576591658344805884704, "size": 109, "commit_id": "f6fde895ef9c77d848061c0517f19d0ec2682f3a", "message": "Validate that a and b are proper sparse tensors\n\nPiperOrigin-RevId: 373274848\nChange-Id: I3a665ac3a29dee9fb69bdf408a939330cb93ea75", "target": 1, "dataset": "other", "idx": 196857}
{"func": "                    a_values_t->shape().DebugString(), \" and \",\n                    b_values_t->shape().DebugString()));\n\n    const int64 a_nnz = a_indices_t->dim_size(0);\n    const int64 b_nnz = b_indices_t->dim_size(0);\n\n    const auto a_values = a_values_t->vec<T>();\n    const auto b_values = b_values_t->vec<T>();\n\n    OP_REQUIRES(\n        ctx, a_values.size() == a_nnz && b_values.size() == b_nnz,\n                    TensorShapeUtils::IsVector(b_shape_t->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape_t->shape().DebugString(), \" and \",\n                    b_shape_t->shape().DebugString()));\n    const int num_dims = a_indices_t->dim_size(1);\n    OP_REQUIRES(\n        ctx, a_shape_t->NumElements() == num_dims,\n        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n                                \"a_shape must match, got \",\n                                num_dims, \" and \", a_shape_t->NumElements()));\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Tensors must not be empty\"));\n    OP_REQUIRES(ctx, a_shape_t->IsSameSize(*b_shape_t),\n                errors::InvalidArgument(\n                    \"Operands do not have the same ranks; got shapes: \",\n                    a_shape_t->SummarizeValue(10), \" and \",\n                    b_shape_t->SummarizeValue(10)));\n      OP_REQUIRES(ctx, a_shape(i) == b_shape(i),\n                  errors::InvalidArgument(\"Operands' shapes do not match: got \",\n                                          a_shape(i), \" and \", b_shape(i),\n                                          \" for dimension \", i));\n    }\n\n    const auto a_indices_mat = a_indices_t->matrix<int64>();\n    const auto b_indices_mat = b_indices_t->matrix<int64>();\n    std::vector<T> a_augmented_values, b_augmented_values;\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    UnionSparseIndicesAndValues(a_indices_mat, a_values, a_nnz, b_indices_mat,", "project": "tensorflow", "hash": 146102284664006539274854620186945524131, "size": 112, "commit_id": "f6fde895ef9c77d848061c0517f19d0ec2682f3a", "message": "Validate that a and b are proper sparse tensors\n\nPiperOrigin-RevId: 373274848\nChange-Id: I3a665ac3a29dee9fb69bdf408a939330cb93ea75", "target": 0, "dataset": "other", "idx": 246590}
{"func": "  bool matches(const Http::RequestHeaderMap& headers) const override {\n    if (BaseMatcherImpl::matchRoute(headers)) {\n      const Http::HeaderString& path = headers.Path()->value();\n      const absl::string_view query_string = Http::Utility::findQueryStringStart(path);\n      absl::string_view path_view = path.getStringView();\n      path_view.remove_suffix(query_string.length());\n      if (path_matcher_->match(path_view)) {", "project": "envoy", "hash": 76257563614559165334076596514064335521, "size": 13, "commit_id": "9371333230b1a6e1be2eccf4868771e11af6253a", "message": "CVE-2021-43824\n\njwt_atuhn: fixed the crash when a CONNECT request is sent to JWT filter\nconfigured with regex match.\n\nSigned-off-by: Yan Avlasov <yavlasov@google.com>", "target": 1, "dataset": "other", "idx": 196858}
{"func": "  bool matches(const Http::RequestHeaderMap& headers) const override {\n    if (BaseMatcherImpl::matchRoute(headers)) {\n      if (headers.Path() == nullptr) {\n        return false;\n      }\n      const Http::HeaderString& path = headers.Path()->value();\n      const absl::string_view query_string = Http::Utility::findQueryStringStart(path);\n      absl::string_view path_view = path.getStringView();\n      path_view.remove_suffix(query_string.length());\n      if (path_matcher_->match(path_view)) {", "project": "envoy", "hash": 234200888989403949850149389072672312631, "size": 16, "commit_id": "9371333230b1a6e1be2eccf4868771e11af6253a", "message": "CVE-2021-43824\n\njwt_atuhn: fixed the crash when a CONNECT request is sent to JWT filter\nconfigured with regex match.\n\nSigned-off-by: Yan Avlasov <yavlasov@google.com>", "target": 0, "dataset": "other", "idx": 246616}
{"func": "    if (f)\n    {\n\twhile (!feof(f))\n\t{\n\t    isstring = false;\n\t    if (fscanf (f, \"%79s %[^\\n]\\n\", def, strparm) == 2)\n\t    {\n\t\tif (strparm[0] == '\"')\n\t\t{\n\t\t    // get a string default\n\t\t    isstring = true;", "project": "doom-vanille", "hash": 82891971845625363789583776832370969270, "size": 74, "commit_id": "8a6d9a02fa991a91ff90ccdc73b5ceabaa6cb9ec", "message": "Fix buffer overflow in M_LoadDefaults\n\nToo much data will most likely result in a crash or freeze, but you can overwrite the stack which can be used to do an arbitrary code execution. (https://twitter.com/notrevenant/status/1268654123903340544)", "target": 1, "dataset": "other", "idx": 196884}
{"func": "    if (f)\n    {\n\twhile (!feof(f))\n\t{\n\t    isstring = false;\n\t    if (fscanf (f, \"%79s %99[^\\n]\\n\", def, strparm) == 2)\n\t    {\n\t\tif (strparm[0] == '\"')\n\t\t{\n\t\t    // get a string default\n\t\t    isstring = true;", "project": "doom-vanille", "hash": 235405443936260339657459276709067885020, "size": 74, "commit_id": "8a6d9a02fa991a91ff90ccdc73b5ceabaa6cb9ec", "message": "Fix buffer overflow in M_LoadDefaults\n\nToo much data will most likely result in a crash or freeze, but you can overwrite the stack which can be used to do an arbitrary code execution. (https://twitter.com/notrevenant/status/1268654123903340544)", "target": 0, "dataset": "other", "idx": 247279}
{"func": "                errors::InvalidArgument(\"tensor_in must be 4-dimensional\"));\n\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));", "project": "tensorflow", "hash": 65175645890002238093586777553466908304, "size": 115, "commit_id": "548b5eaf23685d86f722233d8fbc21d0a4aecb96", "message": "Fix divide by zero error in `fractional_pool_common.cc`.\n\nPiperOrigin-RevId: 371126221\nChange-Id: Iea4b2f363aaeb116ab460e3bc592c687484af344", "target": 1, "dataset": "other", "idx": 196921}
{"func": "\n    std::vector<int> input_size(tensor_in_and_out_dims);\n    std::vector<int> output_size(tensor_in_and_out_dims);\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      input_size[i] = tensor_in.dim_size(i);\n      OP_REQUIRES(\n          context, pooling_ratio_[i] <= input_size[i],\n          errors::InvalidArgument(\n              \"Pooling ratio cannot be bigger than input tensor dim size.\"));\n    }\n    // Output size.\n    for (int i = 0; i < tensor_in_and_out_dims; ++i) {\n      output_size[i] =\n          static_cast<int>(std::floor(input_size[i] / pooling_ratio_[i]));", "project": "tensorflow", "hash": 290851511414157159746753067138127604859, "size": 119, "commit_id": "548b5eaf23685d86f722233d8fbc21d0a4aecb96", "message": "Fix divide by zero error in `fractional_pool_common.cc`.\n\nPiperOrigin-RevId: 371126221\nChange-Id: Iea4b2f363aaeb116ab460e3bc592c687484af344", "target": 0, "dataset": "other", "idx": 248336}
{"func": "static void hash_search(int f,struct sum_struct *s,\n\t\t\tstruct map_struct *buf, OFF_T len)\n{\n\tOFF_T offset, aligned_offset, end;\n\tint32 k, want_i, backup;\n\tchar sum2[SUM_LENGTH];\n\tuint32 s1, s2, sum;\n\tint more;\n\tschar *map;\n\n\ts1 = sum & 0xFFFF;\n\ts2 = sum >> 16;\n\tif (verbose > 3)\n\t\trprintf(FINFO, \"sum=%.8x k=%ld\\n\", sum, (long)k);\n\n\toffset = aligned_offset = 0;\n\n\tend = len + 1 - s->sums[s->count-1].len;\n\n\tif (verbose > 3) {\n\t\trprintf(FINFO, \"hash search s->blength=%ld len=%.0f count=%.0f\\n\",\n\t\t\t/* When updating in-place, the best possible match is\n\t\t\t * one with an identical offset, so we prefer that over\n\t\t\t * the adjacent want_i optimization. */\n\t\t\tif (updating_basis_file) {\n\t\t\t\t/* All the generator's chunks start at blength boundaries. */\n\t\t\t\twhile (aligned_offset < offset)\n\t\t\t\t\taligned_offset += s->blength;\n\t\t\t\tif (offset == aligned_offset) {\n\t\t\t\t\tint32 i2;\n\t\t\t\t\tfor (i2 = i; i2 >= 0; i2 = s->sums[i2].chain) {\n\t\t\t\t\t\tif (s->sums[i2].offset != offset)\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\tif (i2 != i) {\n\t\t\t\t\t\t\tif (sum != s->sums[i2].sum1\n\t\t\t\t\t\t\t || l != s->sums[i2].len\n\t\t\t\t\t\t\t || memcmp(sum2, s->sums[i2].sum2, s->s2length) != 0)\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\ti = i2;\n\t\t\t\t\t\t}\n\t\t\t\t\t\t/* This chunk remained in the same spot in the old and new file. */\n\t\t\t\t\t\ts->sums[i].flags |= SUMFLG_SAME_OFFSET;\n\t\t\t\t\t\twant_i = i;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* we've found a match, but now check to see\n\t\t\t * if want_i can hint at a better match. */\n\t\t\tif (i != want_i && want_i < s->count\n\t\t\t    && (!updating_basis_file || s->sums[want_i].offset >= offset\n\t\t\t     || s->sums[want_i].flags & SUMFLG_SAME_OFFSET)", "project": "rsync", "hash": 13269351929661258568831239064102968322, "size": 176, "commit_id": "c8255147b06b74dad940d32f9cef5fbe17595239", "message": "Optimize finding the sum that matches our --inplace position.", "target": 1, "dataset": "other", "idx": 196927}
{"func": "static void hash_search(int f,struct sum_struct *s,\n\t\t\tstruct map_struct *buf, OFF_T len)\n{\n\tOFF_T offset, aligned_offset, end;\n\tint32 k, want_i, aligned_i, backup;\n\tchar sum2[SUM_LENGTH];\n\tuint32 s1, s2, sum;\n\tint more;\n\tschar *map;\n\n\ts1 = sum & 0xFFFF;\n\ts2 = sum >> 16;\n\tif (verbose > 3)\n\t\trprintf(FINFO, \"sum=%.8x k=%ld\\n\", sum, (long)k);\n\n\toffset = aligned_offset = aligned_i = 0;\n\n\tend = len + 1 - s->sums[s->count-1].len;\n\n\tif (verbose > 3) {\n\t\trprintf(FINFO, \"hash search s->blength=%ld len=%.0f count=%.0f\\n\",\n\t\t\t/* When updating in-place, the best possible match is\n\t\t\t * one with an identical offset, so we prefer that over\n\t\t\t * the adjacent want_i optimization. */\n\t\t\tif (updating_basis_file) {\n\t\t\t\t/* All the generator's chunks start at blength boundaries. */\n\t\t\t\twhile (aligned_offset < offset) {\n\t\t\t\t\taligned_offset += s->blength;\n\t\t\t\t\taligned_i++;\n\t\t\t\t}\n\t\t\t\tif (offset == aligned_offset) {\n\t\t\t\t\tif (i != aligned_i) {\n\t\t\t\t\t\tif (sum != s->sums[aligned_i].sum1\n\t\t\t\t\t\t || l != s->sums[aligned_i].len\n\t\t\t\t\t\t || memcmp(sum2, s->sums[aligned_i].sum2, s->s2length) != 0)\n\t\t\t\t\t\t\tgoto check_want_i;\n\t\t\t\t\t\ti = aligned_i;\n\t\t\t\t\t}\n\t\t\t\t\t/* This identical chunk is in the same spot in the old and new file. */\n\t\t\t\t\ts->sums[i].flags |= SUMFLG_SAME_OFFSET;\n\t\t\t\t\twant_i = i;\n\t\t\t\t}\n\t\t\t}\n\n\t\t  check_want_i:\n\t\t\t/* we've found a match, but now check to see\n\t\t\t * if want_i can hint at a better match. */\n\t\t\tif (i != want_i && want_i < s->count\n\t\t\t    && (!updating_basis_file || s->sums[want_i].offset >= offset\n\t\t\t     || s->sums[want_i].flags & SUMFLG_SAME_OFFSET)", "project": "rsync", "hash": 282231606510893388476838486510622012833, "size": 173, "commit_id": "c8255147b06b74dad940d32f9cef5fbe17595239", "message": "Optimize finding the sum that matches our --inplace position.", "target": 0, "dataset": "other", "idx": 248548}
{"func": "        // so we don't end up with memory corruptions. Our benchmark shows that\n        // the performance impact is quite small\n        // CHECK(input_backprop_index >= in_start && input_backprop_index <\n        // in_end)\n        FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n        input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n      }\n    }\n  };\n\n  const int64 shard_cost = params.tensor_in_rows * params.tensor_in_cols *", "project": "tensorflow", "hash": 68179409679570044569453067278368921696, "size": 151, "commit_id": "a74768f8e4efbda4def9f16ee7e13cf3922ac5f7", "message": "Prevent heap OOB error in `MaxPoolGrad`\n\nPiperOrigin-RevId: 372424854\nChange-Id: Idac0f23867ad8b0601cafbaaa52d5e64269e63a7", "target": 1, "dataset": "other", "idx": 196935}
{"func": "        // so we don't end up with memory corruptions. Our benchmark shows that\n        // the performance impact is quite small\n        // CHECK(input_backprop_index >= in_start && input_backprop_index <\n        // in_end)\n        FastBoundsCheck(input_backprop_index - in_start, in_end - in_start);\n        if (index < out_backprop.NumElements()) {\n          input_backprop_flat(input_backprop_index) += out_backprop_flat(index);\n        }\n      }\n    }\n  };\n\n  const int64 shard_cost = params.tensor_in_rows * params.tensor_in_cols *", "project": "tensorflow", "hash": 19671573270094211449128076626686963676, "size": 153, "commit_id": "a74768f8e4efbda4def9f16ee7e13cf3922ac5f7", "message": "Prevent heap OOB error in `MaxPoolGrad`\n\nPiperOrigin-RevId: 372424854\nChange-Id: Idac0f23867ad8b0601cafbaaa52d5e64269e63a7", "target": 0, "dataset": "other", "idx": 248727}
{"func": "        \"system_call.cc: null response buffer received for the syscall.\");\n  }\n\n  // Copy outputs back into pointer parameters.\n  auto response_reader =\n      asylo::system_call::MessageReader({response_buffer, response_size});\n  const asylo::primitives::PrimitiveStatus response_status =\n      response_reader.Validate();\n  if (!response_status.ok()) {\n    error_handler(\n        \"system_call.cc: Error deserializing response buffer into response \"", "project": "asylo", "hash": 130310869398981487889556909042186639572, "size": 94, "commit_id": "90d7619e9dd99bcdb6cd28c7649d741d254d9a1a", "message": "Add sysno check in MessageReader\n\nThe sysno in MessageReader is interpreted from the Message header passed\nfrom the host. A malicious Message header may provide a modified sysno\nto bypass the validation, and overwrites enclave memory. This change\nadds a check for sysno to make sure it matches the expected value.\n\nThis issue was reported by Qinkun Bao, Zhaofeng Chen, Mingshen Sun, and\nKang Li from Baidu Security.\n\nPiperOrigin-RevId: 377328054\nChange-Id: I3ff6f60694d3390f66da89d139cf7cc7b49abaea", "target": 1, "dataset": "other", "idx": 196939}
{"func": "  }\n\n  // Copy outputs back into pointer parameters.\n  auto response_reader =\n      asylo::system_call::MessageReader({response_buffer, response_size});\n  if (response_reader.sysno() != sysno) {\n    error_handler(\"system_call.cc: Unexpected sysno in response\");\n  }\n  const asylo::primitives::PrimitiveStatus response_status =\n      response_reader.Validate();\n  if (!response_status.ok()) {\n    error_handler(\n        \"system_call.cc: Error deserializing response buffer into response \"", "project": "asylo", "hash": 301481568949052192969768700409775374312, "size": 97, "commit_id": "90d7619e9dd99bcdb6cd28c7649d741d254d9a1a", "message": "Add sysno check in MessageReader\n\nThe sysno in MessageReader is interpreted from the Message header passed\nfrom the host. A malicious Message header may provide a modified sysno\nto bypass the validation, and overwrites enclave memory. This change\nadds a check for sysno to make sure it matches the expected value.\n\nThis issue was reported by Qinkun Bao, Zhaofeng Chen, Mingshen Sun, and\nKang Li from Baidu Security.\n\nPiperOrigin-RevId: 377328054\nChange-Id: I3ff6f60694d3390f66da89d139cf7cc7b49abaea", "target": 0, "dataset": "other", "idx": 248772}
{"func": "            break;\n\n        case 0xa4: /* presentation-context-definition list */\n            if (DEBUG_PRES)\n                printf(\"PRES: pcd list\\n\");\n            bufPos = parsePresentationContextDefinitionList(self, buffer, len, bufPos);\n            break;\n\n        case 0xa5: /* context-definition-result-list */\n\n            bufPos += len;", "project": "libiec61850", "hash": 331339374126968299932527563528983733700, "size": 115, "commit_id": "cfa94cbf10302bedc779703f874ee2e8387a0721", "message": "- fixed - Bug in presentation layer parser can cause infinite loop (LIB61850-302)", "target": 1, "dataset": "other", "idx": 196991}
{"func": "\n        case 0xa4: /* presentation-context-definition list */\n            if (DEBUG_PRES)\n                printf(\"PRES: pcd list\\n\");\n            bufPos = parsePresentationContextDefinitionList(self, buffer, len, bufPos);\n\n            if (bufPos < 0)\n                return -1;\n\n            break;\n\n        case 0xa5: /* context-definition-result-list */\n\n            bufPos += len;", "project": "libiec61850", "hash": 100266899328047651441631665278404681002, "size": 119, "commit_id": "cfa94cbf10302bedc779703f874ee2e8387a0721", "message": "- fixed - Bug in presentation layer parser can cause infinite loop (LIB61850-302)", "target": 0, "dataset": "other", "idx": 249484}
{"func": "\t\tdump = stdout;\n\t\tfprintf(dump, \"* File SDP content *\\n\\n\");\n\t}\n\t//get the movie SDP\n\tgf_isom_sdp_get(file, &sdp, &size);\n\tfprintf(dump, \"%s\", sdp);\n\tfprintf(dump, \"\\r\\n\");\n\n\t//then tracks\n\tfor (i=0; i<gf_isom_get_track_count(file); i++) {\n\t\tif (gf_isom_get_media_type(file, i+1) != GF_ISOM_MEDIA_HINT) continue;", "project": "gpac", "hash": 146034260197706968005738203104188095642, "size": 37, "commit_id": "ce01bd15f711d4575b7424b54b3a395ec64c1784", "message": "fixed #1566", "target": 1, "dataset": "other", "idx": 196997}
{"func": "\t\tdump = stdout;\n\t\tfprintf(dump, \"* File SDP content *\\n\\n\");\n\t}\n\t//get the movie SDP\n\tgf_isom_sdp_get(file, &sdp, &size);\n\tif (sdp && size)\n\t\tfprintf(dump, \"%s\", sdp);\n\tfprintf(dump, \"\\r\\n\");\n\n\t//then tracks\n\tfor (i=0; i<gf_isom_get_track_count(file); i++) {\n\t\tif (gf_isom_get_media_type(file, i+1) != GF_ISOM_MEDIA_HINT) continue;", "project": "gpac", "hash": 129968796869173739557009867841578254601, "size": 38, "commit_id": "ce01bd15f711d4575b7424b54b3a395ec64c1784", "message": "fixed #1566", "target": 0, "dataset": "other", "idx": 249539}
{"func": "        if (r->http_version == HTTP_VERSION_2) return NULL;\n    }\n\n    if (cq->first != cq->last && 0 != olen) {\n        const size_t clen = chunkqueue_length(cq);\n        size_t block = (olen + (16384-1)) & (16384-1);\n        block += (block - olen > 1024 ? 0 : 16384);\n        chunkqueue_compact_mem(cq, block > clen ? clen : block);\n    }\n\n    /* detect if data is added to chunk */", "project": "lighttpd1.4", "hash": 58630683850731888391551309485472068258, "size": 30, "commit_id": "b03b86f47b0d5a553137f081fadc482b4af1372d", "message": "[core] fix merging large headers across mult reads (fixes #3059)\n\n(thx mitd)\n\nx-ref:\n  \"Connections stuck in Close_Wait causing 100% cpu usage\"\n  https://redmine.lighttpd.net/issues/3059", "target": 1, "dataset": "other", "idx": 197066}
{"func": "        if (r->http_version == HTTP_VERSION_2) return NULL;\n    }\n\n    if (cq->first != cq->last && 0 != olen) {\n        const size_t clen = chunkqueue_length(cq);\n        size_t block = (olen + (16384-1)) & ~(16384-1);\n        block += (block - olen > 1024 ? 0 : 16384);\n        chunkqueue_compact_mem(cq, block > clen ? clen : block);\n    }\n\n    /* detect if data is added to chunk */", "project": "lighttpd1.4", "hash": 161459326522428992863096627223237510818, "size": 30, "commit_id": "b03b86f47b0d5a553137f081fadc482b4af1372d", "message": "[core] fix merging large headers across mult reads (fixes #3059)\n\n(thx mitd)\n\nx-ref:\n  \"Connections stuck in Close_Wait causing 100% cpu usage\"\n  https://redmine.lighttpd.net/issues/3059", "target": 0, "dataset": "other", "idx": 251461}
{"func": "    const Tensor& maxvals_tensor = ctx->input(4);\n\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(shape_tensor.shape()),\n        errors::InvalidArgument(\"Input shape should be a vector, got shape: \",\n                                shape_tensor.shape().DebugString()));\n    int32 num_batches = shape_tensor.flat<int32>()(0);\n\n    int32 samples_per_batch = 1;\n    const int32 num_dims = shape_tensor.dim_size(0);\n    for (int32 i = 1; i < num_dims; i++) {", "project": "tensorflow", "hash": 148108695542733870828908833256506778307, "size": 104, "commit_id": "5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8", "message": "Fix breakage in parameterized_truncated_normal_op.cc\n\nPiperOrigin-RevId: 372041718\nChange-Id: Iff79e77a2bb27032423eefcb84211627b27dfe81", "target": 1, "dataset": "other", "idx": 197084}
{"func": "\n    OP_REQUIRES(\n        ctx, TensorShapeUtils::IsVector(shape_tensor.shape()),\n        errors::InvalidArgument(\"Input shape should be a vector, got shape: \",\n                                shape_tensor.shape().DebugString()));\n    OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,\n                errors::InvalidArgument(\"Shape tensor must not be empty, got \",\n                                        shape_tensor.DebugString()));\n    int32 num_batches = shape_tensor.flat<int32>()(0);\n\n    int32 samples_per_batch = 1;\n    const int32 num_dims = shape_tensor.dim_size(0);\n    for (int32 i = 1; i < num_dims; i++) {", "project": "tensorflow", "hash": 320463600272774049798784249056358162496, "size": 107, "commit_id": "5e52ef5a461570cfb68f3bdbbebfe972cb4e0fd8", "message": "Fix breakage in parameterized_truncated_normal_op.cc\n\nPiperOrigin-RevId: 372041718\nChange-Id: Iff79e77a2bb27032423eefcb84211627b27dfe81", "target": 0, "dataset": "other", "idx": 251872}
{"func": "                void *pItem;\n                \n                if (*size == PB_SIZE_MAX)\n                    PB_RETURN_ERROR(stream, \"too many array entries\");\n                \n                (*size)++;\n                if (!allocate_field(stream, iter->pData, iter->pos->data_size, *size))\n                    return false;\n            \n                pItem = *(char**)iter->pData + iter->pos->data_size * (*size - 1);\n                initialize_pointer_field(pItem, iter);\n                return func(stream, iter->pos, pItem);\n            }\n\n        default:", "project": "nanopb", "hash": 213075198701079106378906571658903743081, "size": 122, "commit_id": "aa9d0d1ca78d6adec3adfeecf3a706c7f9df81f2", "message": "Fix invalid free() after failed realloc() (GHSA-gcx3-7m76-287p)", "target": 1, "dataset": "other", "idx": 197114}
{"func": "                void *pItem;\n                \n                if (*size == PB_SIZE_MAX)\n                    PB_RETURN_ERROR(stream, \"too many array entries\");\n                \n                if (!allocate_field(stream, iter->pData, iter->pos->data_size, (size_t)(*size + 1)))\n                    return false;\n            \n                pItem = *(char**)iter->pData + iter->pos->data_size * (*size);\n                (*size)++;\n                initialize_pointer_field(pItem, iter);\n                return func(stream, iter->pos, pItem);\n            }\n\n        default:", "project": "nanopb", "hash": 105540620865014349806580074661016441612, "size": 122, "commit_id": "aa9d0d1ca78d6adec3adfeecf3a706c7f9df81f2", "message": "Fix invalid free() after failed realloc() (GHSA-gcx3-7m76-287p)", "target": 0, "dataset": "other", "idx": 252505}
{"func": "    const Tensor& col_seq_tensor = context->input(3);\n\n    const int64 out_batch = out_backprop.dim_size(0);\n    const int64 out_rows = out_backprop.dim_size(1);\n    const int64 out_cols = out_backprop.dim_size(2);\n    const int64 out_depth = out_backprop.dim_size(3);\n\n    auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n    auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n    auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();\n", "project": "tensorflow", "hash": 220832190112288234713294919372485684625, "size": 116, "commit_id": "12c727cee857fa19be717f336943d95fca4ffe4f", "message": "Validate inputs of `FractionalAvgPoolGrad`.\n\nPiperOrigin-RevId: 372420640\nChange-Id: Icc583928e6cdc3062e12498e4d2337a8fe3da016", "target": 1, "dataset": "other", "idx": 197133}
{"func": "\n    const int64 out_batch = out_backprop.dim_size(0);\n    const int64 out_rows = out_backprop.dim_size(1);\n    const int64 out_cols = out_backprop.dim_size(2);\n    const int64 out_depth = out_backprop.dim_size(3);\n\n    OP_REQUIRES(context, row_seq_tensor.NumElements() > out_rows,\n                errors::InvalidArgument(\"Given out_backprop shape \",\n                                        out_backprop.shape().DebugString(),\n                                        \", row_seq_tensor must have at least \",\n                                        out_rows + 1, \" elements, but got \",\n                                        row_seq_tensor.NumElements()));\n    OP_REQUIRES(context, col_seq_tensor.NumElements() > out_cols,\n                errors::InvalidArgument(\"Given out_backprop shape \",\n                                        out_backprop.shape().DebugString(),\n                                        \", col_seq_tensor must have at least \",\n                                        out_cols + 1, \" elements, but got \",\n                                        col_seq_tensor.NumElements()));\n\n    auto row_seq_tensor_flat = row_seq_tensor.flat<int64>();\n    auto col_seq_tensor_flat = col_seq_tensor.flat<int64>();\n    auto orig_input_tensor_shape_flat = orig_input_tensor_shape.flat<int64>();\n", "project": "tensorflow", "hash": 41801048919539989826079311137357489877, "size": 129, "commit_id": "12c727cee857fa19be717f336943d95fca4ffe4f", "message": "Validate inputs of `FractionalAvgPoolGrad`.\n\nPiperOrigin-RevId: 372420640\nChange-Id: Icc583928e6cdc3062e12498e4d2337a8fe3da016", "target": 0, "dataset": "other", "idx": 253652}
{"func": "\tuint32_t tag;\n\tint count = 0;\n\tint start = -1;\n\tint depth = -1;\n\tint want = 0;\n\tint base = fdt_off_dt_struct(fdt);\n\n\tend = path;\n\t*end = '\\0';\n\tdo {\n\t\tconst struct fdt_property *prop;\n\t\tint offset;\n\t\tint len;\n\n\t\toffset = nextoffset;\n\t\ttag = fdt_next_tag(fdt, offset, &nextoffset);\n\t\tstop_at = nextoffset;\n\n\t\tswitch (tag) {\n\t\tcase FDT_PROP:\n\t\t\tinclude = want >= 2;\n\t\t\tstop_at = offset;\n\n\t\tcase FDT_BEGIN_NODE:\n\t\t\tdepth++;\n\t\t\tif (depth == FDT_MAX_DEPTH)\n\t\t\t\treturn -FDT_ERR_BADSTRUCTURE;\n\t\t\tname = fdt_get_name(fdt, offset, &len);\n\t\t\tif (end - path + 2 + len >= path_len)\n\t\t\t\treturn -FDT_ERR_NOSPACE;\n\t\t\tif (end != path + 1)\n\t\t\t\t*end++ = '/';\n\t\t\tstrcpy(end, name);\n\t\t\t\treturn -FDT_ERR_BADSTRUCTURE;\n\t\t\tinclude = want;\n\t\t\twant = stack[depth--];\n\t\t\twhile (end > path && *--end != '/')\n\t\t\t\t;\n\t\t\t*end = '\\0';\n\t\t\tbreak;\n\n\t\tcase FDT_END:\n\t\t\tinclude = 1;\n\t\t\tbreak;", "project": "u-boot", "hash": 43916771152843850774133375533513142058, "size": 120, "commit_id": "8a7d4cf9820ea16fabd25a6379351b4dc291204b", "message": "fdt_region: Check for a single root node of the correct name\n\nAt present fdt_find_regions() assumes that the FIT is a valid devicetree.\nIf the FIT has two root nodes this is currently not detected in this\nfunction, nor does libfdt's fdt_check_full() notice. Also it is possible\nfor the root node to have a name even though it should not.\n\nAdd checks for these and return -FDT_ERR_BADSTRUCTURE if a problem is\ndetected.\n\nCVE-2021-27097\n\nSigned-off-by: Simon Glass <sjg@chromium.org>\nReported-by: Bruce Monroe <bruce.monroe@intel.com>\nReported-by: Arie Haenel <arie.haenel@intel.com>\nReported-by: Julien Lenoir <julien.lenoir@intel.com>", "target": 1, "dataset": "other", "idx": 197162}
{"func": "\tint count = 0;\n\tint start = -1;\n\tint depth = -1;\n\tint want = 0;\n\tint base = fdt_off_dt_struct(fdt);\n\tbool expect_end = false;\n\n\tend = path;\n\t*end = '\\0';\n\tdo {\n\t\tconst struct fdt_property *prop;\n\t\tint len;\n\n\t\toffset = nextoffset;\n\t\ttag = fdt_next_tag(fdt, offset, &nextoffset);\n\t\tstop_at = nextoffset;\n\n\t\t/* If we see two root nodes, something is wrong */\n\t\tif (expect_end && tag != FDT_END)\n\t\t\treturn -FDT_ERR_BADLAYOUT;\n\n\t\tswitch (tag) {\n\t\tcase FDT_PROP:\n\t\t\tinclude = want >= 2;\n\t\t\tstop_at = offset;\n\t\tcase FDT_BEGIN_NODE:\n\t\t\tdepth++;\n\t\t\tif (depth == FDT_MAX_DEPTH)\n\t\t\t\treturn -FDT_ERR_BADSTRUCTURE;\n\t\t\tname = fdt_get_name(fdt, offset, &len);\n\n\t\t\t/* The root node must have an empty name */\n\t\t\tif (!depth && *name)\n\t\t\t\treturn -FDT_ERR_BADLAYOUT;\n\t\t\tif (end - path + 2 + len >= path_len)\n\t\t\t\treturn -FDT_ERR_NOSPACE;\n\t\t\tif (end != path + 1)\n\t\t\t\t*end++ = '/';\n\t\t\tstrcpy(end, name);\n\t\t\tinclude = want;\n\t\t\twant = stack[depth--];\n\t\t\twhile (end > path && *--end != '/')\n\t\t\t\t;\n\t\t\t*end = '\\0';\n\t\t\tif (depth == -1)\n\t\t\t\texpect_end = true;\n\t\t\tbreak;\n\n\t\tcase FDT_END:\n\t\t\tinclude = 1;\n\t\t\tbreak;", "project": "u-boot", "hash": 238752706778527509335520424068219469599, "size": 131, "commit_id": "8a7d4cf9820ea16fabd25a6379351b4dc291204b", "message": "fdt_region: Check for a single root node of the correct name\n\nAt present fdt_find_regions() assumes that the FIT is a valid devicetree.\nIf the FIT has two root nodes this is currently not detected in this\nfunction, nor does libfdt's fdt_check_full() notice. Also it is possible\nfor the root node to have a name even though it should not.\n\nAdd checks for these and return -FDT_ERR_BADSTRUCTURE if a problem is\ndetected.\n\nCVE-2021-27097\n\nSigned-off-by: Simon Glass <sjg@chromium.org>\nReported-by: Bruce Monroe <bruce.monroe@intel.com>\nReported-by: Arie Haenel <arie.haenel@intel.com>\nReported-by: Julien Lenoir <julien.lenoir@intel.com>", "target": 0, "dataset": "other", "idx": 254234}
{"func": "     * invalid data. */\n    if (!pb_field_iter_find(&old_field, old_tag))\n        PB_RETURN_ERROR(stream, \"invalid union tag\");\n\n    pb_release_single_field(&old_field);\n\n    return true;\n}", "project": "nanopb", "hash": 112412675508427426387109317948700128093, "size": 21, "commit_id": "e2f0ccf939d9f82931d085acb6df8e9a182a4261", "message": "Fix invalid free() with oneof (#647)\n\nNanopb would call free() or realloc() on an invalid\n(attacker controlled) pointer value when all the following\nconditions are true:\n\n- PB_ENABLE_MALLOC is defined at the compile time\n- Message definition contains an oneof field, and the oneof\n  contains at least one pointer type field and at least one\n  non-pointer type field.\n- Data being decoded first contains a non-pointer value for\n  the oneof field, and later contains an overwriting pointer\n  value.\n\nDepending on message layout, the bug may not be exploitable in all\ncases, but it is known to be exploitable at least with string and\nbytes fields. Actual security impact will also depend on the heap\nimplementation used.", "target": 1, "dataset": "other", "idx": 197172}
{"func": "    if (!pb_field_iter_find(&old_field, old_tag))\n        PB_RETURN_ERROR(stream, \"invalid union tag\");\n\n    pb_release_single_field(&old_field);\n\n    if (PB_ATYPE(field->type) == PB_ATYPE_POINTER)\n    {\n        /* Initialize the pointer to NULL to make sure it is valid\n         * even in case of error return. */\n        *(void**)field->pField = NULL;\n        field->pData = NULL;\n    }\n\n    return true;\n}", "project": "nanopb", "hash": 141020607398662900410401211462596595087, "size": 29, "commit_id": "e2f0ccf939d9f82931d085acb6df8e9a182a4261", "message": "Fix invalid free() with oneof (#647)\n\nNanopb would call free() or realloc() on an invalid\n(attacker controlled) pointer value when all the following\nconditions are true:\n\n- PB_ENABLE_MALLOC is defined at the compile time\n- Message definition contains an oneof field, and the oneof\n  contains at least one pointer type field and at least one\n  non-pointer type field.\n- Data being decoded first contains a non-pointer value for\n  the oneof field, and later contains an overwriting pointer\n  value.\n\nDepending on message layout, the bug may not be exploitable in all\ncases, but it is known to be exploitable at least with string and\nbytes fields. Actual security impact will also depend on the heap\nimplementation used.", "target": 0, "dataset": "other", "idx": 254699}
{"func": "\n    /* H323  */\n    if(packet->payload_packet_len >= 4\n       && (packet->payload[0] == 0x03)\n       && (packet->payload[1] == 0x00)) {\n\tstruct tpkt *t = (struct tpkt*)packet->payload;\n\tu_int16_t len = ntohs(t->len);\n\n\tif(packet->payload_packet_len == len) {\n\t  /*\n\t    We need to check if this packet is in reality\n\t    a RDP (Remote Desktop) packet encapsulated on TPTK\n\t   */\n\n\t  if(packet->payload[4] == (packet->payload_packet_len - sizeof(struct tpkt) - 1)) {\n\t    /* ISO 8073/X.224 */\n\t    if((packet->payload[5] == 0xE0 /* CC Connect Request */)\n\t       || (packet->payload[5] == 0xD0 /* CC Connect Confirm */)) {\n\t      NDPI_LOG_INFO(ndpi_struct, \"found RDP\\n\");\n\t      ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_RDP, NDPI_PROTOCOL_UNKNOWN);\n\t      return;\n\t    }\n\t  }\n\n\t  flow->l4.tcp.h323_valid_packets++;\n\n\t  if(flow->l4.tcp.h323_valid_packets >= 2) {\n\t    NDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\t    ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\t  }\n\t} else {\n\t  /* This is not H.323 */\n\t  NDPI_EXCLUDE_PROTO(ndpi_struct, flow);\n\t  return;\n\t}\n      }\n  } else if(packet->udp != NULL) {\n    sport = ntohs(packet->udp->source), dport = ntohs(packet->udp->dest);\n    NDPI_LOG_DBG2(ndpi_struct, \"calculated dport over udp\\n\");\n\n    if(packet->payload_packet_len >= 6 && packet->payload[0] == 0x80 && packet->payload[1] == 0x08 &&\n\tNDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\tndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\treturn;\n      }\n    /* H323  */\n    if(sport == 1719 || dport == 1719)\n      {\n        if(packet->payload[0] == 0x16 && packet->payload[1] == 0x80 && packet->payload[4] == 0x06 && packet->payload[5] == 0x00)\n\t  {\n\t    NDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\t    ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\t    return;\n\t  }\n        else if(packet->payload_packet_len >= 20 && packet->payload_packet_len <= 117)\n\t  {\n\t    NDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\t    ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\t    return;\n\t  }\n        else\n\t  {\n\t    NDPI_EXCLUDE_PROTO(ndpi_struct, flow);\n\t    return;\n\t  }\n      }\n  }\n\n}", "project": "nDPI", "hash": 239776547736060222158186732995430716218, "size": 85, "commit_id": "b7e666e465f138ae48ab81976726e67deed12701", "message": "Added fix to avoid potential heap buffer overflow in H.323 dissector\nModified HTTP report information to make it closer to the HTTP field names", "target": 1, "dataset": "other", "idx": 197178}
{"func": "\n    /* H323  */\n    if(packet->payload_packet_len >= 4\n       && (packet->payload[0] == 0x03)\n       && (packet->payload[1] == 0x00)) {\n      struct tpkt *t = (struct tpkt*)packet->payload;\n      u_int16_t len = ntohs(t->len);\n\n      if(packet->payload_packet_len == len) {\n\t/*\n\t  We need to check if this packet is in reality\n\t  a RDP (Remote Desktop) packet encapsulated on TPTK\n\t*/\n\n\tif(packet->payload[4] == (packet->payload_packet_len - sizeof(struct tpkt) - 1)) {\n\t  /* ISO 8073/X.224 */\n\t  if((packet->payload[5] == 0xE0 /* CC Connect Request */)\n\t     || (packet->payload[5] == 0xD0 /* CC Connect Confirm */)) {\n\t    NDPI_LOG_INFO(ndpi_struct, \"found RDP\\n\");\n\t    ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_RDP, NDPI_PROTOCOL_UNKNOWN);\n\t    return;\n\t  }\n\t}\n\n\tflow->l4.tcp.h323_valid_packets++;\n\n\tif(flow->l4.tcp.h323_valid_packets >= 2) {\n\t  NDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\t  ndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\t}\n      } else {\n\t/* This is not H.323 */\n\tNDPI_EXCLUDE_PROTO(ndpi_struct, flow);\n\treturn;\n      }\n    }\n  } else if(packet->udp != NULL) {\n    sport = ntohs(packet->udp->source), dport = ntohs(packet->udp->dest);\n    NDPI_LOG_DBG2(ndpi_struct, \"calculated dport over udp\\n\");\n\n    if(packet->payload_packet_len >= 6 && packet->payload[0] == 0x80 && packet->payload[1] == 0x08 &&\n\tNDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\tndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\treturn;\n      }\n    /* H323  */\n    if(sport == 1719 || dport == 1719) {\n      if((packet->payload_packet_len >= 5)\n\t && (packet->payload[0] == 0x16)\n\t && (packet->payload[1] == 0x80)\n\t && (packet->payload[4] == 0x06)\n\t && (packet->payload[5] == 0x00)) {\n\tNDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\tndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\treturn;\n      } else if(packet->payload_packet_len >= 20 && packet->payload_packet_len <= 117) {\n\tNDPI_LOG_INFO(ndpi_struct, \"found H323 broadcast\\n\");\n\tndpi_set_detected_protocol(ndpi_struct, flow, NDPI_PROTOCOL_H323, NDPI_PROTOCOL_UNKNOWN);\n\treturn;\n      } else {\n\tNDPI_EXCLUDE_PROTO(ndpi_struct, flow);\n\treturn;\n      }\n    }\n  }\n}", "project": "nDPI", "hash": 333094768327408050308949731073671429220, "size": 82, "commit_id": "b7e666e465f138ae48ab81976726e67deed12701", "message": "Added fix to avoid potential heap buffer overflow in H.323 dissector\nModified HTTP report information to make it closer to the HTTP field names", "target": 0, "dataset": "other", "idx": 254869}
{"func": "  int feature_dim = GetTensorFeatureDimIndex(num_dims, data_format);\n  dims->in_depth = input_shape.dim_size(feature_dim);\n  // The input and output feature dimensions are the second last and last\n  // dimensions of the filter Tensor.\n  VLOG(2) << \"input vs filter_in depth \" << dims->in_depth << \" \"\n          << filter_shape.dim_size(num_dims - 2);\n  if (dims->in_depth % filter_shape.dim_size(num_dims - 2)) {\n    return errors::InvalidArgument(\n        label, \": input depth must be evenly divisible by filter depth\");\n  }\n  dims->out_depth = filter_shape.dim_size(num_dims - 1);", "project": "tensorflow", "hash": 320799004957211890798176933281449360794, "size": 60, "commit_id": "fca9874a9b42a2134f907d2fb46ab774a831404a", "message": "Prevent another division by zero.\n\nPiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2", "target": 1, "dataset": "other", "idx": 197183}
{"func": "  dims->in_depth = input_shape.dim_size(feature_dim);\n  // The input and output feature dimensions are the second last and last\n  // dimensions of the filter Tensor.\n  VLOG(2) << \"input vs filter_in depth \" << dims->in_depth << \" \"\n          << filter_shape.dim_size(num_dims - 2);\n  if (filter_shape.dim_size(num_dims - 2) <= 0) {\n    return errors ::InvalidArgument(\n        label, \": filter depth must be strictly greated than zero\");\n  }\n  if (dims->in_depth % filter_shape.dim_size(num_dims - 2)) {\n    return errors::InvalidArgument(\n        label, \": input depth must be evenly divisible by filter depth\");\n  }\n  dims->out_depth = filter_shape.dim_size(num_dims - 1);", "project": "tensorflow", "hash": 287077448679940441968460163872429604572, "size": 64, "commit_id": "fca9874a9b42a2134f907d2fb46ab774a831404a", "message": "Prevent another division by zero.\n\nPiperOrigin-RevId: 369338598\nChange-Id: I55471d363e401fdcf8d259670ad4eef672b731e2", "target": 0, "dataset": "other", "idx": 255022}
{"func": "\t\t\t\t}\n\t\t\t}\n\t\t\tif(GetType() == MOD_TYPE_IT)\n\t\t\t{\n\t\t\t\t// IT pattern loop start row update - at the end of a pattern loop, set pattern loop start to next row (for upcoming pattern loops with missing SB0)\n\t\t\t\tfor(CHANNELINDEX nChn = 0; nChn < GetNumChannels(); nChn++)\n\t\t\t\t{\n\t\t\t\t\tif((pChn->rowCommand.command == CMD_S3MCMDEX && pChn->rowCommand.param >= 0xB1 && pChn->rowCommand.param <= 0xBF))\n\t\t\t\t\t{\n\t\t\t\t\t\tmemory.chnSettings[nChn].patLoop = memory.elapsedTime;\n\t\t\t\t\t\tmemory.chnSettings[nChn].patLoopSmp = playState.m_lTotalSampleCount;", "project": "openmpt", "hash": 248561327453942912572700036302541191491, "size": 1079, "commit_id": "7ebf02af2e90f03e0dbd0e18b8b3164f372fb97c", "message": "[Fix] Possible out-of-bounds read when computing length of some IT files with pattern loops (OpenMPT: formats that are converted to IT, libopenmpt: IT/ITP/MO3), caught with afl-fuzz.\n\ngit-svn-id: https://source.openmpt.org/svn/openmpt/trunk/OpenMPT@10027 56274372-70c3-4bfc-bfc3-4c3a0b034d27", "target": 1, "dataset": "other", "idx": 197202}
{"func": "\t\t\t\t}\n\t\t\t}\n\t\t\tif(GetType() == MOD_TYPE_IT)\n\t\t\t{\n\t\t\t\t// IT pattern loop start row update - at the end of a pattern loop, set pattern loop start to next row (for upcoming pattern loops with missing SB0)\n\t\t\t\tpChn = playState.Chn;\n\t\t\t\tfor(CHANNELINDEX nChn = 0; nChn < GetNumChannels(); nChn++, pChn++)\n\t\t\t\t{\n\t\t\t\t\tif((pChn->rowCommand.command == CMD_S3MCMDEX && pChn->rowCommand.param >= 0xB1 && pChn->rowCommand.param <= 0xBF))\n\t\t\t\t\t{\n\t\t\t\t\t\tmemory.chnSettings[nChn].patLoop = memory.elapsedTime;\n\t\t\t\t\t\tmemory.chnSettings[nChn].patLoopSmp = playState.m_lTotalSampleCount;", "project": "openmpt", "hash": 82247471402509974896295465140162503884, "size": 1080, "commit_id": "7ebf02af2e90f03e0dbd0e18b8b3164f372fb97c", "message": "[Fix] Possible out-of-bounds read when computing length of some IT files with pattern loops (OpenMPT: formats that are converted to IT, libopenmpt: IT/ITP/MO3), caught with afl-fuzz.\n\ngit-svn-id: https://source.openmpt.org/svn/openmpt/trunk/OpenMPT@10027 56274372-70c3-4bfc-bfc3-4c3a0b034d27", "target": 0, "dataset": "other", "idx": 255181}
{"func": "            {\n                /* Normal repeated field, i.e. only one item at a time. */\n                size_t *size = (size_t*)iter->pSize;\n                void *pItem;\n                \n                (*size)++;\n                if (!allocate_field(stream, iter->pData, iter->pos->data_size, *size))\n                    return false;\n            \n                pItem = *(uint8_t**)iter->pData + iter->pos->data_size * (*size - 1);\n                initialize_pointer_field(pItem, iter);\n                return func(stream, iter->pos, pItem);\n            }\n            \n        default:", "project": "nanopb", "hash": 44925250467000600069522886101534959296, "size": 102, "commit_id": "7b396821ddd06df8e39143f16e1dc0a4645b89a3", "message": "Fix invalid free() after failed realloc() (GHSA-gcx3-7m76-287p)", "target": 1, "dataset": "other", "idx": 197204}
{"func": "            {\n                /* Normal repeated field, i.e. only one item at a time. */\n                size_t *size = (size_t*)iter->pSize;\n                void *pItem;\n                \n                if (!allocate_field(stream, iter->pData, iter->pos->data_size, (size_t)(*size + 1)))\n                    return false;\n            \n                pItem = *(uint8_t**)iter->pData + iter->pos->data_size * (*size);\n                (*size)++;\n                initialize_pointer_field(pItem, iter);\n                return func(stream, iter->pos, pItem);\n            }\n            \n        default:", "project": "nanopb", "hash": 123490975432721035768232801290650758429, "size": 102, "commit_id": "7b396821ddd06df8e39143f16e1dc0a4645b89a3", "message": "Fix invalid free() after failed realloc() (GHSA-gcx3-7m76-287p)", "target": 0, "dataset": "other", "idx": 255326}
{"func": "\tu8 *start;\n\tBool final_flush=GF_FALSE;\n\tu32 pck_size, remain, prev_pck_size;\n\tu64 cts = GF_FILTER_NO_TS;\n\tFLACHeader hdr;\n\n\t//always reparse duration\n\tif (!ctx->duration.num)\n\t\tflac_dmx_check_dur(filter, ctx);\n\n\tif (ctx->opid && !ctx->is_playing)\n\t\t\tu32 dsi_end = 0;\n\t\t\t//we have a header\n\t\t\tgf_bs_reassign_buffer(ctx->bs, ctx->flac_buffer, size);\n\t\t\tu32 magic = gf_bs_read_u32(ctx->bs);\n\t\t\tif (magic != GF_4CC('f','L','a','C')) {\n\n\t\t\t}\n\t\t\twhile (gf_bs_available(ctx->bs)) {\n\t\t\t\tBool last = gf_bs_read_int(ctx->bs, 1);\n\t\t\t\tu32 type = gf_bs_read_int(ctx->bs, 7);\n\t\t\t\tu32 len = gf_bs_read_int(ctx->bs, 24);\n\t\t\t\t\t//ignore the rest for now\n\t\t\t\t\t//TODO: expose metadata, pictures and co\n\t\t\t\t\tgf_bs_skip_bytes(ctx->bs, len);\n\t\t\t\t}\n\t\t\t\tif (last) break;\n\t\t\t}\n\t\t\tflac_dmx_check_pid(filter, ctx, ctx->flac_buffer+4, dsi_end-4);\n\t\t\tremain -= size;\n\t\t\tstart += size;\n\t\t\tctx->initialized = GF_TRUE;\n\t\t\tif (!ctx->is_playing) break;", "project": "gpac", "hash": 181202795714431631737649125597649664622, "size": 226, "commit_id": "da69ad1f970a7e17c865eaec9af98cc84df10d5b", "message": "fixed 1718", "target": 1, "dataset": "other", "idx": 197240}
{"func": "\tBool final_flush=GF_FALSE;\n\tu32 pck_size, remain, prev_pck_size;\n\tu64 cts = GF_FILTER_NO_TS;\n\tFLACHeader hdr;\n\n\tif (ctx->in_error)\n\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\n\t//always reparse duration\n\tif (!ctx->duration.num)\n\t\tflac_dmx_check_dur(filter, ctx);\n\n\tif (ctx->opid && !ctx->is_playing)\n\t\t\tu32 dsi_end = 0;\n\t\t\t//we have a header\n\t\t\tgf_bs_reassign_buffer(ctx->bs, ctx->flac_buffer, size);\n\t\t\tu32 magic = gf_bs_read_u32(ctx->bs);\n\t\t\tif (magic != GF_4CC('f','L','a','C')) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_PARSER, (\"[FLACDmx] invalid FLAC magic\\n\"));\n\t\t\t\tctx->in_error = GF_TRUE;\n\t\t\t\tctx->flac_buffer_size = 0;\n\t\t\t\tif (pck)\n\t\t\t\t\tgf_filter_pid_drop_packet(ctx->ipid);\n\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t}\n\t\t\twhile (gf_bs_available(ctx->bs)) {\n\t\t\t\tBool last = gf_bs_read_int(ctx->bs, 1);\n\t\t\t\tu32 type = gf_bs_read_int(ctx->bs, 7);\n\t\t\t\tu32 len = gf_bs_read_int(ctx->bs, 24);\n\t\t\t\t\t//TODO: expose metadata, pictures and co\n\t\t\t\t\tgf_bs_skip_bytes(ctx->bs, len);\n\t\t\t\t}\n\t\t\t\tif (last) break;\n\t\t\t}\n\t\t\tif (!dsi_end) {\n\t\t\t\tGF_LOG(GF_LOG_ERROR, GF_LOG_PARSER, (\"[FLACDmx] invalid FLAC header\\n\"));\n\t\t\t\tctx->in_error = GF_TRUE;\n\t\t\t\tctx->flac_buffer_size = 0;\n\t\t\t\tif (pck)\n\t\t\t\t\tgf_filter_pid_drop_packet(ctx->ipid);\n\t\t\t\treturn GF_NON_COMPLIANT_BITSTREAM;\n\t\t\t}\n\t\t\tflac_dmx_check_pid(filter, ctx, ctx->flac_buffer+4, dsi_end-4);\n\t\t\tremain -= size;\n\t\t\tstart += size;\n\t\t\tctx->initialized = GF_TRUE;\n\t\t\tif (!ctx->is_playing) break;", "project": "gpac", "hash": 45890451895495219456520744515811279483, "size": 242, "commit_id": "da69ad1f970a7e17c865eaec9af98cc84df10d5b", "message": "fixed 1718", "target": 0, "dataset": "other", "idx": 255791}
{"func": "              if (strcasecmp (arg, \"%f\") == 0)\n                g_string_append_printf (new_exec, \" @@ %s @@\", arg);\n              else if (strcasecmp (arg, \"%u\") == 0)\n                g_string_append_printf (new_exec, \" @@u %s @@\", arg);\n              else if (g_str_has_prefix (arg, \"@@\"))\n                g_print (_(\"Skipping invalid Exec argument %s\\n\"), arg);\n              else\n                g_string_append_printf (new_exec, \" %s\", arg);\n            }\n        }\n      else", "project": "flatpak", "hash": 158761630201972013011166444905331694845, "size": 225, "commit_id": "a7401e638bf0c03102039e216ab1081922f140ae", "message": "dir: Refuse to export .desktop files with suspicious uses of @@ tokens\n\nThis is either a malicious/compromised app trying to do an attack, or\na mistake that will break handling of %f, %u and so on. Either way,\nif we refuse to export the .desktop file, resulting in installation\nfailing, then it makes the rejection more obvious than quietly\nremoving the magic tokens.\n\nSigned-off-by: Simon McVittie <smcv@collabora.com>\n(cherry picked from commit 46b3ede5241561c7d588951048c687c5075a3eac)", "target": 1, "dataset": "other", "idx": 197260}
{"func": "              if (strcasecmp (arg, \"%f\") == 0)\n                g_string_append_printf (new_exec, \" @@ %s @@\", arg);\n              else if (strcasecmp (arg, \"%u\") == 0)\n                g_string_append_printf (new_exec, \" @@u %s @@\", arg);\n              else if (g_str_has_prefix (arg, \"@@\"))\n                {\n                  flatpak_fail_error (error, FLATPAK_ERROR_EXPORT_FAILED,\n                                     _(\"Invalid Exec argument %s\"), arg);\n                  goto out;\n                }\n              else\n                g_string_append_printf (new_exec, \" %s\", arg);\n            }\n        }\n      else", "project": "flatpak", "hash": 177220367868311421112771074203606020515, "size": 229, "commit_id": "a7401e638bf0c03102039e216ab1081922f140ae", "message": "dir: Refuse to export .desktop files with suspicious uses of @@ tokens\n\nThis is either a malicious/compromised app trying to do an attack, or\na mistake that will break handling of %f, %u and so on. Either way,\nif we refuse to export the .desktop file, resulting in installation\nfailing, then it makes the rejection more obvious than quietly\nremoving the magic tokens.\n\nSigned-off-by: Simon McVittie <smcv@collabora.com>\n(cherry picked from commit 46b3ede5241561c7d588951048c687c5075a3eac)", "target": 0, "dataset": "other", "idx": 256099}
{"func": "    // For a single batch, the batch_ptrs are {0, total_nnz}.\n    batch_ptr(0) = 0;\n    ++prev_batch;\n\n    for (int64 i = 0; i < total_nnz; ++i) {\n      // For now, the rows pointers store the corresponding row counts.\n      csr_row_ptr(indices(i, 0) + 1) += 1;\n      csr_col_ind(i) = indices(i, 1);\n    }\n  } else {  // rank == 3\n    for (int64 i = 0; i < total_nnz; ++i) {", "project": "tensorflow", "hash": 11864789206738985824119536060770076338, "size": 69, "commit_id": "1e922ccdf6bf46a3a52641f99fd47d54c1decd13", "message": "Fix crash in `SparseTensorToCSRSparseMatrixCPUFunctor`\n\nPiperOrigin-RevId: 370110290\nChange-Id: I4451e92661a55c2180f80d38b67a9b50bf5edec5", "target": 1, "dataset": "other", "idx": 197301}
{"func": "    batch_ptr(0) = 0;\n    ++prev_batch;\n\n    for (int64 i = 0; i < total_nnz; ++i) {\n      // For now, the rows pointers store the corresponding row counts.\n      int64 ix = indices(i, 0) + 1;\n      if (ix >= csr_row_ptr.size()) {\n        return errors::InvalidArgument(\"Got an index \", ix,\n                                       \" that is outside of csr_row_ptr\");\n      }\n      csr_row_ptr(indices(i, 0) + 1) += 1;\n      csr_col_ind(i) = indices(i, 1);\n    }\n  } else {  // rank == 3\n    for (int64 i = 0; i < total_nnz; ++i) {", "project": "tensorflow", "hash": 184237572670370834113480134201938058069, "size": 74, "commit_id": "1e922ccdf6bf46a3a52641f99fd47d54c1decd13", "message": "Fix crash in `SparseTensorToCSRSparseMatrixCPUFunctor`\n\nPiperOrigin-RevId: 370110290\nChange-Id: I4451e92661a55c2180f80d38b67a9b50bf5edec5", "target": 0, "dataset": "other", "idx": 256387}
{"func": "TfLiteStatus EvalSimple(TfLiteContext* context, TfLiteNode* node,\n                        const TfLiteTensor* lookup, const TfLiteTensor* value,\n                        TfLiteTensor* output) {\n  const int row_size = SizeOfDimension(value, 0);\n  const int row_bytes = value->bytes / row_size;\n\n  char* output_raw = GetTensorData<char>(output);\n  const char* value_raw = GetTensorData<char>(value);\n  const int32_t* lookup_data = GetTensorData<int32_t>(lookup);", "project": "tensorflow", "hash": 95188914225004025743598909042817406240, "size": 25, "commit_id": "f61c57bd425878be108ec787f4d96390579fb83e", "message": "Prevent division by 0\n\nPiperOrigin-RevId: 370966645\nChange-Id: I831bfd96c7eb77b02d7ebb744335f59f6e5728cb", "target": 1, "dataset": "other", "idx": 197303}
{"func": "TfLiteStatus EvalSimple(TfLiteContext* context, TfLiteNode* node,\n                        const TfLiteTensor* lookup, const TfLiteTensor* value,\n                        TfLiteTensor* output) {\n  const int row_size = SizeOfDimension(value, 0);\n  if (row_size == 0) {\n    // Propagate empty tensor if input is empty\n    return kTfLiteOk;\n  }\n  const int row_bytes = value->bytes / row_size;\n\n  char* output_raw = GetTensorData<char>(output);\n  const char* value_raw = GetTensorData<char>(value);\n  const int32_t* lookup_data = GetTensorData<int32_t>(lookup);", "project": "tensorflow", "hash": 246624855501188183765525294917134828445, "size": 29, "commit_id": "f61c57bd425878be108ec787f4d96390579fb83e", "message": "Prevent division by 0\n\nPiperOrigin-RevId: 370966645\nChange-Id: I831bfd96c7eb77b02d7ebb744335f59f6e5728cb", "target": 0, "dataset": "other", "idx": 256405}
{"func": "                }\n            }\n\n            /*  forw_ref */\n            if (output_ins.opcode == I_EQU) {\n                if (!output_ins.label)\n                    nasm_error(ERR_NONFATAL,\n                               \"EQU not preceded by label\");\n\n                if (output_ins.operands == 1 &&\n                    (output_ins.oprs[0].type & IMMEDIATE) &&\n                    output_ins.oprs[0].wrt == NO_SEG) {\n                    define_label(output_ins.label,\n                                 output_ins.oprs[0].segment,\n                                 output_ins.oprs[0].offset, false);\n                } else if (output_ins.operands == 2\n                           && (output_ins.oprs[0].type & IMMEDIATE)", "project": "nasm", "hash": 308312917854211633508209765710891290112, "size": 303, "commit_id": "e996d28c70d45008085322b442b44a9224308548", "message": "labels: Don't nil dereference if no label provided\n\nAn equ without label may cause nil dereference\n\n |\tequ 0x100\n\nFixes 98578071b9d71ecaa2344dd9c185237c1765041e\n\nSigned-off-by: Cyrill Gorcunov <gorcunov@gmail.com>", "target": 1, "dataset": "other", "idx": 197360}
{"func": "                }\n            }\n\n            /*  forw_ref */\n            if (output_ins.opcode == I_EQU) {\n                if (!output_ins.label) {\n                    nasm_error(ERR_NONFATAL, \"EQU not preceded by label\");\n                } else if (output_ins.operands == 1 &&\n                           (output_ins.oprs[0].type & IMMEDIATE) &&\n                           output_ins.oprs[0].wrt == NO_SEG) {\n                    define_label(output_ins.label,\n                                 output_ins.oprs[0].segment,\n                                 output_ins.oprs[0].offset, false);\n                } else if (output_ins.operands == 2\n                           && (output_ins.oprs[0].type & IMMEDIATE)", "project": "nasm", "hash": 217297003284158501046538109816031999494, "size": 301, "commit_id": "e996d28c70d45008085322b442b44a9224308548", "message": "labels: Don't nil dereference if no label provided\n\nAn equ without label may cause nil dereference\n\n |\tequ 0x100\n\nFixes 98578071b9d71ecaa2344dd9c185237c1765041e\n\nSigned-off-by: Cyrill Gorcunov <gorcunov@gmail.com>", "target": 0, "dataset": "other", "idx": 257477}
{"func": "\n\tok = cli_credentials_set_conf(server_credentials, kdc->task->lp_ctx);\n\tif (!ok) {\n\t\tgoto done;\n\t}\n\n\tok = cli_credentials_set_username(server_credentials,\n\t\t\t\t\t  \"kadmin/changepw\",\n\t\t\t\t\t  CRED_SPECIFIED);\n\tif (!ok) {\n\t\tgoto done;\n\t}\n\n\trv = cli_credentials_set_keytab_name(server_credentials,\n\t\t\t\t\t     kdc->task->lp_ctx,\n\t\t\t\t\t     kdc->kpasswd_keytab_name,", "project": "samba", "hash": 291020563554827357122154858202521821517, "size": 324, "commit_id": "52dd9f8f835bc23415ec51dcc344478497e208c3", "message": "CVE-2022-32744 s4:kpasswd: Ensure we pass the kpasswd server principal into krb5_rd_req_ctx()\n\nTo ensure that, when decrypting the kpasswd ticket, we look up the\ncorrect principal and don't trust the sname from the ticket, we should\npass the principal name of the kpasswd service into krb5_rd_req_ctx().\nHowever, gensec_krb5_update_internal() will pass in NULL unless the\nprincipal in our credentials is CRED_SPECIFIED.\n\nAt present, our principal will be considered obtained as CRED_SMB_CONF\n(from the cli_credentials_set_conf() a few lines up), so we explicitly\nset the realm again, but this time as CRED_SPECIFIED. Now the value of\nserver_in_keytab that we provide to smb_krb5_rd_req_decoded() will not\nbe NULL.\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=15074\n\nSigned-off-by: Joseph Sutton <josephsutton@catalyst.net.nz>\nReviewed-by: Andreas Schneider <asn@samba.org>", "target": 1, "dataset": "other", "idx": 197396}
{"func": "\tok = cli_credentials_set_conf(server_credentials, kdc->task->lp_ctx);\n\tif (!ok) {\n\t\tgoto done;\n\t}\n\n\t/*\n\t * After calling cli_credentials_set_conf(), explicitly set the realm\n\t * with CRED_SPECIFIED. We need to do this so the result of\n\t * principal_from_credentials() called from the gensec layer is\n\t * CRED_SPECIFIED rather than CRED_SMB_CONF, avoiding a fallback to\n\t * match-by-key (very undesirable in this case).\n\t */\n\tok = cli_credentials_set_realm(server_credentials,\n\t\t\t\t       lpcfg_realm(kdc->task->lp_ctx),\n\t\t\t\t       CRED_SPECIFIED);\n\tif (!ok) {\n\t\tgoto done;\n\t}\n\n\tok = cli_credentials_set_username(server_credentials,\n\t\t\t\t\t  \"kadmin/changepw\",\n\t\t\t\t\t  CRED_SPECIFIED);\n\tif (!ok) {\n\t\tgoto done;\n\t}\n\n\t/* Check that the server principal is indeed CRED_SPECIFIED. */\n\t{\n\t\tchar *principal = NULL;\n\t\tenum credentials_obtained obtained;\n\n\t\tprincipal = cli_credentials_get_principal_and_obtained(server_credentials,\n\t\t\t\t\t\t\t\t       tmp_ctx,\n\t\t\t\t\t\t\t\t       &obtained);\n\t\tif (obtained < CRED_SPECIFIED) {\n\t\t\tgoto done;\n\t\t}\n\n\t\tTALLOC_FREE(principal);\n\t}\n\n\trv = cli_credentials_set_keytab_name(server_credentials,\n\t\t\t\t\t     kdc->task->lp_ctx,\n\t\t\t\t\t     kdc->kpasswd_keytab_name,", "project": "samba", "hash": 151175540041094190578263195911591115877, "size": 353, "commit_id": "52dd9f8f835bc23415ec51dcc344478497e208c3", "message": "CVE-2022-32744 s4:kpasswd: Ensure we pass the kpasswd server principal into krb5_rd_req_ctx()\n\nTo ensure that, when decrypting the kpasswd ticket, we look up the\ncorrect principal and don't trust the sname from the ticket, we should\npass the principal name of the kpasswd service into krb5_rd_req_ctx().\nHowever, gensec_krb5_update_internal() will pass in NULL unless the\nprincipal in our credentials is CRED_SPECIFIED.\n\nAt present, our principal will be considered obtained as CRED_SMB_CONF\n(from the cli_credentials_set_conf() a few lines up), so we explicitly\nset the realm again, but this time as CRED_SPECIFIED. Now the value of\nserver_in_keytab that we provide to smb_krb5_rd_req_decoded() will not\nbe NULL.\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=15074\n\nSigned-off-by: Joseph Sutton <josephsutton@catalyst.net.nz>\nReviewed-by: Andreas Schneider <asn@samba.org>", "target": 0, "dataset": "other", "idx": 258087}
{"func": "  explicit ReverseSequenceOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"batch_dim\", &batch_dim_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seq_dim\", &seq_dim_));\n  }", "project": "tensorflow", "hash": 309225667495445257046903496583646120315, "size": 5, "commit_id": "ecf768cbe50cedc0a45ce1ee223146a3d3d26d23", "message": "Add missing validations to reverse_sequence_op\n\nPiperOrigin-RevId: 372178683\nChange-Id: Iac97ebab5b342f1262c77a7d9bcb4267b305ce5b", "target": 1, "dataset": "other", "idx": 197404}
{"func": "  explicit ReverseSequenceOp(OpKernelConstruction* context)\n      : OpKernel(context) {\n    OP_REQUIRES_OK(context, context->GetAttr(\"batch_dim\", &batch_dim_));\n    OP_REQUIRES_OK(context, context->GetAttr(\"seq_dim\", &seq_dim_));\n    OP_REQUIRES(context, batch_dim_ >= 0,\n                errors::InvalidArgument(\"Invalid batch_dim \", batch_dim_));\n    OP_REQUIRES(context, seq_dim_ >= 0,\n                errors::InvalidArgument(\"Invalid seq_dim \", seq_dim_));\n  }", "project": "tensorflow", "hash": 206808381287582228720200892092547402342, "size": 9, "commit_id": "ecf768cbe50cedc0a45ce1ee223146a3d3d26d23", "message": "Add missing validations to reverse_sequence_op\n\nPiperOrigin-RevId: 372178683\nChange-Id: Iac97ebab5b342f1262c77a7d9bcb4267b305ce5b", "target": 0, "dataset": "other", "idx": 258254}
{"func": "\t// to fix piror Incorrect use of X509_check_host\n\tif (X509_check_host(cert, tunnel->config->gateway_host,\n\t                    0, 0, NULL) == 1)\n\t\tcert_valid = 1;\n#else\n\tchar common_name[FIELD_SIZE + 1];\n\t// Use explicit Common Name check if native validation not available.\n\t// Note: this will ignore Subject Alternative Name fields.\n\tif (subj\n\t    && X509_NAME_get_text_by_NID(subj, NID_commonName, common_name,\n\t                                 FIELD_SIZE) > 0\n\t    && strncasecmp(common_name, tunnel->config->gateway_host,\n\t                   FIELD_SIZE) == 0)\n\t\tcert_valid = 1;\n#endif\n\n\t// Try to validate certificate using local PKI\n\tif (cert_valid\n\t    && SSL_get_verify_result(tunnel->ssl_handle) == X509_V_OK) {", "project": "openfortivpn", "hash": 231422686038679305050128276823575782422, "size": 94, "commit_id": "6328a070ddaab16faaf008cb9a8a62439c30f2a8", "message": "fix TLS Certificate CommonName NULL Byte Vulnerability\n\nCVE-2020-7043 TLS Certificate CommonName NULL Byte Vulnerability is fixed\nwith this commit\n\nwith #8 hostname validation for the certificate was introduced\nbut unfortunately strncasecmp() was used to compare the byte array\nagainst the expected hostname. This does not correctly treat a CN\nwhich contains a NULL byte. In order to fix this vulnerability\nthe reference implementation from iSECPartners has been included\ninto the code.", "target": 1, "dataset": "other", "idx": 197415}
{"func": "\t// to fix piror Incorrect use of X509_check_host\n\tif (X509_check_host(cert, tunnel->config->gateway_host,\n\t                    0, 0, NULL) == 1)\n\t\tcert_valid = 1;\n#else\n\t// Use validate_hostname form iSECPartners if native validation not available\n\t// in order to avoid TLS Certificate CommonName NULL Byte Vulnerability\n\tif (validate_hostname(tunnel->config->gateway_host, cert) == MatchFound)\n \t\tcert_valid = 1;\n#endif\n\n\t// Try to validate certificate using local PKI\n\tif (cert_valid\n\t    && SSL_get_verify_result(tunnel->ssl_handle) == X509_V_OK) {", "project": "openfortivpn", "hash": 135184877244989477016611417442939527322, "size": 89, "commit_id": "6328a070ddaab16faaf008cb9a8a62439c30f2a8", "message": "fix TLS Certificate CommonName NULL Byte Vulnerability\n\nCVE-2020-7043 TLS Certificate CommonName NULL Byte Vulnerability is fixed\nwith this commit\n\nwith #8 hostname validation for the certificate was introduced\nbut unfortunately strncasecmp() was used to compare the byte array\nagainst the expected hostname. This does not correctly treat a CN\nwhich contains a NULL byte. In order to fix this vulnerability\nthe reference implementation from iSECPartners has been included\ninto the code.", "target": 0, "dataset": "other", "idx": 258435}
{"func": "void CleanWriters(GF_List *writers)\n{\n\twhile (gf_list_count(writers)) {\n\t\tTrackWriter *writer = (TrackWriter*)gf_list_get(writers, 0);\n\t\tgf_isom_box_del(writer->stco);\n\t\tgf_isom_box_del((GF_Box *)writer->stsc);\n\t\tgf_free(writer);\n\t\tgf_list_rem(writers, 0);\n\t}", "project": "gpac", "hash": 165424775529354588660088197830809888026, "size": 10, "commit_id": "5aba27604d957e960d8069d85ccaf868f8a7b07a", "message": "fixed #1661", "target": 1, "dataset": "other", "idx": 197417}
{"func": "void CleanWriters(GF_List *writers)\n{\n\twhile (gf_list_count(writers)) {\n\t\tTrackWriter *writer = (TrackWriter*)gf_list_get(writers, 0);\n\t\t//in case we have an error in the middle of file write, remove our created stco and stsc from sample table\n\t\tgf_list_del_item(writer->stbl->child_boxes, writer->stco);\n\t\tgf_list_del_item(writer->stbl->child_boxes, writer->stsc);\n\t\tgf_isom_box_del(writer->stco);\n\t\tgf_isom_box_del((GF_Box *)writer->stsc);\n\t\tgf_free(writer);\n\t\tgf_list_rem(writers, 0);\n\t}", "project": "gpac", "hash": 34423418132464214395988583124871092916, "size": 13, "commit_id": "5aba27604d957e960d8069d85ccaf868f8a7b07a", "message": "fixed #1661", "target": 0, "dataset": "other", "idx": 258460}
{"func": "\t\t(*descIndex) = ent->sampleDescriptionIndex;\n\t\t(*chunkNumber) = sampleNumber;\n\t\tif (out_ent) *out_ent = ent;\n\t\tif ( stbl->ChunkOffset->type == GF_ISOM_BOX_TYPE_STCO) {\n\t\t\tstco = (GF_ChunkOffsetBox *)stbl->ChunkOffset;\n\t\t\tif (!stco->offsets) return GF_ISOM_INVALID_FILE;\n\n\t\t\t(*offset) = (u64) stco->offsets[sampleNumber - 1];\n\t\t} else {\n\t\t\tco64 = (GF_ChunkLargeOffsetBox *)stbl->ChunkOffset;\n\t\t\tif (!co64->offsets) return GF_ISOM_INVALID_FILE;\n\n\t\t\t(*offset) = co64->offsets[sampleNumber - 1];\n\t\t}\n\t\treturn GF_OK;\n\t}", "project": "gpac", "hash": 114947250168727677329220353898394132242, "size": 143, "commit_id": "2da2f68bffd51d89b1d272d22aa8cc023c1c066e", "message": "fixed #1705", "target": 1, "dataset": "other", "idx": 197433}
{"func": "\t\t(*chunkNumber) = sampleNumber;\n\t\tif (out_ent) *out_ent = ent;\n\t\tif ( stbl->ChunkOffset->type == GF_ISOM_BOX_TYPE_STCO) {\n\t\t\tstco = (GF_ChunkOffsetBox *)stbl->ChunkOffset;\n\t\t\tif (!stco->offsets) return GF_ISOM_INVALID_FILE;\n\t\t\tif (stco->nb_entries < sampleNumber) return GF_ISOM_INVALID_FILE;\n\n\t\t\t(*offset) = (u64) stco->offsets[sampleNumber - 1];\n\t\t} else {\n\t\t\tco64 = (GF_ChunkLargeOffsetBox *)stbl->ChunkOffset;\n\t\t\tif (!co64->offsets) return GF_ISOM_INVALID_FILE;\n\t\t\tif (co64->nb_entries < sampleNumber) return GF_ISOM_INVALID_FILE;\n\n\t\t\t(*offset) = co64->offsets[sampleNumber - 1];\n\t\t}\n\t\treturn GF_OK;\n\t}", "project": "gpac", "hash": 241891346465159261036739589837185271650, "size": 145, "commit_id": "2da2f68bffd51d89b1d272d22aa8cc023c1c066e", "message": "fixed #1705", "target": 0, "dataset": "other", "idx": 258553}
{"func": "        mp_info(log, \"number of files: %d\\n\", mf->nr_of_files);\n\n        goto exit_mf;\n    }\n\n    char *fname = talloc_size(mf, strlen(filename) + 32);\n\n#if HAVE_GLOB\n    if (!strchr(filename, '%')) {\n        strcpy(fname, filename);\n        if (!strchr(filename, '*'))\n        mp_info(log, \"number of files: %d\\n\", mf->nr_of_files);\n        globfree(&gg);\n        goto exit_mf;\n    }\n#endif\n\n    mp_info(log, \"search expr: %s\\n\", filename);\n\n    while (error_count < 5) {\n        sprintf(fname, filename, count++);\n        if (!mp_path_exists(fname)) {\n            error_count++;\n            mp_verbose(log, \"file not found: '%s'\\n\", fname);\n        } else {\n            mf_add(mf, fname);", "project": "mpv", "hash": 274378014879641404725355042981539405209, "size": 109, "commit_id": "d0c530919d8cd4d7a774e38ab064e0fabdae34e6", "message": "demux_mf: improve format string processing\n\nBefore this commit, the user could specify a printf format string\nwhich wasn't verified, and could result in:\n- Undefined behavior due to missing or non-matching arguments.\n- Buffer overflow due to untested result length.\n\nThe offending code was added at commit 103a9609 (2002, mplayer svn):\ngit-svn-id: svn://svn.mplayerhq.hu/mplayer/trunk@4566 b3059339-0415-0410-9bf9-f77b7e298cf2\n\nIt moved around but was not modified meaningfully until now.\n\nNow we reject all conversion specifiers at the format except %%\nand a simple subset of the valid specifiers. Also, we now use\nsnprintf to avoid buffer overflow.\n\nThe format string is provided by the user as part of mf:// URI.\n\nReport and initial patch by Stefan Schiller.\nPatch reviewed by @jeeb, @sfan5, Stefan Schiller.", "target": 1, "dataset": "other", "idx": 197465}
{"func": "        mp_info(log, \"number of files: %d\\n\", mf->nr_of_files);\n\n        goto exit_mf;\n    }\n\n    size_t fname_avail = strlen(filename) + 32;\n    char *fname = talloc_size(mf, fname_avail);\n\n#if HAVE_GLOB\n    if (!strchr(filename, '%')) {\n        strcpy(fname, filename);\n        if (!strchr(filename, '*'))\n        globfree(&gg);\n        goto exit_mf;\n    }\n#endif\n\n    // We're using arbitrary user input as printf format with 1 int argument.\n    // Any format which uses exactly 1 int argument would be valid, but for\n    // simplicity we reject all conversion specifiers except %% and simple\n    // integer specifier: %[.][NUM]d where NUM is 1-3 digits (%.d is valid)\n    const char *f = filename;\n    int MAXDIGS = 3, nspec = 0, bad_spec = 0, c;\n\n    while (nspec < 2 && (c = *f++)) {\n        if (c != '%')\n            continue;\n        if (*f != '%') {\n            nspec++;  // conversion specifier which isn't %%\n            if (*f == '.')\n                f++;\n            for (int ndig = 0; mp_isdigit(*f) && ndig < MAXDIGS; ndig++, f++)\n                /* no-op */;\n            if (*f != 'd') {\n                bad_spec++;  // not int, or beyond our validation capacity\n                break;\n            }\n        }\n        // *f is '%' or 'd'\n        f++;\n    }\n\n    // nspec==0 (zero specifiers) is rejected because fname wouldn't advance.\n    if (bad_spec || nspec != 1) {\n        mp_err(log, \"unsupported expr format: '%s'\\n\", filename);\n        goto exit_mf;\n    }\n\n    mp_info(log, \"search expr: %s\\n\", filename);\n\n    while (error_count < 5) {\n        if (snprintf(fname, fname_avail, filename, count++) >= fname_avail) {\n            mp_err(log, \"format result too long: '%s'\\n\", filename);\n            goto exit_mf;\n        }\n        if (!mp_path_exists(fname)) {\n            error_count++;\n            mp_verbose(log, \"file not found: '%s'\\n\", fname);\n        } else {\n            mf_add(mf, fname);", "project": "mpv", "hash": 167587795732249525600053650581205374378, "size": 144, "commit_id": "d0c530919d8cd4d7a774e38ab064e0fabdae34e6", "message": "demux_mf: improve format string processing\n\nBefore this commit, the user could specify a printf format string\nwhich wasn't verified, and could result in:\n- Undefined behavior due to missing or non-matching arguments.\n- Buffer overflow due to untested result length.\n\nThe offending code was added at commit 103a9609 (2002, mplayer svn):\ngit-svn-id: svn://svn.mplayerhq.hu/mplayer/trunk@4566 b3059339-0415-0410-9bf9-f77b7e298cf2\n\nIt moved around but was not modified meaningfully until now.\n\nNow we reject all conversion specifiers at the format except %%\nand a simple subset of the valid specifiers. Also, we now use\nsnprintf to avoid buffer overflow.\n\nThe format string is provided by the user as part of mf:// URI.\n\nReport and initial patch by Stefan Schiller.\nPatch reviewed by @jeeb, @sfan5, Stefan Schiller.", "target": 0, "dataset": "other", "idx": 259078}
{"func": "      }\n    }\n\n    if (LLVM_UNLIKELY(\n            desc.flags.internalSetter || receiverHandle->isHostObject() ||\n            receiverHandle->isProxyObject())) {\n      SymbolID id{};\n      LAZY_TO_IDENTIFIER(runtime, nameValPrimitiveHandle, id);\n      if (desc.flags.internalSetter) {\n        return internalSetter(\n            receiverHandle,\n            runtime,\n            id,\n            desc.castToNamedPropertyDescriptorRef(),\n            valueHandle,\n            opFlags);\n      } else if (receiverHandle->isHostObject()) {\n        return vmcast<HostObject>(receiverHandle.get())->set(id, *valueHandle);\n      }\n      assert(\n          receiverHandle->isProxyObject() && \"descriptor flags are impossible\");\n      if (*descDefinedRes) {\n        dpf.setValue = 1;\n      } else {\n        dpf = DefinePropertyFlags::getDefaultNewPropertyFlags();\n      }\n      return JSProxy::defineOwnProperty(\n          receiverHandle,\n          runtime,\n          nameValPrimitiveHandle,\n          dpf,\n          valueHandle,\n          opFlags);\n    }\n  }\n\n  /// Can we add more properties?\n  if (LLVM_UNLIKELY(!receiverHandle->isExtensible())) {", "project": "hermes", "hash": 22276050551116012672246483362569889414, "size": 281, "commit_id": "8cb935cd3b2321c46aa6b7ed8454d95c75a7fca0", "message": "Handle set where internalSetter and Proxy are both true\n\nSummary:\nIf putComputed is called on a proxy whose target's prototype\nis an array with a propname of 'length', then internalSetter will be\ntrue, and the receiver will be a proxy.  In that case, proxy needs to\nwin; the behavior may assert or be UB otherwise.\n\nReviewed By: tmikov\n\nDifferential Revision: D23916279\n\nfbshipit-source-id: c760356d48a02ece565fb4bc1acdafd7ccad7c68", "target": 1, "dataset": "other", "idx": 197476}
{"func": "    }\n\n    if (LLVM_UNLIKELY(\n            desc.flags.internalSetter || receiverHandle->isHostObject() ||\n            receiverHandle->isProxyObject())) {\n      // If putComputed is called on a proxy whose target's prototype\n      // is an array with a propname of 'length', then internalSetter\n      // will be true, and the receiver will be a proxy.  In that case,\n      // proxy wins.\n      if (receiverHandle->isProxyObject()) {\n        if (*descDefinedRes) {\n          dpf.setValue = 1;\n        } else {\n          dpf = DefinePropertyFlags::getDefaultNewPropertyFlags();\n        }\n        return JSProxy::defineOwnProperty(\n            receiverHandle,\n            runtime,\n            nameValPrimitiveHandle,\n            dpf,\n            valueHandle,\n            opFlags);\n      }\n      SymbolID id{};\n      LAZY_TO_IDENTIFIER(runtime, nameValPrimitiveHandle, id);\n      if (desc.flags.internalSetter) {\n        return internalSetter(\n            receiverHandle,\n            runtime,\n            id,\n            desc.castToNamedPropertyDescriptorRef(),\n            valueHandle,\n            opFlags);\n      }\n      assert(\n          receiverHandle->isHostObject() && \"descriptor flags are impossible\");\n      return vmcast<HostObject>(receiverHandle.get())->set(id, *valueHandle);\n    }\n  }\n\n  /// Can we add more properties?\n  if (LLVM_UNLIKELY(!receiverHandle->isExtensible())) {", "project": "hermes", "hash": 22945775706399007322739222011717058068, "size": 286, "commit_id": "8cb935cd3b2321c46aa6b7ed8454d95c75a7fca0", "message": "Handle set where internalSetter and Proxy are both true\n\nSummary:\nIf putComputed is called on a proxy whose target's prototype\nis an array with a propname of 'length', then internalSetter will be\ntrue, and the receiver will be a proxy.  In that case, proxy needs to\nwin; the behavior may assert or be UB otherwise.\n\nReviewed By: tmikov\n\nDifferential Revision: D23916279\n\nfbshipit-source-id: c760356d48a02ece565fb4bc1acdafd7ccad7c68", "target": 0, "dataset": "other", "idx": 259369}
{"func": "      col_id_present.resize(num_cols_, true);\n    }\n\n    // Processes the checkpoint source and the provided Tensor name.\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();", "project": "tensorflow", "hash": 52693165629132470358185846768404409366, "size": 226, "commit_id": "77dd114513d7796e1e2b8aece214a380af26fbf4", "message": "Fix a check fail\n\nPiperOrigin-RevId: 372011072\nChange-Id: I1062cfaed0aa16884e9a16312483794d188db76f", "target": 1, "dataset": "other", "idx": 197512}
{"func": "    }\n\n    // Processes the checkpoint source and the provided Tensor name.\n    const Tensor* ckpt_path_t;\n    OP_REQUIRES_OK(context, context->input(\"ckpt_path\", &ckpt_path_t));\n    OP_REQUIRES(\n        context, ckpt_path_t->NumElements() == 1,\n        errors::InvalidArgument(\"The `ckpt_path` tensor must have exactly one \"\n                                \"element, got tensor of shape \",\n                                ckpt_path_t->shape().DebugString()));\n    const string& ckpt_path = ckpt_path_t->scalar<tstring>()();\n    const Tensor* old_tensor_name_t;\n    OP_REQUIRES_OK(context,\n                   context->input(\"old_tensor_name\", &old_tensor_name_t));\n    const string& old_tensor_name = old_tensor_name_t->scalar<tstring>()();", "project": "tensorflow", "hash": 262389193985354269110422891412562548793, "size": 231, "commit_id": "77dd114513d7796e1e2b8aece214a380af26fbf4", "message": "Fix a check fail\n\nPiperOrigin-RevId: 372011072\nChange-Id: I1062cfaed0aa16884e9a16312483794d188db76f", "target": 0, "dataset": "other", "idx": 259623}
{"func": "              pExpr = pRight;\n            }\n            pNew = sqlite3ExprListAppend(pParse, pNew, pExpr);\n            sqlite3TokenInit(&sColname, zColname);\n            sqlite3ExprListSetName(pParse, pNew, &sColname, 0);\n            if( pNew && (p->selFlags & SF_NestedFrom)!=0 ){\n              struct ExprList_item *pX = &pNew->a[pNew->nExpr-1];\n              sqlite3DbFree(db, pX->zEName);\n              if( pSub ){\n                pX->zEName = sqlite3DbStrDup(db, pSub->pEList->a[j].zEName);\n                testcase( pX->zEName==0 );", "project": "sqlite", "hash": 173018535421974116346089355628231803850, "size": 286, "commit_id": "0990c415f65d2556a5e4122cbe5727d500411aeb", "message": "Fix a problem with ALTER TABLE for views that have a nested FROM clause.\nTicket [f50af3e8a565776b].\n\nFossilOrigin-Name: c431b3fd8fd0f6a6974bba3e9366b0430ec003d570e7ce70ceefbcff5fe4b6fa", "target": 1, "dataset": "other", "idx": 197522}
{"func": "              pExpr = pRight;\n            }\n            pNew = sqlite3ExprListAppend(pParse, pNew, pExpr);\n            sqlite3TokenInit(&sColname, zColname);\n            sqlite3ExprListSetName(pParse, pNew, &sColname, 0);\n            if( pNew && (p->selFlags & SF_NestedFrom)!=0 && !IN_RENAME_OBJECT ){\n              struct ExprList_item *pX = &pNew->a[pNew->nExpr-1];\n              sqlite3DbFree(db, pX->zEName);\n              if( pSub ){\n                pX->zEName = sqlite3DbStrDup(db, pSub->pEList->a[j].zEName);\n                testcase( pX->zEName==0 );", "project": "sqlite", "hash": 122260449153361052319909319985506822289, "size": 286, "commit_id": "0990c415f65d2556a5e4122cbe5727d500411aeb", "message": "Fix a problem with ALTER TABLE for views that have a nested FROM clause.\nTicket [f50af3e8a565776b].\n\nFossilOrigin-Name: c431b3fd8fd0f6a6974bba3e9366b0430ec003d570e7ce70ceefbcff5fe4b6fa", "target": 0, "dataset": "other", "idx": 259833}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    const float input_min_float = ctx->input(1).flat<float>()(0);\n    const float input_max_float = ctx->input(2).flat<float>()(0);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;", "project": "tensorflow", "hash": 11062965309142515619891321633938443899, "size": 24, "commit_id": "ef0c008ee84bad91ec6725ddc42091e19a30cf0e", "message": "Fix out of bound read in requantization_range_op.cc\n\nPiperOrigin-RevId: 372129031\nChange-Id: Ie684ab98a3840c5186ead3eafffc0e0ed0e8030d", "target": 1, "dataset": "other", "idx": 197575}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    const Tensor& input = ctx->input(0);\n    OP_REQUIRES(ctx, ctx->input(1).NumElements() > 0,\n                errors::InvalidArgument(\"Input min must not be empty.\"));\n    OP_REQUIRES(ctx, ctx->input(2).NumElements() > 0,\n                errors::InvalidArgument(\"Input max must not be empty.\"));\n    const float input_min_float = ctx->input(1).flat<float>()(0);\n    const float input_max_float = ctx->input(2).flat<float>()(0);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, TensorShape({}), &output_min));\n    Tensor* output_max = nullptr;", "project": "tensorflow", "hash": 249104404907249387447027842196733553165, "size": 28, "commit_id": "ef0c008ee84bad91ec6725ddc42091e19a30cf0e", "message": "Fix out of bound read in requantization_range_op.cc\n\nPiperOrigin-RevId: 372129031\nChange-Id: Ie684ab98a3840c5186ead3eafffc0e0ed0e8030d", "target": 0, "dataset": "other", "idx": 261456}
{"func": "                break;\n            default:\n                // Unknown.  Delete.\n                free (Sections[a].Data);\n                // Move succeding sections back by one to close space in array.\n                memmove(Sections+a, Sections+a+1, sizeof(Section_t) * (SectionsRead-a));\n                SectionsRead -= 1;\n                Modified = TRUE;\n        }\n    }\n    return Modified;", "project": "jhead", "hash": 99506882083525381430266022733946342388, "size": 44, "commit_id": "b8d78e5ec982e86cdd70ebfc1ebbb2273c982eea", "message": "Same error as previous checking in different function", "target": 1, "dataset": "other", "idx": 197609}
{"func": "                break;\n            default:\n                // Unknown.  Delete.\n                free (Sections[a].Data);\n                // Move succeding sections back by one to close space in array.\n                memmove(Sections+a, Sections+a+1, sizeof(Section_t) * (SectionsRead-a-1));\n                SectionsRead -= 1;\n                Modified = TRUE;\n        }\n    }\n    return Modified;", "project": "jhead", "hash": 331719028316894674358311993067451848682, "size": 44, "commit_id": "b8d78e5ec982e86cdd70ebfc1ebbb2273c982eea", "message": "Same error as previous checking in different function", "target": 0, "dataset": "other", "idx": 261985}
{"func": "    }\n\n  retval = 0;\n  goto cleanup;\n\n  /* We have to use the helper program.  */\n helper:;\n\n  pid_t pid = __fork ();\n  if (pid == -1)\n    goto cleanup;\n  else if (pid == 0)\n    {\n      /* We pass the master pseudo terminal as file descriptor PTY_FILENO.  */\n      if (fd != PTY_FILENO)\n\tif (__dup2 (fd, PTY_FILENO) < 0)\n\t  _exit (FAIL_EBADF);\n\n#ifdef CLOSE_ALL_FDS\n      CLOSE_ALL_FDS ();\n#endif\n\n      execle (_PATH_PT_CHOWN, basename (_PATH_PT_CHOWN), NULL, NULL);\n      _exit (FAIL_EXEC);\n    }\n  else\n\t    break;\n\n\t  default:\n\t    assert(! \"getpt: internal error: invalid exit code from pt_chown\");\n\t  }\n    }\n\n cleanup:\n  if (buf != _buf)\n    free (buf);\n", "project": "glibc", "hash": 55488823009452919756852590226678328329, "size": 141, "commit_id": "e4608715e6e1dd2adc91982fd151d5ba4f761d69", "message": "CVE-2013-2207, BZ #15755: Disable pt_chown.\n\nThe helper binary pt_chown tricked into granting access to another\nuser's pseudo-terminal.\n\nPre-conditions for the attack:\n\n * Attacker with local user account\n * Kernel with FUSE support\n * \"user_allow_other\" in /etc/fuse.conf\n * Victim with allocated slave in /dev/pts\n\nUsing the setuid installed pt_chown and a weak check on whether a file\ndescriptor is a tty, an attacker could fake a pty check using FUSE and\ntrick pt_chown to grant ownership of a pty descriptor that the current\nuser does not own.  It cannot access /dev/pts/ptmx however.\n\nIn most modern distributions pt_chown is not needed because devpts\nis enabled by default. The fix for this CVE is to disable building\nand using pt_chown by default. We still provide a configure option\nto enable hte use of pt_chown but distributions do so at their own\nrisk.", "target": 1, "dataset": "other", "idx": 197630}
{"func": "    }\n\n  retval = 0;\n  goto cleanup;\n\n  /* We have to use the helper program if it is available.  */\n helper:;\n\n#ifdef HAVE_PT_CHOWN\n  pid_t pid = __fork ();\n  if (pid == -1)\n    goto cleanup;\n  else if (pid == 0)\n    {\n      /* We pass the master pseudo terminal as file descriptor PTY_FILENO.  */\n      if (fd != PTY_FILENO)\n\tif (__dup2 (fd, PTY_FILENO) < 0)\n\t  _exit (FAIL_EBADF);\n\n# ifdef CLOSE_ALL_FDS\n      CLOSE_ALL_FDS ();\n# endif\n\n      execle (_PATH_PT_CHOWN, basename (_PATH_PT_CHOWN), NULL, NULL);\n      _exit (FAIL_EXEC);\n    }\n  else\n\n\t  default:\n\t    assert(! \"getpt: internal error: invalid exit code from pt_chown\");\n\t  }\n    }\n#endif\n\n cleanup:\n  if (buf != _buf)\n    free (buf);\n", "project": "glibc", "hash": 24069894465064036243353813490499424946, "size": 143, "commit_id": "e4608715e6e1dd2adc91982fd151d5ba4f761d69", "message": "CVE-2013-2207, BZ #15755: Disable pt_chown.\n\nThe helper binary pt_chown tricked into granting access to another\nuser's pseudo-terminal.\n\nPre-conditions for the attack:\n\n * Attacker with local user account\n * Kernel with FUSE support\n * \"user_allow_other\" in /etc/fuse.conf\n * Victim with allocated slave in /dev/pts\n\nUsing the setuid installed pt_chown and a weak check on whether a file\ndescriptor is a tty, an attacker could fake a pty check using FUSE and\ntrick pt_chown to grant ownership of a pty descriptor that the current\nuser does not own.  It cannot access /dev/pts/ptmx however.\n\nIn most modern distributions pt_chown is not needed because devpts\nis enabled by default. The fix for this CVE is to disable building\nand using pt_chown by default. We still provide a configure option\nto enable hte use of pt_chown but distributions do so at their own\nrisk.", "target": 0, "dataset": "other", "idx": 262137}
{"func": "\n    // We partition work first across alphas then across samples-per-alpha to\n    // avoid a couple flops which can be done on a per-alpha basis.\n\n    auto DoWork = [samples_per_alpha, num_alphas, &rng, samples_flat,\n                   alpha_flat](int start_output, int limit_output) {\n      using Eigen::numext::exp;\n      using Eigen::numext::log;\n      using Eigen::numext::log1p;\n      using Eigen::numext::pow;\n", "project": "tensorflow", "hash": 73312061318900783614516195772402837346, "size": 174, "commit_id": "27b417360cbd671ef55915e4bb6bb06af8b8a832", "message": "Prevent `int64` to `int` truncation in `Shard` API usage.\n\nThe function argument in `Shard` must be a function of two `int64` arguments. However, we are passing in a function with two `int` arguments. Thus, for large workloads, these arguments get truncated from positive `int64` values to negative `int` ones, resulting in a buffer out of bounds write.\n\nPiperOrigin-RevId: 332557334\nChange-Id: I236c9a2e7f53580e520571da8ba941a3aa9fa0b5", "target": 1, "dataset": "other", "idx": 197649}
{"func": "\n    // We partition work first across alphas then across samples-per-alpha to\n    // avoid a couple flops which can be done on a per-alpha basis.\n\n    auto DoWork = [samples_per_alpha, num_alphas, &rng, samples_flat,\n                   alpha_flat](int64 start_output, int64 limit_output) {\n      using Eigen::numext::exp;\n      using Eigen::numext::log;\n      using Eigen::numext::log1p;\n      using Eigen::numext::pow;\n", "project": "tensorflow", "hash": 46255362625410649769966248535800319854, "size": 174, "commit_id": "27b417360cbd671ef55915e4bb6bb06af8b8a832", "message": "Prevent `int64` to `int` truncation in `Shard` API usage.\n\nThe function argument in `Shard` must be a function of two `int64` arguments. However, we are passing in a function with two `int` arguments. Thus, for large workloads, these arguments get truncated from positive `int64` values to negative `int` ones, resulting in a buffer out of bounds write.\n\nPiperOrigin-RevId: 332557334\nChange-Id: I236c9a2e7f53580e520571da8ba941a3aa9fa0b5", "target": 0, "dataset": "other", "idx": 262651}
{"func": "        }\n\n        bytestream2_skip(&gb, ac_size);\n    }\n\n    if (dc_size > 0) {\n        unsigned long dest_len = dc_count * 2LL;\n        GetByteContext agb = gb;\n\n        if (dc_count > (6LL * td->xsize * td->ysize + 63) / 64)\n            return AVERROR_INVALIDDATA;\n\n        av_fast_padded_malloc(&td->dc_data, &td->dc_size, FFALIGN(dest_len, 64) * 2);\n        if (!td->dc_data)\n            return AVERROR(ENOMEM);", "project": "FFmpeg", "hash": 164964024217881679158454133523303710683, "size": 185, "commit_id": "26d3c81bc5ef2f8c3f09d45eaeacfb4b1139a777", "message": "avcodec/exr: More strictly check dc_count\n\nFixes: out of array access\nFixes: exr/deneme\n\nFound-by: Burak \u00c7ar\u0131k\u00e7\u0131 <burakcarikci@crypttech.com>\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>", "target": 1, "dataset": "other", "idx": 197652}
{"func": "        }\n\n        bytestream2_skip(&gb, ac_size);\n    }\n\n    {\n        unsigned long dest_len = dc_count * 2LL;\n        GetByteContext agb = gb;\n\n        if (dc_count != dc_w * dc_h * 3)\n            return AVERROR_INVALIDDATA;\n\n        av_fast_padded_malloc(&td->dc_data, &td->dc_size, FFALIGN(dest_len, 64) * 2);\n        if (!td->dc_data)\n            return AVERROR(ENOMEM);", "project": "FFmpeg", "hash": 64835961090131636283596112080756004472, "size": 185, "commit_id": "26d3c81bc5ef2f8c3f09d45eaeacfb4b1139a777", "message": "avcodec/exr: More strictly check dc_count\n\nFixes: out of array access\nFixes: exr/deneme\n\nFound-by: Burak \u00c7ar\u0131k\u00e7\u0131 <burakcarikci@crypttech.com>\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>", "target": 0, "dataset": "other", "idx": 262684}
{"func": "    if (!status.ok()) return status;\n\n    const TensorShape& inputs_shape = (*inputs)->shape();\n\n    if (inputs_shape.dims() != 3) {\n      return errors::InvalidArgument(\"inputs is not a 3-Tensor\");\n    }\n\n    const int64 max_time = inputs_shape.dim_size(0);\n    const int64 batch_size = inputs_shape.dim_size(1);\n", "project": "tensorflow", "hash": 179923620289596823307074921015885142930, "size": 54, "commit_id": "b1b323042264740c398140da32e93fb9c2c9f33e", "message": "Fix SEGV in CTC ops\n\nPiperOrigin-RevId: 372430279\nChange-Id: I7ec2ad9d6f4d0980c33de45d27c6b17df5c6e26f", "target": 1, "dataset": "other", "idx": 197664}
{"func": "\n    const TensorShape& inputs_shape = (*inputs)->shape();\n\n    if (inputs_shape.dims() != 3) {\n      return errors::InvalidArgument(\"inputs is not a 3-Tensor\");\n    }\n    if (inputs_shape.num_elements() == 0) {\n      return errors::InvalidArgument(\"inputs must not be empty\");\n    }\n\n    const int64 max_time = inputs_shape.dim_size(0);\n    const int64 batch_size = inputs_shape.dim_size(1);\n", "project": "tensorflow", "hash": 208622133539229492215272569704905304618, "size": 57, "commit_id": "b1b323042264740c398140da32e93fb9c2c9f33e", "message": "Fix SEGV in CTC ops\n\nPiperOrigin-RevId: 372430279\nChange-Id: I7ec2ad9d6f4d0980c33de45d27c6b17df5c6e26f", "target": 0, "dataset": "other", "idx": 262711}
{"func": "  void ValidateInputs(OpKernelContext* ctx,\n                      const CSRSparseMatrix& sparse_matrix,\n                      const Tensor& permutation_indices, int* batch_size,\n                      int64* num_rows) {\n    OP_REQUIRES(ctx, sparse_matrix.dtype() == DataTypeToEnum<T>::value,\n                errors::InvalidArgument(\n                    \"Asked for a CSRSparseMatrix of type \",\n                    DataTypeString(DataTypeToEnum<T>::value),\n                    \" but saw dtype: \", DataTypeString(sparse_matrix.dtype())));\n\n    const Tensor& dense_shape = sparse_matrix.dense_shape();\n    const int rank = dense_shape.dim_size(0);\n    OP_REQUIRES(ctx, rank == 2 || rank == 3,\n                errors::InvalidArgument(\"sparse matrix must have rank 2 or 3; \",\n                                        \"but dense_shape has size \", rank));\n    const int row_dim = (rank == 2) ? 0 : 1;\n    auto dense_shape_vec = dense_shape.vec<int64>();\n    *num_rows = dense_shape_vec(row_dim);\n    const int64 num_cols = dense_shape_vec(row_dim + 1);\n    OP_REQUIRES(ctx, *num_rows == num_cols,\n                errors::InvalidArgument(\"sparse matrix must be square; got: \",\n                                        *num_rows, \" != \", num_cols));\n    const TensorShape& perm_shape = permutation_indices.shape();\n    OP_REQUIRES(\n        ctx, perm_shape.dims() + 1 == rank,\n        errors::InvalidArgument(\n            \"sparse matrix must have the same rank as permutation; got: \", rank,\n            \" != \", perm_shape.dims(), \" + 1.\"));\n    OP_REQUIRES(\n        ctx, perm_shape.dim_size(rank - 2) == *num_rows,\n        errors::InvalidArgument(\n            \"permutation must have the same number of elements in each batch \"\n            \"as the number of rows in sparse matrix; got: \",\n            perm_shape.dim_size(rank - 2), \" != \", *num_rows));\n\n    *batch_size = sparse_matrix.batch_size();\n    if (*batch_size > 1) {\n      OP_REQUIRES(\n          ctx, perm_shape.dim_size(0) == *batch_size,\n          errors::InvalidArgument(\"permutation must have the same batch size \"\n                                  \"as sparse matrix; got: \",\n                                  perm_shape.dim_size(0), \" != \", *batch_size));\n    }\n  }", "project": "tensorflow", "hash": 336558180459502144170088935707234439720, "size": 44, "commit_id": "e6a7c7cc18c3aaad1ae0872cb0a959f5c923d2bd", "message": "Remove `OP_REQUIRES` call from helper function.\n\nSince `OP_REQUIRES` macro expands to a `return;` (among other), calling it in a helper function only ends the helper function's execution earlier, but the kernel will still run from start to end. Thus, all the expected validations are actually broken/useless as the code ploughs through the next crash anyway.\n\nPiperOrigin-RevId: 369524386\nChange-Id: I54f6cf9328445675ccc392e661b04336b229c9da", "target": 1, "dataset": "other", "idx": 197715}
{"func": "  Status ValidateInputs(const CSRSparseMatrix& sparse_matrix,\n                        const Tensor& permutation_indices, int* batch_size,\n                        int64* num_rows) {\n    if (sparse_matrix.dtype() != DataTypeToEnum<T>::value)\n      return errors::InvalidArgument(\n          \"Asked for a CSRSparseMatrix of type \",\n          DataTypeString(DataTypeToEnum<T>::value),\n          \" but saw dtype: \", DataTypeString(sparse_matrix.dtype()));\n\n    const Tensor& dense_shape = sparse_matrix.dense_shape();\n    const int rank = dense_shape.dim_size(0);\n    if (rank < 2 || rank > 3)\n      return errors::InvalidArgument(\"sparse matrix must have rank 2 or 3; \",\n                                     \"but dense_shape has size \", rank);\n    const int row_dim = (rank == 2) ? 0 : 1;\n    auto dense_shape_vec = dense_shape.vec<int64>();\n    *num_rows = dense_shape_vec(row_dim);\n    const int64 num_cols = dense_shape_vec(row_dim + 1);\n    if (*num_rows != num_cols)\n      return errors::InvalidArgument(\n          \"sparse matrix must be square; got: \", *num_rows, \" != \", num_cols);\n    const TensorShape& perm_shape = permutation_indices.shape();\n    if (perm_shape.dims() + 1 != rank)\n      return errors::InvalidArgument(\n          \"sparse matrix must have the same rank as permutation; got: \", rank,\n          \" != \", perm_shape.dims(), \" + 1.\");\n    if (perm_shape.dim_size(rank - 2) != *num_rows)\n      return errors::InvalidArgument(\n          \"permutation must have the same number of elements in each batch \"\n          \"as the number of rows in sparse matrix; got: \",\n          perm_shape.dim_size(rank - 2), \" != \", *num_rows);\n\n    *batch_size = sparse_matrix.batch_size();\n    if (*batch_size > 1) {\n      if (perm_shape.dim_size(0) != *batch_size)\n        return errors::InvalidArgument(\n            \"permutation must have the same batch size \"\n            \"as sparse matrix; got: \",\n            perm_shape.dim_size(0), \" != \", *batch_size);\n    }\n\n    return Status::OK();\n  }", "project": "tensorflow", "hash": 71539368152818315797596681434801050349, "size": 43, "commit_id": "e6a7c7cc18c3aaad1ae0872cb0a959f5c923d2bd", "message": "Remove `OP_REQUIRES` call from helper function.\n\nSince `OP_REQUIRES` macro expands to a `return;` (among other), calling it in a helper function only ends the helper function's execution earlier, but the kernel will still run from start to end. Thus, all the expected validations are actually broken/useless as the code ploughs through the next crash anyway.\n\nPiperOrigin-RevId: 369524386\nChange-Id: I54f6cf9328445675ccc392e661b04336b229c9da", "target": 0, "dataset": "other", "idx": 262772}
{"func": "\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid)) {\n\t\tir_table = (union dmar_ir_entry *)hpa2hva(dmar_unit->ir_table_addr);\n\t\tir_entry = ir_table + index;\n\t\tir_entry->bits.remap.present = 0x0UL;\n\n\t\tiommu_flush_cache(ir_entry, sizeof(union dmar_ir_entry));", "project": "acrn-hypervisor", "hash": 138533968807108358104095755982502807971, "size": 28, "commit_id": "25c0e3817eb332660dd63d1d4522e63dcc94e79a", "message": "hv: validate input for dmar_free_irte function\n\n Malicious input 'index' may trigger buffer\n overflow on array 'irte_alloc_bitmap[]'.\n\n This patch validate that 'index' shall be\n less than 'CONFIG_MAX_IR_ENTRIES' and also\n remove unnecessary check on 'index' in\n 'ptirq_free_irte()' function with this fix.\n\nTracked-On: #6132\nSigned-off-by: Yonghua Huang <yonghua.huang@intel.com>", "target": 1, "dataset": "other", "idx": 197721}
{"func": "\t\tdmar_unit = device_to_dmaru((uint8_t)intr_src->src.msi.bits.b, intr_src->src.msi.fields.devfun);\n\t} else {\n\t\tdmar_unit = ioapic_to_dmaru(intr_src->src.ioapic_id, &sid);\n\t}\n\n\tif (is_dmar_unit_valid(dmar_unit, sid) && (index < CONFIG_MAX_IR_ENTRIES)) {\n\t\tir_table = (union dmar_ir_entry *)hpa2hva(dmar_unit->ir_table_addr);\n\t\tir_entry = ir_table + index;\n\t\tir_entry->bits.remap.present = 0x0UL;\n\n\t\tiommu_flush_cache(ir_entry, sizeof(union dmar_ir_entry));", "project": "acrn-hypervisor", "hash": 50934453175337732283896052884663765142, "size": 28, "commit_id": "25c0e3817eb332660dd63d1d4522e63dcc94e79a", "message": "hv: validate input for dmar_free_irte function\n\n Malicious input 'index' may trigger buffer\n overflow on array 'irte_alloc_bitmap[]'.\n\n This patch validate that 'index' shall be\n less than 'CONFIG_MAX_IR_ENTRIES' and also\n remove unnecessary check on 'index' in\n 'ptirq_free_irte()' function with this fix.\n\nTracked-On: #6132\nSigned-off-by: Yonghua Huang <yonghua.huang@intel.com>", "target": 0, "dataset": "other", "idx": 262864}
{"func": "    packet->host_line.len = 0, packet->referer_line.ptr = NULL, packet->referer_line.len = 0,\n    packet->content_line.ptr = NULL, packet->content_line.len = 0, packet->accept_line.ptr = NULL,\n    packet->accept_line.len = 0, packet->user_agent_line.ptr = NULL, packet->user_agent_line.len = 0,\n    packet->http_url_name.ptr = NULL, packet->http_url_name.len = 0, packet->http_encoding.ptr = NULL,\n    packet->http_encoding.len = 0, packet->http_transfer_encoding.ptr = NULL, packet->http_transfer_encoding.len = 0,\n    packet->http_contentlen.ptr = NULL, packet->http_contentlen.len = 0, packet->http_cookie.ptr = NULL,\n    packet->http_cookie.len = 0, packet->http_origin.len = 0, packet->http_origin.ptr = NULL,\n    packet->http_x_session_type.ptr = NULL, packet->http_x_session_type.len = 0, packet->server_line.ptr = NULL,\n    packet->server_line.len = 0, packet->http_method.ptr = NULL, packet->http_method.len = 0,\n    packet->http_response.ptr = NULL, packet->http_response.len = 0, packet->http_num_headers = 0;\n}", "project": "nDPI", "hash": 86484384815441304643195803453281909060, "size": 13, "commit_id": "6a9f5e4f7c3fd5ddab3e6727b071904d76773952", "message": "Fixed use after free caused by dangling pointer\n\n * This fix also improved RCE Injection detection\n\nSigned-off-by: Toni Uhlig <matzeton@googlemail.com>", "target": 1, "dataset": "other", "idx": 197747}
{"func": "    packet->host_line.len = 0, packet->referer_line.ptr = NULL, packet->referer_line.len = 0,\n    packet->content_line.ptr = NULL, packet->content_line.len = 0, packet->accept_line.ptr = NULL,\n    packet->accept_line.len = 0, packet->user_agent_line.ptr = NULL, packet->user_agent_line.len = 0,\n    packet->http_url_name.ptr = NULL, packet->http_url_name.len = 0, packet->http_encoding.ptr = NULL,\n    packet->http_encoding.len = 0, packet->http_transfer_encoding.ptr = NULL, packet->http_transfer_encoding.len = 0,\n    packet->http_contentlen.ptr = NULL, packet->http_contentlen.len = 0, packet->content_disposition_line.ptr = NULL,\n    packet->content_disposition_line.len = 0, packet->http_cookie.ptr = NULL,\n    packet->http_cookie.len = 0, packet->http_origin.len = 0, packet->http_origin.ptr = NULL,\n    packet->http_x_session_type.ptr = NULL, packet->http_x_session_type.len = 0, packet->server_line.ptr = NULL,\n    packet->server_line.len = 0, packet->http_method.ptr = NULL, packet->http_method.len = 0,\n    packet->http_response.ptr = NULL, packet->http_response.len = 0, packet->http_num_headers = 0;\n}", "project": "nDPI", "hash": 165714072252586915476330424541219526931, "size": 14, "commit_id": "6a9f5e4f7c3fd5ddab3e6727b071904d76773952", "message": "Fixed use after free caused by dangling pointer\n\n * This fix also improved RCE Injection detection\n\nSigned-off-by: Toni Uhlig <matzeton@googlemail.com>", "target": 0, "dataset": "other", "idx": 263377}
{"func": "    output = nullptr;\n    return false;\n  }\n\n  int16_t klinux_family = input->klinux_sa_family;\n  if (klinux_family == kLinux_AF_UNIX) {\n    struct klinux_sockaddr_un *klinux_sockaddr_un_in =\n        const_cast<struct klinux_sockaddr_un *>(\n            reinterpret_cast<const struct klinux_sockaddr_un *>(input));\n\n    struct sockaddr_un sockaddr_un_out;\n    ReinterpretCopyArray(\n        sockaddr_un_out.sun_path, klinux_sockaddr_un_in->klinux_sun_path,\n        std::min(sizeof(sockaddr_un_out.sun_path),\n                 sizeof(klinux_sockaddr_un_in->klinux_sun_path)));\n    CopySockaddr(&sockaddr_un_out, sizeof(sockaddr_un_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET) {\n    struct klinux_sockaddr_in *klinux_sockaddr_in_in =\n        const_cast<struct klinux_sockaddr_in *>(\n            reinterpret_cast<const struct klinux_sockaddr_in *>(input));\n\n    struct sockaddr_in sockaddr_in_out;\n                          &klinux_sockaddr_in_in->klinux_sin_addr);\n    InitializeToZeroArray(sockaddr_in_out.sin_zero);\n    ReinterpretCopyArray(sockaddr_in_out.sin_zero,\n                         klinux_sockaddr_in_in->klinux_sin_zero);\n    CopySockaddr(&sockaddr_in_out, sizeof(sockaddr_in_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET6) {\n    struct klinux_sockaddr_in6 *klinux_sockaddr_in6_in =\n        const_cast<struct klinux_sockaddr_in6 *>(\n            reinterpret_cast<const struct klinux_sockaddr_in6 *>(input));\n\n    struct sockaddr_in6 sockaddr_in6_out;", "project": "asylo", "hash": 25761054693871753712999801888112976207, "size": 69, "commit_id": "bda9772e7872b0d2b9bee32930cf7a4983837b39", "message": "Check input length in FromLinuxSockAddr\n\nPiperOrigin-RevId: 333785506\nChange-Id: I1d68fb8954665eebc1018d80ff995cbe9e7ed6a9", "target": 1, "dataset": "other", "idx": 197757}
{"func": "    return false;\n  }\n\n  int16_t klinux_family = input->klinux_sa_family;\n  if (klinux_family == kLinux_AF_UNIX) {\n    if (input_len < sizeof(struct klinux_sockaddr_un)) {\n      return false;\n    }\n\n    struct klinux_sockaddr_un *klinux_sockaddr_un_in =\n        const_cast<struct klinux_sockaddr_un *>(\n            reinterpret_cast<const struct klinux_sockaddr_un *>(input));\n\n    struct sockaddr_un sockaddr_un_out;\n        sockaddr_un_out.sun_path, klinux_sockaddr_un_in->klinux_sun_path,\n        std::min(sizeof(sockaddr_un_out.sun_path),\n                 sizeof(klinux_sockaddr_un_in->klinux_sun_path)));\n    CopySockaddr(&sockaddr_un_out, sizeof(sockaddr_un_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET) {\n    if (input_len < sizeof(struct klinux_sockaddr_in)) {\n      return false;\n    }\n    struct klinux_sockaddr_in *klinux_sockaddr_in_in =\n        const_cast<struct klinux_sockaddr_in *>(\n            reinterpret_cast<const struct klinux_sockaddr_in *>(input));\n\n    struct sockaddr_in sockaddr_in_out;\n    InitializeToZeroArray(sockaddr_in_out.sin_zero);\n    ReinterpretCopyArray(sockaddr_in_out.sin_zero,\n                         klinux_sockaddr_in_in->klinux_sin_zero);\n    CopySockaddr(&sockaddr_in_out, sizeof(sockaddr_in_out), output, output_len);\n  } else if (klinux_family == kLinux_AF_INET6) {\n    if (input_len < sizeof(struct klinux_sockaddr_in6)) {\n      return false;\n    }\n\n    struct klinux_sockaddr_in6 *klinux_sockaddr_in6_in =\n        const_cast<struct klinux_sockaddr_in6 *>(\n            reinterpret_cast<const struct klinux_sockaddr_in6 *>(input));\n\n    struct sockaddr_in6 sockaddr_in6_out;", "project": "asylo", "hash": 244700671016873006082210219872496648727, "size": 80, "commit_id": "bda9772e7872b0d2b9bee32930cf7a4983837b39", "message": "Check input length in FromLinuxSockAddr\n\nPiperOrigin-RevId: 333785506\nChange-Id: I1d68fb8954665eebc1018d80ff995cbe9e7ed6a9", "target": 0, "dataset": "other", "idx": 263436}
{"func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const float input_min = context->input(1).flat<float>()(0);\n    const float input_max = context->input(2).flat<float>()(0);\n    const Tensor& mean = context->input(3);\n    const float mean_min = context->input(4).flat<float>()(0);\n    const float mean_max = context->input(5).flat<float>()(0);\n    const Tensor& var = context->input(6);\n    const float var_min = context->input(7).flat<float>()(0);\n    const float var_max = context->input(8).flat<float>()(0);\n    const Tensor& beta = context->input(9);\n    const float beta_min = context->input(10).flat<float>()(0);\n    const float beta_max = context->input(11).flat<float>()(0);\n    const Tensor& gamma = context->input(12);\n    const float gamma_min = context->input(13).flat<float>()(0);\n    const float gamma_max = context->input(14).flat<float>()(0);\n\n    OP_REQUIRES(context, input.dims() == 4,\n                errors::InvalidArgument(\"input must be 4-dimensional\",\n                                        input.shape().DebugString()));\n    OP_REQUIRES(context, mean.dims() == 1,\n    OP_REQUIRES(context, beta.dims() == 1,\n                errors::InvalidArgument(\"beta must be 1-dimensional\",\n                                        beta.shape().DebugString()));\n    OP_REQUIRES(context, gamma.dims() == 1,\n                errors::InvalidArgument(\"gamma must be 1-dimensional\",\n                                        gamma.shape().DebugString()));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n    float output_min;", "project": "tensorflow", "hash": 39891708146580454794486373109005381550, "size": 54, "commit_id": "d6ed5bcfe1dcab9e85a4d39931bd18d99018e75b", "message": "Add missing validation in `QuantizedBatchNormWithGlobalNormalization`\n\nPiperOrigin-RevId: 370123451\nChange-Id: Id234d6dab1ec21230bb8e503dba30f899af87f33", "target": 1, "dataset": "other", "idx": 197761}
{"func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& input = context->input(0);\n    const auto& input_min_tensor = context->input(1);\n    OP_REQUIRES(context, input_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"input_min must have 1 element\"));\n    const float input_min = input_min_tensor.flat<float>()(0);\n    const auto& input_max_tensor = context->input(2);\n    OP_REQUIRES(context, input_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"input_max must have 1 element\"));\n    const float input_max = input_max_tensor.flat<float>()(0);\n    const Tensor& mean = context->input(3);\n    const auto& mean_min_tensor = context->input(4);\n    OP_REQUIRES(context, mean_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"mean_min must have 1 element\"));\n    const float mean_min = mean_min_tensor.flat<float>()(0);\n    const auto& mean_max_tensor = context->input(5);\n    OP_REQUIRES(context, mean_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"mean_max must have 1 element\"));\n    const float mean_max = mean_max_tensor.flat<float>()(0);\n    const Tensor& var = context->input(6);\n    const auto& var_min_tensor = context->input(7);\n    OP_REQUIRES(context, var_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"var_min must have 1 element\"));\n    const float var_min = var_min_tensor.flat<float>()(0);\n    const auto& var_max_tensor = context->input(8);\n    OP_REQUIRES(context, var_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"var_max must have 1 element\"));\n    const float var_max = var_max_tensor.flat<float>()(0);\n    const Tensor& beta = context->input(9);\n    const auto& beta_min_tensor = context->input(10);\n    OP_REQUIRES(context, beta_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"beta_min must have 1 element\"));\n    const float beta_min = beta_min_tensor.flat<float>()(0);\n    const auto& beta_max_tensor = context->input(11);\n    OP_REQUIRES(context, beta_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"beta_max must have 1 element\"));\n    const float beta_max = beta_max_tensor.flat<float>()(0);\n    const Tensor& gamma = context->input(12);\n    const auto& gamma_min_tensor = context->input(13);\n    OP_REQUIRES(context, gamma_min_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"gamma_min must have 1 element\"));\n    const float gamma_min = gamma_min_tensor.flat<float>()(0);\n    const auto& gamma_max_tensor = context->input(14);\n    OP_REQUIRES(context, gamma_max_tensor.NumElements() == 1,\n                errors::InvalidArgument(\"gamma_max must have 1 element\"));\n    const float gamma_max = gamma_max_tensor.flat<float>()(0);\n\n    OP_REQUIRES(context, input.dims() == 4,\n                errors::InvalidArgument(\"input must be 4-dimensional\",\n                                        input.shape().DebugString()));\n    OP_REQUIRES(context, mean.dims() == 1,\n                errors::InvalidArgument(\"beta must be 1-dimensional\",\n                                        beta.shape().DebugString()));\n    OP_REQUIRES(context, gamma.dims() == 1,\n                errors::InvalidArgument(\"gamma must be 1-dimensional\",\n                                        gamma.shape().DebugString()));\n    OP_REQUIRES(context, mean.NumElements() > 1,\n                errors::InvalidArgument(\"Must have at least a mean value\",\n                                        gamma.shape().DebugString()));\n    OP_REQUIRES(context, mean.NumElements() > 1,\n                errors::InvalidArgument(\"Must have at least a mean value\"));\n    const auto last_dim = input.shape().dims() - 1;\n    OP_REQUIRES(context,\n                mean.shape().dim_size(0) == input.shape().dim_size(last_dim),\n                errors::InvalidArgument(\"Must provide as many means as the \"\n                                        \"last dimension of the input tensor: \",\n                                        mean.shape().DebugString(), \" vs. \",\n                                        input.shape().DebugString()));\n    OP_REQUIRES(\n        context, mean.shape().dim_size(0) == var.shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Mean and variance tensors must have the same shape: \",\n            mean.shape().DebugString(), \" vs. \", var.shape().DebugString()));\n    OP_REQUIRES(\n        context, mean.shape().dim_size(0) == beta.shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Mean and beta tensors must have the same shape: \",\n            mean.shape().DebugString(), \" vs. \", beta.shape().DebugString()));\n    OP_REQUIRES(\n        context, mean.shape().dim_size(0) == gamma.shape().dim_size(0),\n        errors::InvalidArgument(\n            \"Mean and gamma tensors must have the same shape: \",\n            mean.shape().DebugString(), \" vs. \", gamma.shape().DebugString()));\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(context,\n                   context->allocate_output(0, input.shape(), &output));\n    float output_min;", "project": "tensorflow", "hash": 115913159429277562366456709982634610677, "size": 111, "commit_id": "d6ed5bcfe1dcab9e85a4d39931bd18d99018e75b", "message": "Add missing validation in `QuantizedBatchNormWithGlobalNormalization`\n\nPiperOrigin-RevId: 370123451\nChange-Id: Id234d6dab1ec21230bb8e503dba30f899af87f33", "target": 0, "dataset": "other", "idx": 263524}
{"func": "ImmutableConstantOp::ImmutableConstantOp(OpKernelConstruction* context)\n    : OpKernel(context) {\n  OP_REQUIRES_OK(context,\n                 context->GetAttr(kMemoryRegionNameAttr, &region_name_));\n  OP_REQUIRES_OK(context, context->GetAttr(kDTypeAttr, &dtype_));\n  OP_REQUIRES_OK(context, context->GetAttr(kShapeAttr, &shape_));\n}", "project": "tensorflow", "hash": 193263969189053696650270005931382566681, "size": 7, "commit_id": "4f663d4b8f0bec1b48da6fa091a7d29609980fa4", "message": "Allowlist certain data types to avoid a seg fault.\n\nPiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31", "target": 1, "dataset": "other", "idx": 197806}
{"func": "ImmutableConstantOp::ImmutableConstantOp(OpKernelConstruction* context)\n    : OpKernel(context) {\n  OP_REQUIRES_OK(context,\n                 context->GetAttr(kMemoryRegionNameAttr, &region_name_));\n  OP_REQUIRES_OK(context, context->GetAttr(kDTypeAttr, &dtype_));\n  OP_REQUIRES(context, dtype_ != DT_RESOURCE && dtype_ != DT_VARIANT,\n              errors::InvalidArgument(\n                  \"Resource and variant dtypes are invalid for this op.\"));\n  OP_REQUIRES_OK(context, context->GetAttr(kShapeAttr, &shape_));\n}", "project": "tensorflow", "hash": 8861752768728197719914030896388110369, "size": 10, "commit_id": "4f663d4b8f0bec1b48da6fa091a7d29609980fa4", "message": "Allowlist certain data types to avoid a seg fault.\n\nPiperOrigin-RevId: 356326671\nChange-Id: I23b65b52e93798cb5a6744632d31b0f88c6b6b31", "target": 0, "dataset": "other", "idx": 264414}
{"func": "    is_pool_empty = buffer_pool_.empty();\n    if (is_pool_empty) {\n      buffers =\n          primitives::AllocateUntrustedBuffers(kPoolIncrement, kPoolEntrySize);\n      for (int i = 0; i < kPoolIncrement; i++) {\n        if (!buffers[i] ||\n            !TrustedPrimitives::IsOutsideEnclave(buffers[i], kPoolEntrySize)) {\n          abort();\n        }\n        buffer_pool_.push(buffers[i]);\n      }\n    }\n    buffer = buffer_pool_.top();\n    buffer_pool_.pop();\n    busy_buffers_.insert(buffer);", "project": "asylo", "hash": 242278763062410738405430919405598678850, "size": 31, "commit_id": "a47ef55db2337d29de19c50cd29b0deb2871d31c", "message": "Fix vulnerability in UntrustedCacheMalloc\n\nThe pointer array is stored in untrusted memory, so we cannot trust the\nvalue even after validation. We should validate the pointer is pointing\nto untrusted memory after it's stored inside the enclave.\n\nPiperOrigin-RevId: 358474391\nChange-Id: I63cf6c251bdaf1b491dbf06cc0dcf77f7b141756", "target": 1, "dataset": "other", "idx": 197832}
{"func": "    is_pool_empty = buffer_pool_.empty();\n    if (is_pool_empty) {\n      buffers =\n          primitives::AllocateUntrustedBuffers(kPoolIncrement, kPoolEntrySize);\n      for (int i = 0; i < kPoolIncrement; i++) {\n        void *buf = buffers[i];\n        if (!buf || !TrustedPrimitives::IsOutsideEnclave(buf, kPoolEntrySize)) {\n          TrustedPrimitives::BestEffortAbort(\n              \"Cached buffer is not outside the enclave\");\n        }\n        buffer_pool_.push(buf);\n      }\n    }\n    buffer = buffer_pool_.top();\n    buffer_pool_.pop();\n    busy_buffers_.insert(buffer);", "project": "asylo", "hash": 49646276951412525923922242548482096096, "size": 32, "commit_id": "a47ef55db2337d29de19c50cd29b0deb2871d31c", "message": "Fix vulnerability in UntrustedCacheMalloc\n\nThe pointer array is stored in untrusted memory, so we cannot trust the\nvalue even after validation. We should validate the pointer is pointing\nto untrusted memory after it's stored inside the enclave.\n\nPiperOrigin-RevId: 358474391\nChange-Id: I63cf6c251bdaf1b491dbf06cc0dcf77f7b141756", "target": 0, "dataset": "other", "idx": 264754}
{"func": "ImagingPcxDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8 n;\n    UINT8* ptr;\n\n    if (strcmp(im->mode, \"1\") == 0 && state->xsize > state->bytes * 8) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    } else if (strcmp(im->mode, \"P\") == 0 && state->xsize > state->bytes) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = buf;", "project": "Pillow", "hash": 91336361060317802518241506144190852925, "size": 73, "commit_id": "6a83e4324738bb0452fbe8074a995b1c73f08de7", "message": "Fix OOB Access on PcxDecode.c", "target": 1, "dataset": "other", "idx": 197848}
{"func": "ImagingPcxDecode(Imaging im, ImagingCodecState state, UINT8* buf, Py_ssize_t bytes)\n{\n    UINT8 n;\n    UINT8* ptr;\n\n    if ((state->xsize * state->bits + 7) / 8 > state->bytes) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n\n    ptr = buf;", "project": "Pillow", "hash": 222319329232490867971448493669699061015, "size": 70, "commit_id": "6a83e4324738bb0452fbe8074a995b1c73f08de7", "message": "Fix OOB Access on PcxDecode.c", "target": 0, "dataset": "other", "idx": 265040}
{"func": "\t\t\t\t\t((GF_IsomInitialObjectDescriptor *)isom_od)->IPMPToolList = NULL;\n\t\t\t\t}\n\n\t\t\t\t//then rewrite the ESDesc\n\t\t\t\tj=0;\n\t\t\t\twhile ((ref = (GF_ES_ID_Ref*)gf_list_enum(isom_od->ES_ID_RefDescriptors, &j))) {\n\t\t\t\t\t//if the ref index is not valid, skip this desc...\n\t\t\t\t\tif (!mpod->trackIDs || gf_isom_get_track_from_id(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1]) == NULL) continue;\n\t\t\t\t\t//OK, get the esd\n\t\t\t\t\te = GetESDForTime(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1], sample->DTS, &esd);\n\t\t\t\t\tif (!e) e = gf_odf_desc_add_desc((GF_Descriptor *) od, (GF_Descriptor *) esd);\n\t\t\t\t\tif (e) {\n\t\t\t\t\t\tgf_odf_desc_del((GF_Descriptor *)od);\n\t\tcase GF_ODF_ESD_UPDATE_TAG:\n\t\t\tesdU = (GF_ESDUpdate *) com;\n\t\t\tesdU2 = (GF_ESDUpdate *) gf_odf_com_new(GF_ODF_ESD_UPDATE_TAG);\n\t\t\tesdU2->ODID = esdU->ODID;\n\t\t\ti=0;\n\t\t\twhile ((ref = (GF_ES_ID_Ref*)gf_list_enum(esdU->ESDescriptors, &i))) {\n\t\t\t\t//if the ref index is not valid, skip this desc...\n\t\t\t\tif (gf_isom_get_track_from_id(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1]) == NULL) continue;\n\t\t\t\t//OK, get the esd\n\t\t\t\te = GetESDForTime(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1], sample->DTS, &esd);\n\t\t\t\tif (e) goto err_exit;\n\t\t\t\te = GF_OUT_OF_MEM;\n\t\t\t\tgoto err_exit;\n\t\t\t}\n\t\t\tskipped = 0;\n\t\t\t//get the ES_ID in the mpod indicated in the ES_ID[]\n\t\t\tfor (i = 0; i < esdR->NbESDs; i++) {\n\t\t\t\t//if the ref index is not valid, remove this desc...\n\t\t\t\tif (gf_isom_get_track_from_id(mdia->mediaTrack->moov, mpod->trackIDs[esdR->ES_ID[i] - 1]) == NULL) {\n\t\t\t\t\tskipped ++;\n\t\t\t\t} else {\n\t\t\t\t\t//the command in the file has the ref index of the trackID in the mpod", "project": "gpac", "hash": 257511211780956322541197733348730262940, "size": 196, "commit_id": "f0ba83717b6e4d7a15a1676d1fe06152e199b011", "message": "fixed #1772 (fuzz)", "target": 1, "dataset": "other", "idx": 197890}
{"func": "\t\t\t\t}\n\n\t\t\t\t//then rewrite the ESDesc\n\t\t\t\tj=0;\n\t\t\t\twhile ((ref = (GF_ES_ID_Ref*)gf_list_enum(isom_od->ES_ID_RefDescriptors, &j))) {\n\t\t\t\t\tif (!mpod->trackIDs || !ref->trackRef || (ref->trackRef>mpod->trackIDCount)) continue;\n\t\t\t\t\t//if the ref index is not valid, skip this desc...\n\t\t\t\t\tif (gf_isom_get_track_from_id(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1]) == NULL) continue;\n\t\t\t\t\t//OK, get the esd\n\t\t\t\t\te = GetESDForTime(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1], sample->DTS, &esd);\n\t\t\t\t\tif (!e) e = gf_odf_desc_add_desc((GF_Descriptor *) od, (GF_Descriptor *) esd);\n\t\t\t\t\tif (e) {\n\t\t\t\t\t\tgf_odf_desc_del((GF_Descriptor *)od);\n\t\t\tesdU = (GF_ESDUpdate *) com;\n\t\t\tesdU2 = (GF_ESDUpdate *) gf_odf_com_new(GF_ODF_ESD_UPDATE_TAG);\n\t\t\tesdU2->ODID = esdU->ODID;\n\t\t\ti=0;\n\t\t\twhile ((ref = (GF_ES_ID_Ref*)gf_list_enum(esdU->ESDescriptors, &i))) {\n\t\t\t\tif (!mpod->trackIDs || !ref->trackRef || (ref->trackRef>mpod->trackIDCount)) continue;\n\t\t\t\t//if the ref index is not valid, skip this desc...\n\t\t\t\tif (gf_isom_get_track_from_id(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1]) == NULL) continue;\n\t\t\t\t//OK, get the esd\n\t\t\t\te = GetESDForTime(mdia->mediaTrack->moov, mpod->trackIDs[ref->trackRef - 1], sample->DTS, &esd);\n\t\t\t\tif (e) goto err_exit;\n\t\t\t\tgoto err_exit;\n\t\t\t}\n\t\t\tskipped = 0;\n\t\t\t//get the ES_ID in the mpod indicated in the ES_ID[]\n\t\t\tfor (i = 0; i < esdR->NbESDs; i++) {\n\t\t\t\tif (!mpod->trackIDs || !esdR->ES_ID[i] || (esdR->ES_ID[i]>mpod->trackIDCount)) continue;\n\t\t\t\t//if the ref index is not valid, remove this desc...\n\t\t\t\tif (gf_isom_get_track_from_id(mdia->mediaTrack->moov, mpod->trackIDs[esdR->ES_ID[i] - 1]) == NULL) {\n\t\t\t\t\tskipped ++;\n\t\t\t\t} else {\n\t\t\t\t\t//the command in the file has the ref index of the trackID in the mpod", "project": "gpac", "hash": 70307577295047254971344546776198646014, "size": 199, "commit_id": "f0ba83717b6e4d7a15a1676d1fe06152e199b011", "message": "fixed #1772 (fuzz)", "target": 0, "dataset": "other", "idx": 265416}
{"func": "  TfLiteTensor* transposed_weights =\n      data->weights_are_transposed\n          ? GetTemporary(context, node, data->transposed_weights_index)\n          : nullptr;\n  const auto* params =\n      reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\n\n  // Resize any deferred dynamic tensors\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));\n  }", "project": "tensorflow", "hash": 290740829738125502038333394047295607616, "size": 123, "commit_id": "801c1c6be5324219689c98e1bd3e0ca365ee834d", "message": "Fix another division by 0 in TFLite\n\nPiperOrigin-RevId: 370800181\nChange-Id: I924809166a6131f5075e6d45c455106538d755f9", "target": 1, "dataset": "other", "idx": 197892}
{"func": "      data->weights_are_transposed\n          ? GetTemporary(context, node, data->transposed_weights_index)\n          : nullptr;\n  const auto* params =\n      reinterpret_cast<TfLiteTransposeConvParams*>(node->builtin_data);\n\n  // Prevent divisions by 0\n  TF_LITE_ENSURE(context, params->stride_height > 0);\n  TF_LITE_ENSURE(context, params->stride_width > 0);\n\n  // Resize any deferred dynamic tensors\n  if (IsDynamicTensor(output)) {\n    TF_LITE_ENSURE_OK(context, ResizeTensor(context, output_shape, output));\n  }", "project": "tensorflow", "hash": 66934807467757635082995213425103966019, "size": 127, "commit_id": "801c1c6be5324219689c98e1bd3e0ca365ee834d", "message": "Fix another division by 0 in TFLite\n\nPiperOrigin-RevId: 370800181\nChange-Id: I924809166a6131f5075e6d45c455106538d755f9", "target": 0, "dataset": "other", "idx": 265428}
{"func": "int ecall_restore(const char *input, uint64_t input_len, char **output,\n                  uint64_t *output_len) {\n  if (!asylo::primitives::TrustedPrimitives::IsOutsideEnclave(input,\n                                                              input_len) ||\n      !asylo::primitives::TrustedPrimitives::IsOutsideEnclave(\n          output_len, sizeof(uint64_t))) {\n    asylo::primitives::TrustedPrimitives::BestEffortAbort(\n        \"ecall_restore: input/output found to not be in untrusted memory.\");\n  }\n  int result = 0;\n  size_t tmp_output_len;", "project": "asylo", "hash": 104108689796384044909836162554974524043, "size": 23, "commit_id": "382da2b8b09cbf928668a2445efb778f76bd9c8a", "message": "Check output of ecall_restore is outside enclave\n\nPiperOrigin-RevId: 334265380\nChange-Id: Ifbaead6bce56f01b2a4d69f53ca508d0138f6f61", "target": 1, "dataset": "other", "idx": 197910}
{"func": "int ecall_restore(const char *input, uint64_t input_len, char **output,\n                  uint64_t *output_len) {\n  if (!asylo::primitives::TrustedPrimitives::IsOutsideEnclave(input,\n                                                              input_len) ||\n      !asylo::primitives::TrustedPrimitives::IsOutsideEnclave(\n          output_len, sizeof(uint64_t)) ||\n      !asylo::primitives::TrustedPrimitives::IsOutsideEnclave(output,\n                                                              *output_len)) {\n    asylo::primitives::TrustedPrimitives::BestEffortAbort(\n        \"ecall_restore: input/output found to not be in untrusted memory.\");\n  }\n  int result = 0;\n  size_t tmp_output_len;", "project": "asylo", "hash": 330875018941072471989270103810819584619, "size": 25, "commit_id": "382da2b8b09cbf928668a2445efb778f76bd9c8a", "message": "Check output of ecall_restore is outside enclave\n\nPiperOrigin-RevId: 334265380\nChange-Id: Ifbaead6bce56f01b2a4d69f53ca508d0138f6f61", "target": 0, "dataset": "other", "idx": 265529}
{"func": "CString CWebSock::GetSkinPath(const CString& sSkinName) {\n    CString sRet = CZNC::Get().GetZNCPath() + \"/webskins/\" + sSkinName;\n\n    if (!CFile::IsDir(sRet)) {\n        sRet = CZNC::Get().GetCurPath() + \"/webskins/\" + sSkinName;\n\n        if (!CFile::IsDir(sRet)) {\n            sRet = CString(_SKINDIR_) + \"/\" + sSkinName;\n        }\n    }\n\n    return sRet + \"/\";\n}", "project": "znc", "hash": 235465360297438929701123024752111071031, "size": 13, "commit_id": "a4a5aeeb17d32937d8c7d743dae9a4cc755ce773", "message": "Don't let web skin name ../../../../ access files outside of usual skins directories.\n\nThanks for Jeriko One <jeriko.one@gmx.us> for finding and reporting this.", "target": 1, "dataset": "other", "idx": 197927}
{"func": "CString CWebSock::GetSkinPath(const CString& sSkinName) {\n    const CString sSkin = sSkinName.Replace_n(\"/\", \"_\").Replace_n(\".\", \"_\");\n\n    CString sRet = CZNC::Get().GetZNCPath() + \"/webskins/\" + sSkin;\n\n    if (!CFile::IsDir(sRet)) {\n        sRet = CZNC::Get().GetCurPath() + \"/webskins/\" + sSkin;\n\n        if (!CFile::IsDir(sRet)) {\n            sRet = CString(_SKINDIR_) + \"/\" + sSkin;\n        }\n    }\n\n    return sRet + \"/\";\n}", "project": "znc", "hash": 9744272250058996937416307326980903460, "size": 15, "commit_id": "a4a5aeeb17d32937d8c7d743dae9a4cc755ce773", "message": "Don't let web skin name ../../../../ access files outside of usual skins directories.\n\nThanks for Jeriko One <jeriko.one@gmx.us> for finding and reporting this.", "target": 0, "dataset": "other", "idx": 265791}
{"func": "GF_Err gf_isom_set_extraction_slc(GF_ISOFile *the_file, u32 trackNumber, u32 StreamDescriptionIndex, const GF_SLConfig *slConfig)\n{\n\tGF_TrackBox *trak;\n\tGF_SampleEntryBox *entry;\n\tGF_Err e;\n\tGF_SLConfig **slc;\n\n\ttrak = gf_isom_get_track_from_file(the_file, trackNumber);\n\tif (!trak) return GF_BAD_PARAM;\n\n\te = Media_GetSampleDesc(trak->Media, StreamDescriptionIndex, &entry, NULL);\n\tif (e) return e;\n\n\t//we must be sure we are not using a remote ESD\n\tswitch (entry->type) {\n\tcase GF_ISOM_BOX_TYPE_MP4S:\n\t\tif (((GF_MPEGSampleEntryBox *)entry)->esd->desc->slConfig->predefined != SLPredef_MP4) return GF_BAD_PARAM;\n\t\tslc = & ((GF_MPEGSampleEntryBox *)entry)->slc;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4A:\n\t\tif (((GF_MPEGAudioSampleEntryBox *)entry)->esd->desc->slConfig->predefined != SLPredef_MP4) return GF_BAD_PARAM;\n\t\tslc = & ((GF_MPEGAudioSampleEntryBox *)entry)->slc;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4V:\n\t\tif (((GF_MPEGVisualSampleEntryBox *)entry)->esd->desc->slConfig->predefined != SLPredef_MP4) return GF_BAD_PARAM;\n\t\tslc = & ((GF_MPEGVisualSampleEntryBox *)entry)->slc;\n\t\tbreak;\n\tdefault:\n\t\treturn GF_BAD_PARAM;\n\t}\n\n\tif (*slc) {\n\t\tgf_odf_desc_del((GF_Descriptor *)*slc);\n\t\t*slc = NULL;\n\t}\n\tif (!slConfig) return GF_OK;\n\t//finally duplicate the SL\n\treturn gf_odf_desc_copy((GF_Descriptor *) slConfig, (GF_Descriptor **) slc);\n}", "project": "gpac", "hash": 24076123659296712931521214875422093085, "size": 39, "commit_id": "ebfa346eff05049718f7b80041093b4c5581c24e", "message": "fixed #1706", "target": 1, "dataset": "other", "idx": 197972}
{"func": "GF_Err gf_isom_get_extraction_slc(GF_ISOFile *the_file, u32 trackNumber, u32 StreamDescriptionIndex, GF_SLConfig **slConfig)\n{\n\tGF_TrackBox *trak;\n\tGF_SampleEntryBox *entry;\n\tGF_Err e;\n\tGF_SLConfig *slc;\n\n\ttrak = gf_isom_get_track_from_file(the_file, trackNumber);\n\tif (!trak) return GF_BAD_PARAM;\n\n\te = Media_GetSampleDesc(trak->Media, StreamDescriptionIndex, &entry, NULL);\n\tif (e) return e;\n\n\t//we must be sure we are not using a remote ESD\n\tslc = NULL;\n\t*slConfig = NULL;\n\tswitch (entry->type) {\n\tcase GF_ISOM_BOX_TYPE_MP4S:\n\t\tif (((GF_MPEGSampleEntryBox *)entry)->esd->desc->slConfig->predefined != SLPredef_MP4) return GF_BAD_PARAM;\n\t\tslc = ((GF_MPEGSampleEntryBox *)entry)->slc;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4A:\n\t\tif (((GF_MPEGAudioSampleEntryBox *)entry)->esd->desc->slConfig->predefined != SLPredef_MP4) return GF_BAD_PARAM;\n\t\tslc = ((GF_MPEGAudioSampleEntryBox *)entry)->slc;\n\t\tbreak;\n\tcase GF_ISOM_BOX_TYPE_MP4V:\n\t\tif (((GF_MPEGVisualSampleEntryBox *)entry)->esd->desc->slConfig->predefined != SLPredef_MP4) return GF_BAD_PARAM;\n\t\tslc = ((GF_MPEGVisualSampleEntryBox *)entry)->slc;\n\t\tbreak;\n\tdefault:\n\t\treturn GF_BAD_PARAM;\n\t}\n\n\tif (!slc) return GF_OK;\n\t//finally duplicate the SL\n\treturn gf_odf_desc_copy((GF_Descriptor *) slc, (GF_Descriptor **) slConfig);\n}", "project": "gpac", "hash": 72759115469568498014505720549617618583, "size": 37, "commit_id": "ebfa346eff05049718f7b80041093b4c5581c24e", "message": "fixed #1706", "target": 0, "dataset": "other", "idx": 267342}
{"func": "inline size_t codepoint_length(const char *s8, size_t l) {\n  if (l) {\n    auto b = static_cast<uint8_t>(s8[0]);\n    if ((b & 0x80) == 0) {\n      return 1;\n    } else if ((b & 0xE0) == 0xC0) {\n      return 2;\n    } else if ((b & 0xF0) == 0xE0) {\n      return 3;\n    } else if ((b & 0xF8) == 0xF0) {\n      return 4;\n    }\n  }\n  return 0;\n}", "project": "cpp-peglib", "hash": 242224596264463421605909250265556736943, "size": 15, "commit_id": "b3b29ce8f3acf3a32733d930105a17d7b0ba347e", "message": "Fix #122", "target": 1, "dataset": "other", "idx": 197987}
{"func": "inline size_t codepoint_length(const char *s8, size_t l) {\n  if (l) {\n    auto b = static_cast<uint8_t>(s8[0]);\n    if ((b & 0x80) == 0) {\n      return 1;\n    } else if ((b & 0xE0) == 0xC0 && l >= 2) {\n      return 2;\n    } else if ((b & 0xF0) == 0xE0 && l >= 3) {\n      return 3;\n    } else if ((b & 0xF8) == 0xF0 && l >= 4) {\n      return 4;\n    }\n  }\n  return 0;\n}", "project": "cpp-peglib", "hash": 173436653321319195624342406646839908574, "size": 15, "commit_id": "b3b29ce8f3acf3a32733d930105a17d7b0ba347e", "message": "Fix #122", "target": 0, "dataset": "other", "idx": 267830}
{"func": "    const CPUDevice& d = context->eigen_device<CPUDevice>();\n\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(reverse_index_map_t->shape()),\n        errors::InvalidArgument(\"reverse_index_map must be a vector, saw: \",\n                                reverse_index_map_t->shape().DebugString()));\n\n    const auto reverse_index_map = reverse_index_map_t->vec<int64>();\n    const auto grad_values = grad_values_t->vec<T>();\n\n    const int64 N = reverse_index_map_t->shape().dim_size(0);\n\n    for (int i = 0; i < N; ++i) {\n      // Locate the index of the output of the forward prop associated\n      // with this location in the input of the forward prop.  Copy\n      // the gradient into it.  Mark it as visited.\n      d_values(i) = grad_values(reverse_index_map(i));\n      visited(reverse_index_map(i)) = true;\n    }\n    for (int j = 0; j < N_full; ++j) {\n      // The default value gradient gets the accumulated remainder of\n      // the backprop values (since the default value was used to fill\n      // in these slots in the forward calculation).", "project": "tensorflow", "hash": 241621722999531832605599622432512754717, "size": 53, "commit_id": "390611e0d45c5793c7066110af37c8514e6a6c54", "message": "Fix heap buffer overflow in `tf.raw_ops.SparseFillEmptyRowsGrad`.\n\nAlso add tests as they were lacking\n\nPiperOrigin-RevId: 332566071\nChange-Id: I44277578e26ff5fb3fdb0dcbba6e91b2ec3e7859", "target": 1, "dataset": "other", "idx": 197988}
{"func": "\n    OP_REQUIRES(\n        context, TensorShapeUtils::IsVector(reverse_index_map_t->shape()),\n        errors::InvalidArgument(\"reverse_index_map must be a vector, saw: \",\n                                reverse_index_map_t->shape().DebugString()));\n    OP_REQUIRES(context, TensorShapeUtils::IsVector(grad_values_t->shape()),\n                errors::InvalidArgument(\"grad_values must be a vector, saw: \",\n                                        grad_values_t->shape().DebugString()));\n\n    const auto reverse_index_map = reverse_index_map_t->vec<int64>();\n    const auto grad_values = grad_values_t->vec<T>();\n\n    const int64 N = reverse_index_map_t->shape().dim_size(0);\n\n    for (int i = 0; i < N; ++i) {\n      // Locate the index of the output of the forward prop associated\n      // with this location in the input of the forward prop.  Copy\n      // the gradient into it.  Mark it as visited.\n      int64 reverse_index = reverse_index_map(i);\n      OP_REQUIRES(\n          context, 0 <= reverse_index && reverse_index < N_full,\n          errors::InvalidArgument(\"Elements in reverse index must be in [0, \",\n                                  N_full, \") but got \", reverse_index));\n      d_values(i) = grad_values(reverse_index);\n      visited(reverse_index) = true;\n    }\n    for (int j = 0; j < N_full; ++j) {\n      // The default value gradient gets the accumulated remainder of\n      // the backprop values (since the default value was used to fill\n      // in these slots in the forward calculation).", "project": "tensorflow", "hash": 220172366288150634974249956340400544400, "size": 61, "commit_id": "390611e0d45c5793c7066110af37c8514e6a6c54", "message": "Fix heap buffer overflow in `tf.raw_ops.SparseFillEmptyRowsGrad`.\n\nAlso add tests as they were lacking\n\nPiperOrigin-RevId: 332566071\nChange-Id: I44277578e26ff5fb3fdb0dcbba6e91b2ec3e7859", "target": 0, "dataset": "other", "idx": 267831}
{"func": "GF_Err stbl_AppendSize(GF_SampleTableBox *stbl, u32 size, u32 nb_pack)\n{\n\tu32 i;\n\tif (!nb_pack) nb_pack = 1;\n\n\tif (!stbl->SampleSize->sampleCount) {\n\t\tstbl->SampleSize->sampleSize = size;\n\t\tstbl->SampleSize->sampleCount += nb_pack;\n\t\treturn GF_OK;\n\t}\n\tif (stbl->SampleSize->sampleSize && (stbl->SampleSize->sampleSize==size)) {\n\t\tstbl->SampleSize->sampleCount += nb_pack;\n\t\treturn GF_OK;\n\t}\n\tif (!stbl->SampleSize->sizes || (stbl->SampleSize->sampleCount+nb_pack > stbl->SampleSize->alloc_size)) {\n\t\tBool init_table = (stbl->SampleSize->sizes==NULL) ? 1 : 0;\n\t\tALLOC_INC(stbl->SampleSize->alloc_size);\n\t\tif (stbl->SampleSize->sampleCount+nb_pack > stbl->SampleSize->alloc_size)\n\t\t\tstbl->SampleSize->alloc_size = stbl->SampleSize->sampleCount+nb_pack;\n\n\t\tstbl->SampleSize->sizes = (u32 *)gf_realloc(stbl->SampleSize->sizes, sizeof(u32)*stbl->SampleSize->alloc_size);\n\t\tif (!stbl->SampleSize->sizes) return GF_OUT_OF_MEM;\n\t\tmemset(&stbl->SampleSize->sizes[stbl->SampleSize->sampleCount], 0, sizeof(u32) * (stbl->SampleSize->alloc_size - stbl->SampleSize->sampleCount) );\n", "project": "gpac", "hash": 40626517707074933311961960171654671734, "size": 40, "commit_id": "77ed81c069e10b3861d88f72e1c6be1277ee7eae", "message": "fixed #1774 (fuzz)", "target": 1, "dataset": "other", "idx": 197993}
{"func": "GF_Err stbl_AppendSize(GF_SampleTableBox *stbl, u32 size, u32 nb_pack)\n{\n\tu32 i;\n\tCHECK_PACK(GF_ISOM_INVALID_FILE)\n\n\tif (!stbl->SampleSize->sampleCount) {\n\t\tstbl->SampleSize->sampleSize = size;\n\t\tstbl->SampleSize->sampleCount += nb_pack;\n\t\treturn GF_OK;\n\t}\n\tif (stbl->SampleSize->sampleSize && (stbl->SampleSize->sampleSize==size)) {\n\t\tstbl->SampleSize->sampleCount += nb_pack;\n\t\treturn GF_OK;\n\t}\n\n\tif (!stbl->SampleSize->sizes || (stbl->SampleSize->sampleCount+nb_pack > stbl->SampleSize->alloc_size)) {\n\t\tBool init_table = (stbl->SampleSize->sizes==NULL) ? 1 : 0;\n\t\tALLOC_INC(stbl->SampleSize->alloc_size);\n\t\tif (stbl->SampleSize->sampleCount+nb_pack > stbl->SampleSize->alloc_size) {\n\t\t\tstbl->SampleSize->alloc_size = stbl->SampleSize->sampleCount+nb_pack;\n\t\t}\n\n\t\tstbl->SampleSize->sizes = (u32 *)gf_realloc(stbl->SampleSize->sizes, sizeof(u32)*stbl->SampleSize->alloc_size);\n\t\tif (!stbl->SampleSize->sizes) return GF_OUT_OF_MEM;\n\t\tmemset(&stbl->SampleSize->sizes[stbl->SampleSize->sampleCount], 0, sizeof(u32) * (stbl->SampleSize->alloc_size - stbl->SampleSize->sampleCount) );\n", "project": "gpac", "hash": 253799151103891324267945073818521020976, "size": 42, "commit_id": "77ed81c069e10b3861d88f72e1c6be1277ee7eae", "message": "fixed #1774 (fuzz)", "target": 0, "dataset": "other", "idx": 267899}
{"func": "        vector_max = max_y;\n        tensor_data = x_data;\n        tensor_num_elements = x.NumElements();\n        tensor_min = min_x;\n        tensor_max = max_x;\n      }\n      VectorTensorAddition<T, Toutput>(\n          vector_data, vector_min, vector_max, vector_num_elements, tensor_data,\n          tensor_min, tensor_max, tensor_num_elements, min_z_value, max_z_value,\n          z_data);\n    } else {", "project": "tensorflow", "hash": 33107701774077650282967373677576892726, "size": 114, "commit_id": "744009c9e5cc5d0447f0dc39d055f917e1fd9e16", "message": "Validate work in `QuantizedAdd`, ensure at least one element.\n\nPiperOrigin-RevId: 370127996\nChange-Id: I57c6f3e01afdeada84737820a131590137463855", "target": 1, "dataset": "other", "idx": 198002}
{"func": "        tensor_data = x_data;\n        tensor_num_elements = x.NumElements();\n        tensor_min = min_x;\n        tensor_max = max_x;\n      }\n      OP_REQUIRES(context, vector_num_elements > 0,\n                  errors::InvalidArgument(\"Must have some elements to add\"));\n      VectorTensorAddition<T, Toutput>(\n          vector_data, vector_min, vector_max, vector_num_elements, tensor_data,\n          tensor_min, tensor_max, tensor_num_elements, min_z_value, max_z_value,\n          z_data);\n    } else {", "project": "tensorflow", "hash": 315245398897635072708517159831180331701, "size": 116, "commit_id": "744009c9e5cc5d0447f0dc39d055f917e1fd9e16", "message": "Validate work in `QuantizedAdd`, ensure at least one element.\n\nPiperOrigin-RevId: 370127996\nChange-Id: I57c6f3e01afdeada84737820a131590137463855", "target": 0, "dataset": "other", "idx": 267926}
{"func": "  // prefix_dim_size == # of elements before the axis\n  // depth == # of elements per axis\n  // suffix_dim_size == # of elements after the axis\n  int prefix_dim_size = 1;\n  for (int i = 0; i < op_context.axis; ++i) {\n    prefix_dim_size *= op_context.indices->dims->data[i];\n  }\n  const int suffix_dim_size = NumElements(op_context.indices) / prefix_dim_size;\n  const int depth = *op_context.depth->data.i32;\n\n  const T on_value = *GetTensorData<T>(op_context.on_value);", "project": "tensorflow", "hash": 227621958541673910375815913303164077833, "size": 32, "commit_id": "3ebedd7e345453d68e279cfc3e4072648e5e12e5", "message": "Prevent division by 0 in OneHot implementation\n\nIf input indices is degenerate, the implementation would do a divide by zero. See https://github.com/tensorflow/tensorflow/blob/745d57df6d5e9bc568666a2a48ed8dd629c27241/tensorflow/lite/kernels/one_hot.cc#L68-L72\n\nPiperOrigin-RevId: 370966870\nChange-Id: Ie018337811c8016b5a1d3a277d00d5f2e19a2058", "target": 1, "dataset": "other", "idx": 198007}
{"func": "  // depth == # of elements per axis\n  // suffix_dim_size == # of elements after the axis\n  int prefix_dim_size = 1;\n  for (int i = 0; i < op_context.axis; ++i) {\n    prefix_dim_size *= op_context.indices->dims->data[i];\n  }\n  if (prefix_dim_size == 0) {\n    // If indices tensor is degenerate, return a degenerate tensor, just like\n    // TensorFlow does.\n    return;\n  }\n  const int suffix_dim_size = NumElements(op_context.indices) / prefix_dim_size;\n  const int depth = *op_context.depth->data.i32;\n\n  const T on_value = *GetTensorData<T>(op_context.on_value);", "project": "tensorflow", "hash": 254202826454691860542754520195247330043, "size": 37, "commit_id": "3ebedd7e345453d68e279cfc3e4072648e5e12e5", "message": "Prevent division by 0 in OneHot implementation\n\nIf input indices is degenerate, the implementation would do a divide by zero. See https://github.com/tensorflow/tensorflow/blob/745d57df6d5e9bc568666a2a48ed8dd629c27241/tensorflow/lite/kernels/one_hot.cc#L68-L72\n\nPiperOrigin-RevId: 370966870\nChange-Id: Ie018337811c8016b5a1d3a277d00d5f2e19a2058", "target": 0, "dataset": "other", "idx": 267934}
{"func": "                num_split_ >= 1 && num_split_ <= input_shape.vec<int64>()(axis),\n                errors::InvalidArgument(\"Input num_split should be between 1 \"\n                                        \"and the splitting dimension size (\",\n                                        input_shape.vec<int64>()(axis),\n                                        \"), got \", num_split_));\n\n    sparse::SparseTensor sparse_tensor;\n    OP_REQUIRES_OK(context,\n                   sparse::SparseTensor::Create(\n                       input_indices, input_values,\n                       TensorShape(input_shape.vec<int64>()), &sparse_tensor));\n\n    std::vector<sparse::SparseTensor> outputs;\n    OP_REQUIRES_OK(context, sparse::SparseTensor::Split<T>(\n                                sparse_tensor, axis, num_split_, &outputs));\n", "project": "tensorflow", "hash": 117534156949568730230637035600140974853, "size": 58, "commit_id": "4c0ee937c0f61c4fc5f5d32d9bb4c67428012a60", "message": "Prevent overflow in sparse op\n\nPiperOrigin-RevId: 372442006\nChange-Id: I60fe31cd7e56fb3501e97c63500caf902ddeee96", "target": 1, "dataset": "other", "idx": 198015}
{"func": "                errors::InvalidArgument(\"Input num_split should be between 1 \"\n                                        \"and the splitting dimension size (\",\n                                        input_shape.vec<int64>()(axis),\n                                        \"), got \", num_split_));\n\n    // Prevent overflow by constructing the dense shape separately\n    TensorShape dense_shape;\n    const auto input_shape_flat = input_shape.flat<int64>();\n    for (int i = 0; i < input_shape.NumElements(); i++) {\n      OP_REQUIRES_OK(context,\n                     dense_shape.AddDimWithStatus(input_shape_flat(i)));\n    }\n\n    sparse::SparseTensor sparse_tensor;\n    OP_REQUIRES_OK(context,\n                   sparse::SparseTensor::Create(input_indices, input_values,\n                                                dense_shape, &sparse_tensor));\n\n    std::vector<sparse::SparseTensor> outputs;\n    OP_REQUIRES_OK(context, sparse::SparseTensor::Split<T>(\n                                sparse_tensor, axis, num_split_, &outputs));\n", "project": "tensorflow", "hash": 197793840772920757051167476588018697571, "size": 65, "commit_id": "4c0ee937c0f61c4fc5f5d32d9bb4c67428012a60", "message": "Prevent overflow in sparse op\n\nPiperOrigin-RevId: 372442006\nChange-Id: I60fe31cd7e56fb3501e97c63500caf902ddeee96", "target": 0, "dataset": "other", "idx": 268106}
{"func": "\t\t}\n\n\t\tesd = gf_isom_get_esd(file, sceneT, 1);\n\t\tif (gf_isom_get_sample_count(file, sceneT)==1) {\n\t\t\tsamp = gf_isom_get_sample(file, sceneT, 1, &descIndex);\n\t\t\tif (gf_hinter_can_embbed_data(samp->data, samp->dataLength, GF_STREAM_SCENE)) {\n\n\t\t\t\tslc.timeScale = slc.timestampResolution = gf_isom_get_media_timescale(file, sceneT);\n\t\t\t\tslc.OCRResolution = 1000;\n\t\t\t\tslc.startCTS = samp->DTS+samp->CTS_Offset;\n\t\t\t\tslc.startDTS = samp->DTS;", "project": "gpac", "hash": 223649118603140396935168133970864337311, "size": 173, "commit_id": "1653f31cf874eb6df964bea88d58d8e9b98b485e", "message": "fixed #1770 (fuzz)", "target": 1, "dataset": "other", "idx": 198018}
{"func": "\t\t}\n\n\t\tesd = gf_isom_get_esd(file, sceneT, 1);\n\t\tif (gf_isom_get_sample_count(file, sceneT)==1) {\n\t\t\tsamp = gf_isom_get_sample(file, sceneT, 1, &descIndex);\n\t\t\tif (samp && gf_hinter_can_embbed_data(samp->data, samp->dataLength, GF_STREAM_SCENE)) {\n\n\t\t\t\tslc.timeScale = slc.timestampResolution = gf_isom_get_media_timescale(file, sceneT);\n\t\t\t\tslc.OCRResolution = 1000;\n\t\t\t\tslc.startCTS = samp->DTS+samp->CTS_Offset;\n\t\t\t\tslc.startDTS = samp->DTS;", "project": "gpac", "hash": 130544004901735494505112604949465488922, "size": 173, "commit_id": "1653f31cf874eb6df964bea88d58d8e9b98b485e", "message": "fixed #1770 (fuzz)", "target": 0, "dataset": "other", "idx": 268147}
{"func": "Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst,\n                                  int input_index) {\n  DataType src_out = src->output_type(output_index);\n  DataType dst_in = dst->input_type(input_index);\n  if (!TypesCompatible(dst_in, src_out)) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(), \" was passed \",", "project": "tensorflow", "hash": 119805169193765224854689007636694320603, "size": 13, "commit_id": "0cc38aaa4064fd9e79101994ce9872c6d91f816b", "message": "Prevent unitialized memory access in `GraphConstructor::MakeEdge`\n\nThe `MakeEdge` implementation assumes that there exists an output at `output_index` of `src` node and an input at `input_index` of `dst` node. However, if this is not the case this results in accessing data out of bounds. Because we are accessing an array that is a private member of a class and only in read only mode, this usually results only in unitialized memory access. However, it is reasonable to think that malicious users could manipulate these indexes to actually read data outside the class, thus resulting in information leakage and further exploits.\n\nPiperOrigin-RevId: 346343288\nChange-Id: I2127da27c2023d27f26efd39afa6c853385cab6f", "target": 1, "dataset": "other", "idx": 198049}
{"func": "Status GraphConstructor::MakeEdge(Node* src, int output_index, Node* dst,\n                                  int input_index) {\n  if (output_index >= src->num_outputs()) {\n    return errors::InvalidArgument(\n        \"Output \", output_index, \" of node \", src->name(),\n        \" does not exist. Node only has \", src->num_outputs(), \" outputs.\");\n  }\n  if (input_index >= dst->num_inputs()) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(),\n        \" does not exist. Node only has \", dst->num_inputs(), \" inputs.\");\n  }\n\n  DataType src_out = src->output_type(output_index);\n  DataType dst_in = dst->input_type(input_index);\n  if (!TypesCompatible(dst_in, src_out)) {\n    return errors::InvalidArgument(\n        \"Input \", input_index, \" of node \", dst->name(), \" was passed \",", "project": "tensorflow", "hash": 213211601759384738680783148753824892622, "size": 24, "commit_id": "0cc38aaa4064fd9e79101994ce9872c6d91f816b", "message": "Prevent unitialized memory access in `GraphConstructor::MakeEdge`\n\nThe `MakeEdge` implementation assumes that there exists an output at `output_index` of `src` node and an input at `input_index` of `dst` node. However, if this is not the case this results in accessing data out of bounds. Because we are accessing an array that is a private member of a class and only in read only mode, this usually results only in unitialized memory access. However, it is reasonable to think that malicious users could manipulate these indexes to actually read data outside the class, thus resulting in information leakage and further exploits.\n\nPiperOrigin-RevId: 346343288\nChange-Id: I2127da27c2023d27f26efd39afa6c853385cab6f", "target": 0, "dataset": "other", "idx": 268323}
{"func": "static int download(struct SPDBDownloader *pd) {\n\tSPDBDownloaderOpt *opt = pd->opt;\n\tchar *curl_cmd = NULL;\n\tchar *extractor_cmd = NULL;\n\tchar *abspath_to_archive = NULL;\n\tchar *abspath_to_file = NULL;\n\tchar *archive_name = NULL;\n\tsize_t archive_name_len = 0;\n\tchar *symbol_store_path = NULL;\n\tchar *dbg_file = NULL;\n\tchar *guid = NULL;\n\tchar *archive_name_escaped  = NULL;\n\tchar *user_agent = NULL;\n\tchar *symbol_server = NULL;\n\n\tint res = 0;\n\tint cmd_ret;\n\tif (!opt->dbg_file || !*opt->dbg_file) {\n\t\t// no pdb debug file\n\t\treturn 0;\n\t}\n\tif (!checkCurl ()) {\n\t\treturn 0;\n\t}\n\t// dbg_file len is > 0\n\tarchive_name_len = strlen (opt->dbg_file);\n\tarchive_name = malloc (archive_name_len + 1);\n\tif (!archive_name) {\n\t\treturn 0;\n\t}\n\tmemcpy (archive_name, opt->dbg_file, archive_name_len + 1);\n\tarchive_name[archive_name_len - 1] = '_';\n\tsymbol_store_path = r_str_escape (opt->symbol_store_path);\n\tdbg_file = r_str_escape (opt->dbg_file);\n\tguid = r_str_escape (opt->guid);\n\tarchive_name_escaped = r_str_escape (archive_name);\n\tuser_agent = r_str_escape (opt->user_agent);\n\tsymbol_server = r_str_escape (opt->symbol_server);\n\n\tabspath_to_archive = r_str_newf (\"%s%s%s%s%s%s%s\",\n\t\t\t    symbol_store_path, R_SYS_DIR,\n\t\t\t    dbg_file, R_SYS_DIR,\n\t\t\t    guid, R_SYS_DIR,\n\t\t\t    archive_name_escaped);\n\n\tabspath_to_file = strdup (abspath_to_archive);\n\tabspath_to_file[strlen (abspath_to_file) - 1] = 'b';\n\tif (r_file_exists (abspath_to_file)) {\n\t\teprintf (\"File already downloaded.\\n\");\n\t\tR_FREE (user_agent);\n\t\tR_FREE (abspath_to_archive);\n\t\tR_FREE (archive_name_escaped);\n\t\tR_FREE (symbol_store_path);\n\t\tR_FREE (dbg_file);\n\t\tR_FREE (guid);\n\t\tR_FREE (archive_name);\n\t\tR_FREE (abspath_to_file);\n\t\tR_FREE (symbol_server);\n\t\treturn 1;\n\t}\n\n\tif (checkExtract () || opt->extract == 0) {\n\t\tres = 1;\n\n\t\tcurl_cmd = r_str_newf (\"curl -sfLA \\\"%s\\\" \\\"%s/%s/%s/%s\\\" --create-dirs -o \\\"%s\\\"\",\n\t\t                       user_agent,\n\t\t                       symbol_server,\n\t\t\t\t\t\t\t   dbg_file,\n\t\t\t\t\t\t\t   guid,\n\t\t                       archive_name_escaped,\n\t\t                       abspath_to_archive);\n#if __WINDOWS__\n\t\tconst char *cabextractor = \"expand\";\n\t\tconst char *format = \"%s %s %s\";\n\n\t\t// extractor_cmd -> %1 %2 %3\n\t\t// %1 - 'expand'\n\t\t// %2 - absolute path to archive\n\t\t// %3 - absolute path to file that will be dearchive\n\t\textractor_cmd = r_str_newf (format, cabextractor,\n\t\t\tabspath_to_archive, abspath_to_file);\n#else\n\t\tconst char *cabextractor = \"cabextract\";\n\t\tconst char *format = \"%s -d \\\"%s\\\" \\\"%s\\\"\";\n\t\tchar *abspath_to_dir = r_file_dirname (abspath_to_archive);\n\t\t// cabextract -d %1 %2\n\t\t// %1 - path to directory where to extract all files from cab archive\n\t\t// %2 - absolute path to cab archive\n\t\textractor_cmd = r_str_newf (format, cabextractor, abspath_to_dir, abspath_to_archive);\n\t\tR_FREE (abspath_to_dir);\n#endif\n\t\teprintf (\"Attempting to download compressed pdb in %s\\n\", abspath_to_archive);\n\t\tif ((cmd_ret = r_sys_cmd (curl_cmd) != 0)) {\n\t\t\teprintf(\"curl exited with error %d\\n\", cmd_ret);\n\t\t\tres = 0;\n\t\t}\n\t\teprintf (\"Attempting to decompress pdb\\n\");\n\t\tif (opt->extract > 0) {\n\t\t\tif (res && ((cmd_ret = r_sys_cmd (extractor_cmd)) != 0)) {\n\t\t\t\teprintf (\"cab extractor exited with error %d\\n\", cmd_ret);\n\t\t\t\tres = 0;\n\t\t\t}\n\t\t\tr_file_rm (abspath_to_archive);\n\t\t}\n\t\tR_FREE (curl_cmd);\n\t}\n\tif (res == 0) {\n\t\teprintf (\"Falling back to uncompressed pdb\\n\");\n\t\tres = 1;\n\n\t\tarchive_name_escaped[strlen (archive_name_escaped) - 1] = 'b';\n\n\t\tcurl_cmd = r_str_newf (\"curl -sfLA \\\"%s\\\" \\\"%s/%s/%s/%s\\\" --create-dirs -o \\\"%s\\\"\",\n\t\t                       opt->user_agent,\n\t\t                       opt->symbol_server,\n\t\t                       opt->dbg_file,\n\t\t                       opt->guid,\n\t\t                       archive_name_escaped,\n\t\t                       abspath_to_file);\n\t\teprintf (\"Attempting to download uncompressed pdb in %s\\n\", abspath_to_file);\n\t\tif ((cmd_ret = r_sys_cmd (curl_cmd) != 0)) {\n\t\t\teprintf(\"curl exited with error %d\\n\", cmd_ret);\n\t\t\tres = 0;\n\t\t}\n\t\tR_FREE (curl_cmd);\n\t}\n\tR_FREE (abspath_to_archive);\n\tR_FREE (abspath_to_file);\n\tR_FREE (archive_name);\n\tR_FREE (extractor_cmd);\n\tR_FREE (symbol_store_path);\n\tR_FREE (dbg_file);\n\tR_FREE (guid);\n\tR_FREE (archive_name_escaped);\n\tR_FREE (user_agent);\n\tR_FREE (symbol_server);\n\treturn res;\n}", "project": "radare2", "hash": 235350912544331798088834006772646860216, "size": 138, "commit_id": "04edfa82c1f3fa2bc3621ccdad2f93bdbf00e4f9", "message": "Fix command injection on PDB download (#16966)\n\n* Fix r_sys_mkdirp with absolute path on Windows\r\n* Fix build with --with-openssl\r\n* Use RBuffer in r_socket_http_answer()\r\n* r_socket_http_answer: Fix read for big responses\r\n* Implement r_str_escape_sh()\r\n* Cleanup r_socket_connect() on Windows\r\n* Fix socket being created without a protocol\r\n* Fix socket connect with SSL ##socket\r\n* Use select() in r_socket_ready()\r\n* Fix read failing if received only protocol answer\r\n* Fix double-free\r\n* r_socket_http_get: Fail if req. SSL with no support\r\n* Follow redirects in r_socket_http_answer()\r\n* Fix r_socket_http_get result length with R2_CURL=1\r\n* Also follow redirects\r\n* Avoid using curl for downloading PDBs\r\n* Use r_socket_http_get() on UNIXs\r\n* Use WinINet API on Windows for r_socket_http_get()\r\n* Fix command injection\r\n* Fix r_sys_cmd_str_full output for binary data\r\n* Validate GUID on PDB download\r\n* Pass depth to socket_http_get_recursive()\r\n* Remove 'r_' and '__' from static function names\r\n* Fix is_valid_guid\r\n* Fix for comments", "target": 1, "dataset": "other", "idx": 198095}
{"func": "static int download(struct SPDBDownloader *pd) {\n\tSPDBDownloaderOpt *opt = pd->opt;\n\tint res = 0;\n\tint cmd_ret;\n\n\tif (!opt->dbg_file || !*opt->dbg_file) {\n\t\t// no pdb debug file\n\t\treturn 0;\n\t}\n\n\tchar *abspath_to_file = r_str_newf (\"%s%s%s%s%s%s%s\",\n\t\topt->symbol_store_path, R_SYS_DIR,\n\t\topt->dbg_file, R_SYS_DIR,\n\t\topt->guid, R_SYS_DIR,\n\t\topt->dbg_file);\n\n\tif (r_file_exists (abspath_to_file)) {\n\t\teprintf (\"File already downloaded.\\n\");\n\t\tfree (abspath_to_file);\n\t\treturn 1;\n\t}\n\n\tif (checkExtract () || opt->extract == 0) {\n\t\tchar *extractor_cmd = NULL;\n\t\tchar *archive_name = strdup (opt->dbg_file);\n\t\tarchive_name[strlen (archive_name) - 1] = '_';\n\t\tchar *abspath_to_archive = r_str_newf (\"%s%s%s%s%s%s%s\",\n\t\t\topt->symbol_store_path, R_SYS_DIR,\n\t\t\topt->dbg_file, R_SYS_DIR,\n\t\t\topt->guid, R_SYS_DIR,\n\t\t\tarchive_name);\n\n\t\teprintf (\"Attempting to download compressed pdb in %s\\n\", abspath_to_archive);\n\t\tchar *abs_arch_esc = r_str_escape_sh (abspath_to_archive);\n#if __WINDOWS__\n\t\tchar *abs_file_esc = r_str_escape_sh (abspath_to_file);\n\t\t// expand %1 %2\n\t\t// %1 - absolute path to archive\n\t\t// %2 - absolute path to file that will be dearchive\n\t\textractor_cmd = r_str_newf (\"expand \\\"%s\\\" \\\"%s\\\"\", abs_arch_esc, abs_file_esc);\n\t\tfree (abs_file_esc);\n#else\n\t\tchar *abspath_to_dir = r_file_dirname (abspath_to_archive);\n\t\tchar *abs_dir_esc = r_str_escape_sh (abspath_to_dir);\n\t\t// cabextract -d %1 %2\n\t\t// %1 - path to directory where to extract all files from cab archive\n\t\t// %2 - absolute path to cab archive\n\t\textractor_cmd = r_str_newf (\"cabextract -d \\\"%s\\\" \\\"%s\\\"\", abs_arch_esc, abs_dir_esc);\n\t\tfree (abs_dir_esc);\n\t\tfree (abspath_to_dir);\n#endif\n\t\tfree (abs_arch_esc);\n\t\tres = download_and_write (opt, archive_name);\n\n\t\tif (opt->extract > 0 && res) {\n\t\t\teprintf (\"Attempting to decompress pdb\\n\");\n\t\t\tif (res && ((cmd_ret = r_sys_cmd (extractor_cmd)) != 0)) {\n\t\t\t\teprintf (\"cab extractor exited with error %d\\n\", cmd_ret);\n\t\t\t\tres = 0;\n\t\t\t}\n\t\t\tr_file_rm (abspath_to_archive);\n\t\t}\n\t\tfree (archive_name);\n\t\tfree (abspath_to_archive);\n\t}\n\tif (res == 0) {\n\t\teprintf (\"Falling back to uncompressed pdb\\n\");\n\t\teprintf (\"Attempting to download uncompressed pdb in %s\\n\", abspath_to_file);\n\t\tres = download_and_write (opt, opt->dbg_file);\n\t}\n\tfree (abspath_to_file);\n\treturn res;\n}", "project": "radare2", "hash": 284642675192813296779193504141476552469, "size": 73, "commit_id": "04edfa82c1f3fa2bc3621ccdad2f93bdbf00e4f9", "message": "Fix command injection on PDB download (#16966)\n\n* Fix r_sys_mkdirp with absolute path on Windows\r\n* Fix build with --with-openssl\r\n* Use RBuffer in r_socket_http_answer()\r\n* r_socket_http_answer: Fix read for big responses\r\n* Implement r_str_escape_sh()\r\n* Cleanup r_socket_connect() on Windows\r\n* Fix socket being created without a protocol\r\n* Fix socket connect with SSL ##socket\r\n* Use select() in r_socket_ready()\r\n* Fix read failing if received only protocol answer\r\n* Fix double-free\r\n* r_socket_http_get: Fail if req. SSL with no support\r\n* Follow redirects in r_socket_http_answer()\r\n* Fix r_socket_http_get result length with R2_CURL=1\r\n* Also follow redirects\r\n* Avoid using curl for downloading PDBs\r\n* Use r_socket_http_get() on UNIXs\r\n* Use WinINet API on Windows for r_socket_http_get()\r\n* Fix command injection\r\n* Fix r_sys_cmd_str_full output for binary data\r\n* Validate GUID on PDB download\r\n* Pass depth to socket_http_get_recursive()\r\n* Remove 'r_' and '__' from static function names\r\n* Fix is_valid_guid\r\n* Fix for comments", "target": 0, "dataset": "other", "idx": 268829}
{"func": "{\n\tservice_info *finger = NULL;\n\turi_type parsed_url;\n\turi_type parsed_url_in;\n\n\tif (table &&\n\t\tparse_uri(eventURLPath, strlen(eventURLPath), &parsed_url_in) ==\n\t\t\tHTTP_SUCCESS) {\n\t\tfinger = table->serviceList;\n\t\twhile (finger) {\n\t\t\tif (finger->eventURL) {\n\t\t\t\tif (parse_uri(finger->eventURL,\n\t\t\t\t\t    strlen(finger->eventURL),", "project": "pupnp", "hash": 48140258843428848992487276517349841836, "size": 28, "commit_id": "c805c1de1141cb22f74c0d94dd5664bda37398e0", "message": "Fixes #177: NULL pointer dereference in FindServiceControlURLPath\n\nAlso fixes its dual bug in FindServiceEventURLPath.", "target": 1, "dataset": "other", "idx": 198108}
{"func": "{\n\tservice_info *finger = NULL;\n\turi_type parsed_url;\n\turi_type parsed_url_in;\n\n\tif (!table || !eventURLPath) {\n\t\treturn NULL;\n\t}\n\tif (parse_uri(eventURLPath, strlen(eventURLPath), &parsed_url_in) ==\n\t\tHTTP_SUCCESS) {\n\t\tfinger = table->serviceList;\n\t\twhile (finger) {\n\t\t\tif (finger->eventURL) {\n\t\t\t\tif (parse_uri(finger->eventURL,\n\t\t\t\t\t    strlen(finger->eventURL),", "project": "pupnp", "hash": 36856745095971474562957251675118738361, "size": 30, "commit_id": "c805c1de1141cb22f74c0d94dd5664bda37398e0", "message": "Fixes #177: NULL pointer dereference in FindServiceControlURLPath\n\nAlso fixes its dual bug in FindServiceEventURLPath.", "target": 0, "dataset": "other", "idx": 269104}
{"func": "{\n\tservice_info *finger = NULL;\n\turi_type parsed_url;\n\turi_type parsed_url_in;\n\n\tif (table && parse_uri(controlURLPath,\n\t\t\t     strlen(controlURLPath),\n\t\t\t     &parsed_url_in) == HTTP_SUCCESS) {\n\t\tfinger = table->serviceList;\n\t\twhile (finger) {\n\t\t\tif (finger->controlURL) {\n\t\t\t\tif (parse_uri(finger->controlURL,\n\t\t\t\t\t    strlen(finger->controlURL),", "project": "pupnp", "hash": 143022199409412141809012906075259083414, "size": 28, "commit_id": "c805c1de1141cb22f74c0d94dd5664bda37398e0", "message": "Fixes #177: NULL pointer dereference in FindServiceControlURLPath\n\nAlso fixes its dual bug in FindServiceEventURLPath.", "target": 1, "dataset": "other", "idx": 198109}
{"func": "{\n\tservice_info *finger = NULL;\n\turi_type parsed_url;\n\turi_type parsed_url_in;\n\n\tif (!table || !controlURLPath) {\n\t\treturn NULL;\n\t}\n\tif (parse_uri(controlURLPath, strlen(controlURLPath), &parsed_url_in) ==\n\t\tHTTP_SUCCESS) {\n\t\tfinger = table->serviceList;\n\t\twhile (finger) {\n\t\t\tif (finger->controlURL) {\n\t\t\t\tif (parse_uri(finger->controlURL,\n\t\t\t\t\t    strlen(finger->controlURL),", "project": "pupnp", "hash": 309107714980423598473483753401169639487, "size": 30, "commit_id": "c805c1de1141cb22f74c0d94dd5664bda37398e0", "message": "Fixes #177: NULL pointer dereference in FindServiceControlURLPath\n\nAlso fixes its dual bug in FindServiceEventURLPath.", "target": 0, "dataset": "other", "idx": 269101}
{"func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& x = context->input(0);\n    const Tensor& y = context->input(1);\n    const float min_x = context->input(2).flat<float>()(0);\n    const float max_x = context->input(3).flat<float>()(0);\n    const float min_y = context->input(4).flat<float>()(0);\n    const float max_y = context->input(5).flat<float>()(0);\n\n    BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n    if (!bcast.IsValid()) {\n      context->SetStatus(errors::InvalidArgument(\n          \"Incompatible shapes: \", x.shape().DebugString(), \" vs. \",", "project": "tensorflow", "hash": 321222250988638329539662374382316933530, "size": 104, "commit_id": "efea03b38fb8d3b81762237dc85e579cc5fc6e87", "message": "Validate inputs to `QuantizedMul`\n\nPiperOrigin-RevId: 369756982\nChange-Id: I00d960cc3b9316fd7a86bd37a44e341c96e17624", "target": 1, "dataset": "other", "idx": 198110}
{"func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& x = context->input(0);\n    const Tensor& y = context->input(1);\n    auto& min_x_tensor = context->input(2);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_x_tensor.shape()),\n                errors::InvalidArgument(\"min_x must be a scalar\"));\n    const float min_x = min_x_tensor.flat<float>()(0);\n    auto& max_x_tensor = context->input(3);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_x_tensor.shape()),\n                errors::InvalidArgument(\"max_x must be a scalar\"));\n    const float max_x = max_x_tensor.flat<float>()(0);\n    auto& min_y_tensor = context->input(4);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_y_tensor.shape()),\n                errors::InvalidArgument(\"min_y must be a scalar\"));\n    const float min_y = min_y_tensor.flat<float>()(0);\n    auto& max_y_tensor = context->input(5);\n    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_y_tensor.shape()),\n                errors::InvalidArgument(\"max_y must be a scalar\"));\n    const float max_y = max_y_tensor.flat<float>()(0);\n\n    BCast bcast(BCast::FromShape(x.shape()), BCast::FromShape(y.shape()));\n    if (!bcast.IsValid()) {\n      context->SetStatus(errors::InvalidArgument(\n          \"Incompatible shapes: \", x.shape().DebugString(), \" vs. \",", "project": "tensorflow", "hash": 190937515744695115531252219907447927152, "size": 116, "commit_id": "efea03b38fb8d3b81762237dc85e579cc5fc6e87", "message": "Validate inputs to `QuantizedMul`\n\nPiperOrigin-RevId: 369756982\nChange-Id: I00d960cc3b9316fd7a86bd37a44e341c96e17624", "target": 0, "dataset": "other", "idx": 269121}
{"func": "inline int MatchingDim(const RuntimeShape& shape1, int index1,\n                       const RuntimeShape& shape2, int index2) {\n  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n  return shape1.Dims(index1);\n}", "project": "tensorflow", "hash": 292684460118693762063488568821033472831, "size": 5, "commit_id": "8ee24e7949a203d234489f9da2c5bf45a7d5157d", "message": "[tflite] Ensure `MatchingDim` does not allow buffer overflow.\n\nWe check in `MatchingDim` that both arguments have the same dimensionality, however that is a `DCHECK` only enabled if building in debug mode. Hence, it could be possible to cause buffer overflows by passing in a tensor with larger dimensions as the second argument. To fix, we now make `MatchingDim` return the minimum of the two sizes.\n\nA much better fix would be to return a status object but that requires refactoring a large part of the codebase for minor benefits.\n\nPiperOrigin-RevId: 332526127\nChange-Id: If627d0d2c80a685217b6e0d1e64b0872dbf1c5e4", "target": 1, "dataset": "other", "idx": 198111}
{"func": "inline int MatchingDim(const RuntimeShape& shape1, int index1,\n                       const RuntimeShape& shape2, int index2) {\n  TFLITE_DCHECK_EQ(shape1.Dims(index1), shape2.Dims(index2));\n  return std::min(shape1.Dims(index1), shape2.Dims(index2));\n}", "project": "tensorflow", "hash": 212828257128375980629692282330012667499, "size": 5, "commit_id": "8ee24e7949a203d234489f9da2c5bf45a7d5157d", "message": "[tflite] Ensure `MatchingDim` does not allow buffer overflow.\n\nWe check in `MatchingDim` that both arguments have the same dimensionality, however that is a `DCHECK` only enabled if building in debug mode. Hence, it could be possible to cause buffer overflows by passing in a tensor with larger dimensions as the second argument. To fix, we now make `MatchingDim` return the minimum of the two sizes.\n\nA much better fix would be to return a status object but that requires refactoring a large part of the codebase for minor benefits.\n\nPiperOrigin-RevId: 332526127\nChange-Id: If627d0d2c80a685217b6e0d1e64b0872dbf1c5e4", "target": 0, "dataset": "other", "idx": 269181}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    // This call processes inputs 1 and 2 to write output 0.\n    ReshapeOp::Compute(ctx);\n\n    const float input_min_float = ctx->input(2).flat<float>()(0);\n    const float input_max_float = ctx->input(3).flat<float>()(0);\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    output_min->flat<float>()(0) = input_min_float;\n\n    Tensor* output_max = nullptr;", "project": "tensorflow", "hash": 138415375282410567846030933274549860111, "size": 14, "commit_id": "a324ac84e573fba362a5e53d4e74d5de6729933e", "message": "Validate arguments to `QuantizedReshape`.\n\nEnsure that validations from `Reshape` also terminate `QuantizedReshape` on failure.\n\nPiperOrigin-RevId: 369775421\nChange-Id: If8c5342267aceea65b7cb83a4b183304886f1ce8", "target": 1, "dataset": "other", "idx": 198140}
{"func": "  void Compute(OpKernelContext* ctx) override {\n    // This call processes inputs 1 and 2 to write output 0.\n    ReshapeOp::Compute(ctx);\n    if (!ctx->status().ok()) {\n      return;\n    }\n\n    const auto& input_min_float_tensor = ctx->input(2);\n    const auto& input_min_float_shape = input_min_float_tensor.shape();\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsScalar(input_min_float_shape) ||\n                    (TensorShapeUtils::IsVector(input_min_float_shape) &&\n                     (input_min_float_shape.dim_size(0) == 1)),\n                errors::InvalidArgument(\n                    \"input_min must be a scalar or a vector of 1 element\"));\n    const float input_min_float = input_min_float_tensor.flat<float>()(0);\n    const auto& input_max_float_tensor = ctx->input(3);\n    const auto& input_max_float_shape = input_max_float_tensor.shape();\n    OP_REQUIRES(ctx,\n                TensorShapeUtils::IsScalar(input_max_float_shape) ||\n                    (TensorShapeUtils::IsVector(input_max_float_shape) &&\n                     (input_max_float_shape.dim_size(0) == 1)),\n                errors::InvalidArgument(\n                    \"input_max must be a scalar or a vector of 1 element\"));\n    const float input_max_float = input_max_float_tensor.flat<float>()(0);\n\n    Tensor* output_min = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, TensorShape({}), &output_min));\n    output_min->flat<float>()(0) = input_min_float;\n\n    Tensor* output_max = nullptr;", "project": "tensorflow", "hash": 159137396648765332305552157383255155319, "size": 34, "commit_id": "a324ac84e573fba362a5e53d4e74d5de6729933e", "message": "Validate arguments to `QuantizedReshape`.\n\nEnsure that validations from `Reshape` also terminate `QuantizedReshape` on failure.\n\nPiperOrigin-RevId: 369775421\nChange-Id: If8c5342267aceea65b7cb83a4b183304886f1ce8", "target": 0, "dataset": "other", "idx": 269702}
{"func": "    Emit(\"-new-window\", params.url, \"\", params.disposition, \"\", params.referrer,\n         params.post_data);\n    return nullptr;\n  }\n\n  // Give user a chance to cancel navigation.\n  if (Emit(\"will-navigate\", params.url))\n    return nullptr;\n\n  // Don't load the URL if the web contents was marked as destroyed from a\n  // will-navigate event listener\n  if (IsDestroyed())\n    return nullptr;\n\n  return CommonWebContentsDelegate::OpenURLFromTab(source, params);\n}", "project": "electron", "hash": 266355652945031614418015863162348662454, "size": 20, "commit_id": "18613925610ba319da7f497b6deed85ad712c59b", "message": "refactor: wire will-navigate up to a navigation throttle instead of OpenURL (#25108)\n\n* refactor: wire will-navigate up to a navigation throttle instead of OpenURL (#25065)\r\n\r\n* refactor: wire will-navigate up to a navigation throttle instead of OpenURL\r\n\r\n* spec: add test for x-site _top navigation\r\n\r\n* chore: old code be old", "target": 1, "dataset": "other", "idx": 198143}
{"func": "  if (params.disposition != WindowOpenDisposition::CURRENT_TAB) {\n    Emit(\"-new-window\", params.url, \"\", params.disposition, \"\", params.referrer,\n         params.post_data);\n    return nullptr;\n  }\n\n  if (IsDestroyed())\n    return nullptr;\n\n  return CommonWebContentsDelegate::OpenURLFromTab(source, params);\n}", "project": "electron", "hash": 49577923825265945582623513612554035537, "size": 14, "commit_id": "18613925610ba319da7f497b6deed85ad712c59b", "message": "refactor: wire will-navigate up to a navigation throttle instead of OpenURL (#25108)\n\n* refactor: wire will-navigate up to a navigation throttle instead of OpenURL (#25065)\r\n\r\n* refactor: wire will-navigate up to a navigation throttle instead of OpenURL\r\n\r\n* spec: add test for x-site _top navigation\r\n\r\n* chore: old code be old", "target": 0, "dataset": "other", "idx": 269738}
{"func": "otError Commissioner::GeneratePskc(const char *              aPassPhrase,\n                                   const char *              aNetworkName,\n                                   const Mac::ExtendedPanId &aExtPanId,\n                                   Pskc &                    aPskc)\n{\n    otError     error      = OT_ERROR_NONE;\n    const char *saltPrefix = \"Thread\";\n    uint8_t     salt[OT_PBKDF2_SALT_MAX_LEN];\n    uint16_t    saltLen = 0;\n\n    VerifyOrExit((strlen(aPassPhrase) >= OT_COMMISSIONING_PASSPHRASE_MIN_SIZE) &&\n                     (strlen(aPassPhrase) <= OT_COMMISSIONING_PASSPHRASE_MAX_SIZE) &&\n                     (strlen(aNetworkName) <= OT_NETWORK_NAME_MAX_SIZE),\n                 error = OT_ERROR_INVALID_ARGS);\n\n    memset(salt, 0, sizeof(salt));\n    memcpy(salt, saltPrefix, strlen(saltPrefix));\n    saltLen += static_cast<uint16_t>(strlen(saltPrefix));\n\n    memcpy(salt + saltLen, aExtPanId.m8, sizeof(aExtPanId));\n    saltLen += OT_EXT_PAN_ID_SIZE;\n\n    memcpy(salt + saltLen, aNetworkName, strlen(aNetworkName));\n    saltLen += static_cast<uint16_t>(strlen(aNetworkName));\n\n    otPbkdf2Cmac(reinterpret_cast<const uint8_t *>(aPassPhrase), static_cast<uint16_t>(strlen(aPassPhrase)),\n                 reinterpret_cast<const uint8_t *>(salt), saltLen, 16384, OT_PSKC_MAX_SIZE, aPskc.m8);\n\nexit:\n    return error;\n}", "project": "openthread", "hash": 269174587310881469223710673598831931018, "size": 31, "commit_id": "c3a3a0c424322009fec3ab735fb20ce8f6e19e70", "message": "[commissioner] use strnlen instead of strlen (#4404)", "target": 1, "dataset": "other", "idx": 198147}
{"func": "otError Commissioner::GeneratePskc(const char *              aPassPhrase,\n                                   const char *              aNetworkName,\n                                   const Mac::ExtendedPanId &aExtPanId,\n                                   Pskc &                    aPskc)\n{\n    otError    error        = OT_ERROR_NONE;\n    const char saltPrefix[] = \"Thread\";\n    uint8_t    salt[OT_PBKDF2_SALT_MAX_LEN];\n    uint16_t   saltLen = 0;\n    uint16_t   passphraseLen;\n    uint8_t    networkNameLen;\n\n    passphraseLen  = static_cast<uint16_t>(strnlen(aPassPhrase, OT_COMMISSIONING_PASSPHRASE_MAX_SIZE + 1));\n    networkNameLen = static_cast<uint8_t>(strnlen(aNetworkName, OT_NETWORK_NAME_MAX_SIZE + 1));\n\n    VerifyOrExit((passphraseLen >= OT_COMMISSIONING_PASSPHRASE_MIN_SIZE) &&\n                     (passphraseLen <= OT_COMMISSIONING_PASSPHRASE_MAX_SIZE) &&\n                     (networkNameLen <= OT_NETWORK_NAME_MAX_SIZE),\n                 error = OT_ERROR_INVALID_ARGS);\n\n    memset(salt, 0, sizeof(salt));\n    memcpy(salt, saltPrefix, sizeof(saltPrefix) - 1);\n    saltLen += static_cast<uint16_t>(sizeof(saltPrefix) - 1);\n\n    memcpy(salt + saltLen, aExtPanId.m8, sizeof(aExtPanId));\n    saltLen += OT_EXT_PAN_ID_SIZE;\n\n    memcpy(salt + saltLen, aNetworkName, networkNameLen);\n    saltLen += networkNameLen;\n\n    otPbkdf2Cmac(reinterpret_cast<const uint8_t *>(aPassPhrase), passphraseLen, reinterpret_cast<const uint8_t *>(salt),\n                 saltLen, 16384, OT_PSKC_MAX_SIZE, aPskc.m8);\n\nexit:\n    return error;\n}", "project": "openthread", "hash": 130996794969779372185141123675944404951, "size": 36, "commit_id": "c3a3a0c424322009fec3ab735fb20ce8f6e19e70", "message": "[commissioner] use strnlen instead of strlen (#4404)", "target": 0, "dataset": "other", "idx": 269900}
{"func": "        return AVERROR_INVALIDDATA;\n    }\n\n    if (h->pps.sps_id != h->current_sps_id ||\n        h0->sps_buffers[h->pps.sps_id]->new) {\n        h0->sps_buffers[h->pps.sps_id]->new = 0;\n\n        h->current_sps_id = h->pps.sps_id;\n        h->sps            = *h0->sps_buffers[h->pps.sps_id];\n\n        if (h->mb_width  != h->sps.mb_width ||\n            h->mb_height != h->sps.mb_height * (2 - h->sps.frame_mbs_only_flag) ||\n            h->avctx->bits_per_raw_sample != h->sps.bit_depth_luma ||\n    }\n\n    if (h->ref_count[0]) h->er.last_pic = &h->ref_list[0][0];\n    if (h->ref_count[1]) h->er.next_pic = &h->ref_list[1][0];\n    h->er.ref_count = h->ref_count[0];\n    h0->au_pps_id = pps_id;\n\n    if (h->avctx->debug & FF_DEBUG_PICT_INFO) {\n        av_log(h->avctx, AV_LOG_DEBUG,\n               \"slice:%d %s mb:%d %c%s%s pps:%u frame:%d poc:%d/%d ref:%d/%d qp:%d loop:%d:%d:%d weight:%d%s %s\\n\",\n               h->slice_num,", "project": "FFmpeg", "hash": 24783762682806170514184972114597660646, "size": 694, "commit_id": "8a3b85f3a7952c54a2c36ba1797f7e0cde9f85aa", "message": "avcodec/h264: update current_sps & sps->new only after the whole slice header decoder and init code finished\n\nThis avoids them being cleared before the full initialization finished\n\nFixes out of array read\nFixes: asan_heap-oob_f0c5e6_7071_cov_1605985132_mov_h264_aac__Demo_FlagOfOurFathers.mov\nFound-by: Mateusz \"j00ru\" Jurczyk and Gynvael Coldwind\nSigned-off-by: Michael Niedermayer <michaelni@gmx.at>", "target": 1, "dataset": "other", "idx": 198173}
{"func": "        return AVERROR_INVALIDDATA;\n    }\n\n    if (h->pps.sps_id != h->current_sps_id ||\n        h0->sps_buffers[h->pps.sps_id]->new) {\n\n        h->sps            = *h0->sps_buffers[h->pps.sps_id];\n\n        if (h->mb_width  != h->sps.mb_width ||\n            h->mb_height != h->sps.mb_height * (2 - h->sps.frame_mbs_only_flag) ||\n            h->avctx->bits_per_raw_sample != h->sps.bit_depth_luma ||\n\n    if (h->ref_count[0]) h->er.last_pic = &h->ref_list[0][0];\n    if (h->ref_count[1]) h->er.next_pic = &h->ref_list[1][0];\n    h->er.ref_count = h->ref_count[0];\n    h0->au_pps_id = pps_id;\n    h->sps.new =\n    h0->sps_buffers[h->pps.sps_id]->new = 0;\n    h->current_sps_id = h->pps.sps_id;\n\n    if (h->avctx->debug & FF_DEBUG_PICT_INFO) {\n        av_log(h->avctx, AV_LOG_DEBUG,\n               \"slice:%d %s mb:%d %c%s%s pps:%u frame:%d poc:%d/%d ref:%d/%d qp:%d loop:%d:%d:%d weight:%d%s %s\\n\",\n               h->slice_num,", "project": "FFmpeg", "hash": 200232776860561206754163513001485909644, "size": 695, "commit_id": "8a3b85f3a7952c54a2c36ba1797f7e0cde9f85aa", "message": "avcodec/h264: update current_sps & sps->new only after the whole slice header decoder and init code finished\n\nThis avoids them being cleared before the full initialization finished\n\nFixes out of array read\nFixes: asan_heap-oob_f0c5e6_7071_cov_1605985132_mov_h264_aac__Demo_FlagOfOurFathers.mov\nFound-by: Mateusz \"j00ru\" Jurczyk and Gynvael Coldwind\nSigned-off-by: Michael Niedermayer <michaelni@gmx.at>", "target": 0, "dataset": "other", "idx": 270122}
{"func": "  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      outputs->push_back(Tensor(*context.mutable_output(i)));\n    }\n  }\n  return Status::OK();\n}", "project": "tensorflow", "hash": 268532713607530076905202853458432555638, "size": 77, "commit_id": "da8558533d925694483d2c136a9220d6d49d843c", "message": "Fix undefined behavior in `tf.raw_ops.Switch` in eager mode.\n\nPiperOrigin-RevId: 332578058\nChange-Id: I9727571d2f21476b10d8aa27c1b7176564b76ac9", "target": 1, "dataset": "other", "idx": 198174}
{"func": "  if (!context.status().ok()) return context.status();\n\n  if (outputs != nullptr) {\n    outputs->clear();\n    for (int i = 0; i < context.num_outputs(); ++i) {\n      const auto* output_tensor = context.mutable_output(i);\n      if (output_tensor != nullptr) {\n        outputs->push_back(Tensor(*output_tensor));\n      } else {\n        outputs->push_back(Tensor());\n      }\n    }\n  }\n  return Status::OK();\n}", "project": "tensorflow", "hash": 159214455117997485478071511420441997014, "size": 82, "commit_id": "da8558533d925694483d2c136a9220d6d49d843c", "message": "Fix undefined behavior in `tf.raw_ops.Switch` in eager mode.\n\nPiperOrigin-RevId: 332578058\nChange-Id: I9727571d2f21476b10d8aa27c1b7176564b76ac9", "target": 0, "dataset": "other", "idx": 270145}
{"func": "        ctx, size >= 0,\n        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;", "project": "tensorflow", "hash": 98707188461233797046371132573913629241, "size": 40, "commit_id": "eebb96c2830d48597d055d247c0e9aebaea94cd5", "message": "Fix an invalid address vulnerability in `tf.raw_ops.RaggedBincount`.\n\nPiperOrigin-RevId: 368293153\nChange-Id: I4b4e493d3fd05e7dc55a55de3a041a80a4f275c3", "target": 1, "dataset": "other", "idx": 198180}
{"func": "        errors::InvalidArgument(\"size (\", size, \") must be non-negative\"));\n\n    int num_rows = splits.size() - 1;\n    int num_values = values.size();\n    int batch_idx = 0;\n\n    OP_REQUIRES(ctx, splits(0) == 0,\n                errors::InvalidArgument(\"Splits must start with 0, not with \",\n                                        splits(0)));\n\n    OP_REQUIRES(ctx, splits(num_rows) == num_values,\n                errors::InvalidArgument(\n                    \"Splits must end with the number of values, got \",\n                    splits(num_rows), \" instead of \", num_values));\n\n    Tensor* out_t;\n    OP_REQUIRES_OK(\n        ctx, ctx->allocate_output(0, TensorShape({num_rows, size}), &out_t));\n    functor::SetZeroFunctor<Device, T> fill;", "project": "tensorflow", "hash": 127889019311252322646574831716871048286, "size": 49, "commit_id": "eebb96c2830d48597d055d247c0e9aebaea94cd5", "message": "Fix an invalid address vulnerability in `tf.raw_ops.RaggedBincount`.\n\nPiperOrigin-RevId: 368293153\nChange-Id: I4b4e493d3fd05e7dc55a55de3a041a80a4f275c3", "target": 0, "dataset": "other", "idx": 270171}
{"func": "    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    TensorShape tensor_input_shape(input_shape->vec<int64>());\n    gtl::InlinedVector<int64, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    auto input_shape_t = input_shape->vec<int64>();\n    const int64 N = input_shape_t(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_t.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});", "project": "tensorflow", "hash": 334418536661201182685376817370728187459, "size": 116, "commit_id": "69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c", "message": "Fix overflow CHECK issue with `tf.raw_ops.AddManySparseToTensorsMap`.\n\nPiperOrigin-RevId: 369492969\nChange-Id: I1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81", "target": 1, "dataset": "other", "idx": 198191}
{"func": "    OP_REQUIRES(\n        context, rank > 1,\n        errors::InvalidArgument(\n            \"Rank of input SparseTensor should be > 1, but saw rank: \", rank));\n\n    auto input_shape_vec = input_shape->vec<int64>();\n    int new_num_elements = 1;\n    bool overflow_ocurred = false;\n    for (int i = 0; i < input_shape_vec.size(); i++) {\n      new_num_elements =\n          MultiplyWithoutOverflow(new_num_elements, input_shape_vec(i));\n      if (new_num_elements < 0) {\n        overflow_ocurred = true;\n      }\n    }\n\n    OP_REQUIRES(\n        context, !overflow_ocurred,\n        errors::Internal(\"Encountered overflow from large input shape.\"));\n\n    TensorShape tensor_input_shape(input_shape_vec);\n    gtl::InlinedVector<int64, 8> std_order(rank);\n    std::iota(std_order.begin(), std_order.end(), 0);\n    SparseTensor input_st;\n    OP_REQUIRES_OK(context, SparseTensor::Create(*input_indices, *input_values,\n                                                 tensor_input_shape, std_order,\n                                                 &input_st));\n\n    const int64 N = input_shape_vec(0);\n\n    Tensor sparse_handles(DT_INT64, TensorShape({N}));\n    auto sparse_handles_t = sparse_handles.vec<int64>();\n\n    OP_REQUIRES_OK(context, input_st.IndicesValid());\n\n    // We can generate the output shape proto string now, for all\n    // minibatch entries.\n    TensorShape output_shape;\n    OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(\n                                input_shape_vec.data() + 1,\n                                input_shape->NumElements() - 1, &output_shape));\n\n    // Get groups by minibatch dimension\n    std::unordered_set<int64> visited;\n    sparse::GroupIterable minibatch = input_st.group({0});", "project": "tensorflow", "hash": 89479228034469738692764404438746447809, "size": 130, "commit_id": "69c68ecbb24dff3fa0e46da0d16c821a2dd22d7c", "message": "Fix overflow CHECK issue with `tf.raw_ops.AddManySparseToTensorsMap`.\n\nPiperOrigin-RevId: 369492969\nChange-Id: I1d70d6c0c92e3d7a25bc3b3aa2a0c0ac9688bf81", "target": 0, "dataset": "other", "idx": 270289}
{"func": "                seg = av_malloc(sizeof(struct segment));\n                if (!seg) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                seg->duration = duration;\n                seg->key_type = key_type;\n                if (has_iv) {\n                    memcpy(seg->iv, iv, sizeof(iv));\n                } else {\n                    int seq = pls->start_seq_no + pls->n_segments;\n                    memset(seg->iv, 0, sizeof(seg->iv));\n                    av_free(seg->key);\n                    av_free(seg);\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n\n                dynarray_add(&pls->segments, &pls->n_segments, seg);\n                is_segment = 0;\n\n                seg->size = seg_size;\n                if (seg_size >= 0) {", "project": "FFmpeg", "hash": 322685302452455197251925283777640246074, "size": 203, "commit_id": "6959358683c7533f586c07a766acc5fe9544d8b2", "message": "avformat/hls: check segment duration value of EXTINF\n\nfix ticket: 8673\nset the default EXTINF duration to 1ms if duration is smaller than 1ms\n\nSigned-off-by: Steven Liu <lq@chinaffmpeg.org>\n(cherry picked from commit 9dfb19baeb86a8bb02c53a441682c6e9a6e104cc)", "target": 1, "dataset": "other", "idx": 198208}
{"func": "                }\n                seg = av_malloc(sizeof(struct segment));\n                if (!seg) {\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n                if (has_iv) {\n                    memcpy(seg->iv, iv, sizeof(iv));\n                } else {\n                    int seq = pls->start_seq_no + pls->n_segments;\n                    memset(seg->iv, 0, sizeof(seg->iv));\n                    av_free(seg);\n                    ret = AVERROR(ENOMEM);\n                    goto fail;\n                }\n\n                if (duration < 0.001 * AV_TIME_BASE) {\n                    duration = 0.001 * AV_TIME_BASE;\n                }\n                seg->duration = duration;\n                seg->key_type = key_type;\n                dynarray_add(&pls->segments, &pls->n_segments, seg);\n                is_segment = 0;\n\n                seg->size = seg_size;\n                if (seg_size >= 0) {", "project": "FFmpeg", "hash": 143241207040584752751126445822656075184, "size": 206, "commit_id": "6959358683c7533f586c07a766acc5fe9544d8b2", "message": "avformat/hls: check segment duration value of EXTINF\n\nfix ticket: 8673\nset the default EXTINF duration to 1ms if duration is smaller than 1ms\n\nSigned-off-by: Steven Liu <lq@chinaffmpeg.org>\n(cherry picked from commit 9dfb19baeb86a8bb02c53a441682c6e9a6e104cc)", "target": 0, "dataset": "other", "idx": 270450}
{"func": "static int cardos_have_2048bit_package(sc_card_t *card)\n{\n\tsc_apdu_t apdu;\n        u8        rbuf[SC_MAX_APDU_BUFFER_SIZE];\n        int       r;\n\tconst u8  *p = rbuf, *q;\n\tsize_t    len, tlen = 0, ilen = 0;\n\n\tsc_format_apdu(card, &apdu, SC_APDU_CASE_2_SHORT, 0xca, 0x01, 0x88);\n\tapdu.resp    = rbuf;\n\tapdu.resplen = sizeof(rbuf);\n\tif ((len = apdu.resplen) == 0)\n\t\t/* looks like no package has been installed  */\n\t\treturn 0;\n\n\twhile (len != 0) {\n\t\tp = sc_asn1_find_tag(card->ctx, p, len, 0xe1, &tlen);\n\t\tif (p == NULL)\n\t\t\treturn 0;\n\t\tq = sc_asn1_find_tag(card->ctx, p, tlen, 0x01, &ilen);\n\t\tif (q == NULL || ilen != 4)\n\t\t\treturn 0;\n\t\tif (q[0] == 0x1c)\n\t\t\treturn 1;\n\t\tp   += tlen;", "project": "OpenSC", "hash": 92357617617140958826267024638443033197, "size": 35, "commit_id": "1252aca9f10771ef5ba8405e73cf2da50827958f", "message": "cardos: Correctly calculate the left bytes to avoid buffer overrun\n\nThanks oss-fuzz\n\nhttps://bugs.chromium.org/p/oss-fuzz/issues/detail?id=29912", "target": 1, "dataset": "other", "idx": 198234}
{"func": "static int cardos_have_2048bit_package(sc_card_t *card)\n{\n\tsc_apdu_t apdu;\n        u8        rbuf[SC_MAX_APDU_BUFFER_SIZE];\n        int       r;\n\tconst u8  *p = rbuf, *q, *pp;\n\tsize_t    len, tlen = 0, ilen = 0;\n\n\tsc_format_apdu(card, &apdu, SC_APDU_CASE_2_SHORT, 0xca, 0x01, 0x88);\n\tapdu.resp    = rbuf;\n\tapdu.resplen = sizeof(rbuf);\n\tif ((len = apdu.resplen) == 0)\n\t\t/* looks like no package has been installed  */\n\t\treturn 0;\n\n\twhile (len != 0) {\n\t\tpp = sc_asn1_find_tag(card->ctx, p, len, 0xe1, &tlen);\n\t\tif (pp == NULL)\n\t\t\treturn 0;\n\t\tq = sc_asn1_find_tag(card->ctx, pp, tlen, 0x01, &ilen);\n\t\tif (q == NULL || ilen != 4)\n\t\t\treturn 0;\n\t\tif (q[0] == 0x1c)\n\t\t\treturn 1;\n\t\tp   += tlen;", "project": "OpenSC", "hash": 331177212470604586059658277731450158649, "size": 35, "commit_id": "1252aca9f10771ef5ba8405e73cf2da50827958f", "message": "cardos: Correctly calculate the left bytes to avoid buffer overrun\n\nThanks oss-fuzz\n\nhttps://bugs.chromium.org/p/oss-fuzz/issues/detail?id=29912", "target": 0, "dataset": "other", "idx": 270673}
{"func": "GF_Err latm_dmx_process(GF_Filter *filter)\n{\n\tGF_LATMDmxCtx *ctx = gf_filter_get_udta(filter);\n\tGF_FilterPacket *pck, *dst_pck;\n\tu32 pos;\n\tu8 *data, *output;\n\tu32 pck_size, prev_pck_size;\n\tu64 cts = GF_FILTER_NO_TS;\n\n\tif (ctx->in_error)\n\t\treturn ctx->in_error;\n\n\t\t\t\tctx->src_pck = NULL;\n\t\t\t\treturn GF_EOS;\n\t\t\t}\n\t\t} else {\n\t\t\treturn GF_OK;\n\t\t}\n\t}\n\n\tdata = (char *) gf_filter_pck_get_data(pck, &pck_size);\n\n\t//input pid sets some timescale - we flushed pending data , update cts\n\tif (ctx->timescale && pck) {\n\t\tcts = gf_filter_pck_get_cts(pck);\n\t}", "project": "gpac", "hash": 129393460693122238437209841613363417813, "size": 138, "commit_id": "b2db2f99b4c30f96e17b9a14537c776da6cb5dca", "message": "fixed #1728", "target": 1, "dataset": "other", "idx": 198278}
{"func": "GF_Err latm_dmx_process(GF_Filter *filter)\n{\n\tGF_LATMDmxCtx *ctx = gf_filter_get_udta(filter);\n\tGF_FilterPacket *pck, *dst_pck;\n\tu32 pos;\n\tu8 *data=NULL, *output;\n\tu32 pck_size=0, prev_pck_size;\n\tu64 cts = GF_FILTER_NO_TS;\n\n\tif (ctx->in_error)\n\t\treturn ctx->in_error;\n\n\t\t\t\treturn GF_EOS;\n\t\t\t}\n\t\t} else {\n\t\t\treturn GF_OK;\n\t\t}\n\t} else {\n\t\tdata = (char *) gf_filter_pck_get_data(pck, &pck_size);\n\t}\n\n\t//input pid sets some timescale - we flushed pending data , update cts\n\tif (ctx->timescale && pck) {\n\t\tcts = gf_filter_pck_get_cts(pck);\n\t}", "project": "gpac", "hash": 10482263121380563115286320698149929915, "size": 138, "commit_id": "b2db2f99b4c30f96e17b9a14537c776da6cb5dca", "message": "fixed #1728", "target": 0, "dataset": "other", "idx": 271474}
{"func": "\t\t\t\t\tpos = gf_list_del_item(mov->TopBoxes, mov->brand);\n\t\t\t\t\tgf_isom_box_del((GF_Box *) mov->brand);\n\t\t\t\t\tmov->brand = brand;\n\t\t\t\t\tif (pos<0) pos=0;\n\t\t\t\t\tgf_list_insert(mov->TopBoxes, brand, pos);\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase GF_ISOM_BOX_TYPE_PDIN:\n\t\t\t/*ONE AND ONLY ONE PDIN*/", "project": "gpac", "hash": 86450605193316092897460625068419387462, "size": 474, "commit_id": "fe5155cf047252d1c4cb91602048bfa682af0ea7", "message": "fixed #1783 (fuzz)", "target": 1, "dataset": "other", "idx": 198286}
{"func": "\t\t\t\t\tgf_isom_box_del((GF_Box *) mov->brand);\n\t\t\t\t\tmov->brand = brand;\n\t\t\t\t\tif (pos<0) pos=0;\n\t\t\t\t\tgf_list_insert(mov->TopBoxes, brand, pos);\n\t\t\t\t}\n\t\t\t\tgf_isom_box_del(a);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase GF_ISOM_BOX_TYPE_PDIN:\n\t\t\t/*ONE AND ONLY ONE PDIN*/", "project": "gpac", "hash": 61440798691784431491544950992000959629, "size": 475, "commit_id": "fe5155cf047252d1c4cb91602048bfa682af0ea7", "message": "fixed #1783 (fuzz)", "target": 0, "dataset": "other", "idx": 271715}
{"func": "            perf_push(PERF_PROC_IN_LINK);\n            lsi = get_link_socket_info(c);\n            orig_buf = c->c2.buf.data;\n            if (process_incoming_link_part1(c, lsi, floated))\n            {\n                if (floated)\n                {\n                    multi_process_float(m, m->pending);\n                }\n\n                process_incoming_link_part2(c, lsi, orig_buf);", "project": "openvpn", "hash": 227170740362661013093459332997021567710, "size": 252, "commit_id": "37bc691e7d26ea4eb61a8a434ebd7a9ae76225ab", "message": "Fix illegal client float (CVE-2020-11810)\n\nThere is a time frame between allocating peer-id and initializing data\nchannel key (which is performed on receiving push request or on async\npush-reply) in which the existing peer-id float checks do not work right.\n\nIf a \"rogue\" data channel packet arrives during that time frame from\nanother address and  with same peer-id, this would cause client to float\nto that new address. This is because:\n\n - tls_pre_decrypt() sets packet length to zero if\n   data channel key has not been initialized, which leads to\n\n - openvpn_decrypt() returns true if packet length is zero,\n   which leads to\n\n - process_incoming_link_part1() returns true, which\n   calls multi_process_float(), which commits float\n\nNote that problem doesn't happen when data channel key is initialized,\nsince in this case openvpn_decrypt() returns false.\n\nThe net effect of this behaviour is that the VPN session for the\n\"victim client\" is broken.  Since the \"attacker client\" does not have\nsuitable keys, it can not inject or steal VPN traffic from the other\nsession.  The time window is small and it can not be used to attack\na specific client's session, unless some other way is found to make it\ndisconnect and reconnect first.\n\nCVE-2020-11810 has been assigned to acknowledge this risk.\n\nFix illegal float by adding buffer length check (\"is this packet still\nconsidered valid\") before calling multi_process_float().\n\nTrac: #1272\nCVE: 2020-11810\n\nSigned-off-by: Lev Stipakov <lev@openvpn.net>\nAcked-by: Arne Schwabe <arne@rfc2549.org>\nAcked-by: Antonio Quartulli <antonio@openvpn.net>\nAcked-by: Gert Doering <gert@greenie.muc.de>\nMessage-Id: <20200415073017.22839-1-lstipakov@gmail.com>\nURL: https://www.mail-archive.com/openvpn-devel@lists.sourceforge.net/msg19720.html\nSigned-off-by: Gert Doering <gert@greenie.muc.de>", "target": 1, "dataset": "other", "idx": 198315}
{"func": "            perf_push(PERF_PROC_IN_LINK);\n            lsi = get_link_socket_info(c);\n            orig_buf = c->c2.buf.data;\n            if (process_incoming_link_part1(c, lsi, floated))\n            {\n                /* nonzero length means that we have a valid, decrypted packed */\n                if (floated && c->c2.buf.len > 0)\n                {\n                    multi_process_float(m, m->pending);\n                }\n\n                process_incoming_link_part2(c, lsi, orig_buf);", "project": "openvpn", "hash": 293548728228125118158520364779443227300, "size": 253, "commit_id": "37bc691e7d26ea4eb61a8a434ebd7a9ae76225ab", "message": "Fix illegal client float (CVE-2020-11810)\n\nThere is a time frame between allocating peer-id and initializing data\nchannel key (which is performed on receiving push request or on async\npush-reply) in which the existing peer-id float checks do not work right.\n\nIf a \"rogue\" data channel packet arrives during that time frame from\nanother address and  with same peer-id, this would cause client to float\nto that new address. This is because:\n\n - tls_pre_decrypt() sets packet length to zero if\n   data channel key has not been initialized, which leads to\n\n - openvpn_decrypt() returns true if packet length is zero,\n   which leads to\n\n - process_incoming_link_part1() returns true, which\n   calls multi_process_float(), which commits float\n\nNote that problem doesn't happen when data channel key is initialized,\nsince in this case openvpn_decrypt() returns false.\n\nThe net effect of this behaviour is that the VPN session for the\n\"victim client\" is broken.  Since the \"attacker client\" does not have\nsuitable keys, it can not inject or steal VPN traffic from the other\nsession.  The time window is small and it can not be used to attack\na specific client's session, unless some other way is found to make it\ndisconnect and reconnect first.\n\nCVE-2020-11810 has been assigned to acknowledge this risk.\n\nFix illegal float by adding buffer length check (\"is this packet still\nconsidered valid\") before calling multi_process_float().\n\nTrac: #1272\nCVE: 2020-11810\n\nSigned-off-by: Lev Stipakov <lev@openvpn.net>\nAcked-by: Arne Schwabe <arne@rfc2549.org>\nAcked-by: Antonio Quartulli <antonio@openvpn.net>\nAcked-by: Gert Doering <gert@greenie.muc.de>\nMessage-Id: <20200415073017.22839-1-lstipakov@gmail.com>\nURL: https://www.mail-archive.com/openvpn-devel@lists.sourceforge.net/msg19720.html\nSigned-off-by: Gert Doering <gert@greenie.muc.de>", "target": 0, "dataset": "other", "idx": 272280}
{"func": "\n\t\t\t\t/*if we have only 1 sai_offsets, assume that its type is cenc*/\n\t\t\t\tif ((aux_info_type == GF_ISOM_CENC_SCHEME) || (aux_info_type == GF_ISOM_CBC_SCHEME) ||\n\t\t\t\t\t(aux_info_type == GF_ISOM_CENS_SCHEME) || (aux_info_type == GF_ISOM_CBCS_SCHEME) ||\n\t\t\t\t\t(gf_list_count(traf->sai_offsets) == 1)) {\n\t\t\t\t\toffset = saio->offsets[0] + moof_offset;\n\t\t\t\t\tnb_saio = saio->entry_count;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor (i = 0; i < gf_list_count(traf->sai_sizes); i++) {\n\t\t\t\tsaiz = (GF_SampleAuxiliaryInfoSizeBox *)gf_list_get(traf->sai_sizes, i);\n\t\t\t\taux_info_type = saiz->aux_info_type;\n\t\t\t\tif (!aux_info_type) aux_info_type = scheme_type;\n\t\t\t\t/*if we have only 1 sai_sizes, assume that its type is cenc*/\n\t\t\t\tif ((aux_info_type == GF_ISOM_CENC_SCHEME) || (aux_info_type == GF_ISOM_CBC_SCHEME) ||\n\t\t\t\t\t(aux_info_type == GF_ISOM_CENS_SCHEME) || (aux_info_type == GF_ISOM_CBCS_SCHEME) ||\n\t\t\t\t\t(gf_list_count(traf->sai_sizes) == 1)) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (saiz && saio && senc) {\n\t\t\t\tfor (i = 0; i < saiz->sample_count; i++) {\n\t\t\t\t\tGF_CENCSampleAuxInfo *sai;\n\t\t\t\t\tconst u8 *key_info=NULL;", "project": "gpac", "hash": 149457244667447276186807458417399459366, "size": 677, "commit_id": "df8fffd839fe5ae9acd82d26fd48280a397411d9", "message": "fixed #1736", "target": 1, "dataset": "other", "idx": 198316}
{"func": "\n\t\t\t\t/*if we have only 1 sai_offsets, assume that its type is cenc*/\n\t\t\t\tif ((aux_info_type == GF_ISOM_CENC_SCHEME) || (aux_info_type == GF_ISOM_CBC_SCHEME) ||\n\t\t\t\t\t(aux_info_type == GF_ISOM_CENS_SCHEME) || (aux_info_type == GF_ISOM_CBCS_SCHEME) ||\n\t\t\t\t\t(gf_list_count(traf->sai_offsets) == 1)) {\n\t\t\t\t\tif (saio->offsets && saio->entry_count) {\n\t\t\t\t\t\toffset = saio->offsets[0] + moof_offset;\n\t\t\t\t\t\tnb_saio = saio->entry_count;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tsaio = NULL;\n\t\t\t}\n\t\t\tfor (i = 0; i < gf_list_count(traf->sai_sizes); i++) {\n\t\t\t\tsaiz = (GF_SampleAuxiliaryInfoSizeBox *)gf_list_get(traf->sai_sizes, i);\n\t\t\t\taux_info_type = saiz->aux_info_type;\n\t\t\t\tif (!aux_info_type) aux_info_type = scheme_type;\n\t\t\t\tif ((aux_info_type == GF_ISOM_CENC_SCHEME) || (aux_info_type == GF_ISOM_CBC_SCHEME) ||\n\t\t\t\t\t(aux_info_type == GF_ISOM_CENS_SCHEME) || (aux_info_type == GF_ISOM_CBCS_SCHEME) ||\n\t\t\t\t\t(gf_list_count(traf->sai_sizes) == 1)) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tsaiz = NULL;\n\t\t\t}\n\t\t\tif (saiz && saio && senc) {\n\t\t\t\tfor (i = 0; i < saiz->sample_count; i++) {\n\t\t\t\t\tGF_CENCSampleAuxInfo *sai;\n\t\t\t\t\tconst u8 *key_info=NULL;", "project": "gpac", "hash": 303288141822567692112362620916464223543, "size": 681, "commit_id": "df8fffd839fe5ae9acd82d26fd48280a397411d9", "message": "fixed #1736", "target": 0, "dataset": "other", "idx": 272325}
{"func": "    const TfLiteTensor* t;\n    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, i, &t));\n    TF_LITE_ENSURE_EQ(context, t->dims->size, t0->dims->size);\n    TF_LITE_ENSURE_EQ(context, t->type, input_type);\n    for (int d = 0; d < t0->dims->size; ++d) {\n      if (d == axis) {\n        sum_axis += t->dims->data[axis];\n      } else {\n        TF_LITE_ENSURE_EQ(context, t->dims->data[d], t0->dims->data[d]);\n      }\n    }", "project": "tensorflow", "hash": 92324200070320272661822081558700268693, "size": 72, "commit_id": "4253f96a58486ffe84b61c0415bb234a4632ee73", "message": "Fix integer overflow in TFLite concat\n\nPiperOrigin-RevId: 371013841\nChange-Id: I6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29", "target": 1, "dataset": "other", "idx": 198349}
{"func": "    TF_LITE_ENSURE_OK(context, GetInputSafe(context, node, i, &t));\n    TF_LITE_ENSURE_EQ(context, t->dims->size, t0->dims->size);\n    TF_LITE_ENSURE_EQ(context, t->type, input_type);\n    for (int d = 0; d < t0->dims->size; ++d) {\n      if (d == axis) {\n        // Avoid integer overflow in sum_axis below\n        TF_LITE_ENSURE(context, t->dims->data[axis] >= 0);\n        TF_LITE_ENSURE(context, t->dims->data[axis] <=\n                                    std::numeric_limits<int>::max() - sum_axis);\n        sum_axis += t->dims->data[axis];\n      } else {\n        TF_LITE_ENSURE_EQ(context, t->dims->data[d], t0->dims->data[d]);\n      }\n    }", "project": "tensorflow", "hash": 173727750294110908854288838016785698395, "size": 76, "commit_id": "4253f96a58486ffe84b61c0415bb234a4632ee73", "message": "Fix integer overflow in TFLite concat\n\nPiperOrigin-RevId: 371013841\nChange-Id: I6a4782ce7ca753e23ff31e7fb6aeb7f9d412cd29", "target": 0, "dataset": "other", "idx": 273050}
{"func": "\tPong(const std::string& cookie, const std::string& server = \"\")\n\t\t: ClientProtocol::Message(\"PONG\", ServerInstance->Config->GetServerName())\n\t{\n\t\tPushParamRef(ServerInstance->Config->GetServerName());\n\t\tif (!server.empty())\n\t\t\tPushParamRef(server);\n\t\tPushParamRef(cookie);\n\t}", "project": "inspircd", "hash": 261166146508544882843094418005234060063, "size": 8, "commit_id": "4350a11c663b0d75f8119743bffb7736d87abd4d", "message": "Fix sending malformed pong messages in some cases.", "target": 1, "dataset": "other", "idx": 198370}
{"func": "\tPong(const std::string& cookie, const std::string& server = \"\")\n\t\t: ClientProtocol::Message(\"PONG\", ServerInstance->Config->GetServerName())\n\t{\n\t\tif (server.empty())\n\t\t\tPushParamRef(ServerInstance->Config->GetServerName());\n\t\telse\n\t\t\tPushParam(server);\n\t\tPushParamRef(cookie);\n\t}", "project": "inspircd", "hash": 221906848297919622116301621609254196785, "size": 9, "commit_id": "4350a11c663b0d75f8119743bffb7736d87abd4d", "message": "Fix sending malformed pong messages in some cases.", "target": 0, "dataset": "other", "idx": 273215}
{"func": "            }\n        }\n\n        if (marker == JPEG_MARKER_SOS) {\n            length = AV_RB16(frag->data + start);\n\n            data_ref = NULL;\n            data     = av_malloc(end - start +\n                                 AV_INPUT_BUFFER_PADDING_SIZE);\n            if (!data)\n                return AVERROR(ENOMEM);", "project": "FFmpeg", "hash": 97423740291215508096520627238927806950, "size": 136, "commit_id": "a3a3730b5456ca00587455004d40c047f7b20a99", "message": "avcodec/cbs_jpeg: Check length for SOS\n\nFixes: out of array access\nFixes: 19734/clusterfuzz-testcase-minimized-ffmpeg_BSF_TRACE_HEADERS_fuzzer-5673507031875584\nFixes: 19353/clusterfuzz-testcase-minimized-ffmpeg_BSF_TRACE_HEADERS_fuzzer-5703944462663680\n\nFound-by: continuous fuzzing process https://github.com/google/oss-fuzz/tree/master/projects/ffmpeg\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>\n(cherry picked from commit 1812352d767ccf5431aa440123e2e260a4db2726)\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>", "target": 1, "dataset": "other", "idx": 198372}
{"func": "        }\n\n        if (marker == JPEG_MARKER_SOS) {\n            length = AV_RB16(frag->data + start);\n\n            if (length > end - start)\n                return AVERROR_INVALIDDATA;\n\n            data_ref = NULL;\n            data     = av_malloc(end - start +\n                                 AV_INPUT_BUFFER_PADDING_SIZE);\n            if (!data)\n                return AVERROR(ENOMEM);", "project": "FFmpeg", "hash": 182498168350715750569467212904850217645, "size": 139, "commit_id": "a3a3730b5456ca00587455004d40c047f7b20a99", "message": "avcodec/cbs_jpeg: Check length for SOS\n\nFixes: out of array access\nFixes: 19734/clusterfuzz-testcase-minimized-ffmpeg_BSF_TRACE_HEADERS_fuzzer-5673507031875584\nFixes: 19353/clusterfuzz-testcase-minimized-ffmpeg_BSF_TRACE_HEADERS_fuzzer-5703944462663680\n\nFound-by: continuous fuzzing process https://github.com/google/oss-fuzz/tree/master/projects/ffmpeg\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>\n(cherry picked from commit 1812352d767ccf5431aa440123e2e260a4db2726)\nSigned-off-by: Michael Niedermayer <michael@niedermayer.cc>", "target": 0, "dataset": "other", "idx": 273279}
{"func": "    axis_value = *GetTensorData<int>(axis);\n  }\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  // Copy the input dimensions to output except the axis dimension.\n  TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n  int j = 0;\n  for (int i = 0; i < NumDimensions(input); ++i) {\n    if (i != axis_value) {", "project": "tensorflow", "hash": 85730760625429324821980003967303687262, "size": 24, "commit_id": "c59c37e7b2d563967da813fa50fe20b21f4da683", "message": "Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224", "target": 1, "dataset": "other", "idx": 198396}
{"func": "  }\n  if (axis_value < 0) {\n    axis_value += NumDimensions(input);\n  }\n\n  TF_LITE_ENSURE(context, axis_value >= 0);\n  TF_LITE_ENSURE(context, axis_value < NumDimensions(input));\n\n  // Copy the input dimensions to output except the axis dimension.\n  TfLiteIntArray* output_dims = TfLiteIntArrayCreate(NumDimensions(input) - 1);\n  int j = 0;\n  for (int i = 0; i < NumDimensions(input); ++i) {\n    if (i != axis_value) {", "project": "tensorflow", "hash": 224958909197921227858368302688275106543, "size": 27, "commit_id": "c59c37e7b2d563967da813fa50fe20b21f4da683", "message": "Prevent array write out-of-bounds.\n\nIf user passes an invalid axis, then we copy one too many dimensions to the output in the loop below these checks. Even if we didn't do that, there will be further issues with an invalid axis, so we check for that right now.\n\nPiperOrigin-RevId: 371023299\nChange-Id: I9eca37ffc2b29e8e48710f500701270ef0790224", "target": 0, "dataset": "other", "idx": 273769}
{"func": "  void *p2;\n\n  p2 = mrb_realloc_simple(mrb, p, len);\n  if (len == 0) return p2;\n  if (p2 == NULL) {\n    mrb_free(mrb, p);\n    mrb->gc.out_of_memory = TRUE;\n    mrb_raise_nomemory(mrb);\n  }\n  else {\n    mrb->gc.out_of_memory = FALSE;", "project": "mruby", "hash": 162088573447200069676744299327088726317, "size": 17, "commit_id": "97319697c8f9f6ff27b32589947e1918e3015503", "message": "Cancel 9cdf439\n\nShould not free the pointer in `realloc` since it can cause\nuse-after-free problem.", "target": 1, "dataset": "other", "idx": 198402}
{"func": "{\n  void *p2;\n\n  p2 = mrb_realloc_simple(mrb, p, len);\n  if (len == 0) return p2;\n  if (p2 == NULL) {\n    mrb->gc.out_of_memory = TRUE;\n    mrb_raise_nomemory(mrb);\n  }\n  else {\n    mrb->gc.out_of_memory = FALSE;", "project": "mruby", "hash": 233155307309393879841958010235802131920, "size": 16, "commit_id": "97319697c8f9f6ff27b32589947e1918e3015503", "message": "Cancel 9cdf439\n\nShould not free the pointer in `realloc` since it can cause\nuse-after-free problem.", "target": 0, "dataset": "other", "idx": 274032}
{"func": "  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();", "project": "tensorflow", "hash": 8875073086629769890154937833779819001, "size": 20, "commit_id": "301ae88b331d37a2a16159b65b255f4f9eb39314", "message": "Fix null ptr deref in tf.raw_ops.RaggedTensorToTensor\n\nPiperOrigin-RevId: 384257511\nChange-Id: I0484ad285039d132d6c41b284a7fcdd2b774a38e", "target": 1, "dataset": "other", "idx": 198407}
{"func": "  Status GetFirstDimensionSize(OpKernelContext* context, INDEX_TYPE* result) {\n    const Tensor first_partition_tensor =\n        context->input(kFirstPartitionInputIndex);\n    if (row_partition_types_.empty()) {\n      return errors::InvalidArgument(\"No row_partition_types given.\");\n    }\n    const RowPartitionType first_partition_type = row_partition_types_[0];\n    switch (first_partition_type) {\n      case RowPartitionType::FIRST_DIM_SIZE:\n        *result = first_partition_tensor.scalar<INDEX_TYPE>()();\n        return Status::OK();", "project": "tensorflow", "hash": 111412195870420450026567644267237811330, "size": 23, "commit_id": "301ae88b331d37a2a16159b65b255f4f9eb39314", "message": "Fix null ptr deref in tf.raw_ops.RaggedTensorToTensor\n\nPiperOrigin-RevId: 384257511\nChange-Id: I0484ad285039d132d6c41b284a7fcdd2b774a38e", "target": 0, "dataset": "other", "idx": 274041}
{"func": "                errors::InvalidArgument(\n                    \"Input indices should be matrices but received shapes: \",\n                    a_indices->shape().DebugString(), \" and \",\n                    b_indices->shape().DebugString()));\n    const int64 a_nnz = a_indices->dim_size(0);\n    const int64 b_nnz = b_indices->dim_size(0);\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n\n    OP_REQUIRES(ctx,\n                    TensorShapeUtils::IsVector(b_shape->shape()),\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape->shape().DebugString(), \" and \",\n                    b_shape->shape().DebugString()));\n    OP_REQUIRES(\n        ctx, a_shape->IsSameSize(*b_shape),\n        errors::InvalidArgument(\n            \"Operands do not have the same ranks; got shapes: \",\n            a_shape->SummarizeValue(10), \" and \", b_shape->SummarizeValue(10)));\n    const auto a_shape_flat = a_shape->flat<int64>();\n    auto a_indices_mat = a_indices->matrix<int64>();\n    auto b_indices_mat = b_indices->matrix<int64>();\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    entries_to_copy.reserve(a_nnz + b_nnz);\n    std::vector<T> out_values;\n    const int num_dims = a_shape->dim_size(0);\n\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Invalid input_a shape. Received: \",\n                                        a_shape->DebugString()));\n\n    // The input and output sparse tensors are assumed to be ordered along\n    // increasing dimension number.\n    int64 i = 0, j = 0;\n    T s;", "project": "tensorflow", "hash": 49579086162774256975067637533248462719, "size": 141, "commit_id": "41727ff06111117bdf86b37db198217fd7a143cc", "message": "Validate that a and b are proper sparse tensors\n\nPiperOrigin-RevId: 373248068\nChange-Id: I0a2041a0747901b3f00387a6a3bce9bca6b0b3b1", "target": 1, "dataset": "other", "idx": 198409}
{"func": "                    \"Input indices should be matrices but received shapes: \",\n                    a_indices->shape().DebugString(), \" and \",\n                    b_indices->shape().DebugString()));\n    const int64 a_nnz = a_indices->dim_size(0);\n    const int64 b_nnz = b_indices->dim_size(0);\n    const int num_dims = a_indices->dim_size(1);\n    OP_REQUIRES(ctx, b_indices->dim_size(1) == num_dims,\n                errors::InvalidArgument(\n                    \"Input indices must have the same dimension, got \",\n                    num_dims, \" and \", b_indices->dim_size(1)));\n\n    OP_REQUIRES_OK(ctx, ctx->input(\"a_values\", &a_values_t));\n    OP_REQUIRES_OK(ctx, ctx->input(\"b_values\", &b_values_t));\n\n    OP_REQUIRES(ctx,\n                errors::InvalidArgument(\n                    \"Input shapes should be a vector but received shapes \",\n                    a_shape->shape().DebugString(), \" and \",\n                    b_shape->shape().DebugString()));\n    OP_REQUIRES(\n        ctx, a_shape->NumElements() == num_dims,\n        errors::InvalidArgument(\"Second dimension of a_indices and length of \"\n                                \"a_shape must match, got \",\n                                num_dims, \" and \", a_shape->NumElements()));\n    OP_REQUIRES(ctx, num_dims > 0,\n                errors::InvalidArgument(\"Tesors must not be empty\"));\n    OP_REQUIRES(\n        ctx, a_shape->IsSameSize(*b_shape),\n        errors::InvalidArgument(\n            \"Operands do not have the same ranks; got shapes: \",\n            a_shape->SummarizeValue(10), \" and \", b_shape->SummarizeValue(10)));\n    const auto a_shape_flat = a_shape->flat<int64>();\n    // (1) do a pass over inputs, and append values and indices to vectors\n    auto a_indices_mat = a_indices->matrix<int64>();\n    auto b_indices_mat = b_indices->matrix<int64>();\n    std::vector<std::pair<bool, int64>> entries_to_copy;  // from_a?, idx\n    entries_to_copy.reserve(a_nnz + b_nnz);\n    std::vector<T> out_values;\n\n    // The input and output sparse tensors are assumed to be ordered along\n    // increasing dimension number.\n    int64 i = 0, j = 0;\n    T s;", "project": "tensorflow", "hash": 16228492521577769374139271035359656492, "size": 148, "commit_id": "41727ff06111117bdf86b37db198217fd7a143cc", "message": "Validate that a and b are proper sparse tensors\n\nPiperOrigin-RevId: 373248068\nChange-Id: I0a2041a0747901b3f00387a6a3bce9bca6b0b3b1", "target": 0, "dataset": "other", "idx": 274064}
{"func": "\t\tif (strcasecmp(\"MaxNotificationPerConn\", policy_name) == 0) {\n\t\t\tconn->limits.max_notifications = policy_value;\n\t\t\tcontinue;\n\t\t}\n\t\tif (strcasecmp(\"MaxQueryDuration\", policy_name) == 0) {\n\t\t\tconn->limits.search_timeout = policy_value;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\treturn 0;", "project": "samba", "hash": 179110453965118510639557808253855764974, "size": 104, "commit_id": "f9b2267c6eb8138fc94df7a138ad5d87526f1d79", "message": "CVE-2021-3670 ldap_server: Ensure value of MaxQueryDuration is greater than zero\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=14694\n\nSigned-off-by: Joseph Sutton <josephsutton@catalyst.net.nz>\nReviewed-by: Douglas Bagnall <douglas.bagnall@catalyst.net.nz>\n(cherry picked from commit e1ab0c43629686d1d2c0b0b2bcdc90057a792049)", "target": 1, "dataset": "other", "idx": 198425}
{"func": "\t\tif (strcasecmp(\"MaxNotificationPerConn\", policy_name) == 0) {\n\t\t\tconn->limits.max_notifications = policy_value;\n\t\t\tcontinue;\n\t\t}\n\t\tif (strcasecmp(\"MaxQueryDuration\", policy_name) == 0) {\n\t\t\tif (policy_value > 0) {\n\t\t\t\tconn->limits.search_timeout = policy_value;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\treturn 0;", "project": "samba", "hash": 20233092815787754852088965792044905240, "size": 106, "commit_id": "f9b2267c6eb8138fc94df7a138ad5d87526f1d79", "message": "CVE-2021-3670 ldap_server: Ensure value of MaxQueryDuration is greater than zero\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=14694\n\nSigned-off-by: Joseph Sutton <josephsutton@catalyst.net.nz>\nReviewed-by: Douglas Bagnall <douglas.bagnall@catalyst.net.nz>\n(cherry picked from commit e1ab0c43629686d1d2c0b0b2bcdc90057a792049)", "target": 0, "dataset": "other", "idx": 274270}
{"func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& image = context->input(0);\n    OP_REQUIRES(context, image.dims() == 3,\n                errors::InvalidArgument(\"image must be 3-dimensional\",\n                                        image.shape().DebugString()));\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(image.NumElements(), std::numeric_limits<int32>::max()),\n        errors::InvalidArgument(\"image cannot have >= int32 max elements\"));\n    const int32 height = static_cast<int32>(image.dim_size(0));", "project": "tensorflow", "hash": 252338118123250428276547279507438563877, "size": 43, "commit_id": "26eb323554ffccd173e8a79a8c05c15b685ae4d1", "message": "Fix null CHECK issue with `tf.raw_ops.EncodePng`.\n\nPiperOrigin-RevId: 369717714\nChange-Id: I24136cd99c20b8466671f4f93b670ef6f6dd1250", "target": 1, "dataset": "other", "idx": 198430}
{"func": "  void Compute(OpKernelContext* context) override {\n    const Tensor& image = context->input(0);\n    OP_REQUIRES(context, image.dims() == 3,\n                errors::InvalidArgument(\"image must be 3-dimensional\",\n                                        image.shape().DebugString()));\n    OP_REQUIRES(context, image.NumElements() > 0,\n                errors::Internal(\"Invalid image provided.\"));\n    OP_REQUIRES(\n        context,\n        FastBoundsCheck(image.NumElements(), std::numeric_limits<int32>::max()),\n        errors::InvalidArgument(\"image cannot have >= int32 max elements\"));\n    const int32 height = static_cast<int32>(image.dim_size(0));", "project": "tensorflow", "hash": 232608082282450676739866634543277634841, "size": 45, "commit_id": "26eb323554ffccd173e8a79a8c05c15b685ae4d1", "message": "Fix null CHECK issue with `tf.raw_ops.EncodePng`.\n\nPiperOrigin-RevId: 369717714\nChange-Id: I24136cd99c20b8466671f4f93b670ef6f6dd1250", "target": 0, "dataset": "other", "idx": 274583}
{"func": "    scanner_context.context_status_flags |= PARSER_SCANNING_SUCCESSFUL;\n#endif /* !JERRY_NDEBUG */\n  }\n  PARSER_CATCH\n  {\n    /* Ignore the errors thrown by the lexer. */\n    if (context_p->error != PARSER_ERR_OUT_OF_MEMORY)\n    {\n      context_p->error = PARSER_ERR_NO_ERROR;\n    }\n\n#if ENABLED (JERRY_ES2015)\n    while (scanner_context.active_binding_list_p != NULL)\n    {\n      scanner_pop_binding_list (&scanner_context);\n    }\n#endif /* ENABLED (JERRY_ES2015) */\n\n    /* The following code may allocate memory, so it is enclosed in a try/catch. */\n    PARSER_TRY (context_p->try_buffer)\n    {\n#if ENABLED (JERRY_ES2015)\n      if (scanner_context.status_flags & SCANNER_CONTEXT_THROW_ERR_ASYNC_FUNCTION)\n      {\n        JERRY_ASSERT (scanner_context.async_source_p != NULL);\n\n        scanner_info_t *info_p;\n        info_p = scanner_insert_info (context_p, scanner_context.async_source_p, sizeof (scanner_info_t));\n        info_p->type = SCANNER_TYPE_ERR_ASYNC_FUNCTION;\n      }\n#endif /* ENABLED (JERRY_ES2015) */\n\n      while (scanner_context.active_literal_pool_p != NULL)\n      {\n        scanner_pop_literal_pool (context_p, &scanner_context);\n      }\n    }\n    PARSER_CATCH\n    {\n      JERRY_ASSERT (context_p->error == PARSER_ERR_NO_ERROR);\n\n      while (scanner_context.active_literal_pool_p != NULL)\n      {\n        scanner_literal_pool_t *literal_pool_p = scanner_context.active_literal_pool_p;\n\n        scanner_context.active_literal_pool_p = literal_pool_p->prev_p;\n\n        parser_list_free (&literal_pool_p->literal_pool);\n        scanner_free (literal_pool_p, sizeof (scanner_literal_pool_t));\n      }\n    }\n    PARSER_TRY_END\n\n#if ENABLED (JERRY_ES2015)\n    context_p->status_flags &= (uint32_t) ~PARSER_IS_GENERATOR_FUNCTION;\n#endif /* ENABLED (JERRY_ES2015) */\n  }\n  PARSER_TRY_END\n\n  context_p->status_flags = scanner_context.context_status_flags;\n  scanner_reverse_info_list (context_p);", "project": "jerryscript", "hash": 188299918774090447744906316734921320013, "size": 1234, "commit_id": "69f8e78c2f8d562bd6d8002b5488f1662ac30d24", "message": "Fix error handling in scanner when in case of OOM (#3793)\n\nThis patch fixes #3786 and fixes #3788.\r\n\r\nJerryScript-DCO-1.0-Signed-off-by: Robert Fancsik frobert@inf.u-szeged.hu", "target": 1, "dataset": "other", "idx": 198440}
{"func": "#ifndef JERRY_NDEBUG\n    scanner_context.context_status_flags |= PARSER_SCANNING_SUCCESSFUL;\n#endif /* !JERRY_NDEBUG */\n  }\n  PARSER_CATCH\n  {\n#if ENABLED (JERRY_ES2015)\n    while (scanner_context.active_binding_list_p != NULL)\n    {\n      scanner_pop_binding_list (&scanner_context);\n    }\n#endif /* ENABLED (JERRY_ES2015) */\n\n    if (JERRY_UNLIKELY (context_p->error != PARSER_ERR_OUT_OF_MEMORY))\n    {\n      /* Ignore the errors thrown by the lexer. */\n      context_p->error = PARSER_ERR_NO_ERROR;\n\n      /* The following code may allocate memory, so it is enclosed in a try/catch. */\n      PARSER_TRY (context_p->try_buffer)\n      {\n  #if ENABLED (JERRY_ES2015)\n        if (scanner_context.status_flags & SCANNER_CONTEXT_THROW_ERR_ASYNC_FUNCTION)\n        {\n          JERRY_ASSERT (scanner_context.async_source_p != NULL);\n\n          scanner_info_t *info_p;\n          info_p = scanner_insert_info (context_p, scanner_context.async_source_p, sizeof (scanner_info_t));\n          info_p->type = SCANNER_TYPE_ERR_ASYNC_FUNCTION;\n        }\n  #endif /* ENABLED (JERRY_ES2015) */\n\n        while (scanner_context.active_literal_pool_p != NULL)\n        {\n          scanner_pop_literal_pool (context_p, &scanner_context);\n        }\n      }\n      PARSER_CATCH\n      {\n        JERRY_ASSERT (context_p->error == PARSER_ERR_OUT_OF_MEMORY);\n      }\n      PARSER_TRY_END\n    }\n\n    JERRY_ASSERT (context_p->error == PARSER_ERR_NO_ERROR || context_p->error == PARSER_ERR_OUT_OF_MEMORY);\n\n    if (context_p->error == PARSER_ERR_OUT_OF_MEMORY)\n    {\n      while (scanner_context.active_literal_pool_p != NULL)\n      {\n        scanner_literal_pool_t *literal_pool_p = scanner_context.active_literal_pool_p;\n\n        scanner_context.active_literal_pool_p = literal_pool_p->prev_p;\n\n        parser_list_free (&literal_pool_p->literal_pool);\n        scanner_free (literal_pool_p, sizeof (scanner_literal_pool_t));\n      }\n\n      parser_stack_free (context_p);\n      return;\n    }\n  }\n  PARSER_TRY_END\n\n  context_p->status_flags = scanner_context.context_status_flags;\n  scanner_reverse_info_list (context_p);", "project": "jerryscript", "hash": 279987808649331350784855300505332773163, "size": 1238, "commit_id": "69f8e78c2f8d562bd6d8002b5488f1662ac30d24", "message": "Fix error handling in scanner when in case of OOM (#3793)\n\nThis patch fixes #3786 and fixes #3788.\r\n\r\nJerryScript-DCO-1.0-Signed-off-by: Robert Fancsik frobert@inf.u-szeged.hu", "target": 0, "dataset": "other", "idx": 274775}
{"func": "                 data_type == kTfLiteFloat32 || data_type == kTfLiteUInt8 ||\n                     data_type == kTfLiteInt8 || data_type == kTfLiteInt32 ||\n                     data_type == kTfLiteInt64);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  const int block_size = params->block_size;\n  const int input_height = input->dims->data[1];\n  const int input_width = input->dims->data[2];\n  int output_height = input_height / block_size;\n  int output_width = input_width / block_size;\n", "project": "tensorflow", "hash": 31179933714560962811339968929323691110, "size": 39, "commit_id": "0d45ea1ca641b21b73bcf9c00e0179cda284e7e7", "message": "Prevent one more div by 0 in TFLite\n\nPiperOrigin-RevId: 370800114\nChange-Id: I6b956aeb8c458cc6f514408d2e89ffacfe249e57", "target": 1, "dataset": "other", "idx": 198448}
{"func": "                     data_type == kTfLiteInt8 || data_type == kTfLiteInt32 ||\n                     data_type == kTfLiteInt64);\n  TF_LITE_ENSURE_TYPES_EQ(context, input->type, output->type);\n\n  const int block_size = params->block_size;\n  TF_LITE_ENSURE(context, block_size > 0);\n  const int input_height = input->dims->data[1];\n  const int input_width = input->dims->data[2];\n  int output_height = input_height / block_size;\n  int output_width = input_width / block_size;\n", "project": "tensorflow", "hash": 53852501749742148609964241513124776788, "size": 40, "commit_id": "0d45ea1ca641b21b73bcf9c00e0179cda284e7e7", "message": "Prevent one more div by 0 in TFLite\n\nPiperOrigin-RevId: 370800114\nChange-Id: I6b956aeb8c458cc6f514408d2e89ffacfe249e57", "target": 0, "dataset": "other", "idx": 274812}
{"func": "\t\t\tif (sgdesc->grouping_type!=GF_ISOM_SAMPLE_GROUP_SEIG) continue;\n\t\t\tif (sgdesc->default_description_index)\n\t\t\t\tseig_entry = gf_list_get(sgdesc->group_descriptions, sgdesc->default_description_index-1);\n\t\t\telse\n\t\t\t\tseig_entry = gf_list_get(sgdesc->group_descriptions, 0);\n\t\t\tif (!seig_entry->key_info[0])\n\t\t\t\tseig_entry = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tif (seig_entry) {\n\t\t\tif (default_IsEncrypted) *default_IsEncrypted = seig_entry->IsProtected;", "project": "gpac", "hash": 294092012517353484945000599386797471926, "size": 93, "commit_id": "3b84ffcbacf144ce35650df958432f472b6483f8", "message": "fixed #1735", "target": 1, "dataset": "other", "idx": 198469}
{"func": "\t\t\tif (sgdesc->grouping_type!=GF_ISOM_SAMPLE_GROUP_SEIG) continue;\n\t\t\tif (sgdesc->default_description_index)\n\t\t\t\tseig_entry = gf_list_get(sgdesc->group_descriptions, sgdesc->default_description_index-1);\n\t\t\telse\n\t\t\t\tseig_entry = gf_list_get(sgdesc->group_descriptions, 0);\n\t\t\tif (seig_entry && !seig_entry->key_info[0])\n\t\t\t\tseig_entry = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tif (seig_entry) {\n\t\t\tif (default_IsEncrypted) *default_IsEncrypted = seig_entry->IsProtected;", "project": "gpac", "hash": 297446364103780389560620090307720640177, "size": 93, "commit_id": "3b84ffcbacf144ce35650df958432f472b6483f8", "message": "fixed #1735", "target": 0, "dataset": "other", "idx": 275434}
{"func": "  }\n\n  const int residual = split_dim_size % num_split;\n  for (int i = 0; i < input_tensor.indices().dim_size(0); ++i) {\n    const int dim = input_tensor.indices().matrix<int64>()(i, split_dim);\n    int slice_index = GetSliceIndex(dim, split_size, residual);\n    num_values[slice_index]++;\n  }\n\n  for (int i = 0; i < num_split; ++i) {\n    // TODO(ataei): Pass an allocator to avoid allocating large memory buffer.", "project": "tensorflow", "hash": 146709249587504964012296817765190543788, "size": 79, "commit_id": "8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31", "message": "Fix heap-buffer-overflow issue with `tf.raw_ops.SparseSplit`.\n\nPiperOrigin-RevId: 371242872\nChange-Id: I482bb3d12602c7c3cc9446f97fb9f584bb98e9a4", "target": 1, "dataset": "other", "idx": 198557}
{"func": "\n  const int residual = split_dim_size % num_split;\n  for (int i = 0; i < input_tensor.indices().dim_size(0); ++i) {\n    const int dim = input_tensor.indices().matrix<int64>()(i, split_dim);\n    int slice_index = GetSliceIndex(dim, split_size, residual);\n    if (slice_index >= num_values.size()) {\n      return errors::InvalidArgument(\"Slice index \", slice_index,\n                                     \" is larger than num_split.\");\n    }\n    num_values[slice_index]++;\n  }\n\n  for (int i = 0; i < num_split; ++i) {\n    // TODO(ataei): Pass an allocator to avoid allocating large memory buffer.", "project": "tensorflow", "hash": 10706522302590550572564966482861572860, "size": 83, "commit_id": "8ba6fa29cd8bf9cef9b718dc31c78c73081f5b31", "message": "Fix heap-buffer-overflow issue with `tf.raw_ops.SparseSplit`.\n\nPiperOrigin-RevId: 371242872\nChange-Id: I482bb3d12602c7c3cc9446f97fb9f584bb98e9a4", "target": 0, "dataset": "other", "idx": 277030}
{"func": "    uint len = r_size(pscratch);\n    uint code;\n\n    if (len < devlen)\n        return_error(gs_error_rangecheck);     /* not even room for device len */\n    memcpy((char *)pscratch->value.bytes, iodev->dname, devlen);\n    code = iodev->procs.enumerate_next(pfen, (char *)pscratch->value.bytes + devlen,\n                len - devlen);\n    if (code == ~(uint) 0) {    /* all done */\n        esp -= 5;               /* pop proc, pfen, devlen, iodev , mark */\n        return o_pop_estack;\n    } else if (code > len)      /* overran string */\n        return_error(gs_error_rangecheck);\n    else {\n        push(1);\n        ref_assign(op, pscratch);\n        r_set_size(op, code + devlen);\n        push_op_estack(file_continue);  /* come again */\n        *++esp = pscratch[2];   /* proc */\n        return o_push_estack;\n    }\n}", "project": "ghostpdl", "hash": 138719953418437027565060145640300598234, "size": 29, "commit_id": "ab109aaeb3ddba59518b036fb288402a65cf7ce8", "message": "Bug 694724: Have filenameforall and getenv honor SAFER", "target": 1, "dataset": "other", "idx": 198559}
{"func": "    uint len = r_size(pscratch);\n    uint code;\n\n    if (len < devlen)\n        return_error(gs_error_rangecheck);     /* not even room for device len */\n\n    do {\n        memcpy((char *)pscratch->value.bytes, iodev->dname, devlen);\n        code = iodev->procs.enumerate_next(pfen, (char *)pscratch->value.bytes + devlen,\n                    len - devlen);\n        if (code == ~(uint) 0) {    /* all done */\n            esp -= 5;               /* pop proc, pfen, devlen, iodev , mark */\n            return o_pop_estack;\n        } else if (code > len)      /* overran string */\n            return_error(gs_error_rangecheck);\n        else if (iodev != iodev_default(imemory)\n              || (check_file_permissions_reduced(i_ctx_p, (char *)pscratch->value.bytes, code + devlen, \"PermitFileReading\")) == 0) {\n            push(1);\n            ref_assign(op, pscratch);\n            r_set_size(op, code + devlen);\n            push_op_estack(file_continue);  /* come again */\n            *++esp = pscratch[2];   /* proc */\n            return o_push_estack;\n        }\n    } while(1);\n}", "project": "ghostpdl", "hash": 160037816602598820891675771187589764736, "size": 33, "commit_id": "ab109aaeb3ddba59518b036fb288402a65cf7ce8", "message": "Bug 694724: Have filenameforall and getenv honor SAFER", "target": 0, "dataset": "other", "idx": 277036}
{"func": "\t\t   int size,\n\t\t   LevelRoundingMode rmode)\n{\n    for (int i = 0; i < numLevels; i++)\n    {\n        int l = levelSize (min, max, i, rmode);\n        if (l > std::numeric_limits<int>::max() - size + 1)\n            throw IEX_NAMESPACE::ArgExc (\"Invalid size.\");\n\n        numTiles[i] = (l + size - 1) / size;\n    }\n}", "project": "openexr", "hash": 245031744365829335891668455873877538315, "size": 15, "commit_id": "2a18ed424a854598c2a20b5dd7e782b436a1e753", "message": "Avoid overflow in calculateNumTiles when size=MAX_INT (#825)\n\n* Avoid overflow in calculateNumTiles when size=MAX_INT\r\n\r\nSigned-off-by: Cary Phillips <cary@ilm.com>\r\n\r\n* Compute level size with 64 bits to avoid overflow\r\n\r\nSigned-off-by: Cary Phillips <cary@ilm.com>", "target": 1, "dataset": "other", "idx": 198573}
{"func": "\t\t   int size,\n\t\t   LevelRoundingMode rmode)\n{\n    for (int i = 0; i < numLevels; i++)\n    {\n        // use 64 bits to avoid int overflow if size is large.\n        Int64 l = levelSize (min, max, i, rmode);\n        numTiles[i] = (l + size - 1) / size;\n    }\n}", "project": "openexr", "hash": 70611141410577235686014934945173630473, "size": 13, "commit_id": "2a18ed424a854598c2a20b5dd7e782b436a1e753", "message": "Avoid overflow in calculateNumTiles when size=MAX_INT (#825)\n\n* Avoid overflow in calculateNumTiles when size=MAX_INT\r\n\r\nSigned-off-by: Cary Phillips <cary@ilm.com>\r\n\r\n* Compute level size with 64 bits to avoid overflow\r\n\r\nSigned-off-by: Cary Phillips <cary@ilm.com>", "target": 0, "dataset": "other", "idx": 277662}
{"func": "    }\n    else\n    {\n\tchar_u\t*tmp_ptr = ptr;\n\n\tif (compl_status_adding())\n\t{\n\t    tmp_ptr += compl_length;\n\t    // Skip if already inside a word.\n\t    if (vim_iswordp(tmp_ptr))\n\t\treturn NULL;", "project": "vim", "hash": 205333609836782956561185481131919699922, "size": 90, "commit_id": "a6f9e300161f4cb54713da22f65b261595e8e614", "message": "patch 9.0.0102: reading past end of line with insert mode completion\n\nProblem:    Reading past end of line with insert mode completion.\nSolution:   Check text length.", "target": 1, "dataset": "other", "idx": 198585}
{"func": "    }\n    else\n    {\n\tchar_u\t*tmp_ptr = ptr;\n\n\tif (compl_status_adding() && compl_length <= (int)STRLEN(tmp_ptr))\n\t{\n\t    tmp_ptr += compl_length;\n\t    // Skip if already inside a word.\n\t    if (vim_iswordp(tmp_ptr))\n\t\treturn NULL;", "project": "vim", "hash": 219722132728553087998374150001249172766, "size": 90, "commit_id": "a6f9e300161f4cb54713da22f65b261595e8e614", "message": "patch 9.0.0102: reading past end of line with insert mode completion\n\nProblem:    Reading past end of line with insert mode completion.\nSolution:   Check text length.", "target": 0, "dataset": "other", "idx": 277954}
{"func": "nosy_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct client *client = file->private_data;\n\tspinlock_t *client_list_lock = &client->lynx->client_list_lock;\n\tstruct nosy_stats stats;\n\n\tswitch (cmd) {\n\tcase NOSY_IOC_GET_STATS:\n\t\tspin_lock_irq(client_list_lock);\n\t\tstats.total_packet_count = client->buffer.total_packet_count;\n\t\tif (copy_to_user((void __user *) arg, &stats, sizeof stats))\n\t\t\treturn -EFAULT;\n\t\telse\n\t\t\treturn 0;\n\n\tcase NOSY_IOC_START:\n\t\tspin_lock_irq(client_list_lock);\n\t\tlist_add_tail(&client->link, &client->lynx->client_list);\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn 0;\n\n\tcase NOSY_IOC_STOP:\n\t\tspin_lock_irq(client_list_lock);\n\t\tlist_del_init(&client->link);\n\t\tspin_unlock_irq(client_list_lock);", "project": "linux", "hash": 195769529492278534124032952899358865154, "size": 44, "commit_id": "829933ef05a951c8ff140e814656d73e74915faf", "message": "firewire: nosy: Fix a use-after-free bug in nosy_ioctl()\n\nFor each device, the nosy driver allocates a pcilynx structure.\nA use-after-free might happen in the following scenario:\n\n 1. Open nosy device for the first time and call ioctl with command\n    NOSY_IOC_START, then a new client A will be malloced and added to\n    doubly linked list.\n 2. Open nosy device for the second time and call ioctl with command\n    NOSY_IOC_START, then a new client B will be malloced and added to\n    doubly linked list.\n 3. Call ioctl with command NOSY_IOC_START for client A, then client A\n    will be readded to the doubly linked list. Now the doubly linked\n    list is messed up.\n 4. Close the first nosy device and nosy_release will be called. In\n    nosy_release, client A will be unlinked and freed.\n 5. Close the second nosy device, and client A will be referenced,\n    resulting in UAF.\n\nThe root cause of this bug is that the element in the doubly linked list\nis reentered into the list.\n\nFix this bug by adding a check before inserting a client.  If a client\nis already in the linked list, don't insert it.\n\nThe following KASAN report reveals it:\n\n   BUG: KASAN: use-after-free in nosy_release+0x1ea/0x210\n   Write of size 8 at addr ffff888102ad7360 by task poc\n   CPU: 3 PID: 337 Comm: poc Not tainted 5.12.0-rc5+ #6\n   Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS rel-1.12.0-59-gc9ba5276e321-prebuilt.qemu.org 04/01/2014\n   Call Trace:\n     nosy_release+0x1ea/0x210\n     __fput+0x1e2/0x840\n     task_work_run+0xe8/0x180\n     exit_to_user_mode_prepare+0x114/0x120\n     syscall_exit_to_user_mode+0x1d/0x40\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\n   Allocated by task 337:\n     nosy_open+0x154/0x4d0\n     misc_open+0x2ec/0x410\n     chrdev_open+0x20d/0x5a0\n     do_dentry_open+0x40f/0xe80\n     path_openat+0x1cf9/0x37b0\n     do_filp_open+0x16d/0x390\n     do_sys_openat2+0x11d/0x360\n     __x64_sys_open+0xfd/0x1a0\n     do_syscall_64+0x33/0x40\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\n   Freed by task 337:\n     kfree+0x8f/0x210\n     nosy_release+0x158/0x210\n     __fput+0x1e2/0x840\n     task_work_run+0xe8/0x180\n     exit_to_user_mode_prepare+0x114/0x120\n     syscall_exit_to_user_mode+0x1d/0x40\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\n   The buggy address belongs to the object at ffff888102ad7300 which belongs to the cache kmalloc-128 of size 128\n   The buggy address is located 96 bytes inside of 128-byte region [ffff888102ad7300, ffff888102ad7380)\n\n[ Modified to use 'list_empty()' inside proper lock  - Linus ]\n\nLink: https://lore.kernel.org/lkml/1617433116-5930-1-git-send-email-zheyuma97@gmail.com/\nReported-and-tested-by: \u9a6c\u54f2\u5b87 (Zheyu Ma) <zheyuma97@gmail.com>\nSigned-off-by: Zheyu Ma <zheyuma97@gmail.com>\nCc: Greg Kroah-Hartman <greg@kroah.com>\nCc: Stefan Richter <stefanr@s5r6.in-berlin.de>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 198639}
{"func": "nosy_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct client *client = file->private_data;\n\tspinlock_t *client_list_lock = &client->lynx->client_list_lock;\n\tstruct nosy_stats stats;\n\tint ret;\n\n\tswitch (cmd) {\n\tcase NOSY_IOC_GET_STATS:\n\t\tspin_lock_irq(client_list_lock);\n\t\tstats.total_packet_count = client->buffer.total_packet_count;\n\t\t\treturn -EFAULT;\n\t\telse\n\t\t\treturn 0;\n\n\tcase NOSY_IOC_START:\n\t\tret = -EBUSY;\n\t\tspin_lock_irq(client_list_lock);\n\t\tif (list_empty(&client->link)) {\n\t\t\tlist_add_tail(&client->link, &client->lynx->client_list);\n\t\t\tret = 0;\n\t\t}\n\t\tspin_unlock_irq(client_list_lock);\n\n\t\treturn ret;\n\n\tcase NOSY_IOC_STOP:\n\t\tspin_lock_irq(client_list_lock);\n\t\tlist_del_init(&client->link);\n\t\tspin_unlock_irq(client_list_lock);", "project": "linux", "hash": 228155701602816497915464580422124821519, "size": 49, "commit_id": "829933ef05a951c8ff140e814656d73e74915faf", "message": "firewire: nosy: Fix a use-after-free bug in nosy_ioctl()\n\nFor each device, the nosy driver allocates a pcilynx structure.\nA use-after-free might happen in the following scenario:\n\n 1. Open nosy device for the first time and call ioctl with command\n    NOSY_IOC_START, then a new client A will be malloced and added to\n    doubly linked list.\n 2. Open nosy device for the second time and call ioctl with command\n    NOSY_IOC_START, then a new client B will be malloced and added to\n    doubly linked list.\n 3. Call ioctl with command NOSY_IOC_START for client A, then client A\n    will be readded to the doubly linked list. Now the doubly linked\n    list is messed up.\n 4. Close the first nosy device and nosy_release will be called. In\n    nosy_release, client A will be unlinked and freed.\n 5. Close the second nosy device, and client A will be referenced,\n    resulting in UAF.\n\nThe root cause of this bug is that the element in the doubly linked list\nis reentered into the list.\n\nFix this bug by adding a check before inserting a client.  If a client\nis already in the linked list, don't insert it.\n\nThe following KASAN report reveals it:\n\n   BUG: KASAN: use-after-free in nosy_release+0x1ea/0x210\n   Write of size 8 at addr ffff888102ad7360 by task poc\n   CPU: 3 PID: 337 Comm: poc Not tainted 5.12.0-rc5+ #6\n   Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS rel-1.12.0-59-gc9ba5276e321-prebuilt.qemu.org 04/01/2014\n   Call Trace:\n     nosy_release+0x1ea/0x210\n     __fput+0x1e2/0x840\n     task_work_run+0xe8/0x180\n     exit_to_user_mode_prepare+0x114/0x120\n     syscall_exit_to_user_mode+0x1d/0x40\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\n   Allocated by task 337:\n     nosy_open+0x154/0x4d0\n     misc_open+0x2ec/0x410\n     chrdev_open+0x20d/0x5a0\n     do_dentry_open+0x40f/0xe80\n     path_openat+0x1cf9/0x37b0\n     do_filp_open+0x16d/0x390\n     do_sys_openat2+0x11d/0x360\n     __x64_sys_open+0xfd/0x1a0\n     do_syscall_64+0x33/0x40\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\n   Freed by task 337:\n     kfree+0x8f/0x210\n     nosy_release+0x158/0x210\n     __fput+0x1e2/0x840\n     task_work_run+0xe8/0x180\n     exit_to_user_mode_prepare+0x114/0x120\n     syscall_exit_to_user_mode+0x1d/0x40\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\n   The buggy address belongs to the object at ffff888102ad7300 which belongs to the cache kmalloc-128 of size 128\n   The buggy address is located 96 bytes inside of 128-byte region [ffff888102ad7300, ffff888102ad7380)\n\n[ Modified to use 'list_empty()' inside proper lock  - Linus ]\n\nLink: https://lore.kernel.org/lkml/1617433116-5930-1-git-send-email-zheyuma97@gmail.com/\nReported-and-tested-by: \u9a6c\u54f2\u5b87 (Zheyu Ma) <zheyuma97@gmail.com>\nSigned-off-by: Zheyu Ma <zheyuma97@gmail.com>\nCc: Greg Kroah-Hartman <greg@kroah.com>\nCc: Stefan Richter <stefanr@s5r6.in-berlin.de>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 279632}
{"func": "      result=(MagickRealType) (QuantumRange*(ampl*sin((double) (2.0*MagickPI*\n        (freq*QuantumScale*pixel + phase/360.0) )) + bias ) );\n      break;\n    }\n    case ArcsinFunction:\n    {\n      /* Arcsin Function  (peged at range limits for invalid results)\n       * Parameters:   Width, Center, Range, Bias\n       */\n      double  width,range,center,bias;\n      width  = ( number_parameters >= 1 ) ? parameters[0] : 1.0;\n      center = ( number_parameters >= 2 ) ? parameters[1] : 0.5;\n      range  = ( number_parameters >= 3 ) ? parameters[2] : 1.0;\n      bias   = ( number_parameters >= 4 ) ? parameters[3] : 0.5;\n      result = 2.0/width*(QuantumScale*pixel - center);\n      if ( result <= -1.0 )\n        result = bias - range/2.0;\n      else if ( result >= 1.0 )\n        result = bias + range/2.0;\n      else\n        result=(MagickRealType) (range/MagickPI*asin((double) result)+bias);\n      result *= QuantumRange;\n      break;\n    }\n    case ArctanFunction:\n    {\n      /* Arctan Function", "project": "ImageMagick6", "hash": 304799564303649187232279385135928444641, "size": 81, "commit_id": "072d7b10dbe74d1cf4ec0d008990c1a28c076f9e", "message": "https://github.com/ImageMagick/ImageMagick/issues/3332", "target": 1, "dataset": "other", "idx": 198640}
{"func": "        (freq*QuantumScale*pixel + phase/360.0) )) + bias ) );\n      break;\n    }\n    case ArcsinFunction:\n    {\n      double\n        bias,\n        center,\n        range,\n        width;\n\n      /* Arcsin Function  (peged at range limits for invalid results)\n       * Parameters:   Width, Center, Range, Bias\n       */\n      width=(number_parameters >= 1) ? parameters[0] : 1.0;\n      center=(number_parameters >= 2) ? parameters[1] : 0.5;\n      range=(number_parameters >= 3) ? parameters[2] : 1.0;\n      bias=(number_parameters >= 4) ? parameters[3] : 0.5;\n      result=2.0*PerceptibleReciprocal(width)*(QuantumScale*pixel-center);\n      if (result <= -1.0)\n        result=bias-range/2.0;\n      else\n        if (result >= 1.0)\n          result=bias+range/2.0;\n        else\n          result=(MagickRealType) (range/MagickPI*asin((double) result)+bias);\n      result*=QuantumRange;\n      break;\n    }\n    case ArctanFunction:\n    {\n      /* Arctan Function", "project": "ImageMagick6", "hash": 315303757418884079516642944898609784871, "size": 87, "commit_id": "072d7b10dbe74d1cf4ec0d008990c1a28c076f9e", "message": "https://github.com/ImageMagick/ImageMagick/issues/3332", "target": 0, "dataset": "other", "idx": 279663}
{"func": "\t\t\t      uint32_t filter,\n\t\t\t      bool recursive)\n{\n\tsize_t len = fsp_fullbasepath(fsp, NULL, 0);\n\tchar fullpath[len+1];\n\tNTSTATUS status = NT_STATUS_NOT_IMPLEMENTED;\n\n\tif (fsp->notify != NULL) {\n\t\tDEBUG(1, (\"change_notify_create: fsp->notify != NULL, \"\n\t\t\t  \"fname = %s\\n\", fsp->fsp_name->base_name));\n\t\treturn NT_STATUS_INVALID_PARAMETER;", "project": "samba", "hash": 309795918628451459392198770553612485822, "size": 41, "commit_id": "c300a85848350635e7ddd8129b31c4d439dc0f8a", "message": "s3: smbd: Ensure change notifies can't get set unless the directory handle is open for SEC_DIR_LIST.\n\nRemove knownfail entry.\n\nCVE-2020-14318\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=14434\n\nSigned-off-by: Jeremy Allison <jra@samba.org>", "target": 1, "dataset": "other", "idx": 198641}
{"func": "\t\t\t      bool recursive)\n{\n\tsize_t len = fsp_fullbasepath(fsp, NULL, 0);\n\tchar fullpath[len+1];\n\tNTSTATUS status = NT_STATUS_NOT_IMPLEMENTED;\n\n\t/*\n\t * Setting a changenotify needs READ/LIST access\n\t * on the directory handle.\n\t */\n\tif (!(fsp->access_mask & SEC_DIR_LIST)) {\n\t\treturn NT_STATUS_ACCESS_DENIED;\n\t}\n\n\tif (fsp->notify != NULL) {\n\t\tDEBUG(1, (\"change_notify_create: fsp->notify != NULL, \"\n\t\t\t  \"fname = %s\\n\", fsp->fsp_name->base_name));\n\t\treturn NT_STATUS_INVALID_PARAMETER;", "project": "samba", "hash": 105712949554026737390193230545599570192, "size": 49, "commit_id": "c300a85848350635e7ddd8129b31c4d439dc0f8a", "message": "s3: smbd: Ensure change notifies can't get set unless the directory handle is open for SEC_DIR_LIST.\n\nRemove knownfail entry.\n\nCVE-2020-14318\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=14434\n\nSigned-off-by: Jeremy Allison <jra@samba.org>", "target": 0, "dataset": "other", "idx": 279699}
{"func": "bool ItemStackMetadata::setString(const std::string &name, const std::string &var)\n{\n\tbool result = Metadata::setString(name, var);\n\tif (name == TOOLCAP_KEY)\n\t\tupdateToolCapabilities();\n\treturn result;\n}", "project": "minetest", "hash": 260790760425028914771767596212402179523, "size": 7, "commit_id": "b5956bde259faa240a81060ff4e598e25ad52dae", "message": "Sanitize ItemStack meta text", "target": 1, "dataset": "other", "idx": 198663}
{"func": "bool ItemStackMetadata::setString(const std::string &name, const std::string &var)\n{\n\tstd::string clean_name = name;\n\tstd::string clean_var = var;\n\tsanitize_string(clean_name);\n\tsanitize_string(clean_var);\n\n\tbool result = Metadata::setString(clean_name, clean_var);\n\tif (clean_name == TOOLCAP_KEY)\n\t\tupdateToolCapabilities();\n\treturn result;\n}", "project": "minetest", "hash": 29693985862186695302198532605370901986, "size": 12, "commit_id": "b5956bde259faa240a81060ff4e598e25ad52dae", "message": "Sanitize ItemStack meta text", "target": 0, "dataset": "other", "idx": 279961}
{"func": "\tc = this_cpu_ptr(s->cpu_slab);\n\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = c->freelist;\n\n\t\tif (unlikely(!object)) {\n\t\t\t/*\n\t\t\t * Invoking slow path likely have side-effect\n\t\t\t * of re-populating per CPU c->freelist\n\t\t\t */\n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,", "project": "linux", "hash": 124517195750956527135706940064747760759, "size": 60, "commit_id": "fd4d9c7d0c71866ec0c2825189ebd2ce35bd95b8", "message": "mm: slub: add missing TID bump in kmem_cache_alloc_bulk()\n\nWhen kmem_cache_alloc_bulk() attempts to allocate N objects from a percpu\nfreelist of length M, and N > M > 0, it will first remove the M elements\nfrom the percpu freelist, then call ___slab_alloc() to allocate the next\nelement and repopulate the percpu freelist. ___slab_alloc() can re-enable\nIRQs via allocate_slab(), so the TID must be bumped before ___slab_alloc()\nto properly commit the freelist head change.\n\nFix it by unconditionally bumping c->tid when entering the slowpath.\n\nCc: stable@vger.kernel.org\nFixes: ebe909e0fdb3 (\"slub: improve bulk alloc strategy\")\nSigned-off-by: Jann Horn <jannh@google.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 198668}
{"func": "\n\tfor (i = 0; i < size; i++) {\n\t\tvoid *object = c->freelist;\n\n\t\tif (unlikely(!object)) {\n\t\t\t/*\n\t\t\t * We may have removed an object from c->freelist using\n\t\t\t * the fastpath in the previous iteration; in that case,\n\t\t\t * c->tid has not been bumped yet.\n\t\t\t * Since ___slab_alloc() may reenable interrupts while\n\t\t\t * allocating memory, we should bump c->tid now.\n\t\t\t */\n\t\t\tc->tid = next_tid(c->tid);\n\n\t\t\t/*\n\t\t\t * Invoking slow path likely have side-effect\n\t\t\t * of re-populating per CPU c->freelist\n\t\t\t */\n\t\t\tp[i] = ___slab_alloc(s, flags, NUMA_NO_NODE,", "project": "linux", "hash": 286573245831184637475846363298679212083, "size": 69, "commit_id": "fd4d9c7d0c71866ec0c2825189ebd2ce35bd95b8", "message": "mm: slub: add missing TID bump in kmem_cache_alloc_bulk()\n\nWhen kmem_cache_alloc_bulk() attempts to allocate N objects from a percpu\nfreelist of length M, and N > M > 0, it will first remove the M elements\nfrom the percpu freelist, then call ___slab_alloc() to allocate the next\nelement and repopulate the percpu freelist. ___slab_alloc() can re-enable\nIRQs via allocate_slab(), so the TID must be bumped before ___slab_alloc()\nto properly commit the freelist head change.\n\nFix it by unconditionally bumping c->tid when entering the slowpath.\n\nCc: stable@vger.kernel.org\nFixes: ebe909e0fdb3 (\"slub: improve bulk alloc strategy\")\nSigned-off-by: Jann Horn <jannh@google.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 280052}
{"func": "decode_NXAST_RAW_ENCAP(const struct nx_action_encap *nae,\n                       enum ofp_version ofp_version OVS_UNUSED,\n                       struct ofpbuf *out)\n{\n    struct ofpact_encap *encap;\n    const struct ofp_ed_prop_header *ofp_prop;\n    size_t props_len;\n    uint16_t n_props = 0;\n    int err;\n\n    encap = ofpact_put_ENCAP(out);\n        err = decode_ed_prop(&ofp_prop, out, &props_len);\n        if (err) {\n            return err;\n        }\n        n_props++;\n    }\n    encap->n_props = n_props;\n    out->header = &encap->ofpact;\n    ofpact_finish_ENCAP(out, &encap);\n\n    return 0;", "project": "ovs", "hash": 58951518954942858611729288502768220883, "size": 39, "commit_id": "65c61b0c23a0d474696d7b1cea522a5016a8aeb3", "message": "ofp-actions: Fix use-after-free while decoding RAW_ENCAP.\n\nWhile decoding RAW_ENCAP action, decode_ed_prop() might re-allocate\nofpbuf if there is no enough space left.  However, function\n'decode_NXAST_RAW_ENCAP' continues to use old pointer to 'encap'\nstructure leading to write-after-free and incorrect decoding.\n\n  ==3549105==ERROR: AddressSanitizer: heap-use-after-free on address\n  0x60600000011a at pc 0x0000005f6cc6 bp 0x7ffc3a2d4410 sp 0x7ffc3a2d4408\n  WRITE of size 2 at 0x60600000011a thread T0\n    #0 0x5f6cc5 in decode_NXAST_RAW_ENCAP lib/ofp-actions.c:4461:20\n    #1 0x5f0551 in ofpact_decode ./lib/ofp-actions.inc2:4777:16\n    #2 0x5ed17c in ofpacts_decode lib/ofp-actions.c:7752:21\n    #3 0x5eba9a in ofpacts_pull_openflow_actions__ lib/ofp-actions.c:7791:13\n    #4 0x5eb9fc in ofpacts_pull_openflow_actions lib/ofp-actions.c:7835:12\n    #5 0x64bb8b in ofputil_decode_packet_out lib/ofp-packet.c:1113:17\n    #6 0x65b6f4 in ofp_print_packet_out lib/ofp-print.c:148:13\n    #7 0x659e3f in ofp_to_string__ lib/ofp-print.c:1029:16\n    #8 0x659b24 in ofp_to_string lib/ofp-print.c:1244:21\n    #9 0x65a28c in ofp_print lib/ofp-print.c:1288:28\n    #10 0x540d11 in ofctl_ofp_parse utilities/ovs-ofctl.c:2814:9\n    #11 0x564228 in ovs_cmdl_run_command__ lib/command-line.c:247:17\n    #12 0x56408a in ovs_cmdl_run_command lib/command-line.c:278:5\n    #13 0x5391ae in main utilities/ovs-ofctl.c:179:9\n    #14 0x7f6911ce9081 in __libc_start_main (/lib64/libc.so.6+0x27081)\n    #15 0x461fed in _start (utilities/ovs-ofctl+0x461fed)\n\nFix that by getting a new pointer before using.\n\nCredit to OSS-Fuzz.\n\nFuzzer regression test will fail only with AddressSanitizer enabled.\n\nReported-at: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=27851\nFixes: f839892a206a (\"OF support and translation of generic encap and decap\")\nAcked-by: William Tu <u9012063@gmail.com>\nSigned-off-by: Ilya Maximets <i.maximets@ovn.org>", "target": 1, "dataset": "other", "idx": 198672}
{"func": "                       enum ofp_version ofp_version OVS_UNUSED,\n                       struct ofpbuf *out)\n{\n    struct ofpact_encap *encap;\n    const struct ofp_ed_prop_header *ofp_prop;\n    const size_t encap_ofs = out->size;\n    size_t props_len;\n    uint16_t n_props = 0;\n    int err;\n\n    encap = ofpact_put_ENCAP(out);\n        if (err) {\n            return err;\n        }\n        n_props++;\n    }\n    encap = ofpbuf_at_assert(out, encap_ofs, sizeof *encap);\n    encap->n_props = n_props;\n    out->header = &encap->ofpact;\n    ofpact_finish_ENCAP(out, &encap);\n\n    return 0;", "project": "ovs", "hash": 102959166777006843366923731847257991889, "size": 41, "commit_id": "65c61b0c23a0d474696d7b1cea522a5016a8aeb3", "message": "ofp-actions: Fix use-after-free while decoding RAW_ENCAP.\n\nWhile decoding RAW_ENCAP action, decode_ed_prop() might re-allocate\nofpbuf if there is no enough space left.  However, function\n'decode_NXAST_RAW_ENCAP' continues to use old pointer to 'encap'\nstructure leading to write-after-free and incorrect decoding.\n\n  ==3549105==ERROR: AddressSanitizer: heap-use-after-free on address\n  0x60600000011a at pc 0x0000005f6cc6 bp 0x7ffc3a2d4410 sp 0x7ffc3a2d4408\n  WRITE of size 2 at 0x60600000011a thread T0\n    #0 0x5f6cc5 in decode_NXAST_RAW_ENCAP lib/ofp-actions.c:4461:20\n    #1 0x5f0551 in ofpact_decode ./lib/ofp-actions.inc2:4777:16\n    #2 0x5ed17c in ofpacts_decode lib/ofp-actions.c:7752:21\n    #3 0x5eba9a in ofpacts_pull_openflow_actions__ lib/ofp-actions.c:7791:13\n    #4 0x5eb9fc in ofpacts_pull_openflow_actions lib/ofp-actions.c:7835:12\n    #5 0x64bb8b in ofputil_decode_packet_out lib/ofp-packet.c:1113:17\n    #6 0x65b6f4 in ofp_print_packet_out lib/ofp-print.c:148:13\n    #7 0x659e3f in ofp_to_string__ lib/ofp-print.c:1029:16\n    #8 0x659b24 in ofp_to_string lib/ofp-print.c:1244:21\n    #9 0x65a28c in ofp_print lib/ofp-print.c:1288:28\n    #10 0x540d11 in ofctl_ofp_parse utilities/ovs-ofctl.c:2814:9\n    #11 0x564228 in ovs_cmdl_run_command__ lib/command-line.c:247:17\n    #12 0x56408a in ovs_cmdl_run_command lib/command-line.c:278:5\n    #13 0x5391ae in main utilities/ovs-ofctl.c:179:9\n    #14 0x7f6911ce9081 in __libc_start_main (/lib64/libc.so.6+0x27081)\n    #15 0x461fed in _start (utilities/ovs-ofctl+0x461fed)\n\nFix that by getting a new pointer before using.\n\nCredit to OSS-Fuzz.\n\nFuzzer regression test will fail only with AddressSanitizer enabled.\n\nReported-at: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=27851\nFixes: f839892a206a (\"OF support and translation of generic encap and decap\")\nAcked-by: William Tu <u9012063@gmail.com>\nSigned-off-by: Ilya Maximets <i.maximets@ovn.org>", "target": 0, "dataset": "other", "idx": 280721}
{"func": "\t};\n\n\t*batch++ = MI_ARB_ON_OFF | MI_ARB_DISABLE;\n\n\t/* WaFlushCoherentL3CacheLinesAtContextSwitch:skl,bxt,glk */\n\tbatch = gen8_emit_flush_coherentl3_wa(engine, batch);\n\n\tbatch = emit_lri(batch, lri, ARRAY_SIZE(lri));\n\n\t/* WaMediaPoolStateCmdInWABB:bxt,glk */\n\tif (HAS_POOLED_EU(engine->i915)) {", "project": "linux", "hash": 201520787713853255332939412295673036655, "size": 63, "commit_id": "bc8a76a152c5f9ef3b48104154a65a68a8b76946", "message": "drm/i915/gen9: Clear residual context state on context switch\n\nIntel ID: PSIRT-TA-201910-001\nCVEID: CVE-2019-14615\n\nIntel GPU Hardware prior to Gen11 does not clear EU state\nduring a context switch. This can result in information\nleakage between contexts.\n\nFor Gen8 and Gen9, hardware provides a mechanism for\nfast cleardown of the EU state, by issuing a PIPE_CONTROL\nwith bit 27 set. We can use this in a context batch buffer\nto explicitly cleardown the state on every context switch.\n\nAs this workaround is already in place for gen8, we can borrow\nthe code verbatim for Gen9.\n\nSigned-off-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>\nSigned-off-by: Akeem G Abodunrin <akeem.g.abodunrin@intel.com>\nCc: Kumar Valsan Prathap <prathap.kumar.valsan@intel.com>\nCc: Chris Wilson <chris.p.wilson@intel.com>\nCc: Balestrieri Francesco <francesco.balestrieri@intel.com>\nCc: Bloomfield Jon <jon.bloomfield@intel.com>\nCc: Dutt Sudeep <sudeep.dutt@intel.com>", "target": 1, "dataset": "other", "idx": 198697}
{"func": "\n\t*batch++ = MI_ARB_ON_OFF | MI_ARB_DISABLE;\n\n\t/* WaFlushCoherentL3CacheLinesAtContextSwitch:skl,bxt,glk */\n\tbatch = gen8_emit_flush_coherentl3_wa(engine, batch);\n\n\t/* WaClearSlmSpaceAtContextSwitch:skl,bxt,kbl,glk,cfl */\n\tbatch = gen8_emit_pipe_control(batch,\n\t\t\t\t       PIPE_CONTROL_FLUSH_L3 |\n\t\t\t\t       PIPE_CONTROL_STORE_DATA_INDEX |\n\t\t\t\t       PIPE_CONTROL_CS_STALL |\n\t\t\t\t       PIPE_CONTROL_QW_WRITE,\n\t\t\t\t       LRC_PPHWSP_SCRATCH_ADDR);\n\n\tbatch = emit_lri(batch, lri, ARRAY_SIZE(lri));\n\n\t/* WaMediaPoolStateCmdInWABB:bxt,glk */\n\tif (HAS_POOLED_EU(engine->i915)) {", "project": "linux", "hash": 287263079175771631246087515336497738963, "size": 71, "commit_id": "bc8a76a152c5f9ef3b48104154a65a68a8b76946", "message": "drm/i915/gen9: Clear residual context state on context switch\n\nIntel ID: PSIRT-TA-201910-001\nCVEID: CVE-2019-14615\n\nIntel GPU Hardware prior to Gen11 does not clear EU state\nduring a context switch. This can result in information\nleakage between contexts.\n\nFor Gen8 and Gen9, hardware provides a mechanism for\nfast cleardown of the EU state, by issuing a PIPE_CONTROL\nwith bit 27 set. We can use this in a context batch buffer\nto explicitly cleardown the state on every context switch.\n\nAs this workaround is already in place for gen8, we can borrow\nthe code verbatim for Gen9.\n\nSigned-off-by: Mika Kuoppala <mika.kuoppala@linux.intel.com>\nSigned-off-by: Akeem G Abodunrin <akeem.g.abodunrin@intel.com>\nCc: Kumar Valsan Prathap <prathap.kumar.valsan@intel.com>\nCc: Chris Wilson <chris.p.wilson@intel.com>\nCc: Balestrieri Francesco <francesco.balestrieri@intel.com>\nCc: Bloomfield Jon <jon.bloomfield@intel.com>\nCc: Dutt Sudeep <sudeep.dutt@intel.com>", "target": 0, "dataset": "other", "idx": 281552}
{"func": "{\n    ZZIP_FILE* file = zzip_file_open (disk, name, 0);\n    if (file) \n    {\n\tchar buffer[1024]; int len;\n\twhile ((len = zzip_file_read (file, buffer, 1024))) \n\t{\n\t    fwrite (buffer, 1, len, out);\n\t}\n\t\n\tzzip_file_close (file);", "project": "zziplib", "hash": 213540364375667917653366276803369040606, "size": 14, "commit_id": "ac9ae39ef419e9f0f83da1e583314d8c7cda34a6", "message": "#68 ssize_t return value of zzip_file_read is a signed value being possibly -1", "target": 1, "dataset": "other", "idx": 198733}
{"func": "{\n    ZZIP_FILE* file = zzip_file_open (disk, name, 0);\n    if (file) \n    {\n\tchar buffer[1024]; int len;\n\twhile (0 < (len = zzip_file_read (file, buffer, 1024))) \n\t{\n\t    fwrite (buffer, 1, len, out);\n\t}\n\t\n\tzzip_file_close (file);", "project": "zziplib", "hash": 253924670115102704045568268367383544838, "size": 14, "commit_id": "ac9ae39ef419e9f0f83da1e583314d8c7cda34a6", "message": "#68 ssize_t return value of zzip_file_read is a signed value being possibly -1", "target": 0, "dataset": "other", "idx": 282823}
{"func": "                    new_off += sub_dissected;\n\n                    if ((sub_dissected <= bb_data_len) && (sub_dissected >= DVB_S2_GSE_MINSIZE)) {\n                        bb_data_len -= sub_dissected;\n                        if (bb_data_len < DVB_S2_GSE_MINSIZE)\n                            bb_data_len = 0;\n                    }\n                }\n            }\n        } else {\n            proto_tree_add_item(dvb_s2_bb_tree, hf_dvb_s2_bb_df, tvb, new_off, bb_data_len, ENC_NA);", "project": "wireshark", "hash": 210796043791253976716955079241114200072, "size": 147, "commit_id": "0137c24d60934f131b25506a88c9464e4dc827de", "message": "DVB-S2-BB: Prevent infinite loop\n\nCommit 4bf4ee88f0544727e7f89f3f288c6afd2f650a4c removed an else\nstatement that broke out of the BBFrame processing loop. Without\nit, infinite loops might be possible if the GSE frames have bit errors\nin the length field.", "target": 1, "dataset": "other", "idx": 198763}
{"func": "\n                    if ((sub_dissected <= bb_data_len) && (sub_dissected >= DVB_S2_GSE_MINSIZE)) {\n                        bb_data_len -= sub_dissected;\n                        if (bb_data_len < DVB_S2_GSE_MINSIZE)\n                            bb_data_len = 0;\n                    } else {\n                        bb_data_len = 0;\n                    }\n                }\n            }\n        } else {\n            proto_tree_add_item(dvb_s2_bb_tree, hf_dvb_s2_bb_df, tvb, new_off, bb_data_len, ENC_NA);", "project": "wireshark", "hash": 330364968381030963517562341505385463003, "size": 149, "commit_id": "0137c24d60934f131b25506a88c9464e4dc827de", "message": "DVB-S2-BB: Prevent infinite loop\n\nCommit 4bf4ee88f0544727e7f89f3f288c6afd2f650a4c removed an else\nstatement that broke out of the BBFrame processing loop. Without\nit, infinite loops might be possible if the GSE frames have bit errors\nin the length field.", "target": 0, "dataset": "other", "idx": 283344}
{"func": "static void set_error_response(h2_stream *stream, int http_status)\n{\n    if (!h2_stream_is_ready(stream)) {\n        stream->rtmp->http_status = http_status;\n    }\n}", "project": "httpd", "hash": 172068702792085315081162612263931698574, "size": 6, "commit_id": "f990e5ecad40b100a8a5c7c1033c46044a9cb244", "message": "mod_htt2: fix incomplete sync with latest changes in github, adjust version number.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/httpd/httpd/trunk@1889119 13f79535-47bb-0310-9956-ffa450edef68", "target": 1, "dataset": "other", "idx": 198837}
{"func": "static void set_error_response(h2_stream *stream, int http_status)\n{\n    if (!h2_stream_is_ready(stream) && stream->rtmp) {\n        stream->rtmp->http_status = http_status;\n    }\n}", "project": "httpd", "hash": 163842922705465616802565387268745890925, "size": 6, "commit_id": "f990e5ecad40b100a8a5c7c1033c46044a9cb244", "message": "mod_htt2: fix incomplete sync with latest changes in github, adjust version number.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/httpd/httpd/trunk@1889119 13f79535-47bb-0310-9956-ffa450edef68", "target": 0, "dataset": "other", "idx": 284266}
{"func": "parse_reg_exp (re_string_t *regexp, regex_t *preg, re_token_t *token,\n\t       reg_syntax_t syntax, Idx nest, reg_errcode_t *err)\n{\n  re_dfa_t *dfa = preg->buffer;\n  bin_tree_t *tree, *branch = NULL;\n  tree = parse_branch (regexp, preg, token, syntax, nest, err);\n  if (BE (*err != REG_NOERROR && tree == NULL, 0))\n    return NULL;\n\n  while (token->type == OP_ALT)\n    {\n      fetch_token (token, regexp, syntax | RE_CARET_ANCHORS_HERE);\n      if (token->type != OP_ALT && token->type != END_OF_RE\n\t  && (nest == 0 || token->type != OP_CLOSE_SUBEXP))\n\t{\n\t  branch = parse_branch (regexp, preg, token, syntax, nest, err);\n\t  if (BE (*err != REG_NOERROR && branch == NULL, 0))\n\t    {\n\t      if (tree != NULL)\n\t\tpostorder (tree, free_tree, NULL);\n\t      return NULL;\n\t    }\n\t}\n      else\n\tbranch = NULL;\n      tree = create_tree (dfa, tree, branch, OP_ALT);\n      if (BE (tree == NULL, 0))", "project": "gnulib", "hash": 273676495705910113330018924005947558068, "size": 34, "commit_id": "5513b40999149090987a0341c018d05d3eea1272", "message": "Diagnose ERE '()|\\1'\n\nProblem reported by Hanno B\u00f6ck in: http://bugs.gnu.org/21513\n* lib/regcomp.c (parse_reg_exp): While parsing alternatives, keep\ntrack of the set of previously-completed subexpressions available\nbefore the first alternative, and restore this set just before\nparsing each subsequent alternative.  This lets us diagnose the\ninvalid back-reference in the ERE '()|\\1'.", "target": 1, "dataset": "other", "idx": 198942}
{"func": "parse_reg_exp (re_string_t *regexp, regex_t *preg, re_token_t *token,\n\t       reg_syntax_t syntax, Idx nest, reg_errcode_t *err)\n{\n  re_dfa_t *dfa = preg->buffer;\n  bin_tree_t *tree, *branch = NULL;\n  bitset_word_t initial_bkref_map = dfa->completed_bkref_map;\n  tree = parse_branch (regexp, preg, token, syntax, nest, err);\n  if (BE (*err != REG_NOERROR && tree == NULL, 0))\n    return NULL;\n\n  while (token->type == OP_ALT)\n    {\n      fetch_token (token, regexp, syntax | RE_CARET_ANCHORS_HERE);\n      if (token->type != OP_ALT && token->type != END_OF_RE\n\t  && (nest == 0 || token->type != OP_CLOSE_SUBEXP))\n\t{\n\t  bitset_word_t accumulated_bkref_map = dfa->completed_bkref_map;\n\t  dfa->completed_bkref_map = initial_bkref_map;\n\t  branch = parse_branch (regexp, preg, token, syntax, nest, err);\n\t  if (BE (*err != REG_NOERROR && branch == NULL, 0))\n\t    {\n\t      if (tree != NULL)\n\t\tpostorder (tree, free_tree, NULL);\n\t      return NULL;\n\t    }\n\t  dfa->completed_bkref_map |= accumulated_bkref_map;\n\t}\n      else\n\tbranch = NULL;\n      tree = create_tree (dfa, tree, branch, OP_ALT);\n      if (BE (tree == NULL, 0))", "project": "gnulib", "hash": 313711019416566665334745674532461859910, "size": 38, "commit_id": "5513b40999149090987a0341c018d05d3eea1272", "message": "Diagnose ERE '()|\\1'\n\nProblem reported by Hanno B\u00f6ck in: http://bugs.gnu.org/21513\n* lib/regcomp.c (parse_reg_exp): While parsing alternatives, keep\ntrack of the set of previously-completed subexpressions available\nbefore the first alternative, and restore this set just before\nparsing each subsequent alternative.  This lets us diagnose the\ninvalid back-reference in the ERE '()|\\1'.", "target": 0, "dataset": "other", "idx": 285543}
{"func": "\t\tctxt->sax->ignorableWhitespace = soap_ignorableWhitespace;\n\t\tctxt->sax->comment = soap_Comment;\n\t\tctxt->sax->warning = NULL;\n\t\tctxt->sax->error = NULL;\n\t\t/*ctxt->sax->fatalError = NULL;*/\n\t\told = php_libxml_disable_entity_loader(1);\n\t\txmlParseDocument(ctxt);\n\t\tphp_libxml_disable_entity_loader(old);\n\t\tif (ctxt->wellFormed) {\n\t\t\tret = ctxt->myDoc;\n\t\t\tif (ret->URL == NULL && ctxt->directory != NULL) {\n\t\t\t\tret->URL = xmlCharStrdup(ctxt->directory);\n\t\t\t}", "project": "php-src", "hash": 149172573949708914156035129804477784284, "size": 50, "commit_id": "fcd4b5335a6df4e0676ee32e2267ca71d70fe623", "message": "Fix TSRM (after afc1debb)", "target": 1, "dataset": "other", "idx": 198943}
{"func": "\t\tctxt->sax->ignorableWhitespace = soap_ignorableWhitespace;\n\t\tctxt->sax->comment = soap_Comment;\n\t\tctxt->sax->warning = NULL;\n\t\tctxt->sax->error = NULL;\n\t\t/*ctxt->sax->fatalError = NULL;*/\n\t\told = php_libxml_disable_entity_loader(1 TSRMLS_CC);\n\t\txmlParseDocument(ctxt);\n\t\tphp_libxml_disable_entity_loader(old TSRMLS_CC);\n\t\tif (ctxt->wellFormed) {\n\t\t\tret = ctxt->myDoc;\n\t\t\tif (ret->URL == NULL && ctxt->directory != NULL) {\n\t\t\t\tret->URL = xmlCharStrdup(ctxt->directory);\n\t\t\t}", "project": "php-src", "hash": 81104495453836081457129860350581051957, "size": 50, "commit_id": "fcd4b5335a6df4e0676ee32e2267ca71d70fe623", "message": "Fix TSRM (after afc1debb)", "target": 0, "dataset": "other", "idx": 285598}
{"func": "xmlDocPtr soap_xmlParseMemory(const void *buf, size_t buf_size)\n{\n\txmlParserCtxtPtr ctxt = NULL;\n\txmlDocPtr ret;\n\n/*\n\txmlInitParser();\n*/\n\tctxt = xmlCreateMemoryParserCtxt(buf, buf_size);\n\t\tctxt->sax->error = NULL;\n\t\t/*ctxt->sax->fatalError = NULL;*/\n#if LIBXML_VERSION >= 20703\n\t\tctxt->options |= XML_PARSE_HUGE;\n#endif\n\t\told = php_libxml_disable_entity_loader(1);\n\t\txmlParseDocument(ctxt);\n\t\tphp_libxml_disable_entity_loader(old);\n\t\tif (ctxt->wellFormed) {\n\t\t\tret = ctxt->myDoc;\n\t\t\tif (ret->URL == NULL && ctxt->directory != NULL) {\n\t\t\t\tret->URL = xmlCharStrdup(ctxt->directory);\n\t\t\t}", "project": "php-src", "hash": 134084765210679426355834754154630655990, "size": 49, "commit_id": "fcd4b5335a6df4e0676ee32e2267ca71d70fe623", "message": "Fix TSRM (after afc1debb)", "target": 1, "dataset": "other", "idx": 198944}
{"func": "xmlDocPtr soap_xmlParseMemory(const void *buf, size_t buf_size)\n{\n\txmlParserCtxtPtr ctxt = NULL;\n\txmlDocPtr ret;\n\n\tTSRMLS_FETCH();\n\n/*\n\txmlInitParser();\n*/\n\tctxt = xmlCreateMemoryParserCtxt(buf, buf_size);\n\t\tctxt->sax->error = NULL;\n\t\t/*ctxt->sax->fatalError = NULL;*/\n#if LIBXML_VERSION >= 20703\n\t\tctxt->options |= XML_PARSE_HUGE;\n#endif\n\t\told = php_libxml_disable_entity_loader(1 TSRMLS_CC);\n\t\txmlParseDocument(ctxt);\n\t\tphp_libxml_disable_entity_loader(old TSRMLS_CC);\n\t\tif (ctxt->wellFormed) {\n\t\t\tret = ctxt->myDoc;\n\t\t\tif (ret->URL == NULL && ctxt->directory != NULL) {\n\t\t\t\tret->URL = xmlCharStrdup(ctxt->directory);\n\t\t\t}", "project": "php-src", "hash": 101208883108715639540706738822841294770, "size": 51, "commit_id": "fcd4b5335a6df4e0676ee32e2267ca71d70fe623", "message": "Fix TSRM (after afc1debb)", "target": 0, "dataset": "other", "idx": 285589}
{"func": "size_t intsetBlobLen(intset *is) {\n    return sizeof(intset)+intrev32ifbe(is->length)*intrev32ifbe(is->encoding);\n}", "project": "redis", "hash": 61618186628749728684651827175100336534, "size": 3, "commit_id": "789f10156009b404950ad717642a9496ed887083", "message": "Fix integer overflow in intset (CVE-2021-29478)\n\nAn integer overflow bug in Redis 6.2 could be exploited to corrupt the heap and\npotentially result with remote code execution.\n\nThe vulnerability involves changing the default set-max-intset-entries\nconfiguration value, creating a large set key that consists of integer values\nand using the COPY command to duplicate it.\n\nThe integer overflow bug exists in all versions of Redis starting with 2.6,\nwhere it could result with a corrupted RDB or DUMP payload, but not exploited\nthrough COPY (which did not exist before 6.2).\n\n(cherry picked from commit 29900d4e6bccdf3691bedf0ea9a5d84863fa3592)", "target": 1, "dataset": "other", "idx": 199227}
{"func": "size_t intsetBlobLen(intset *is) {\n    return sizeof(intset)+(size_t)intrev32ifbe(is->length)*intrev32ifbe(is->encoding);\n}", "project": "redis", "hash": 270360219559243121696705431487155837807, "size": 3, "commit_id": "789f10156009b404950ad717642a9496ed887083", "message": "Fix integer overflow in intset (CVE-2021-29478)\n\nAn integer overflow bug in Redis 6.2 could be exploited to corrupt the heap and\npotentially result with remote code execution.\n\nThe vulnerability involves changing the default set-max-intset-entries\nconfiguration value, creating a large set key that consists of integer values\nand using the COPY command to duplicate it.\n\nThe integer overflow bug exists in all versions of Redis starting with 2.6,\nwhere it could result with a corrupted RDB or DUMP payload, but not exploited\nthrough COPY (which did not exist before 6.2).\n\n(cherry picked from commit 29900d4e6bccdf3691bedf0ea9a5d84863fa3592)", "target": 0, "dataset": "other", "idx": 290629}
{"func": "static int php_openssl_validate_iv(char **piv, size_t *piv_len, size_t iv_required_len,\n\t\tzend_bool *free_iv, EVP_CIPHER_CTX *cipher_ctx, struct php_openssl_cipher_mode *mode) /* {{{ */\n{\n\tchar *iv_new;\n\n\t/* Best case scenario, user behaved */\n\tif (*piv_len == iv_required_len) {\n\t\treturn SUCCESS;\n\t}\n\n\tif (mode->is_aead) {\n\t\tif (EVP_CIPHER_CTX_ctrl(cipher_ctx, mode->aead_ivlen_flag, *piv_len, NULL) != 1) {\n\t\t\tphp_error_docref(NULL, E_WARNING, \"Setting of IV length for AEAD mode failed\");\n\t\t\treturn FAILURE;\n\t\t}\n\t\treturn SUCCESS;\n\t}\n\n\tiv_new = ecalloc(1, iv_required_len + 1);\n", "project": "php-src", "hash": 232840072465422852413169490519972262918, "size": 50, "commit_id": "0216630ea2815a5789a24279a1211ac398d4de79", "message": "Fix bug #79601 (Wrong ciphertext/tag in AES-CCM encryption for a 12 bytes IV)", "target": 1, "dataset": "other", "idx": 199700}
{"func": "static int php_openssl_validate_iv(char **piv, size_t *piv_len, size_t iv_required_len,\n\t\tzend_bool *free_iv, EVP_CIPHER_CTX *cipher_ctx, struct php_openssl_cipher_mode *mode) /* {{{ */\n{\n\tchar *iv_new;\n\n\tif (mode->is_aead) {\n\t\tif (EVP_CIPHER_CTX_ctrl(cipher_ctx, mode->aead_ivlen_flag, *piv_len, NULL) != 1) {\n\t\t\tphp_error_docref(NULL, E_WARNING, \"Setting of IV length for AEAD mode failed\");\n\t\t\treturn FAILURE;\n\t\t}\n\t\treturn SUCCESS;\n\t}\n\n\t/* Best case scenario, user behaved */\n\tif (*piv_len == iv_required_len) {\n\t\treturn SUCCESS;\n\t}\n\n\tiv_new = ecalloc(1, iv_required_len + 1);\n", "project": "php-src", "hash": 248839184558979669461751117706652524696, "size": 50, "commit_id": "0216630ea2815a5789a24279a1211ac398d4de79", "message": "Fix bug #79601 (Wrong ciphertext/tag in AES-CCM encryption for a 12 bytes IV)", "target": 0, "dataset": "other", "idx": 291448}
{"func": "\tclear_tsk_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->exit_signal = -1;\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tif (clone_flags & CLONE_PARENT)\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t\telse\n\t\t\tp->exit_signal = args->exit_signal;\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);", "project": "linux", "hash": 173143261973963860691601241949264591134, "size": 533, "commit_id": "b4e00444cab4c3f3fec876dc0cccc8cbb0d1a948", "message": "fork: fix copy_process(CLONE_PARENT) race with the exiting ->real_parent\n\ncurrent->group_leader->exit_signal may change during copy_process() if\ncurrent->real_parent exits.\n\nMove the assignment inside tasklist_lock to avoid the race.\n\nSigned-off-by: Eddy Wu <eddy_wu@trendmicro.com>\nAcked-by: Oleg Nesterov <oleg@redhat.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 199840}
{"func": "#endif\n\tclear_tsk_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t\tif (clone_flags & CLONE_THREAD)\n\t\t\tp->exit_signal = -1;\n\t\telse\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t\tp->exit_signal = args->exit_signal;\n\t}\n\n\tklp_copy_process(p);\n\n\tspin_lock(&current->sighand->siglock);", "project": "linux", "hash": 308794257608074143275963692734333829208, "size": 533, "commit_id": "b4e00444cab4c3f3fec876dc0cccc8cbb0d1a948", "message": "fork: fix copy_process(CLONE_PARENT) race with the exiting ->real_parent\n\ncurrent->group_leader->exit_signal may change during copy_process() if\ncurrent->real_parent exits.\n\nMove the assignment inside tasklist_lock to avoid the race.\n\nSigned-off-by: Eddy Wu <eddy_wu@trendmicro.com>\nAcked-by: Oleg Nesterov <oleg@redhat.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 293695}
{"func": "\t}\n\n\tStream_Read_UINT16(s, rect16->left);   /* left (2 bytes) */\n\tStream_Read_UINT16(s, rect16->top);    /* top (2 bytes) */\n\tStream_Read_UINT16(s, rect16->right);  /* right (2 bytes) */\n\tStream_Read_UINT16(s, rect16->bottom); /* bottom (2 bytes) */\n\treturn CHANNEL_RC_OK;\n}", "project": "FreeRDP", "hash": 138858215555612283726888611189591944586, "size": 14, "commit_id": "40393700642ad38437982e8a3afc34ff33ccf28e", "message": "Fixed input sanitation in rdpgfx_recv_solid_fill_pdu\n\nThe input rectangle must be checked for plausibility.\n\nThanks to Sunglin and HuanGMz of the Knownsec 404 security team and pangzi of pwnzen", "target": 1, "dataset": "other", "idx": 199860}
{"func": "\n\tStream_Read_UINT16(s, rect16->left);   /* left (2 bytes) */\n\tStream_Read_UINT16(s, rect16->top);    /* top (2 bytes) */\n\tStream_Read_UINT16(s, rect16->right);  /* right (2 bytes) */\n\tStream_Read_UINT16(s, rect16->bottom); /* bottom (2 bytes) */\n\tif (rect16->left >= rect16->right)\n\t\treturn ERROR_INVALID_DATA;\n\tif (rect16->top >= rect16->bottom)\n\t\treturn ERROR_INVALID_DATA;\n\treturn CHANNEL_RC_OK;\n}", "project": "FreeRDP", "hash": 165617928321411316983723181749205350057, "size": 18, "commit_id": "40393700642ad38437982e8a3afc34ff33ccf28e", "message": "Fixed input sanitation in rdpgfx_recv_solid_fill_pdu\n\nThe input rectangle must be checked for plausibility.\n\nThanks to Sunglin and HuanGMz of the Knownsec 404 security team and pangzi of pwnzen", "target": 0, "dataset": "other", "idx": 294212}
{"func": "\t\tWLog_ERR(TAG, \"Stream_GetRemainingLength(s) < 2\");\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, updateType); /* updateType (2 bytes) */\n\tWLog_Print(update->log, WLOG_TRACE, \"%s Update Data PDU\", UPDATE_TYPE_STRINGS[updateType]);\n\n\tif (!update_begin_paint(update))\n\t\tgoto fail;\n\n\tswitch (updateType)", "project": "FreeRDP", "hash": 241599745468172248070055396754810927395, "size": 78, "commit_id": "0332cad015fdf7fac7e5c6863484f18a554e0fcf", "message": "Fixed oob read in update_recv\n\nproperly use update_type_to_string to print update type.\nThanks to hac425 CVE-2020-11019", "target": 1, "dataset": "other", "idx": 199894}
{"func": "\t\tWLog_ERR(TAG, \"Stream_GetRemainingLength(s) < 2\");\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT16(s, updateType); /* updateType (2 bytes) */\n\tWLog_Print(update->log, WLOG_TRACE, \"%s Update Data PDU\", update_type_to_string(updateType));\n\n\tif (!update_begin_paint(update))\n\t\tgoto fail;\n\n\tswitch (updateType)", "project": "FreeRDP", "hash": 210712111577567156501672184535618537131, "size": 78, "commit_id": "0332cad015fdf7fac7e5c6863484f18a554e0fcf", "message": "Fixed oob read in update_recv\n\nproperly use update_type_to_string to print update type.\nThanks to hac425 CVE-2020-11019", "target": 0, "dataset": "other", "idx": 295081}
{"func": "static int may_create_in_sticky(struct dentry * const dir,\n\t\t\t\tstruct inode * const inode)\n{\n\tif ((!sysctl_protected_fifos && S_ISFIFO(inode->i_mode)) ||\n\t    (!sysctl_protected_regular && S_ISREG(inode->i_mode)) ||\n\t    likely(!(dir->d_inode->i_mode & S_ISVTX)) ||\n\t    uid_eq(inode->i_uid, dir->d_inode->i_uid) ||\n\t    uid_eq(current_fsuid(), inode->i_uid))\n\t\treturn 0;\n\n\tif (likely(dir->d_inode->i_mode & 0002) ||\n\t    (dir->d_inode->i_mode & 0020 &&\n\t     ((sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) ||\n\t      (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode))))) {\n\t\tconst char *operation = S_ISFIFO(inode->i_mode) ?\n\t\t\t\t\t\"sticky_create_fifo\" :\n\t\t\t\t\t\"sticky_create_regular\";", "project": "linux", "hash": 20871760417441141942075932160806615554, "size": 22, "commit_id": "d0cb50185ae942b03c4327be322055d622dc79f6", "message": "do_last(): fetch directory ->i_mode and ->i_uid before it's too late\n\nmay_create_in_sticky() call is done when we already have dropped the\nreference to dir.\n\nFixes: 30aba6656f61e (namei: allow restricted O_CREAT of FIFOs and regular files)\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>", "target": 1, "dataset": "other", "idx": 199916}
{"func": "static int may_create_in_sticky(umode_t dir_mode, kuid_t dir_uid,\n\t\t\t\tstruct inode * const inode)\n{\n\tif ((!sysctl_protected_fifos && S_ISFIFO(inode->i_mode)) ||\n\t    (!sysctl_protected_regular && S_ISREG(inode->i_mode)) ||\n\t    likely(!(dir_mode & S_ISVTX)) ||\n\t    uid_eq(inode->i_uid, dir_uid) ||\n\t    uid_eq(current_fsuid(), inode->i_uid))\n\t\treturn 0;\n\n\tif (likely(dir_mode & 0002) ||\n\t    (dir_mode & 0020 &&\n\t     ((sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) ||\n\t      (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode))))) {\n\t\tconst char *operation = S_ISFIFO(inode->i_mode) ?\n\t\t\t\t\t\"sticky_create_fifo\" :\n\t\t\t\t\t\"sticky_create_regular\";", "project": "linux", "hash": 265796941351907819719857276883338893990, "size": 22, "commit_id": "d0cb50185ae942b03c4327be322055d622dc79f6", "message": "do_last(): fetch directory ->i_mode and ->i_uid before it's too late\n\nmay_create_in_sticky() call is done when we already have dropped the\nreference to dir.\n\nFixes: 30aba6656f61e (namei: allow restricted O_CREAT of FIFOs and regular files)\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>", "target": 0, "dataset": "other", "idx": 295331}
{"func": "  runtime->setCurrentIP(ip);         \\\n  dst = expr;                        \\\n  ip = runtime->getCurrentIP();      \\\n  runtime->invalidateCurrentIP();\n\n#endif // NDEBUG\n\n  LLVM_DEBUG(dbgs() << \"interpretFunction() called\\n\");\n\n  ScopedNativeDepthTracker depthTracker{runtime};\n  if (LLVM_UNLIKELY(depthTracker.overflowed())) {\n        ip = NEXTINST(CompleteGenerator);\n        DISPATCH;\n      }\n\n      CASE(SaveGenerator) {\n        nextIP = IPADD(ip->iSaveGenerator.op1);\n        goto doSaveGen;\n      }\n      CASE(SaveGeneratorLong) {\n        nextIP = IPADD(ip->iSaveGeneratorLong.op1);\n        goto doSaveGen;\n      }\n\n    doSaveGen : {\n      auto *innerFn = vmcast<GeneratorInnerFunction>(\n          runtime->getCurrentFrame().getCalleeClosure());\n\n      innerFn->saveStack(runtime);\n      innerFn->setNextIP(nextIP);\n      innerFn->setState(GeneratorInnerFunction::State::SuspendedYield);\n      ip = NEXTINST(SaveGenerator);\n      DISPATCH;\n    }\n\n      CASE(StartGenerator) {\n        auto *innerFn = vmcast<GeneratorInnerFunction>(\n            runtime->getCurrentFrame().getCalleeClosure());\n        if (innerFn->getState() ==", "project": "hermes", "hash": 239731606711042238751809560389055742004, "size": 2753, "commit_id": "b2021df620824627f5a8c96615edbd1eb7fdddfc", "message": "Fix CVE-2020-1914 by using NEXTINST for SaveGeneratorLong\n\nSummary:\nIf `SaveGeneratorLong` was emitted, it would accidentally jump to the\nwrong next instruction, based on how long SaveGenerator was.\n\nMake a callout function to handle the common case, and handle the dispatch\nwithin each case of the interpreter loop.\n\nFixes CVE-2020-1914\n\nReviewed By: neildhar\n\nDifferential Revision: D24024242\n\nfbshipit-source-id: 3bcb88daa740f0d50e91771a49eb212551ce8bd8", "target": 1, "dataset": "other", "idx": 199924}
{"func": "  dst = expr;                        \\\n  ip = runtime->getCurrentIP();      \\\n  runtime->invalidateCurrentIP();\n\n#endif // NDEBUG\n\n/// \\def DONT_CAPTURE_IP(expr)\n/// \\param expr A call expression to a function external to the interpreter. The\n///   expression should not make any allocations and the IP should be set\n///   immediately following this macro.\n#define DONT_CAPTURE_IP(expr)      \\\n  do {                             \\\n    NoAllocScope noAlloc(runtime); \\\n    (void)expr;                    \\\n  } while (false)\n\n  LLVM_DEBUG(dbgs() << \"interpretFunction() called\\n\");\n\n  ScopedNativeDepthTracker depthTracker{runtime};\n  if (LLVM_UNLIKELY(depthTracker.overflowed())) {\n        ip = NEXTINST(CompleteGenerator);\n        DISPATCH;\n      }\n\n      CASE(SaveGenerator) {\n        DONT_CAPTURE_IP(\n            saveGenerator(runtime, frameRegs, IPADD(ip->iSaveGenerator.op1)));\n        ip = NEXTINST(SaveGenerator);\n        DISPATCH;\n      }\n      CASE(SaveGeneratorLong) {\n        DONT_CAPTURE_IP(saveGenerator(\n            runtime, frameRegs, IPADD(ip->iSaveGeneratorLong.op1)));\n        ip = NEXTINST(SaveGeneratorLong);\n        DISPATCH;\n      }\n\n      CASE(StartGenerator) {\n        auto *innerFn = vmcast<GeneratorInnerFunction>(\n            runtime->getCurrentFrame().getCalleeClosure());\n        if (innerFn->getState() ==", "project": "hermes", "hash": 289242997866177378156795426889881895518, "size": 2756, "commit_id": "b2021df620824627f5a8c96615edbd1eb7fdddfc", "message": "Fix CVE-2020-1914 by using NEXTINST for SaveGeneratorLong\n\nSummary:\nIf `SaveGeneratorLong` was emitted, it would accidentally jump to the\nwrong next instruction, based on how long SaveGenerator was.\n\nMake a callout function to handle the common case, and handle the dispatch\nwithin each case of the interpreter loop.\n\nFixes CVE-2020-1914\n\nReviewed By: neildhar\n\nDifferential Revision: D24024242\n\nfbshipit-source-id: 3bcb88daa740f0d50e91771a49eb212551ce8bd8", "target": 0, "dataset": "other", "idx": 295511}
{"func": "{\n    rfbFramebufferUpdateRectHeader rect;\n    int nlines;\n    int bytesPerLine = w * (cl->format.bitsPerPixel / 8);\n    char *fbptr = (cl->scaledScreen->frameBuffer + (cl->scaledScreen->paddedWidthInBytes * y)\n                   + (x * (cl->scaledScreen->bitsPerPixel / 8)));\n\n    /* Flush the buffer to guarantee correct alignment for translateFn(). */\n    if (cl->ublen > 0) {\n        if (!rfbSendUpdateBuf(cl))\n            return FALSE;", "project": "libvncserver", "hash": 102805718309305214589410374720369949147, "size": 64, "commit_id": "673c07a75ed844d74676f3ccdcfdc706a7052dba", "message": "libvncserver/rfbserver: fix possible divide-by-zero\n\nCloses #409", "target": 1, "dataset": "other", "idx": 199948}
{"func": "    rfbFramebufferUpdateRectHeader rect;\n    int nlines;\n    int bytesPerLine = w * (cl->format.bitsPerPixel / 8);\n    char *fbptr = (cl->scaledScreen->frameBuffer + (cl->scaledScreen->paddedWidthInBytes * y)\n                   + (x * (cl->scaledScreen->bitsPerPixel / 8)));\n\n    if(!h || !w)\n\treturn TRUE; /* nothing to send */\n\n    /* Flush the buffer to guarantee correct alignment for translateFn(). */\n    if (cl->ublen > 0) {\n        if (!rfbSendUpdateBuf(cl))\n            return FALSE;", "project": "libvncserver", "hash": 244759484708758165967165727952471421179, "size": 67, "commit_id": "673c07a75ed844d74676f3ccdcfdc706a7052dba", "message": "libvncserver/rfbserver: fix possible divide-by-zero\n\nCloses #409", "target": 0, "dataset": "other", "idx": 295864}
{"func": "{\nl_int32    i, j, w, h, wplc, wplm, wpld, ncolors, index;\nl_int32    rval, gval, bval, val, minval, maxval;\nl_int32   *lut;\nl_uint32  *datac, *datam, *datad, *linec, *linem, *lined;\nPIX       *pixc, *pixm, *pixg, *pixd;\nPIXCMAP   *cmap, *cmapd;\n\n    PROCNAME(\"pixFewColorsOctcubeQuantMixed\");\n\n    if (!pixs || pixGetDepth(pixs) != 32)\n    if (diffthresh <= 0) diffthresh = 20;\n    if (minfract <= 0.0) minfract = 0.05;\n    if (maxspan <= 2) maxspan = 15;\n\n        /* Start with a simple fixed octcube quantizer. */\n    if ((pixc = pixFewColorsOctcubeQuant1(pixs, level)) == NULL)\n        return (PIX *)ERROR_PTR(\"too many colors\", procName, NULL);\n\n        /* Identify and save color entries in the colormap.  Set up a LUT\n         * that returns -1 for any gray pixel. */\n    cmap = pixGetColormap(pixc);\n    ncolors = pixcmapGetCount(cmap);", "project": "leptonica", "hash": 189520668983998653093674301627277618692, "size": 96, "commit_id": "5ee24b398bb67666f6d173763eaaedd9c36fb1e5", "message": "Fixed issue 22140 in oss-fuzz: Heap-buffer-overflow\n* color quantized pix must be 8 bpp before extra colors are added.", "target": 1, "dataset": "other", "idx": 199976}
{"func": "{\nl_int32    i, j, w, h, wplc, wplm, wpld, ncolors, index;\nl_int32    rval, gval, bval, val, minval, maxval;\nl_int32   *lut;\nl_uint32  *datac, *datam, *datad, *linec, *linem, *lined;\nPIX       *pix1, *pixc, *pixm, *pixg, *pixd;\nPIXCMAP   *cmap, *cmapd;\n\n    PROCNAME(\"pixFewColorsOctcubeQuantMixed\");\n\n    if (!pixs || pixGetDepth(pixs) != 32)\n    if (diffthresh <= 0) diffthresh = 20;\n    if (minfract <= 0.0) minfract = 0.05;\n    if (maxspan <= 2) maxspan = 15;\n\n        /* Start with a simple fixed octcube quantizer. */\n    if ((pix1 = pixFewColorsOctcubeQuant1(pixs, level)) == NULL)\n        return (PIX *)ERROR_PTR(\"too many colors\", procName, NULL);\n    pixc = pixConvertTo8(pix1, 1);  /* must be 8 bpp */\n    pixDestroy(&pix1);\n\n        /* Identify and save color entries in the colormap.  Set up a LUT\n         * that returns -1 for any gray pixel. */\n    cmap = pixGetColormap(pixc);\n    ncolors = pixcmapGetCount(cmap);", "project": "leptonica", "hash": 140231359431133457710971366870294387407, "size": 98, "commit_id": "5ee24b398bb67666f6d173763eaaedd9c36fb1e5", "message": "Fixed issue 22140 in oss-fuzz: Heap-buffer-overflow\n* color quantized pix must be 8 bpp before extra colors are added.", "target": 0, "dataset": "other", "idx": 296022}
{"func": "             case 2: pixel_value = *((unsigned short *)srcptr2); break;\n             case 1: pixel_value = *((unsigned char *)srcptr2);  break;\n             default:\n               /* fixme: endianness problem? */\n               for (z = 0; z < bytesPerPixel; z++)\n                 pixel_value += (srcptr2[z] << (8 * z));\n                break;\n              }\n              /*\n              srcptr2 += bytesPerPixel;\n              */", "project": "libvncserver", "hash": 295503009586478475692075987935889055828, "size": 122, "commit_id": "a6788d1da719ae006605b78d22f5a9f170b423af", "message": "libvncserver: scale: cast to 64 bit before shifting\n\nSince pixel_value is 64 bit the data type of the shift operand should\nbe 64 bit too to prevent integer overflows.", "target": 1, "dataset": "other", "idx": 199993}
{"func": "             case 2: pixel_value = *((unsigned short *)srcptr2); break;\n             case 1: pixel_value = *((unsigned char *)srcptr2);  break;\n             default:\n               /* fixme: endianness problem? */\n               for (z = 0; z < bytesPerPixel; z++)\n                 pixel_value += ((unsigned long)srcptr2[z] << (8 * z));\n                break;\n              }\n              /*\n              srcptr2 += bytesPerPixel;\n              */", "project": "libvncserver", "hash": 85794347889118649475691237939536661528, "size": 122, "commit_id": "a6788d1da719ae006605b78d22f5a9f170b423af", "message": "libvncserver: scale: cast to 64 bit before shifting\n\nSince pixel_value is 64 bit the data type of the shift operand should\nbe 64 bit too to prevent integer overflows.", "target": 0, "dataset": "other", "idx": 296799}
{"func": "    TSK_FS_META *fs_meta;\n    TSK_FS_FILE *fs_file;\n    YAFFSFS_INFO *yfs = (YAFFSFS_INFO *)fs;\n    char ls[12];\n    YAFFSFS_PRINT_ADDR print;\n    char timeBuf[32];\n    YaffsCacheObject * obj = NULL;\n    YaffsCacheVersion * version = NULL;\n    YaffsHeader * header = NULL;\n\n    yaffscache_version_find_by_inode(yfs, inum, &version, &obj);", "project": "sleuthkit", "hash": 315622372000357176371289570991066181436, "size": 116, "commit_id": "459ae818fc8dae717549810150de4d191ce158f1", "message": "Fix stack buffer overflow in yaffsfs_istat\n\nPrevent a stack buffer overflow in yaffsfs_istat by increasing the buffer size to the size required by tsk_fs_time_to_str.", "target": 1, "dataset": "other", "idx": 200108}
{"func": "    TSK_FS_META *fs_meta;\n    TSK_FS_FILE *fs_file;\n    YAFFSFS_INFO *yfs = (YAFFSFS_INFO *)fs;\n    char ls[12];\n    YAFFSFS_PRINT_ADDR print;\n    char timeBuf[128];\n    YaffsCacheObject * obj = NULL;\n    YaffsCacheVersion * version = NULL;\n    YaffsHeader * header = NULL;\n\n    yaffscache_version_find_by_inode(yfs, inum, &version, &obj);", "project": "sleuthkit", "hash": 172760896741279150306338467662761860668, "size": 116, "commit_id": "459ae818fc8dae717549810150de4d191ce158f1", "message": "Fix stack buffer overflow in yaffsfs_istat\n\nPrevent a stack buffer overflow in yaffsfs_istat by increasing the buffer size to the size required by tsk_fs_time_to_str.", "target": 0, "dataset": "other", "idx": 299134}
{"func": "    if (sax != NULL)\n\tctxt->sax = oldsax;\n    xmlFreeParserCtxt(ctxt);\n    newDoc->intSubset = NULL;\n    newDoc->extSubset = NULL;\n    newDoc->oldNs = NULL;\n    xmlFreeDoc(newDoc);\n\n    return(ret);\n}", "project": "libxml2", "hash": 277853729283418525736722021066110830468, "size": 138, "commit_id": "5a02583c7e683896d84878bd90641d8d9b0d0549", "message": "Fix memory leak in xmlParseBalancedChunkMemoryRecover\n\nWhen doc is NULL, namespace created in xmlTreeEnsureXMLDecl\nis bind to newDoc->oldNs, in this case, set newDoc->oldNs to\nNULL and free newDoc will cause a memory leak.\n\nFound with libFuzzer.\n\nCloses #82.", "target": 1, "dataset": "other", "idx": 200109}
{"func": "    if (sax != NULL)\n\tctxt->sax = oldsax;\n    xmlFreeParserCtxt(ctxt);\n    newDoc->intSubset = NULL;\n    newDoc->extSubset = NULL;\n    if(doc != NULL)\n\tnewDoc->oldNs = NULL;\n    xmlFreeDoc(newDoc);\n\n    return(ret);\n}", "project": "libxml2", "hash": 142613781580222883758493860092065921257, "size": 139, "commit_id": "5a02583c7e683896d84878bd90641d8d9b0d0549", "message": "Fix memory leak in xmlParseBalancedChunkMemoryRecover\n\nWhen doc is NULL, namespace created in xmlTreeEnsureXMLDecl\nis bind to newDoc->oldNs, in this case, set newDoc->oldNs to\nNULL and free newDoc will cause a memory leak.\n\nFound with libFuzzer.\n\nCloses #82.", "target": 0, "dataset": "other", "idx": 299179}
{"func": "    else width = precision = slen;\n\n    /* Check string space, and add the string to the buffer if ok. If\n    not OK, add part of the string (debugging uses this to show as\n    much as possible). */\n\n    if (p >= last - width)\n      {\n      yield = FALSE;\n      width = precision = last - p - 1;\n      }\n    sprintf(CS p, \"%*.*s\", width, precision, s);\n    if (fp[-1] == 'S')\n      while (*p) { *p = tolower(*p); p++; }\n    else", "project": "exim", "hash": 320005744502620398274420421250934351064, "size": 234, "commit_id": "24c929a27415c7cfc7126c47e4cad39acf3efa6b", "message": "Buffer overrun fix. fixes: bug #787", "target": 1, "dataset": "other", "idx": 200316}
{"func": "\n    /* Check string space, and add the string to the buffer if ok. If\n    not OK, add part of the string (debugging uses this to show as\n    much as possible). */\n\n    if (p == last)\n      {\n      yield = FALSE;\n      goto END_FORMAT;\n      }\n    if (p >= last - width)\n      {\n      yield = FALSE;\n      width = precision = last - p - 1;\n      if (width < 0) width = 0;\n      if (precision < 0) precision = 0;\n      }\n    sprintf(CS p, \"%*.*s\", width, precision, s);\n    if (fp[-1] == 'S')\n      while (*p) { *p = tolower(*p); p++; }\n    else", "project": "exim", "hash": 29493622242225405401141093763393293838, "size": 241, "commit_id": "24c929a27415c7cfc7126c47e4cad39acf3efa6b", "message": "Buffer overrun fix. fixes: bug #787", "target": 0, "dataset": "other", "idx": 301211}
{"func": "\t\t/* we already duplicated this pointer */\n\t\treturn *old_p;\n\t}\n\tretval = ZCG(mem);;\n\tZCG(mem) = (void*)(((char*)ZCG(mem)) + ZEND_ALIGNED_SIZE(size));\n\tmemcpy(retval, source, size);\n\tif (free_source) {\n\t\tinterned_efree((char*)source);\n\t}\n\tzend_shared_alloc_register_xlat_entry(source, retval);\n\treturn retval;\n}", "project": "php-src", "hash": 299352734408075147829433838475904463433, "size": 17, "commit_id": "0a8f28b43212cc2ddbc1f2df710e37b1bec0addd", "message": "Fixed bug #68677 (Use After Free in OPcache)\n\n(cherry picked from commit 777c39f4042327eac4b63c7ee87dc1c7a09a3115)", "target": 1, "dataset": "other", "idx": 200327}
{"func": "\t\treturn *old_p;\n\t}\n\tretval = ZCG(mem);;\n\tZCG(mem) = (void*)(((char*)ZCG(mem)) + ZEND_ALIGNED_SIZE(size));\n\tmemcpy(retval, source, size);\n\tzend_shared_alloc_register_xlat_entry(source, retval);\n\tif (free_source) {\n\t\tinterned_efree((char*)source);\n\t}\n\treturn retval;\n}", "project": "php-src", "hash": 122228126921376440226435604693610853668, "size": 17, "commit_id": "0a8f28b43212cc2ddbc1f2df710e37b1bec0addd", "message": "Fixed bug #68677 (Use After Free in OPcache)\n\n(cherry picked from commit 777c39f4042327eac4b63c7ee87dc1c7a09a3115)", "target": 0, "dataset": "other", "idx": 301530}
{"func": "\t        } else\n\t\t */\n\t\t*out++ = *cur;\n\t    } else {\n\t\t/*\n\t\t * We assume we have UTF-8 input.\n\t\t */\n\t\tchar buf[11], *ptr;\n\t\tint val = 0, l = 1;\n\n\t\tif (*cur < 0xC0) {\n\t\t    xmlEntitiesErr(XML_CHECK_NOT_UTF8,\n\t\t\t    \"xmlEncodeEntities: input not UTF-8\");\n\t\t    if (doc != NULL)\n\t\t\tdoc->encoding = xmlStrdup(BAD_CAST \"ISO-8859-1\");\n\t\t    snprintf(buf, sizeof(buf), \"&#%d;\", *cur);", "project": "libxml2", "hash": 295556253642103349169033769733719269138, "size": 185, "commit_id": "bf22713507fe1fc3a2c4b525cf0a88c2dc87a3a2", "message": "Validate UTF8 in xmlEncodeEntities\n\nCode is currently assuming UTF-8 without validating. Truncated UTF-8\ninput can cause out-of-bounds array access.\n\nAdds further checks to partial fix in 50f06b3e.\n\nFixes #178", "target": 1, "dataset": "other", "idx": 200381}
{"func": "\t\t */\n\t\t*out++ = *cur;\n\t    } else {\n\t\t/*\n\t\t * We assume we have UTF-8 input.\n\t\t * It must match either:\n\t\t *   110xxxxx 10xxxxxx\n\t\t *   1110xxxx 10xxxxxx 10xxxxxx\n\t\t *   11110xxx 10xxxxxx 10xxxxxx 10xxxxxx\n\t\t * That is:\n\t\t *   cur[0] is 11xxxxxx\n\t\t *   cur[1] is 10xxxxxx\n\t\t *   cur[2] is 10xxxxxx if cur[0] is 111xxxxx\n\t\t *   cur[3] is 10xxxxxx if cur[0] is 1111xxxx\n\t\t *   cur[0] is not 11111xxx\n\t\t */\n\t\tchar buf[11], *ptr;\n\t\tint val = 0, l = 1;\n\n\t\tif (((cur[0] & 0xC0) != 0xC0) ||\n\t\t    ((cur[1] & 0xC0) != 0x80) ||\n\t\t    (((cur[0] & 0xE0) == 0xE0) && ((cur[2] & 0xC0) != 0x80)) ||\n\t\t    (((cur[0] & 0xF0) == 0xF0) && ((cur[3] & 0xC0) != 0x80)) ||\n\t\t    (((cur[0] & 0xF8) == 0xF8))) {\n\t\t    xmlEntitiesErr(XML_CHECK_NOT_UTF8,\n\t\t\t    \"xmlEncodeEntities: input not UTF-8\");\n\t\t    if (doc != NULL)\n\t\t\tdoc->encoding = xmlStrdup(BAD_CAST \"ISO-8859-1\");\n\t\t    snprintf(buf, sizeof(buf), \"&#%d;\", *cur);", "project": "libxml2", "hash": 116537115844529615353413364142450888793, "size": 199, "commit_id": "bf22713507fe1fc3a2c4b525cf0a88c2dc87a3a2", "message": "Validate UTF8 in xmlEncodeEntities\n\nCode is currently assuming UTF-8 without validating. Truncated UTF-8\ninput can cause out-of-bounds array access.\n\nAdds further checks to partial fix in 50f06b3e.\n\nFixes #178", "target": 0, "dataset": "other", "idx": 302155}
{"func": "{\n    int rc;\n    BerElement *ber;\n    struct berval *bvp;\n    char *uuid;\n    Slapi_Attr *attr;\n    Slapi_Value *val;\n\n    if (type == LDAP_SYNC_NONE || ctrlp == NULL || (ber = der_alloc()) == NULL) {\n        return (LDAP_OPERATIONS_ERROR);\n    }\n\n    *ctrlp = NULL;\n\n    slapi_entry_attr_find(e, SLAPI_ATTR_UNIQUEID, &attr);\n    slapi_attr_first_value(attr, &val);\n    uuid = sync_nsuniqueid2uuid(slapi_value_get_string(val));\n    if ((rc = ber_printf(ber, \"{eo\", type, uuid, 16)) != -1) {\n        if (cookie) {\n            char *cookiestr = sync_cookie2str(cookie);\n            rc = ber_printf(ber, \"s}\", cookiestr);", "project": "389-ds-base", "hash": 282180238928437160976107800273334393083, "size": 47, "commit_id": "2e5b526012612d1d6ccace46398bee679a730271", "message": "Issue 4711 - SIGSEV with sync_repl (#4738)\n\nBug description:\n\tsync_repl sends back entries identified with a unique\n\tidentifier that is 'nsuniqueid'. If 'nsuniqueid' is\n\tmissing, then it may crash\n\nFix description:\n\tCheck a nsuniqueid is available else returns OP_ERR\n\nrelates: https://github.com/389ds/389-ds-base/issues/4711\n\nReviewed by: Pierre Rogier, James Chapman, William Brown (Thanks!)\n\nPlatforms tested:  F33", "target": 1, "dataset": "other", "idx": 200413}
{"func": "{\n    int rc;\n    BerElement *ber;\n    struct berval *bvp;\n    char *uuid;\n    Slapi_Attr *attr = NULL;\n    Slapi_Value *val = NULL;\n\n    if (type == LDAP_SYNC_NONE || ctrlp == NULL || (ber = der_alloc()) == NULL) {\n        return (LDAP_OPERATIONS_ERROR);\n    }\n\n    *ctrlp = NULL;\n\n    slapi_entry_attr_find(e, SLAPI_ATTR_UNIQUEID, &attr);\n    slapi_attr_first_value(attr, &val);\n    if ((attr == NULL) || (val == NULL)) {\n        /* It may happen with entries in special backends\n         * such like cn=config, cn=shema, cn=monitor...\n         */\n        slapi_log_err(SLAPI_LOG_ERR, SYNC_PLUGIN_SUBSYSTEM,\n\t\t      \"sync_create_state_control - Entries are missing nsuniqueid. Unable to proceed.\\n\");\n        return (LDAP_OPERATIONS_ERROR);\n    }\n    uuid = sync_nsuniqueid2uuid(slapi_value_get_string(val));\n    if ((rc = ber_printf(ber, \"{eo\", type, uuid, 16)) != -1) {\n        if (cookie) {\n            char *cookiestr = sync_cookie2str(cookie);\n            rc = ber_printf(ber, \"s}\", cookiestr);", "project": "389-ds-base", "hash": 215302344452509467283235451332534852906, "size": 55, "commit_id": "2e5b526012612d1d6ccace46398bee679a730271", "message": "Issue 4711 - SIGSEV with sync_repl (#4738)\n\nBug description:\n\tsync_repl sends back entries identified with a unique\n\tidentifier that is 'nsuniqueid'. If 'nsuniqueid' is\n\tmissing, then it may crash\n\nFix description:\n\tCheck a nsuniqueid is available else returns OP_ERR\n\nrelates: https://github.com/389ds/389-ds-base/issues/4711\n\nReviewed by: Pierre Rogier, James Chapman, William Brown (Thanks!)\n\nPlatforms tested:  F33", "target": 0, "dataset": "other", "idx": 302826}
{"func": "static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n                   int32_t leftoverblock, int32_t ntbytes, int32_t maxbytes,\n                   const uint8_t* src, const int32_t offset, uint8_t* dest,\n                   uint8_t* tmp, uint8_t* tmp2) {\n  blosc2_context* context = thread_context->parent_context;\n  int dont_split = (context->header_flags & 0x10) >> 4;\n  int dict_training = context->use_dict && context->dict_cdict == NULL;\n    // See if we have a run here\n    const uint8_t* ip = (uint8_t*)_src + j * neblock;\n    const uint8_t* ipbound = (uint8_t*)_src + (j + 1) * neblock;\n    if (get_run(ip, ipbound)) {\n      // A run.  Encode the repeated byte as a negative length in the length of the split.\n      int32_t value = _src[j * neblock];\n      _sw32(dest - 4, -value);\n      continue;\n    }\n\n    maxout = neblock;\n  #if defined(HAVE_SNAPPY)\n    if (context->compcode == BLOSC_SNAPPY) {\n      maxout = (int32_t)snappy_max_compressed_length((size_t)neblock);\n    }\n  #endif /*  HAVE_SNAPPY */\n    if (ntbytes + maxout > maxbytes) {\n      /* avoid buffer * overrun */\n      maxout = (int64_t)maxbytes - (int64_t)ntbytes;\n      if (maxout <= 0) {\n        return 0;                  /* non-compressible block */\n      }\n    }\n    if (dict_training) {\n    if (!dict_training) {\n      if (cbytes == 0 || cbytes == neblock) {\n        /* The compressor has been unable to compress data at all. */\n        /* Before doing the copy, check that we are not running into a\n           buffer overflow. */\n        if ((ntbytes + neblock) > maxbytes) {\n          return 0;    /* Non-compressible data */\n        }\n        memcpy(dest, _src + j * neblock, (unsigned int)neblock);\n        cbytes = neblock;\n      }", "project": "c-blosc2", "hash": 33213175756612861371893504542614898630, "size": 170, "commit_id": "c4c6470e88210afc95262c8b9fcc27e30ca043ee", "message": "Fixed asan heap buffer overflow when not enough space to write compressed block size.", "target": 1, "dataset": "other", "idx": 200450}
{"func": "static int blosc_c(struct thread_context* thread_context, int32_t bsize,\n                   int32_t leftoverblock, int32_t ntbytes, int32_t destsize,\n                   const uint8_t* src, const int32_t offset, uint8_t* dest,\n                   uint8_t* tmp, uint8_t* tmp2) {\n  blosc2_context* context = thread_context->parent_context;\n  int dont_split = (context->header_flags & 0x10) >> 4;\n  int dict_training = context->use_dict && context->dict_cdict == NULL;\n    const uint8_t* ip = (uint8_t*)_src + j * neblock;\n    const uint8_t* ipbound = (uint8_t*)_src + (j + 1) * neblock;\n    if (get_run(ip, ipbound)) {\n      // A run.  Encode the repeated byte as a negative length in the length of the split.\n      int32_t value = _src[j * neblock];\n      if (ntbytes > destsize) {\n        /* Not enough space to write out compressed block size */\n        return -1;\n      }\n      _sw32(dest - 4, -value);\n      continue;\n    }\n\n    maxout = neblock;\n  #if defined(HAVE_SNAPPY)\n    if (context->compcode == BLOSC_SNAPPY) {\n      maxout = (int32_t)snappy_max_compressed_length((size_t)neblock);\n    }\n  #endif /*  HAVE_SNAPPY */\n    if (ntbytes + maxout > destsize) {\n      /* avoid buffer * overrun */\n      maxout = (int64_t)destsize - (int64_t)ntbytes;\n      if (maxout <= 0) {\n        return 0;                  /* non-compressible block */\n      }\n    }\n    if (dict_training) {\n    if (!dict_training) {\n      if (cbytes == 0 || cbytes == neblock) {\n        /* The compressor has been unable to compress data at all. */\n        /* Before doing the copy, check that we are not running into a\n           buffer overflow. */\n        if ((ntbytes + neblock) > destsize) {\n          return 0;    /* Non-compressible data */\n        }\n        memcpy(dest, _src + j * neblock, (unsigned int)neblock);\n        cbytes = neblock;\n      }", "project": "c-blosc2", "hash": 111230234594530939158768630350968281026, "size": 174, "commit_id": "c4c6470e88210afc95262c8b9fcc27e30ca043ee", "message": "Fixed asan heap buffer overflow when not enough space to write compressed block size.", "target": 0, "dataset": "other", "idx": 303085}
{"func": "copy_ciphersuites(gnutls_session_t session,\n\t\t  gnutls_buffer_st * cdata, int add_scsv)\n{\n\tint ret;\n\tuint8_t cipher_suites[MAX_CIPHERSUITE_SIZE + 2]; /* allow space for SCSV */\n\tint cipher_suites_size;\n\tsize_t init_length = cdata->length;\n\n\tret =\n\t    _gnutls_supported_ciphersuites(session, cipher_suites,\n\n\t\tret = _gnutls_ext_sr_send_cs(session);\n\t\tif (ret < 0)\n\t\t\treturn gnutls_assert_val(ret);\n\t}\n\n\tret =\n\t    _gnutls_buffer_append_data_prefix(cdata, 16, cipher_suites,\n\t\t\t\t\t      cipher_suites_size);\n\tif (ret < 0)\n\t\treturn gnutls_assert_val(ret);", "project": "gnutls", "hash": 266014162039389037407122294619920511067, "size": 51, "commit_id": "21f89efad7014a5ee0debd4cd3d59e27774b29e6", "message": "handshake: add FALLBACK_SCSV priority option\n\nThis allows clients to enable the TLS_FALLBACK_SCSV mechanism during\nthe handshake, as defined in RFC7507.", "target": 1, "dataset": "other", "idx": 200462}
{"func": "copy_ciphersuites(gnutls_session_t session,\n\t\t  gnutls_buffer_st * cdata, int add_scsv)\n{\n\tint ret;\n\tuint8_t cipher_suites[MAX_CIPHERSUITE_SIZE + 4]; /* allow space for SCSV */\n\tint cipher_suites_size;\n\tsize_t init_length = cdata->length;\n\n\tret =\n\t    _gnutls_supported_ciphersuites(session, cipher_suites,\n\t\tret = _gnutls_ext_sr_send_cs(session);\n\t\tif (ret < 0)\n\t\t\treturn gnutls_assert_val(ret);\n\t}\n\n\tif (session->internals.priorities.fallback) {\n\t\tcipher_suites[cipher_suites_size] =\n\t\t\tGNUTLS_FALLBACK_SCSV_MAJOR;\n\t\tcipher_suites[cipher_suites_size + 1] =\n\t\t\tGNUTLS_FALLBACK_SCSV_MINOR;\n\t\tcipher_suites_size += 2;\n\t}\n\n\tret =\n\t    _gnutls_buffer_append_data_prefix(cdata, 16, cipher_suites,\n\t\t\t\t\t      cipher_suites_size);\n\tif (ret < 0)\n\t\treturn gnutls_assert_val(ret);", "project": "gnutls", "hash": 247092967394331613424979076891944757849, "size": 59, "commit_id": "21f89efad7014a5ee0debd4cd3d59e27774b29e6", "message": "handshake: add FALLBACK_SCSV priority option\n\nThis allows clients to enable the TLS_FALLBACK_SCSV mechanism during\nthe handshake, as defined in RFC7507.", "target": 0, "dataset": "other", "idx": 303482}
{"func": "\tint ret;\n\n\tret = XpmReadFileToXpmImage(filename, &image, &info);\n\tif (ret != XpmSuccess) {\n\t\treturn 0;\n\t}\n\n\tif (!(im = gdImageCreate(image.width, image.height))) {\n\t\tgoto done;\n\t}\n\n\tnumber = image.ncolors;\n\tcolors = (int *) safe_emalloc(number, sizeof(int), 0);\n\tfor (i = 0; i < number; i++) {\n\t\tswitch (strlen (image.colorTable[i].c_color)) {\n\t\t\tcase 4:\n\t\t\t\tbuf[1] = '\\0';", "project": "php-src", "hash": 52478760808659547393823496374843686575, "size": 116, "commit_id": "cf4753691dc55999373d1c576f62ecb298723420", "message": "Fixed Bug #66901 php-gd 'c_color' NULL pointer dereference\n\nUpstream https://bitbucket.org/libgd/gd-libgd/commits/463c3bd09bfe8e924e19acad7a2a6af16953a704\n\nNotice: this fix don't manage monochrome/monovisual values\nbut just fix the security issue CVE-2014-2497\nfailing when trying to load such an image", "target": 1, "dataset": "other", "idx": 200525}
{"func": "\n\tret = XpmReadFileToXpmImage(filename, &image, &info);\n\tif (ret != XpmSuccess) {\n\t\treturn 0;\n\t}\n\tnumber = image.ncolors;\n\tfor(i = 0; i < number; i++) {\n\t\tif (!image.colorTable[i].c_color) {\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\tif (!(im = gdImageCreate(image.width, image.height))) {\n\t\tgoto done;\n\t}\n\n\tcolors = (int *) safe_emalloc(number, sizeof(int), 0);\n\tfor (i = 0; i < number; i++) {\n\t\tswitch (strlen (image.colorTable[i].c_color)) {\n\t\t\tcase 4:\n\t\t\t\tbuf[1] = '\\0';", "project": "php-src", "hash": 108615700401399758911645753719088620741, "size": 121, "commit_id": "cf4753691dc55999373d1c576f62ecb298723420", "message": "Fixed Bug #66901 php-gd 'c_color' NULL pointer dereference\n\nUpstream https://bitbucket.org/libgd/gd-libgd/commits/463c3bd09bfe8e924e19acad7a2a6af16953a704\n\nNotice: this fix don't manage monochrome/monovisual values\nbut just fix the security issue CVE-2014-2497\nfailing when trying to load such an image", "target": 0, "dataset": "other", "idx": 304857}
{"func": "        logprintf(STDERR_FILENO,\n                  \"SWTPM_NVRAM_Lock_Lockfile: Could not asprintf lock filename\\n\");\n        return TPM_FAIL;\n    }\n\n    *fd = open(lockfile, O_WRONLY|O_CREAT|O_TRUNC, 0660);\n    if (*fd < 0) {\n        logprintf(STDERR_FILENO,\n                  \"SWTPM_NVRAM_Lock_Lockfile: Could not open lockfile: %s\\n\",\n                  strerror(errno));\n        rc = TPM_FAIL;", "project": "swtpm", "hash": 300279316673534479735790981739540156268, "size": 40, "commit_id": "cae5991423826f21b11f7a5bc7f7b2b538bde2a2", "message": "swtpm: Do not follow symlinks when opening lockfile (CVE-2020-28407)\n\nThis patch addresses CVE-2020-28407.\n\nPrevent us from following symliks when we open the lockfile\nfor writing.\n\nSigned-off-by: Stefan Berger <stefanb@linux.ibm.com>", "target": 1, "dataset": "other", "idx": 200558}
{"func": "        logprintf(STDERR_FILENO,\n                  \"SWTPM_NVRAM_Lock_Lockfile: Could not asprintf lock filename\\n\");\n        return TPM_FAIL;\n    }\n\n    *fd = open(lockfile, O_WRONLY|O_CREAT|O_TRUNC|O_NOFOLLOW, 0660);\n    if (*fd < 0) {\n        logprintf(STDERR_FILENO,\n                  \"SWTPM_NVRAM_Lock_Lockfile: Could not open lockfile: %s\\n\",\n                  strerror(errno));\n        rc = TPM_FAIL;", "project": "swtpm", "hash": 227292187414569762413855551049878784834, "size": 40, "commit_id": "cae5991423826f21b11f7a5bc7f7b2b538bde2a2", "message": "swtpm: Do not follow symlinks when opening lockfile (CVE-2020-28407)\n\nThis patch addresses CVE-2020-28407.\n\nPrevent us from following symliks when we open the lockfile\nfor writing.\n\nSigned-off-by: Stefan Berger <stefanb@linux.ibm.com>", "target": 0, "dataset": "other", "idx": 305346}
{"func": "\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\treg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);", "project": "linux", "hash": 207420910822850441672571201896199052237, "size": 119, "commit_id": "998912346c0da53a6dbb71fab3a138586b596b30", "message": "media: ov519: add missing endpoint sanity checks\n\nMake sure to check that we have at least one endpoint before accessing\nthe endpoint array to avoid dereferencing a NULL-pointer on stream\nstart.\n\nNote that these sanity checks are not redundant as the driver is mixing\nlooking up altsettings by index and by number, which need not coincide.\n\nFixes: 1876bb923c98 (\"V4L/DVB (12079): gspca_ov519: add support for the ov511 bridge\")\nFixes: b282d87332f5 (\"V4L/DVB (12080): gspca_ov519: Fix ov518+ with OV7620AE (Trust spacecam 320)\")\nCc: stable <stable@vger.kernel.org>     # 2.6.31\nCc: Hans de Goede <hdegoede@redhat.com>\nSigned-off-by: Johan Hovold <johan@kernel.org>\nSigned-off-by: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 1, "dataset": "other", "idx": 200621}
{"func": "\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1) {\n\t\tsd->gspca_dev.usb_err = -ENODEV;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\treg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);", "project": "linux", "hash": 66031367225502192060724962502262196162, "size": 124, "commit_id": "998912346c0da53a6dbb71fab3a138586b596b30", "message": "media: ov519: add missing endpoint sanity checks\n\nMake sure to check that we have at least one endpoint before accessing\nthe endpoint array to avoid dereferencing a NULL-pointer on stream\nstart.\n\nNote that these sanity checks are not redundant as the driver is mixing\nlooking up altsettings by index and by number, which need not coincide.\n\nFixes: 1876bb923c98 (\"V4L/DVB (12079): gspca_ov519: add support for the ov511 bridge\")\nFixes: b282d87332f5 (\"V4L/DVB (12080): gspca_ov519: Fix ov518+ with OV7620AE (Trust spacecam 320)\")\nCc: stable <stable@vger.kernel.org>     # 2.6.31\nCc: Hans de Goede <hdegoede@redhat.com>\nSigned-off-by: Johan Hovold <johan@kernel.org>\nSigned-off-by: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 0, "dataset": "other", "idx": 306343}
{"func": "\n\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);", "project": "linux", "hash": 263247290990414325966345455466875763387, "size": 133, "commit_id": "998912346c0da53a6dbb71fab3a138586b596b30", "message": "media: ov519: add missing endpoint sanity checks\n\nMake sure to check that we have at least one endpoint before accessing\nthe endpoint array to avoid dereferencing a NULL-pointer on stream\nstart.\n\nNote that these sanity checks are not redundant as the driver is mixing\nlooking up altsettings by index and by number, which need not coincide.\n\nFixes: 1876bb923c98 (\"V4L/DVB (12079): gspca_ov519: add support for the ov511 bridge\")\nFixes: b282d87332f5 (\"V4L/DVB (12080): gspca_ov519: Fix ov518+ with OV7620AE (Trust spacecam 320)\")\nCc: stable <stable@vger.kernel.org>     # 2.6.31\nCc: Hans de Goede <hdegoede@redhat.com>\nSigned-off-by: Johan Hovold <johan@kernel.org>\nSigned-off-by: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 1, "dataset": "other", "idx": 200622}
{"func": "\tintf = usb_ifnum_to_if(sd->gspca_dev.dev, sd->gspca_dev.iface);\n\talt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);\n\tif (!alt) {\n\t\tgspca_err(gspca_dev, \"Couldn't get altsetting\\n\");\n\t\tsd->gspca_dev.usb_err = -EIO;\n\t\treturn;\n\t}\n\n\tif (alt->desc.bNumEndpoints < 1) {\n\t\tsd->gspca_dev.usb_err = -ENODEV;\n\t\treturn;\n\t}\n\n\tpacket_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);\n\tov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);", "project": "linux", "hash": 205218704415973742418430089136769528517, "size": 138, "commit_id": "998912346c0da53a6dbb71fab3a138586b596b30", "message": "media: ov519: add missing endpoint sanity checks\n\nMake sure to check that we have at least one endpoint before accessing\nthe endpoint array to avoid dereferencing a NULL-pointer on stream\nstart.\n\nNote that these sanity checks are not redundant as the driver is mixing\nlooking up altsettings by index and by number, which need not coincide.\n\nFixes: 1876bb923c98 (\"V4L/DVB (12079): gspca_ov519: add support for the ov511 bridge\")\nFixes: b282d87332f5 (\"V4L/DVB (12080): gspca_ov519: Fix ov518+ with OV7620AE (Trust spacecam 320)\")\nCc: stable <stable@vger.kernel.org>     # 2.6.31\nCc: Hans de Goede <hdegoede@redhat.com>\nSigned-off-by: Johan Hovold <johan@kernel.org>\nSigned-off-by: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 0, "dataset": "other", "idx": 306292}
{"func": "        // ones.\n        list[i].flags.cached = storeGetPublic(list[i].url, m) ? 1 : 0;\n        ++i;\n    }\n\n    debugs(52, 3, \"urnParseReply: Found \" << i << \" URLs\");\n    return list;\n}", "project": "squid", "hash": 111230701440294531243041412971285423945, "size": 48, "commit_id": "47a085ff06598b64817875769022b8707a0af7db", "message": "Bug 5104: Memory leak in RFC 2169 response parsing (#778)\n\nA temporary parsing buffer was not being released when\nparsing completed.", "target": 1, "dataset": "other", "idx": 200632}
{"func": "        list[i].flags.cached = storeGetPublic(list[i].url, m) ? 1 : 0;\n        ++i;\n    }\n\n    debugs(52, 3, \"urnParseReply: Found \" << i << \" URLs\");\n    xfree(buf);\n    return list;\n}", "project": "squid", "hash": 263080633568694329448297146517615992184, "size": 49, "commit_id": "47a085ff06598b64817875769022b8707a0af7db", "message": "Bug 5104: Memory leak in RFC 2169 response parsing (#778)\n\nA temporary parsing buffer was not being released when\nparsing completed.", "target": 0, "dataset": "other", "idx": 306747}
{"func": "\t  } else if (ctrl & PAM_ICASE_ARG) {\n\t    compare = -2;\n\t  } else {\n\t    cryptpw = crypt (pass, data.dptr);\n\n\t    if (cryptpw) {\n\t      compare = strncasecmp (data.dptr, cryptpw, data.dsize);\n\t    } else {\n\t      compare = -2;\n\t      if (ctrl & PAM_DEBUG_ARG) {\n\t\tpam_syslog(pamh, LOG_INFO, \"crypt() returned NULL\");\n\t      }\n\t    };\n\n\t  };\n", "project": "linux-pam", "hash": 112434099628297315187231061836027553571, "size": 169, "commit_id": "57a1e2b274d0a6376d92ada9926e5c5741e7da20", "message": "pam_userdb: fix password hash comparison\n\nStarting with commit Linux-PAM-0-77-28-g0b3e583 that introduced hashed\npasswords support in pam_userdb, hashes are compared case-insensitively.\nThis bug leads to accepting hashes for completely different passwords in\naddition to those that should be accepted.\n\nAdditionally, commit Linux-PAM-1_1_6-13-ge2a8187 that added support for\nmodern password hashes with different lengths and settings, did not\nupdate the hash comparison accordingly, which leads to accepting\ncomputed hashes longer than stored hashes when the latter is a prefix\nof the former.\n\n* modules/pam_userdb/pam_userdb.c (user_lookup): Reject the computed\nhash whose length differs from the stored hash length.\nCompare computed and stored hashes case-sensitively.\nFixes CVE-2013-7041.\n\nBug-Debian: http://bugs.debian.org/731368", "target": 1, "dataset": "other", "idx": 200647}
{"func": "\t  } else if (ctrl & PAM_ICASE_ARG) {\n\t    compare = -2;\n\t  } else {\n\t    cryptpw = crypt (pass, data.dptr);\n\n\t    if (cryptpw && strlen(cryptpw) == (size_t)data.dsize) {\n\t      compare = memcmp(data.dptr, cryptpw, data.dsize);\n\t    } else {\n\t      compare = -2;\n\t      if (ctrl & PAM_DEBUG_ARG) {\n\t\tif (cryptpw)\n\t\t  pam_syslog(pamh, LOG_INFO, \"lengths of computed and stored hashes differ\");\n\t\telse\n\t\t  pam_syslog(pamh, LOG_INFO, \"crypt() returned NULL\");\n\t      }\n\t    };\n\n\t  };\n", "project": "linux-pam", "hash": 121873383550116916687105523985397910680, "size": 172, "commit_id": "57a1e2b274d0a6376d92ada9926e5c5741e7da20", "message": "pam_userdb: fix password hash comparison\n\nStarting with commit Linux-PAM-0-77-28-g0b3e583 that introduced hashed\npasswords support in pam_userdb, hashes are compared case-insensitively.\nThis bug leads to accepting hashes for completely different passwords in\naddition to those that should be accepted.\n\nAdditionally, commit Linux-PAM-1_1_6-13-ge2a8187 that added support for\nmodern password hashes with different lengths and settings, did not\nupdate the hash comparison accordingly, which leads to accepting\ncomputed hashes longer than stored hashes when the latter is a prefix\nof the former.\n\n* modules/pam_userdb/pam_userdb.c (user_lookup): Reject the computed\nhash whose length differs from the stored hash length.\nCompare computed and stored hashes case-sensitively.\nFixes CVE-2013-7041.\n\nBug-Debian: http://bugs.debian.org/731368", "target": 0, "dataset": "other", "idx": 307216}
{"func": "       return XML_TOK_INVALID, since the BOM is still in the buffer\n    */\n    else if (tok == XML_TOK_BOM && next == end && !ps_finalBuffer) {\n      *nextPtr = next;\n      return XML_ERROR_NONE;\n    }\n    start = next;\n    eventPtr = start;\n  }\n}", "project": "libexpat", "hash": 337998318068132835707143746781218093260, "size": 65, "commit_id": "c4bf96bb51dd2a1b0e185374362ee136fe2c9d7f", "message": "xmlparse.c: Fix external entity infinite loop bug (CVE-2017-9233)", "target": 1, "dataset": "other", "idx": 200711}
{"func": "    */\n    else if (tok == XML_TOK_BOM && next == end && !ps_finalBuffer) {\n      *nextPtr = next;\n      return XML_ERROR_NONE;\n    }\n    /* If we get this token, we have the start of what might be a\n       normal tag, but not a declaration (i.e. it doesn't begin with\n       \"<!\").  In a DTD context, that isn't legal.\n    */\n    else if (tok == XML_TOK_INSTANCE_START) {\n      *nextPtr = next;\n      return XML_ERROR_SYNTAX;\n    }\n    start = next;\n    eventPtr = start;\n  }\n}", "project": "libexpat", "hash": 315722906188758225822115197049039800003, "size": 73, "commit_id": "c4bf96bb51dd2a1b0e185374362ee136fe2c9d7f", "message": "xmlparse.c: Fix external entity infinite loop bug (CVE-2017-9233)", "target": 0, "dataset": "other", "idx": 308321}
{"func": "\t\t\t/* no more pseudo-headers, time to build the request line */\n\t\t\tsl = h2_prepare_htx_reqline(fields, phdr_val, htx, msgf);\n\t\t\tif (!sl)\n\t\t\t\tgoto fail;\n\t\t\tfields |= H2_PHDR_FND_NONE;\n\t\t}\n\n\t\tif (isteq(list[idx].n, ist(\"host\")))\n\t\t\tfields |= H2_PHDR_FND_HOST;\n\n\t\tif (isteq(list[idx].n, ist(\"content-length\"))) {\n\t\t\tret = h2_parse_cont_len_header(msgf, &list[idx].v, body_len);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto fail;\n\t}\n\n\t/* update the start line with last detected header info */\n\tsl->flags |= sl_flags;\n\n\t/* complete with missing Host if needed */\n\tif ((fields & (H2_PHDR_FND_HOST|H2_PHDR_FND_AUTH)) == H2_PHDR_FND_AUTH) {\n\t\t/* missing Host field, use :authority instead */\n\t\tif (!htx_add_header(htx, ist(\"host\"), phdr_val[H2_PHDR_IDX_AUTH]))\n\t\t\tgoto fail;\n\t}", "project": "haproxy", "hash": 15719751322204847922271508601509064692, "size": 203, "commit_id": "b5d2b9e154d78e4075db163826c5e0f6d31b2ab1", "message": "BUG/MEDIUM: h2: give :authority precedence over Host\n\nThe wording regarding Host vs :authority in RFC7540 is ambiguous as it\nsays that an intermediary must produce a host header from :authority if\nHost is missing, but, contrary to HTTP/1.1, doesn't say anything regarding\nthe possibility that Host and :authority differ, which leaves Host with\nhigher precedence there. In addition it mentions that clients should use\n:authority *instead* of Host, and that H1->H2 should use :authority only\nif the original request was in authority form. This leaves some gray\narea in the middle of the chain for fully valid H2 requests arboring a\nHost header that are forwarded to the other side where it's possible to\ndrop the Host header and use the authority only after forwarding to a\nsecond H2 layer, thus possibly seeing two different values of Host at\na different stage. There's no such issue when forwarding from H2 to H1\nas the authority is dropped only only the Host is kept.\n\nNote that the following request is sufficient to re-normalize such a\nrequest:\n\n   http-request set-header host %[req.hdr(host)]\n\nThe new spec in progress (draft-ietf-httpbis-http2bis-03) addresses\nthis trouble by being a bit is stricter on these rules. It clarifies\nthat :authority must always be used instead of Host and that Host ought\nto be ignored. This is much saner as it avoids to convey two distinct\nvalues along the chain. This becomes the protocol-level equivalent of:\n\n   http-request set-uri %[url]\n\nSo this patch does exactly this, which we were initially a bit reluctant\nto do initially by lack of visibility about other implementations'\nexpectations. In addition it slightly simplifies the Host header field\ncreation by always placing it first in the list of headers instead of\nlast; this could also speed up the look up a little bit.\n\nThis needs to be backported to 2.0. Non-HTX versions are safe regarding\nthis because they drop the URI during the conversion to HTTP/1.1 so\nonly Host is used and transmitted.\n\nThanks to Tim D\ufffdsterhus for reporting that one.", "target": 1, "dataset": "other", "idx": 200767}
{"func": "\t\t\t/* no more pseudo-headers, time to build the request line */\n\t\t\tsl = h2_prepare_htx_reqline(fields, phdr_val, htx, msgf);\n\t\t\tif (!sl)\n\t\t\t\tgoto fail;\n\t\t\tfields |= H2_PHDR_FND_NONE;\n\n\t\t\t/* http2bis draft recommends to drop Host in favor of :authority when\n\t\t\t * the latter is present. This is required to make sure there is no\n\t\t\t * discrepancy between the authority and the host header, especially\n\t\t\t * since routing rules usually involve Host. Here we already know if\n\t\t\t * :authority was found so we can emit it right now and mark the host\n\t\t\t * as filled so that it's skipped later.\n\t\t\t */\n\t\t\tif (fields & H2_PHDR_FND_AUTH) {\n\t\t\t\tif (!htx_add_header(htx, ist(\"host\"), phdr_val[H2_PHDR_IDX_AUTH]))\n\t\t\t\t\tgoto fail;\n\t\t\t\tfields |= H2_PHDR_FND_HOST;\n\t\t\t}\n\t\t}\n\n\t\tif (isteq(list[idx].n, ist(\"host\"))) {\n\t\t\tif (fields & H2_PHDR_FND_HOST)\n\t\t\t\tcontinue;\n\n\t\t\tfields |= H2_PHDR_FND_HOST;\n\t\t}\n\n\t\tif (isteq(list[idx].n, ist(\"content-length\"))) {\n\t\t\tret = h2_parse_cont_len_header(msgf, &list[idx].v, body_len);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto fail;\n\t}\n\n\t/* update the start line with last detected header info */\n\tsl->flags |= sl_flags;\n\n\t/* complete with missing Host if needed (we may validate this test if\n\t * no regular header was found).\n\t */\n\tif ((fields & (H2_PHDR_FND_HOST|H2_PHDR_FND_AUTH)) == H2_PHDR_FND_AUTH) {\n\t\t/* missing Host field, use :authority instead */\n\t\tif (!htx_add_header(htx, ist(\"host\"), phdr_val[H2_PHDR_IDX_AUTH]))\n\t\t\tgoto fail;\n\t}", "project": "haproxy", "hash": 266045474504989259384704744141280674190, "size": 222, "commit_id": "b5d2b9e154d78e4075db163826c5e0f6d31b2ab1", "message": "BUG/MEDIUM: h2: give :authority precedence over Host\n\nThe wording regarding Host vs :authority in RFC7540 is ambiguous as it\nsays that an intermediary must produce a host header from :authority if\nHost is missing, but, contrary to HTTP/1.1, doesn't say anything regarding\nthe possibility that Host and :authority differ, which leaves Host with\nhigher precedence there. In addition it mentions that clients should use\n:authority *instead* of Host, and that H1->H2 should use :authority only\nif the original request was in authority form. This leaves some gray\narea in the middle of the chain for fully valid H2 requests arboring a\nHost header that are forwarded to the other side where it's possible to\ndrop the Host header and use the authority only after forwarding to a\nsecond H2 layer, thus possibly seeing two different values of Host at\na different stage. There's no such issue when forwarding from H2 to H1\nas the authority is dropped only only the Host is kept.\n\nNote that the following request is sufficient to re-normalize such a\nrequest:\n\n   http-request set-header host %[req.hdr(host)]\n\nThe new spec in progress (draft-ietf-httpbis-http2bis-03) addresses\nthis trouble by being a bit is stricter on these rules. It clarifies\nthat :authority must always be used instead of Host and that Host ought\nto be ignored. This is much saner as it avoids to convey two distinct\nvalues along the chain. This becomes the protocol-level equivalent of:\n\n   http-request set-uri %[url]\n\nSo this patch does exactly this, which we were initially a bit reluctant\nto do initially by lack of visibility about other implementations'\nexpectations. In addition it slightly simplifies the Host header field\ncreation by always placing it first in the list of headers instead of\nlast; this could also speed up the look up a little bit.\n\nThis needs to be backported to 2.0. Non-HTX versions are safe regarding\nthis because they drop the URI during the conversion to HTTP/1.1 so\nonly Host is used and transmitted.\n\nThanks to Tim D\ufffdsterhus for reporting that one.", "target": 0, "dataset": "other", "idx": 309702}
{"func": "\tif (power_zone->ops->get_max_energy_range_uj)\n\t\tpower_zone->zone_dev_attrs[count++] =\n\t\t\t\t\t&dev_attr_max_energy_range_uj.attr;\n\tif (power_zone->ops->get_energy_uj) {\n\t\tif (power_zone->ops->reset_energy_uj)\n\t\t\tdev_attr_energy_uj.attr.mode = S_IWUSR | S_IRUGO;\n\t\telse\n\t\t\tdev_attr_energy_uj.attr.mode = S_IRUGO;\n\t\tpower_zone->zone_dev_attrs[count++] =\n\t\t\t\t\t&dev_attr_energy_uj.attr;\n\t}\n\tif (power_zone->ops->get_power_uw)\n\t\tpower_zone->zone_dev_attrs[count++] =", "project": "linux", "hash": 52467361752463996065834465628288893561, "size": 26, "commit_id": "949dd0104c496fa7c14991a23c03c62e44637e71", "message": "powercap: restrict energy meter to root access\n\nRemove non-privileged user access to power data contained in\n/sys/class/powercap/intel-rapl*/*/energy_uj\n\nNon-privileged users currently have read access to power data and can\nuse this data to form a security attack. Some privileged\ndrivers/applications need read access to this data, but don't expose it\nto non-privileged users.\n\nFor example, thermald uses this data to ensure that power management\nworks correctly. Thus removing non-privileged access is preferred over\ncompletely disabling this power reporting capability with\nCONFIG_INTEL_RAPL=n.\n\nFixes: 95677a9a3847 (\"PowerCap: Fix mode for energy counter\")\n\nSigned-off-by: Len Brown <len.brown@intel.com>\nCc: stable@vger.kernel.org", "target": 1, "dataset": "other", "idx": 200832}
{"func": "\tif (power_zone->ops->get_max_energy_range_uj)\n\t\tpower_zone->zone_dev_attrs[count++] =\n\t\t\t\t\t&dev_attr_max_energy_range_uj.attr;\n\tif (power_zone->ops->get_energy_uj) {\n\t\tif (power_zone->ops->reset_energy_uj)\n\t\t\tdev_attr_energy_uj.attr.mode = S_IWUSR | S_IRUSR;\n\t\telse\n\t\t\tdev_attr_energy_uj.attr.mode = S_IRUSR;\n\t\tpower_zone->zone_dev_attrs[count++] =\n\t\t\t\t\t&dev_attr_energy_uj.attr;\n\t}\n\tif (power_zone->ops->get_power_uw)\n\t\tpower_zone->zone_dev_attrs[count++] =", "project": "linux", "hash": 212688182015870014729078339472072838837, "size": 26, "commit_id": "949dd0104c496fa7c14991a23c03c62e44637e71", "message": "powercap: restrict energy meter to root access\n\nRemove non-privileged user access to power data contained in\n/sys/class/powercap/intel-rapl*/*/energy_uj\n\nNon-privileged users currently have read access to power data and can\nuse this data to form a security attack. Some privileged\ndrivers/applications need read access to this data, but don't expose it\nto non-privileged users.\n\nFor example, thermald uses this data to ensure that power management\nworks correctly. Thus removing non-privileged access is preferred over\ncompletely disabling this power reporting capability with\nCONFIG_INTEL_RAPL=n.\n\nFixes: 95677a9a3847 (\"PowerCap: Fix mode for energy counter\")\n\nSigned-off-by: Len Brown <len.brown@intel.com>\nCc: stable@vger.kernel.org", "target": 0, "dataset": "other", "idx": 310345}
{"func": "\t{\n\t\tjpeg_component_info *compptr=&dinfo->comp_info[i];\n\t\tint ih;\n\t\tiw[i]=compptr->width_in_blocks*dctsize;\n\t\tih=compptr->height_in_blocks*dctsize;\n\t\tpw[i]=PAD(dinfo->output_width, dinfo->max_h_samp_factor)\n\t\t\t*compptr->h_samp_factor/dinfo->max_h_samp_factor;\n\t\tph[i]=PAD(dinfo->output_height, dinfo->max_v_samp_factor)\n\t\t\t*compptr->v_samp_factor/dinfo->max_v_samp_factor;\n\t\tif(iw[i]!=pw[i] || ih!=ph[i]) usetmpbuf=1;\n\t\tth[i]=compptr->v_samp_factor*dctsize;\n\t\ttmpbufsize+=iw[i]*th[i];\n\t\tif((outbuf[i]=(JSAMPROW *)malloc(sizeof(JSAMPROW)*ph[i]))==NULL)\n\t\t\t_throw(\"tjDecompressToYUVPlanes(): Memory allocation failure\");", "project": "libjpeg-turbo", "hash": 169277103039170376241754039304908975481, "size": 180, "commit_id": "dab6be4cfb2f9307b5378d2d1dc74d9080383dc2", "message": "tjDecompressToYUV*(): Fix OOB write/double free\n\n... when attempting to decompress grayscale JPEG images with sampling\nfactors != 1.\n\nFixes #387", "target": 1, "dataset": "other", "idx": 200866}
{"func": "\t{\n\t\tjpeg_component_info *compptr=&dinfo->comp_info[i];\n\t\tint ih;\n\t\tiw[i]=compptr->width_in_blocks*dctsize;\n\t\tih=compptr->height_in_blocks*dctsize;\n\t\tpw[i]=tjPlaneWidth(i, dinfo->output_width, jpegSubsamp);\n\t\tph[i]=tjPlaneHeight(i, dinfo->output_height, jpegSubsamp);\n\t\tif(iw[i]!=pw[i] || ih!=ph[i]) usetmpbuf=1;\n\t\tth[i]=compptr->v_samp_factor*dctsize;\n\t\ttmpbufsize+=iw[i]*th[i];\n\t\tif((outbuf[i]=(JSAMPROW *)malloc(sizeof(JSAMPROW)*ph[i]))==NULL)\n\t\t\t_throw(\"tjDecompressToYUVPlanes(): Memory allocation failure\");", "project": "libjpeg-turbo", "hash": 117971814043960851220573612576267403876, "size": 178, "commit_id": "dab6be4cfb2f9307b5378d2d1dc74d9080383dc2", "message": "tjDecompressToYUV*(): Fix OOB write/double free\n\n... when attempting to decompress grayscale JPEG images with sampling\nfactors != 1.\n\nFixes #387", "target": 0, "dataset": "other", "idx": 311112}
{"func": "\t\t\t * no local regular files yet\n\t\t\t */\n\t\t\tif (S_ISREG(mode))\n\t\t\t\treturn __this_address;\n\t\t\tif (di_size > XFS_DFORK_DSIZE(dip, mp))\n\t\t\t\treturn __this_address;\n\t\t\t/* fall through */\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\tcase XFS_DINODE_FMT_BTREE:\n\t\t\tbreak;\n\t\tdefault:\n\t\treturn __this_address;\n\t}\n\n\tif (XFS_DFORK_Q(dip)) {\n\t\tswitch (dip->di_aformat) {\n\t\tcase XFS_DINODE_FMT_LOCAL:\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\tcase XFS_DINODE_FMT_BTREE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn __this_address;\n\t\t}\n\t}\n\n\t/* only version 3 or greater inodes are extensively verified here */\n\tif (dip->di_version < 3)\n\t\treturn NULL;", "project": "linux", "hash": 101398170972394773795376226616025225850, "size": 125, "commit_id": "b42db0860e13067fcc7cbfba3966c9e652668bbc", "message": "xfs: enhance dinode verifier\n\nAdd several more validations to xfs_dinode_verify:\n\n- For LOCAL data fork formats, di_nextents must be 0.\n- For LOCAL attr fork formats, di_anextents must be 0.\n- For inodes with no attr fork offset,\n  - format must be XFS_DINODE_FMT_EXTENTS if set at all\n  - di_anextents must be 0.\n\nThanks to dchinner for pointing out a couple related checks I had\nforgotten to add.\n\nSigned-off-by: Eric Sandeen <sandeen@redhat.com>\nBugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=199377\nReviewed-by: Darrick J. Wong <darrick.wong@oracle.com>\nSigned-off-by: Darrick J. Wong <darrick.wong@oracle.com>", "target": 1, "dataset": "other", "idx": 200867}
{"func": "\t\t\t */\n\t\t\tif (S_ISREG(mode))\n\t\t\t\treturn __this_address;\n\t\t\tif (di_size > XFS_DFORK_DSIZE(dip, mp))\n\t\t\t\treturn __this_address;\n\t\t\tif (dip->di_nextents)\n\t\t\t\treturn __this_address;\n\t\t\t/* fall through */\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\tcase XFS_DINODE_FMT_BTREE:\n\t\t\tbreak;\n\t\tdefault:\n\t}\n\n\tif (XFS_DFORK_Q(dip)) {\n\t\tswitch (dip->di_aformat) {\n\t\tcase XFS_DINODE_FMT_LOCAL:\n\t\t\tif (dip->di_anextents)\n\t\t\t\treturn __this_address;\n\t\t/* fall through */\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\tcase XFS_DINODE_FMT_BTREE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn __this_address;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If there is no fork offset, this may be a freshly-made inode\n\t\t * in a new disk cluster, in which case di_aformat is zeroed.\n\t\t * Otherwise, such an inode must be in EXTENTS format; this goes\n\t\t * for freed inodes as well.\n\t\t */\n\t\tswitch (dip->di_aformat) {\n\t\tcase 0:\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn __this_address;\n\t\t}\n\t\tif (dip->di_anextents)\n\t\t\treturn __this_address;\n\t}\n\n\t/* only version 3 or greater inodes are extensively verified here */\n\tif (dip->di_version < 3)\n\t\treturn NULL;", "project": "linux", "hash": 153900552675857552774292769547701110591, "size": 146, "commit_id": "b42db0860e13067fcc7cbfba3966c9e652668bbc", "message": "xfs: enhance dinode verifier\n\nAdd several more validations to xfs_dinode_verify:\n\n- For LOCAL data fork formats, di_nextents must be 0.\n- For LOCAL attr fork formats, di_anextents must be 0.\n- For inodes with no attr fork offset,\n  - format must be XFS_DINODE_FMT_EXTENTS if set at all\n  - di_anextents must be 0.\n\nThanks to dchinner for pointing out a couple related checks I had\nforgotten to add.\n\nSigned-off-by: Eric Sandeen <sandeen@redhat.com>\nBugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=199377\nReviewed-by: Darrick J. Wong <darrick.wong@oracle.com>\nSigned-off-by: Darrick J. Wong <darrick.wong@oracle.com>", "target": 0, "dataset": "other", "idx": 311147}
{"func": "unserialize_uep(bufinfo_T *bi, int *error, char_u *file_name)\n{\n    int\t\ti;\n    u_entry_T\t*uep;\n    char_u\t**array;\n    char_u\t*line;\n    int\t\tline_len;\n\n    uep = (u_entry_T *)U_ALLOC_LINE(sizeof(u_entry_T));\n    if (uep == NULL)\n    uep->ue_bot = undo_read_4c(bi);\n    uep->ue_lcount = undo_read_4c(bi);\n    uep->ue_size = undo_read_4c(bi);\n    if (uep->ue_size > 0)\n    {\n\tarray = (char_u **)U_ALLOC_LINE(sizeof(char_u *) * uep->ue_size);\n\tif (array == NULL)\n\t{\n\t    *error = TRUE;\n\t    return uep;\n\t}\n\tvim_memset(array, 0, sizeof(char_u *) * uep->ue_size);\n    }\n    else\n\tarray = NULL;\n    uep->ue_array = array;\n\n    for (i = 0; i < uep->ue_size; ++i)\n    {\n\tline_len = undo_read_4c(bi);", "project": "vim", "hash": 220930485853172651933814434689792902407, "size": 52, "commit_id": "0c8485f0e4931463c0f7986e1ea84a7d79f10c75", "message": "patch 8.0.0378: possible overflow when reading corrupted undo file\n\nProblem:    Another possible overflow when reading corrupted undo file.\nSolution:   Check if allocated size is not too big. (King)", "target": 1, "dataset": "other", "idx": 200929}
{"func": "unserialize_uep(bufinfo_T *bi, int *error, char_u *file_name)\n{\n    int\t\ti;\n    u_entry_T\t*uep;\n    char_u\t**array = NULL;\n    char_u\t*line;\n    int\t\tline_len;\n\n    uep = (u_entry_T *)U_ALLOC_LINE(sizeof(u_entry_T));\n    if (uep == NULL)\n    uep->ue_bot = undo_read_4c(bi);\n    uep->ue_lcount = undo_read_4c(bi);\n    uep->ue_size = undo_read_4c(bi);\n    if (uep->ue_size > 0)\n    {\n\tif (uep->ue_size < LONG_MAX / (int)sizeof(char_u *))\n\t    array = (char_u **)U_ALLOC_LINE(sizeof(char_u *) * uep->ue_size);\n\tif (array == NULL)\n\t{\n\t    *error = TRUE;\n\t    return uep;\n\t}\n\tvim_memset(array, 0, sizeof(char_u *) * uep->ue_size);\n    }\n    uep->ue_array = array;\n\n    for (i = 0; i < uep->ue_size; ++i)\n    {\n\tline_len = undo_read_4c(bi);", "project": "vim", "hash": 113002822976245590429503739327047580863, "size": 51, "commit_id": "0c8485f0e4931463c0f7986e1ea84a7d79f10c75", "message": "patch 8.0.0378: possible overflow when reading corrupted undo file\n\nProblem:    Another possible overflow when reading corrupted undo file.\nSolution:   Check if allocated size is not too big. (King)", "target": 0, "dataset": "other", "idx": 313017}
{"func": "      if (aptr + rr_len > abuf + alen)\n        {\n          status = ARES_EBADRESP;\n          break;\n        }\n      /* RR must contain at least 7 bytes = 2 x int16 + 3 x name */\n      if (rr_len < 7)\n        {\n          status = ARES_EBADRESP;\n          break;\n        }\n\n      /* Check if we are really looking at a NAPTR record */\n      if (rr_class == C_IN && rr_type == T_NAPTR)\n        {\n          /* parse the NAPTR record itself */\n\n          /* Allocate storage for this NAPTR answer appending it to the list */\n          naptr_curr = ares_malloc_data(ARES_DATATYPE_NAPTR_REPLY);\n          if (!naptr_curr)\n            {", "project": "c-ares", "hash": 92817310620354776498914521339641206707, "size": 145, "commit_id": "18ea99693d63f957ecb670045adbd2c1da8a4641", "message": "ares_parse_naptr_reply: make buffer length check more accurate\n\n9478908a490a6bf009ba58d81de8c1d06d50a117 introduced a length check\nfor records parsed by `ares_parse_naptr_reply()`. However, that\nfunction is designed to parse replies which also contain non-NAPTR\nrecords; for A records, the `rr_len > 7` check will fail as there\nare only 4 bytes of payload.\nIn particular, parsing ANY replies for NAPTR records was broken\nby that patch.\n\nFix that by moving the check into the case in which it is already\nknown that the record is a NAPTR record.", "target": 1, "dataset": "other", "idx": 201328}
{"func": "      aptr += RRFIXEDSZ;\n      if (aptr + rr_len > abuf + alen)\n        {\n          status = ARES_EBADRESP;\n          break;\n        }\n\n      /* Check if we are really looking at a NAPTR record */\n      if (rr_class == C_IN && rr_type == T_NAPTR)\n        {\n          /* parse the NAPTR record itself */\n\n          /* RR must contain at least 7 bytes = 2 x int16 + 3 x name */\n          if (rr_len < 7)\n            {\n              status = ARES_EBADRESP;\n              break;\n            }\n\n          /* Allocate storage for this NAPTR answer appending it to the list */\n          naptr_curr = ares_malloc_data(ARES_DATATYPE_NAPTR_REPLY);\n          if (!naptr_curr)\n            {", "project": "c-ares", "hash": 198334055271059055129109951088542752271, "size": 146, "commit_id": "18ea99693d63f957ecb670045adbd2c1da8a4641", "message": "ares_parse_naptr_reply: make buffer length check more accurate\n\n9478908a490a6bf009ba58d81de8c1d06d50a117 introduced a length check\nfor records parsed by `ares_parse_naptr_reply()`. However, that\nfunction is designed to parse replies which also contain non-NAPTR\nrecords; for A records, the `rr_len > 7` check will fail as there\nare only 4 bytes of payload.\nIn particular, parsing ANY replies for NAPTR records was broken\nby that patch.\n\nFix that by moving the check into the case in which it is already\nknown that the record is a NAPTR record.", "target": 0, "dataset": "other", "idx": 316665}
{"func": "          theta.p=atan2(box_p[1].y-center.y,box_p[1].x-center.x);\n          theta.q=atan2(box_p[2].y-center.y,box_p[2].x-center.x);\n          if (theta.p < theta.q)\n            theta.p+=2.0*MagickPI;\n          arc_segments=(size_t) CastDoubleToLong(ceil((double) ((theta.p-\n            theta.q)/(2.0*sqrt((double) (1.0/mid))))));\n          CheckPathExtent(arc_segments+MaxStrokePad,MaxStrokePad);\n          stroke_p[p++]=box_p[1];\n          for (j=1; j < (ssize_t) arc_segments; j++)\n          {\n            delta_theta=(double) (j*(theta.q-theta.p)/arc_segments);", "project": "ImageMagick6", "hash": 300731361585622607899630210795115991097, "size": 503, "commit_id": "9a94877f7823b0b8a41d50638dd105229d91fa89", "message": "https://github.com/ImageMagick/ImageMagick/issues/3339", "target": 1, "dataset": "other", "idx": 201329}
{"func": "          theta.p=atan2(box_p[1].y-center.y,box_p[1].x-center.x);\n          theta.q=atan2(box_p[2].y-center.y,box_p[2].x-center.x);\n          if (theta.p < theta.q)\n            theta.p+=2.0*MagickPI;\n          arc_segments=(size_t) CastDoubleToLong(ceil((double) ((theta.p-\n            theta.q)/(2.0*sqrt((double) (PerceptibleReciprocal(mid)))))));\n          CheckPathExtent(arc_segments+MaxStrokePad,MaxStrokePad);\n          stroke_p[p++]=box_p[1];\n          for (j=1; j < (ssize_t) arc_segments; j++)\n          {\n            delta_theta=(double) (j*(theta.q-theta.p)/arc_segments);", "project": "ImageMagick6", "hash": 23882718594748933673379097067447163572, "size": 503, "commit_id": "9a94877f7823b0b8a41d50638dd105229d91fa89", "message": "https://github.com/ImageMagick/ImageMagick/issues/3339", "target": 0, "dataset": "other", "idx": 316687}
{"func": "rpmRC hdrblobInit(const void *uh, size_t uc,\n\t\trpmTagVal regionTag, int exact_size,\n\t\tstruct hdrblob_s *blob, char **emsg)\n{\n    rpmRC rc = RPMRC_FAIL;\n\n    memset(blob, 0, sizeof(*blob));\n    blob->ei = (int32_t *) uh; /* discards const */\n    blob->il = ntohl(blob->ei[0]);\n    blob->dl = ntohl(blob->ei[1]);\n    blob->pe = (entryInfo) &(blob->ei[2]);\n    blob->pvlen = sizeof(blob->il) + sizeof(blob->dl) +\n\t\t  (blob->il * sizeof(*blob->pe)) + blob->dl;\n    blob->dataStart = (uint8_t *) (blob->pe + blob->il);\n    blob->dataEnd = blob->dataStart + blob->dl;", "project": "rpm", "hash": 221870817089961992144312998012828797980, "size": 35, "commit_id": "8f4b3c3cab8922a2022b9e47c71f1ecf906077ef", "message": "hdrblobInit() needs bounds checks too\n\nUsers can pass untrusted data to hdrblobInit() and it must be robust\nagainst this.", "target": 1, "dataset": "other", "idx": 201363}
{"func": "rpmRC hdrblobInit(const void *uh, size_t uc,\n\t\trpmTagVal regionTag, int exact_size,\n\t\tstruct hdrblob_s *blob, char **emsg)\n{\n    rpmRC rc = RPMRC_FAIL;\n    memset(blob, 0, sizeof(*blob));\n    if (uc && uc < 8) {\n\trasprintf(emsg, _(\"hdr length: BAD\"));\n\tgoto exit;\n    }\n\n    blob->ei = (int32_t *) uh; /* discards const */\n    blob->il = ntohl((uint32_t)(blob->ei[0]));\n    blob->dl = ntohl((uint32_t)(blob->ei[1]));\n    if (hdrblobVerifyLengths(regionTag, blob->il, blob->dl, emsg) != RPMRC_OK)\n\tgoto exit;\n\n    blob->pe = (entryInfo) &(blob->ei[2]);\n    blob->pvlen = sizeof(blob->il) + sizeof(blob->dl) +\n\t\t  (blob->il * sizeof(*blob->pe)) + blob->dl;\n    blob->dataStart = (uint8_t *) (blob->pe + blob->il);\n    blob->dataEnd = blob->dataStart + blob->dl;", "project": "rpm", "hash": 315069287843193856684881977313831062424, "size": 42, "commit_id": "8f4b3c3cab8922a2022b9e47c71f1ecf906077ef", "message": "hdrblobInit() needs bounds checks too\n\nUsers can pass untrusted data to hdrblobInit() and it must be robust\nagainst this.", "target": 0, "dataset": "other", "idx": 318167}
{"func": "static x3f_huffnode_t *new_node(x3f_hufftree_t *tree)\n{\n  x3f_huffnode_t *t = &tree->nodes[tree->free_node_index];\n\n  t->branch[0] = NULL;\n  t->branch[1] = NULL;\n  t->leaf = UNDEFINED_LEAF;", "project": "LibRaw", "hash": 123416413605001745719792443480718508652, "size": 12, "commit_id": "11c4db253ef2c9bb44247b578f5caa57c66a1eeb", "message": "X3F: check huffman tree size", "target": 1, "dataset": "other", "idx": 201415}
{"func": "static x3f_huffnode_t *new_node(x3f_hufftree_t *tree)\n{\n\tif (tree->free_node_index >= tree->total_node_index)\n\t\tthrow LIBRAW_EXCEPTION_IO_CORRUPT;\n  x3f_huffnode_t *t = &tree->nodes[tree->free_node_index];\n\n  t->branch[0] = NULL;\n  t->branch[1] = NULL;\n  t->leaf = UNDEFINED_LEAF;", "project": "LibRaw", "hash": 271692711028990488717832420931979417790, "size": 14, "commit_id": "11c4db253ef2c9bb44247b578f5caa57c66a1eeb", "message": "X3F: check huffman tree size", "target": 0, "dataset": "other", "idx": 319168}
{"func": "\tpr_debug(\"sock=%p\\n\", sock);\n\n\tif ((sock->type != SOCK_SEQPACKET) && (sock->type != SOCK_RAW))\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tif (sock->type == SOCK_RAW)\n\t\tsock->ops = &rawsock_raw_ops;\n\telse\n\t\tsock->ops = &rawsock_ops;\n\n\tsk = sk_alloc(net, PF_NFC, GFP_ATOMIC, nfc_proto->proto, kern);\n\tif (!sk)\n\t\treturn -ENOMEM;\n", "project": "linux", "hash": 76158006236225806046621099350006167099, "size": 32, "commit_id": "26896f01467a28651f7a536143fe5ac8449d4041", "message": "net/nfc/rawsock.c: add CAP_NET_RAW check.\n\nWhen creating a raw AF_NFC socket, CAP_NET_RAW needs to be checked first.\n\nSigned-off-by: Qingyu Li <ieatmuttonchuan@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 201423}
{"func": "\tpr_debug(\"sock=%p\\n\", sock);\n\n\tif ((sock->type != SOCK_SEQPACKET) && (sock->type != SOCK_RAW))\n\t\treturn -ESOCKTNOSUPPORT;\n\n\tif (sock->type == SOCK_RAW) {\n\t\tif (!capable(CAP_NET_RAW))\n\t\t\treturn -EPERM;\n\t\tsock->ops = &rawsock_raw_ops;\n\t} else {\n\t\tsock->ops = &rawsock_ops;\n\t}\n\n\tsk = sk_alloc(net, PF_NFC, GFP_ATOMIC, nfc_proto->proto, kern);\n\tif (!sk)\n\t\treturn -ENOMEM;\n", "project": "linux", "hash": 284654099723487976419252796131361945863, "size": 35, "commit_id": "26896f01467a28651f7a536143fe5ac8449d4041", "message": "net/nfc/rawsock.c: add CAP_NET_RAW check.\n\nWhen creating a raw AF_NFC socket, CAP_NET_RAW needs to be checked first.\n\nSigned-off-by: Qingyu Li <ieatmuttonchuan@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 319215}
{"func": "  if (jh->sraw)\n  {\n    FORC(4) jh->huff[2 + c] = jh->huff[1];\n    FORC(jh->sraw) jh->huff[1 + c] = jh->huff[0];\n  }\n  jh->row = (ushort *)calloc(jh->wide * jh->clrs, 4);\n  merror(jh->row, \"ljpeg_start()\");\n  return zero_after_ff = 1;\n}", "project": "LibRaw", "hash": 17018590345339261485668035454434752810, "size": 72, "commit_id": "a6937d4046a7c4742b683a04c8564605fd9be4fb", "message": "more room for ljpeg row", "target": 1, "dataset": "other", "idx": 201452}
{"func": "  if (jh->sraw)\n  {\n    FORC(4) jh->huff[2 + c] = jh->huff[1];\n    FORC(jh->sraw) jh->huff[1 + c] = jh->huff[0];\n  }\n  jh->row = (ushort *)calloc(jh->wide * jh->clrs, 16);\n  merror(jh->row, \"ljpeg_start()\");\n  return zero_after_ff = 1;\n}", "project": "LibRaw", "hash": 172933929170343697197974643398947672843, "size": 72, "commit_id": "a6937d4046a7c4742b683a04c8564605fd9be4fb", "message": "more room for ljpeg row", "target": 0, "dataset": "other", "idx": 319453}
{"func": "\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {\n\t\t\tif (extent == HPAGE_PMD_SIZE) {\n\t\t\t\tbool moved;\n\t\t\t\t/* See comment in move_ptes() */\n\t\t\t\tif (need_rmap_locks)\n\t\t\t\t\ttake_rmap_locks(vma);", "project": "linux", "hash": 322406925399934688200484412754502215496, "size": 77, "commit_id": "5bfea2d9b17f1034a68147a8b03b9789af5700f9", "message": "mm: Fix mremap not considering huge pmd devmap\n\nThe original code in mm/mremap.c checks huge pmd by:\n\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {\n\nHowever, a DAX mapped nvdimm is mapped as huge page (by default) but it\nis not transparent huge page (_PAGE_PSE | PAGE_DEVMAP).  This commit\nchanges the condition to include the case.\n\nThis addresses CVE-2020-10757.\n\nFixes: 5c7fb56e5e3f (\"mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd\")\nCc: <stable@vger.kernel.org>\nReported-by: Fan Yang <Fan_Yang@sjtu.edu.cn>\nSigned-off-by: Fan Yang <Fan_Yang@sjtu.edu.cn>\nTested-by: Fan Yang <Fan_Yang@sjtu.edu.cn>\nTested-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nAcked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 201788}
{"func": "\t\tif (!old_pmd)\n\t\t\tcontinue;\n\t\tnew_pmd = alloc_new_pmd(vma->vm_mm, vma, new_addr);\n\t\tif (!new_pmd)\n\t\t\tbreak;\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd) || pmd_devmap(*old_pmd)) {\n\t\t\tif (extent == HPAGE_PMD_SIZE) {\n\t\t\t\tbool moved;\n\t\t\t\t/* See comment in move_ptes() */\n\t\t\t\tif (need_rmap_locks)\n\t\t\t\t\ttake_rmap_locks(vma);", "project": "linux", "hash": 70690057593979156175552450811161004469, "size": 77, "commit_id": "5bfea2d9b17f1034a68147a8b03b9789af5700f9", "message": "mm: Fix mremap not considering huge pmd devmap\n\nThe original code in mm/mremap.c checks huge pmd by:\n\n\t\tif (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {\n\nHowever, a DAX mapped nvdimm is mapped as huge page (by default) but it\nis not transparent huge page (_PAGE_PSE | PAGE_DEVMAP).  This commit\nchanges the condition to include the case.\n\nThis addresses CVE-2020-10757.\n\nFixes: 5c7fb56e5e3f (\"mm, dax: dax-pmd vs thp-pmd vs hugetlbfs-pmd\")\nCc: <stable@vger.kernel.org>\nReported-by: Fan Yang <Fan_Yang@sjtu.edu.cn>\nSigned-off-by: Fan Yang <Fan_Yang@sjtu.edu.cn>\nTested-by: Fan Yang <Fan_Yang@sjtu.edu.cn>\nTested-by: Dan Williams <dan.j.williams@intel.com>\nReviewed-by: Dan Williams <dan.j.williams@intel.com>\nAcked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 321574}
{"func": "Sfdouble_t sh_strnum(Shell_t *shp, const char *str, char **ptr, int mode) {\n    Sfdouble_t d;\n    char *last;\n\n    if (*str == 0) {\n        if (ptr) *ptr = (char *)str;\n        return 0;\n    }\n    errno = 0;\n    d = number(str, &last, shp->inarith ? 0 : 10, NULL);\n    if (*last) {\n        if (*last != '.' || last[1] != '.') {\n            d = strval(shp, str, &last, arith, mode);\n            Varsubscript = true;\n        }\n        if (!ptr && *last && mode > 0) errormsg(SH_DICT, ERROR_exit(1), e_lexbadchar, *last, str);\n    } else if (!d && *str == '-') {\n        d = -0.0;\n    }\n    if (ptr) *ptr = last;\n    return d;\n}", "project": "ast", "hash": 309916825304971174344136967868648417012, "size": 22, "commit_id": "c7de8b641266bac7c77942239ac659edfee9ecd2", "message": "Harden env var imports", "target": 1, "dataset": "other", "idx": 201806}
{"func": "Sfdouble_t sh_strnum(Shell_t *shp, const char *str, char **ptr, int mode) {\n    Sfdouble_t d;\n    char *last;\n\n    if (*str == 0) {\n        d = 0.0;\n        last = (char *)str;\n    } else {\n        d = number(str, &last, shp->inarith ? 0 : 10, NULL);\n        if (*last && !shp->inarith && sh_isstate(shp, SH_INIT)) {\n            // This call is to handle \"base#value\" literals if we're importing untrusted env vars.\n            d = number(str, &last, 0, NULL);\n        }\n        if (*last) {\n            if (sh_isstate(shp, SH_INIT)) {\n                // Initializing means importing untrusted env vars. Since the string does not appear\n                // to be a recognized numeric literal give up. We can't safely call strval() since\n                // that allows arbitrary expressions which would create a security vulnerability.\n                d = 0.0;\n            } else {\n                if (*last != '.' || last[1] != '.') {\n                    d = strval(shp, str, &last, arith, mode);\n                    Varsubscript = true;\n                }\n                if (!ptr && *last && mode > 0) {\n                    errormsg(SH_DICT, ERROR_exit(1), e_lexbadchar, *last, str);\n                }\n            }\n        } else if (d == 0.0 && *str == '-') {\n            d = -0.0;\n        }\n    }\n    if (ptr) *ptr = last;\n    return d;\n}", "project": "ast", "hash": 84029568395809826570711187865149217968, "size": 35, "commit_id": "c7de8b641266bac7c77942239ac659edfee9ecd2", "message": "Harden env var imports", "target": 0, "dataset": "other", "idx": 321871}
{"func": "int pidfile_write(pid_t pid)\n{\n    FILE *f;\n\n    if (g_pidfile) {\n        f = fopen(g_pidfile, \"w+\");\n    } else if (pidfilefd >= 0) {\n        f = fdopen(pidfilefd, \"w\");\n        if (f) {\n            g_pidfile = fd_to_filename(pidfilefd);\n            if (!g_pidfile)\n                goto error;\n        }\n    } else {\n        return 0;\n    }\n\n    if (!f) {\n        logprintf(STDERR_FILENO, \"Could not open pidfile %s : %s\\n\",\n                  g_pidfile, strerror(errno));\n        goto error;\n    }\n\n    if (fprintf(f, \"%d\", pid) < 0) {\n        logprintf(STDERR_FILENO, \"Could not write to pidfile : %s\\n\",\n                  strerror(errno));\n        goto error;\n    }\n\n    fclose(f);\n\n    return 0;\n\nerror:\n    if (f)\n        fclose(f);\n    return -1;\n}", "project": "swtpm", "hash": 34254869638229224588717382596225694876, "size": 38, "commit_id": "634b6294000fb785b9f12e13b852c18a0888b01e", "message": "swtpm: Switch to open() from fopen() for the pidfile (CVE-2020-28407)\n\nThis patch addresses CVE-2020-28407.\n\nUse the open() call rather than the fopen() call when creating a pidfile.\nAlso prevent us from following symbolic links when opening the pidfile for\nwriting.\n\nSigned-off-by: Stefan Berger <stefanb@linux.ibm.com>", "target": 1, "dataset": "other", "idx": 201812}
{"func": "int pidfile_write(pid_t pid)\n{\n    int fd;\n    char buffer[32];\n    ssize_t nwritten;\n\n    if (g_pidfile) {\n        fd = open(g_pidfile, O_WRONLY|O_CREAT|O_TRUNC|O_NOFOLLOW,\n                  S_IRUSR|S_IWUSR|S_IRGRP|S_IROTH);\n    } else if (pidfilefd >= 0) {\n        fd = pidfilefd;\n        g_pidfile = fd_to_filename(pidfilefd);\n        if (!g_pidfile)\n            goto error;\n    } else {\n        return 0;\n    }\n\n    if (fd < 0) {\n        logprintf(STDERR_FILENO, \"Could not open pidfile %s : %s\\n\",\n                  g_pidfile, strerror(errno));\n        goto error;\n    }\n\n    if (snprintf(buffer, sizeof(buffer), \"%d\", pid) >= (int)sizeof(buffer)) {\n        logprintf(STDERR_FILENO, \"Could not write pid to buffer\\n\");\n        goto error_close;\n    }\n\n    nwritten = write_full(fd, buffer, strlen(buffer));\n    if (nwritten < 0 || nwritten != (ssize_t)strlen(buffer)) {\n        logprintf(STDERR_FILENO, \"Could not write to pidfile : %s\\n\",\n                  strerror(errno));\n        goto error_close;\n    }\n\n    close(fd);\n\n    return 0;\n\nerror_close:\n    if (fd != pidfilefd)\n        close(fd);\n\nerror:\n    return -1;\n}", "project": "swtpm", "hash": 20375191730023779922313833710750581834, "size": 47, "commit_id": "634b6294000fb785b9f12e13b852c18a0888b01e", "message": "swtpm: Switch to open() from fopen() for the pidfile (CVE-2020-28407)\n\nThis patch addresses CVE-2020-28407.\n\nUse the open() call rather than the fopen() call when creating a pidfile.\nAlso prevent us from following symbolic links when opening the pidfile for\nwriting.\n\nSigned-off-by: Stefan Berger <stefanb@linux.ibm.com>", "target": 0, "dataset": "other", "idx": 322084}
{"func": "extern int x11_set_xauth(char *xauthority, char *cookie,\n\t\t\t char *host, uint16_t display)\n{\n\tint i=0, status;\n\tchar *result;\n\tchar **xauth_argv;\n\n\txauth_argv = xmalloc(sizeof(char *) * 10);\n\txauth_argv[i++] = xstrdup(\"xauth\");\n\txauth_argv[i++] = xstrdup(\"-v\");\n\txauth_argv[i++] = xstrdup(\"-f\");\n\txauth_argv[i++] = xstrdup(xauthority);\n\txauth_argv[i++] = xstrdup(\"add\");\n\txauth_argv[i++] = xstrdup_printf(\"%s/unix:%u\", host, display);\n\txauth_argv[i++] = xstrdup(\"MIT-MAGIC-COOKIE-1\");\n\txauth_argv[i++] = xstrdup(cookie);\n\txauth_argv[i++] = NULL;\n\txassert(i < 10);\n\n\tresult = run_command(\"xauth\", XAUTH_PATH, xauth_argv, 10000, 0,\n\t\t\t     &status);\n\n\tfree_command_argv(xauth_argv);\n\n\tdebug2(\"%s: result from xauth: %s\", __func__, result);\n\txfree(result);\n\n\treturn status;\n}", "project": "slurm", "hash": 84215474414950344258573648975773029635, "size": 29, "commit_id": "07309deb45c33e735e191faf9dd31cca1054a15c", "message": "X11 forwarding - avoid unsafe use of magic cookie as arg to xauth command.\n\nMagic cookie can leak through /proc this way. There is a race here between\nthis usually short-lived xauth command running and an attacker scraping\nthe value from /proc. This can be exacerbated through use of\nX11Parameters=home_xauthority on a cluster with a shared home directory\nunder heavy load.\n\nCVE-2020-27746.", "target": 1, "dataset": "other", "idx": 201848}
{"func": "extern int x11_delete_xauth(char *xauthority, char *host, uint16_t display)\n{\n\tint i=0, status;\n\tchar *result;\n\tchar **xauth_argv;\n\n\txauth_argv = xmalloc(sizeof(char *) * 10);\n\txauth_argv[i++] = xstrdup(\"xauth\");\n\txauth_argv[i++] = xstrdup(\"-v\");\n\txauth_argv[i++] = xstrdup(\"-f\");\n\txauth_argv[i++] = xstrdup(xauthority);\n\txauth_argv[i++] = xstrdup(\"remove\");\n\txauth_argv[i++] = xstrdup_printf(\"%s/unix:%u\", host, display);\n\txauth_argv[i++] = NULL;\n\txassert(i < 10);\n\n\tresult = run_command(\"xauth\", XAUTH_PATH, xauth_argv, 10000, 0,\n\t\t\t     &status);\n\n\tfree_command_argv(xauth_argv);\n\n\tdebug2(\"%s: result from xauth: %s\", __func__, result);\n\txfree(result);\n\n\treturn status;\n}", "project": "slurm", "hash": 128496414230349176263839705694684206156, "size": 26, "commit_id": "07309deb45c33e735e191faf9dd31cca1054a15c", "message": "X11 forwarding - avoid unsafe use of magic cookie as arg to xauth command.\n\nMagic cookie can leak through /proc this way. There is a race here between\nthis usually short-lived xauth command running and an attacker scraping\nthe value from /proc. This can be exacerbated through use of\nX11Parameters=home_xauthority on a cluster with a shared home directory\nunder heavy load.\n\nCVE-2020-27746.", "target": 0, "dataset": "other", "idx": 323088}
{"func": "#endif\n\n\twrite = madvise_need_mmap_write(behavior);\n\tif (write) {\n\t\tif (down_write_killable(&current->mm->mmap_sem))\n\t\t\treturn -EINTR;\n\t} else {\n\t\tdown_read(&current->mm->mmap_sem);\n\t}\n\n\t/*", "project": "linux", "hash": 112062203693169171312729510010872998987, "size": 97, "commit_id": "bc0c4d1e176eeb614dc8734fc3ace34292771f11", "message": "mm: check that mm is still valid in madvise()\n\nIORING_OP_MADVISE can end up basically doing mprotect() on the VM of\nanother process, which means that it can race with our crazy core dump\nhandling which accesses the VM state without holding the mmap_sem\n(because it incorrectly thinks that it is the final user).\n\nThis is clearly a core dumping problem, but we've never fixed it the\nright way, and instead have the notion of \"check that the mm is still\nok\" using mmget_still_valid() after getting the mmap_sem for writing in\nany situation where we're not the original VM thread.\n\nSee commit 04f5866e41fb (\"coredump: fix race condition between\nmmget_not_zero()/get_task_mm() and core dumping\") for more background on\nthis whole mmget_still_valid() thing.  You might want to have a barf bag\nhandy when you do.\n\nWe're discussing just fixing this properly in the only remaining core\ndumping routines.  But even if we do that, let's make do_madvise() do\nthe right thing, and then when we fix core dumping, we can remove all\nthese mmget_still_valid() checks.\n\nReported-and-tested-by: Jann Horn <jannh@google.com>\nFixes: c1ca757bd6f4 (\"io_uring: add IORING_OP_MADVISE\")\nAcked-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 201869}
{"func": "\n\twrite = madvise_need_mmap_write(behavior);\n\tif (write) {\n\t\tif (down_write_killable(&current->mm->mmap_sem))\n\t\t\treturn -EINTR;\n\n\t\t/*\n\t\t * We may have stolen the mm from another process\n\t\t * that is undergoing core dumping.\n\t\t *\n\t\t * Right now that's io_ring, in the future it may\n\t\t * be remote process management and not \"current\"\n\t\t * at all.\n\t\t *\n\t\t * We need to fix core dumping to not do this,\n\t\t * but for now we have the mmget_still_valid()\n\t\t * model.\n\t\t */\n\t\tif (!mmget_still_valid(current->mm)) {\n\t\t\tup_write(&current->mm->mmap_sem);\n\t\t\treturn -EINTR;\n\t\t}\n\t} else {\n\t\tdown_read(&current->mm->mmap_sem);\n\t}\n\n\t/*", "project": "linux", "hash": 262379304761782825761220862266520319644, "size": 114, "commit_id": "bc0c4d1e176eeb614dc8734fc3ace34292771f11", "message": "mm: check that mm is still valid in madvise()\n\nIORING_OP_MADVISE can end up basically doing mprotect() on the VM of\nanother process, which means that it can race with our crazy core dump\nhandling which accesses the VM state without holding the mmap_sem\n(because it incorrectly thinks that it is the final user).\n\nThis is clearly a core dumping problem, but we've never fixed it the\nright way, and instead have the notion of \"check that the mm is still\nok\" using mmget_still_valid() after getting the mmap_sem for writing in\nany situation where we're not the original VM thread.\n\nSee commit 04f5866e41fb (\"coredump: fix race condition between\nmmget_not_zero()/get_task_mm() and core dumping\") for more background on\nthis whole mmget_still_valid() thing.  You might want to have a barf bag\nhandy when you do.\n\nWe're discussing just fixing this properly in the only remaining core\ndumping routines.  But even if we do that, let's make do_madvise() do\nthe right thing, and then when we fix core dumping, we can remove all\nthese mmget_still_valid() checks.\n\nReported-and-tested-by: Jann Horn <jannh@google.com>\nFixes: c1ca757bd6f4 (\"io_uring: add IORING_OP_MADVISE\")\nAcked-by: Jens Axboe <axboe@kernel.dk>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 323578}
{"func": "\t\tif (p >= end) {\n\t\t\tas->length = p - as->s;\n\t\t\tas->s[as->length] = '\\0';\n\t\t\t/* Re-allocate buffer for MBS. */\n\t\t\tif (archive_string_ensure(as,\n\t\t\t    as->length + len * 2 + 1) == NULL)\n\t\t\t\treturn (-1);\n\t\t\tp = as->s + as->length;\n\t\t\tend = as->s + as->buffer_length - MB_CUR_MAX -1;\n\t\t}\n#if HAVE_WCRTOMB", "project": "libarchive", "hash": 267706670982642914719821262234524237398, "size": 63, "commit_id": "4f085eea879e2be745f4d9bf57e8513ae48157f4", "message": "Fix a possible heap-buffer-overflow in archive_string_append_from_wcs()\n\nWhen we grow the archive_string buffer, we have to make sure it fits\nat least one maximum-sized multibyte character in the current locale\nand the null character.\n\nFixes #1298", "target": 1, "dataset": "other", "idx": 201874}
{"func": "\t\tif (p >= end) {\n\t\t\tas->length = p - as->s;\n\t\t\tas->s[as->length] = '\\0';\n\t\t\t/* Re-allocate buffer for MBS. */\n\t\t\tif (archive_string_ensure(as,\n\t\t\t    as->length + max(len * 2,\n\t\t\t    (size_t)MB_CUR_MAX) + 1) == NULL)\n\t\t\t\treturn (-1);\n\t\t\tp = as->s + as->length;\n\t\t\tend = as->s + as->buffer_length - MB_CUR_MAX -1;\n\t\t}\n#if HAVE_WCRTOMB", "project": "libarchive", "hash": 85036731968145354958341451146284390103, "size": 64, "commit_id": "4f085eea879e2be745f4d9bf57e8513ae48157f4", "message": "Fix a possible heap-buffer-overflow in archive_string_append_from_wcs()\n\nWhen we grow the archive_string buffer, we have to make sure it fits\nat least one maximum-sized multibyte character in the current locale\nand the null character.\n\nFixes #1298", "target": 0, "dataset": "other", "idx": 325834}
{"func": "#ifdef TTY_SOFT_SAK\n\ttty_hangup(tty);\n#else\n\tstruct task_struct *g, *p;\n\tstruct pid *session;\n\tint\t\ti;\n\n\tif (!tty)\n\t\treturn;\n\tsession = tty->session;\n\n\ttty_ldisc_flush(tty);\n\n\ttty_driver_flush_buffer(tty);\n\n\t\t\t\t   task_pid_nr(p), p->comm, i - 1);\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t}\n\t\ttask_unlock(p);\n\t} while_each_thread(g, p);\n\tread_unlock(&tasklist_lock);\n#endif\n}", "project": "linux", "hash": 327113028686969306396725554617278047338, "size": 45, "commit_id": "c8bcd9c5be24fb9e6132e97da5a35e55a83e36b9", "message": "tty: Fix ->session locking\n\nCurrently, locking of ->session is very inconsistent; most places\nprotect it using the legacy tty mutex, but disassociate_ctty(),\n__do_SAK(), tiocspgrp() and tiocgsid() don't.\nTwo of the writers hold the ctrl_lock (because they already need it for\n->pgrp), but __proc_set_tty() doesn't do that yet.\n\nOn a PREEMPT=y system, an unprivileged user can theoretically abuse\nthis broken locking to read 4 bytes of freed memory via TIOCGSID if\ntiocgsid() is preempted long enough at the right point. (Other things\nmight also go wrong, especially if root-only ioctls are involved; I'm\nnot sure about that.)\n\nChange the locking on ->session such that:\n\n - tty_lock() is held by all writers: By making disassociate_ctty()\n   hold it. This should be fine because the same lock can already be\n   taken through the call to tty_vhangup_session().\n   The tricky part is that we need to shorten the area covered by\n   siglock to be able to take tty_lock() without ugly retry logic; as\n   far as I can tell, this should be fine, since nothing in the\n   signal_struct is touched in the `if (tty)` branch.\n - ctrl_lock is held by all writers: By changing __proc_set_tty() to\n   hold the lock a little longer.\n - All readers that aren't holding tty_lock() hold ctrl_lock: By\n   adding locking to tiocgsid() and __do_SAK(), and expanding the area\n   covered by ctrl_lock in tiocspgrp().\n\nCc: stable@kernel.org\nSigned-off-by: Jann Horn <jannh@google.com>\nReviewed-by: Jiri Slaby <jirislaby@kernel.org>\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 1, "dataset": "other", "idx": 201881}
{"func": "\ttty_hangup(tty);\n#else\n\tstruct task_struct *g, *p;\n\tstruct pid *session;\n\tint\t\ti;\n\tunsigned long flags;\n\n\tif (!tty)\n\t\treturn;\n\n\tspin_lock_irqsave(&tty->ctrl_lock, flags);\n\tsession = get_pid(tty->session);\n\tspin_unlock_irqrestore(&tty->ctrl_lock, flags);\n\n\ttty_ldisc_flush(tty);\n\n\ttty_driver_flush_buffer(tty);\n\n\t\t\tgroup_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n\t\t}\n\t\ttask_unlock(p);\n\t} while_each_thread(g, p);\n\tread_unlock(&tasklist_lock);\n\tput_pid(session);\n#endif\n}", "project": "linux", "hash": 232074957705892957788904660986151519370, "size": 50, "commit_id": "c8bcd9c5be24fb9e6132e97da5a35e55a83e36b9", "message": "tty: Fix ->session locking\n\nCurrently, locking of ->session is very inconsistent; most places\nprotect it using the legacy tty mutex, but disassociate_ctty(),\n__do_SAK(), tiocspgrp() and tiocgsid() don't.\nTwo of the writers hold the ctrl_lock (because they already need it for\n->pgrp), but __proc_set_tty() doesn't do that yet.\n\nOn a PREEMPT=y system, an unprivileged user can theoretically abuse\nthis broken locking to read 4 bytes of freed memory via TIOCGSID if\ntiocgsid() is preempted long enough at the right point. (Other things\nmight also go wrong, especially if root-only ioctls are involved; I'm\nnot sure about that.)\n\nChange the locking on ->session such that:\n\n - tty_lock() is held by all writers: By making disassociate_ctty()\n   hold it. This should be fine because the same lock can already be\n   taken through the call to tty_vhangup_session().\n   The tricky part is that we need to shorten the area covered by\n   siglock to be able to take tty_lock() without ugly retry logic; as\n   far as I can tell, this should be fine, since nothing in the\n   signal_struct is touched in the `if (tty)` branch.\n - ctrl_lock is held by all writers: By changing __proc_set_tty() to\n   hold the lock a little longer.\n - All readers that aren't holding tty_lock() hold ctrl_lock: By\n   adding locking to tiocgsid() and __do_SAK(), and expanding the area\n   covered by ctrl_lock in tiocspgrp().\n\nCc: stable@kernel.org\nSigned-off-by: Jann Horn <jannh@google.com>\nReviewed-by: Jiri Slaby <jirislaby@kernel.org>\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 0, "dataset": "other", "idx": 326074}
{"func": "  char* ds;\n  int rc;\n\n  ds = d;\n\n  for (si = s; si < se; /* empty */) {\n    st = si;\n    c = uv__utf8_decode1(&si, se);\n\n    if (c != '.')\n      if (c != 0x3002)  /* \u3002 */\n        if (c != 0xFF0E)  /* \uff0e */\n          if (c != 0xFF61)  /* \uff61 */", "project": "libuv", "hash": 330726136045724320865601654107082283716, "size": 42, "commit_id": "b7466e31e4bee160d82a68fca11b1f61d46debae", "message": "idna: fix OOB read in punycode decoder\n\nlibuv was vulnerable to out-of-bounds reads in the uv__idna_toascii()\nfunction which is used to convert strings to ASCII. This is called by\nthe DNS resolution function and can lead to information disclosures or\ncrashes.\n\nReported by Eric Sesterhenn in collaboration with Cure53 and ExpressVPN.\n\nReported-By: Eric Sesterhenn <eric.sesterhenn@x41-dsec.de>\nFixes: https://github.com/libuv/libuv/issues/3147\nPR-URL: https://github.com/libuv/libuv-private/pull/1\nRefs: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22918\nReviewed-By: Colin Ihrig <cjihrig@gmail.com>\nReviewed-By: Richard Lau <riclau@uk.ibm.com>", "target": 1, "dataset": "other", "idx": 201892}
{"func": "  char* ds;\n  int rc;\n\n  ds = d;\n\n  si = s;\n  while (si < se) {\n    st = si;\n    c = uv__utf8_decode1(&si, se);\n\n    if (c == -1u)\n      return UV_EINVAL;\n\n    if (c != '.')\n      if (c != 0x3002)  /* \u3002 */\n        if (c != 0xFF0E)  /* \uff0e */\n          if (c != 0xFF61)  /* \uff61 */", "project": "libuv", "hash": 143364374050282701014374410463146753253, "size": 46, "commit_id": "b7466e31e4bee160d82a68fca11b1f61d46debae", "message": "idna: fix OOB read in punycode decoder\n\nlibuv was vulnerable to out-of-bounds reads in the uv__idna_toascii()\nfunction which is used to convert strings to ASCII. This is called by\nthe DNS resolution function and can lead to information disclosures or\ncrashes.\n\nReported by Eric Sesterhenn in collaboration with Cure53 and ExpressVPN.\n\nReported-By: Eric Sesterhenn <eric.sesterhenn@x41-dsec.de>\nFixes: https://github.com/libuv/libuv/issues/3147\nPR-URL: https://github.com/libuv/libuv-private/pull/1\nRefs: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-22918\nReviewed-By: Colin Ihrig <cjihrig@gmail.com>\nReviewed-By: Richard Lau <riclau@uk.ibm.com>", "target": 0, "dataset": "other", "idx": 326145}
{"func": "\t}\n\n\terr = nf_tables_set_alloc_name(&ctx, set, name);\n\tkfree(name);\n\tif (err < 0)\n\t\tgoto err_set_alloc_name;\n\n\tif (nla[NFTA_SET_EXPR]) {\n\t\texpr = nft_set_elem_expr_alloc(&ctx, set, nla[NFTA_SET_EXPR]);\n\t\tif (IS_ERR(expr)) {\n\t\t\terr = PTR_ERR(expr);\n\t\t\tgoto err_set_alloc_name;\n\t\t}\n\t\tset->exprs[0] = expr;\n\t\tset->num_exprs++;\n\t} else if (nla[NFTA_SET_EXPRESSIONS]) {\n\t\tstruct nft_expr *expr;\n\t\tstruct nlattr *tmp;\n\t\tint left;\n\n\t\tif (!(flags & NFT_SET_EXPR)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_set_alloc_name;\n\t\t}\n\t\ti = 0;\n\t\tnla_for_each_nested(tmp, nla[NFTA_SET_EXPRESSIONS], left) {\n\t\t\tif (i == NFT_SET_EXPR_MAX) {\n\t\t\t\terr = -E2BIG;\n\t\t\t\tgoto err_set_init;\n\t\t\t}\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_set_init;\n\t\t\t}\n\t\t\texpr = nft_set_elem_expr_alloc(&ctx, set, tmp);\n\t\t\tif (IS_ERR(expr)) {\n\t\t\t\terr = PTR_ERR(expr);\n\t\t\t\tgoto err_set_init;\n\t\t\t}\n\t\t\tset->exprs[i++] = expr;\n\t\t\tset->num_exprs++;\n\t\t}\n\t}\n\n\tudata = NULL;\n\tif (udlen) {\n\t\tudata = set->data + size;\n\t\tnla_memcpy(udata, nla[NFTA_SET_USERDATA], udlen);\n\t}\n\n\tINIT_LIST_HEAD(&set->bindings);\n\tINIT_LIST_HEAD(&set->catchall_list);\n\tset->table = table;\n\twrite_pnet(&set->net, net);\n\tset->ops   = ops;\n\tset->ktype = ktype;\n\tset->klen  = desc.klen;\n\tset->dtype = dtype;\n\tset->objtype = objtype;\n\tset->dlen  = desc.dlen;\n\tset->flags = flags;\n\tset->size  = desc.size;\n\tset->policy = policy;\n\tset->udlen  = udlen;\n\tset->udata  = udata;\n\tset->timeout = timeout;\n\tset->gc_int = gc_int;\n\tset->handle = nf_tables_alloc_handle(table);\n\n\tset->field_count = desc.field_count;\n\tfor (i = 0; i < desc.field_count; i++)\n\t\tset->field_len[i] = desc.field_len[i];\n\n\terr = ops->init(set, &desc, nla);\n\tif (err < 0)\n\t\tgoto err_set_init;\n\n\terr = nft_trans_set_add(&ctx, NFT_MSG_NEWSET, set);\n\tif (err < 0)\n\t\tgoto err_set_trans;\n\n\tlist_add_tail_rcu(&set->list, &table->sets);\n\ttable->use++;\n\treturn 0;\n\nerr_set_trans:\n\tops->destroy(set);\nerr_set_init:\n\tfor (i = 0; i < set->num_exprs; i++)\n\t\tnft_expr_destroy(&ctx, set->exprs[i]);\nerr_set_alloc_name:\n\tkfree(set->name);\nerr_set_name:\n\tkvfree(set);\n\treturn err;\n}", "project": "linux", "hash": 193021357358532201027347093134489495686, "size": 269, "commit_id": "ad9f151e560b016b6ad3280b48e42fa11e1a5440", "message": "netfilter: nf_tables: initialize set before expression setup\n\nnft_set_elem_expr_alloc() needs an initialized set if expression sets on\nthe NFT_EXPR_GC flag. Move set fields initialization before expression\nsetup.\n\n[4512935.019450] ==================================================================\n[4512935.019456] BUG: KASAN: null-ptr-deref in nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019487] Read of size 8 at addr 0000000000000070 by task nft/23532\n[4512935.019494] CPU: 1 PID: 23532 Comm: nft Not tainted 5.12.0-rc4+ #48\n[...]\n[4512935.019502] Call Trace:\n[4512935.019505]  dump_stack+0x89/0xb4\n[4512935.019512]  ? nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019536]  ? nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019560]  kasan_report.cold.12+0x5f/0xd8\n[4512935.019566]  ? nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019590]  nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019615]  nf_tables_newset+0xc7f/0x1460 [nf_tables]\n\nReported-by: syzbot+ce96ca2b1d0b37c6422d@syzkaller.appspotmail.com\nFixes: 65038428b2c6 (\"netfilter: nf_tables: allow to specify stateful expression in set definition\")\nSigned-off-by: Pablo Neira Ayuso <pablo@netfilter.org>", "target": 1, "dataset": "other", "idx": 202069}
{"func": "\t}\n\n\terr = nf_tables_set_alloc_name(&ctx, set, name);\n\tkfree(name);\n\tif (err < 0)\n\t\tgoto err_set_name;\n\n\tudata = NULL;\n\tif (udlen) {\n\t\tudata = set->data + size;\n\t\tnla_memcpy(udata, nla[NFTA_SET_USERDATA], udlen);\n\t}\n\n\tINIT_LIST_HEAD(&set->bindings);\n\tINIT_LIST_HEAD(&set->catchall_list);\n\tset->table = table;\n\twrite_pnet(&set->net, net);\n\tset->ops = ops;\n\tset->ktype = ktype;\n\tset->klen = desc.klen;\n\tset->dtype = dtype;\n\tset->objtype = objtype;\n\tset->dlen = desc.dlen;\n\tset->flags = flags;\n\tset->size = desc.size;\n\tset->policy = policy;\n\tset->udlen = udlen;\n\tset->udata = udata;\n\tset->timeout = timeout;\n\tset->gc_int = gc_int;\n\n\tset->field_count = desc.field_count;\n\tfor (i = 0; i < desc.field_count; i++)\n\t\tset->field_len[i] = desc.field_len[i];\n\n\terr = ops->init(set, &desc, nla);\n\tif (err < 0)\n\t\tgoto err_set_init;\n\n\tif (nla[NFTA_SET_EXPR]) {\n\t\texpr = nft_set_elem_expr_alloc(&ctx, set, nla[NFTA_SET_EXPR]);\n\t\tif (IS_ERR(expr)) {\n\t\t\terr = PTR_ERR(expr);\n\t\t\tgoto err_set_expr_alloc;\n\t\t}\n\t\tset->exprs[0] = expr;\n\t\tset->num_exprs++;\n\t} else if (nla[NFTA_SET_EXPRESSIONS]) {\n\t\tstruct nft_expr *expr;\n\t\tstruct nlattr *tmp;\n\t\tint left;\n\n\t\tif (!(flags & NFT_SET_EXPR)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto err_set_expr_alloc;\n\t\t}\n\t\ti = 0;\n\t\tnla_for_each_nested(tmp, nla[NFTA_SET_EXPRESSIONS], left) {\n\t\t\tif (i == NFT_SET_EXPR_MAX) {\n\t\t\t\terr = -E2BIG;\n\t\t\t\tgoto err_set_expr_alloc;\n\t\t\t}\n\t\t\tif (nla_type(tmp) != NFTA_LIST_ELEM) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_set_expr_alloc;\n\t\t\t}\n\t\t\texpr = nft_set_elem_expr_alloc(&ctx, set, tmp);\n\t\t\tif (IS_ERR(expr)) {\n\t\t\t\terr = PTR_ERR(expr);\n\t\t\t\tgoto err_set_expr_alloc;\n\t\t\t}\n\t\t\tset->exprs[i++] = expr;\n\t\t\tset->num_exprs++;\n\t\t}\n\t}\n\n\tset->handle = nf_tables_alloc_handle(table);\n\n\terr = nft_trans_set_add(&ctx, NFT_MSG_NEWSET, set);\n\tif (err < 0)\n\t\tgoto err_set_expr_alloc;\n\n\tlist_add_tail_rcu(&set->list, &table->sets);\n\ttable->use++;\n\treturn 0;\n\nerr_set_expr_alloc:\n\tfor (i = 0; i < set->num_exprs; i++)\n\t\tnft_expr_destroy(&ctx, set->exprs[i]);\n\n\tops->destroy(set);\nerr_set_init:\n\tkfree(set->name);\nerr_set_name:\n\tkvfree(set);\n\treturn err;\n}", "project": "linux", "hash": 70145795789126680677504544991909643805, "size": 270, "commit_id": "ad9f151e560b016b6ad3280b48e42fa11e1a5440", "message": "netfilter: nf_tables: initialize set before expression setup\n\nnft_set_elem_expr_alloc() needs an initialized set if expression sets on\nthe NFT_EXPR_GC flag. Move set fields initialization before expression\nsetup.\n\n[4512935.019450] ==================================================================\n[4512935.019456] BUG: KASAN: null-ptr-deref in nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019487] Read of size 8 at addr 0000000000000070 by task nft/23532\n[4512935.019494] CPU: 1 PID: 23532 Comm: nft Not tainted 5.12.0-rc4+ #48\n[...]\n[4512935.019502] Call Trace:\n[4512935.019505]  dump_stack+0x89/0xb4\n[4512935.019512]  ? nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019536]  ? nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019560]  kasan_report.cold.12+0x5f/0xd8\n[4512935.019566]  ? nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019590]  nft_set_elem_expr_alloc+0x84/0xd0 [nf_tables]\n[4512935.019615]  nf_tables_newset+0xc7f/0x1460 [nf_tables]\n\nReported-by: syzbot+ce96ca2b1d0b37c6422d@syzkaller.appspotmail.com\nFixes: 65038428b2c6 (\"netfilter: nf_tables: allow to specify stateful expression in set definition\")\nSigned-off-by: Pablo Neira Ayuso <pablo@netfilter.org>", "target": 0, "dataset": "other", "idx": 328360}
{"func": "static void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->smin_value;\n\tu32 umin_val = src_reg->umin_value;\n\n\t/* Assuming scalar64_min_max_or will be called so it is safe\n\t * to skip updating register for known case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->umin_value;\n\t\tdst_reg->s32_max_value = dst_reg->umax_value;\n\t}\n}", "project": "linux", "hash": 165771617226406414034272893159919382683, "size": 34, "commit_id": "5b9fbeb75b6a98955f628e205ac26689bcb1383e", "message": "bpf: Fix scalar32_min_max_or bounds tracking\n\nSimon reported an issue with the current scalar32_min_max_or() implementation.\nThat is, compared to the other 32 bit subreg tracking functions, the code in\nscalar32_min_max_or() stands out that it's using the 64 bit registers instead\nof 32 bit ones. This leads to bounds tracking issues, for example:\n\n  [...]\n  8: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  8: (79) r1 = *(u64 *)(r0 +0)\n   R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  9: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  9: (b7) r0 = 1\n  10: R0_w=inv1 R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  10: (18) r2 = 0x600000002\n  12: R0_w=inv1 R1_w=inv(id=0) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  12: (ad) if r1 < r2 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: (95) exit\n  14: R0_w=inv1 R1_w=inv(id=0,umax_value=25769803777,var_off=(0x0; 0x7ffffffff)) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  14: (25) if r1 > 0x0 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: (95) exit\n  16: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=25769803777,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  16: (47) r1 |= 0\n  17: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=32212254719,var_off=(0x1; 0x700000000),s32_max_value=1,u32_max_value=1) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  [...]\n\nThe bound tests on the map value force the upper unsigned bound to be 25769803777\nin 64 bit (0b11000000000000000000000000000000001) and then lower one to be 1. By\nusing OR they are truncated and thus result in the range [1,1] for the 32 bit reg\ntracker. This is incorrect given the only thing we know is that the value must be\npositive and thus 2147483647 (0b1111111111111111111111111111111) at max for the\nsubregs. Fix it by using the {u,s}32_{min,max}_value vars instead. This also makes\nsense, for example, for the case where we update dst_reg->s32_{min,max}_value in\nthe else branch we need to use the newly computed dst_reg->u32_{min,max}_value as\nwe know that these are positive. Previously, in the else branch the 64 bit values\nof umin_value=1 and umax_value=32212254719 were used and latter got truncated to\nbe 1 as upper bound there. After the fix the subreg range is now correct:\n\n  [...]\n  8: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  8: (79) r1 = *(u64 *)(r0 +0)\n   R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  9: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  9: (b7) r0 = 1\n  10: R0_w=inv1 R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  10: (18) r2 = 0x600000002\n  12: R0_w=inv1 R1_w=inv(id=0) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  12: (ad) if r1 < r2 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: (95) exit\n  14: R0_w=inv1 R1_w=inv(id=0,umax_value=25769803777,var_off=(0x0; 0x7ffffffff)) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  14: (25) if r1 > 0x0 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: (95) exit\n  16: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=25769803777,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  16: (47) r1 |= 0\n  17: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=32212254719,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  [...]\n\nFixes: 3f50f132d840 (\"bpf: Verifier, do explicit ALU32 bounds tracking\")\nReported-by: Simon Scannell <scannell.smn@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nReviewed-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Alexei Starovoitov <ast@kernel.org>", "target": 1, "dataset": "other", "idx": 202076}
{"func": "static void scalar_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t      struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_is_const(src_reg->var_off);\n\tbool dst_known = tnum_is_const(dst_reg->var_off);\n\ts64 smin_val = src_reg->smin_value;\n\tu64 umin_val = src_reg->umin_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg_known(dst_reg, dst_reg->var_off.value |\n\t\t\t\t\t  src_reg->var_off.value);\n\t\treturn;\n\t}\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->umin_value = max(dst_reg->umin_value, umin_val);\n\tdst_reg->umax_value = dst_reg->var_off.value | dst_reg->var_off.mask;\n\tif (dst_reg->smin_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->smin_value = S64_MIN;\n\t\tdst_reg->smax_value = S64_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->smin_value = dst_reg->umin_value;\n\t\tdst_reg->smax_value = dst_reg->umax_value;\n\t}\n\t/* We may learn something more from the var_off */\n\t__update_reg_bounds(dst_reg);\n}", "project": "linux", "hash": 16041466753298829583829099804318410019, "size": 35, "commit_id": "5b9fbeb75b6a98955f628e205ac26689bcb1383e", "message": "bpf: Fix scalar32_min_max_or bounds tracking\n\nSimon reported an issue with the current scalar32_min_max_or() implementation.\nThat is, compared to the other 32 bit subreg tracking functions, the code in\nscalar32_min_max_or() stands out that it's using the 64 bit registers instead\nof 32 bit ones. This leads to bounds tracking issues, for example:\n\n  [...]\n  8: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  8: (79) r1 = *(u64 *)(r0 +0)\n   R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  9: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  9: (b7) r0 = 1\n  10: R0_w=inv1 R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  10: (18) r2 = 0x600000002\n  12: R0_w=inv1 R1_w=inv(id=0) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  12: (ad) if r1 < r2 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: (95) exit\n  14: R0_w=inv1 R1_w=inv(id=0,umax_value=25769803777,var_off=(0x0; 0x7ffffffff)) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  14: (25) if r1 > 0x0 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: (95) exit\n  16: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=25769803777,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  16: (47) r1 |= 0\n  17: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=32212254719,var_off=(0x1; 0x700000000),s32_max_value=1,u32_max_value=1) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  [...]\n\nThe bound tests on the map value force the upper unsigned bound to be 25769803777\nin 64 bit (0b11000000000000000000000000000000001) and then lower one to be 1. By\nusing OR they are truncated and thus result in the range [1,1] for the 32 bit reg\ntracker. This is incorrect given the only thing we know is that the value must be\npositive and thus 2147483647 (0b1111111111111111111111111111111) at max for the\nsubregs. Fix it by using the {u,s}32_{min,max}_value vars instead. This also makes\nsense, for example, for the case where we update dst_reg->s32_{min,max}_value in\nthe else branch we need to use the newly computed dst_reg->u32_{min,max}_value as\nwe know that these are positive. Previously, in the else branch the 64 bit values\nof umin_value=1 and umax_value=32212254719 were used and latter got truncated to\nbe 1 as upper bound there. After the fix the subreg range is now correct:\n\n  [...]\n  8: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  8: (79) r1 = *(u64 *)(r0 +0)\n   R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R10=fp0 fp-8=mmmmmmmm\n  9: R0=map_value(id=0,off=0,ks=4,vs=48,imm=0) R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  9: (b7) r0 = 1\n  10: R0_w=inv1 R1_w=inv(id=0) R10=fp0 fp-8=mmmmmmmm\n  10: (18) r2 = 0x600000002\n  12: R0_w=inv1 R1_w=inv(id=0) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  12: (ad) if r1 < r2 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: R0_w=inv1 R1_w=inv(id=0,umin_value=25769803778) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  13: (95) exit\n  14: R0_w=inv1 R1_w=inv(id=0,umax_value=25769803777,var_off=(0x0; 0x7ffffffff)) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  14: (25) if r1 > 0x0 goto pc+1\n   R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: R0_w=inv1 R1_w=inv(id=0,umax_value=0,var_off=(0x0; 0x7fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  15: (95) exit\n  16: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=25769803777,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  16: (47) r1 |= 0\n  17: R0_w=inv1 R1_w=inv(id=0,umin_value=1,umax_value=32212254719,var_off=(0x0; 0x77fffffff),u32_max_value=2147483647) R2_w=inv25769803778 R10=fp0 fp-8=mmmmmmmm\n  [...]\n\nFixes: 3f50f132d840 (\"bpf: Verifier, do explicit ALU32 bounds tracking\")\nReported-by: Simon Scannell <scannell.smn@gmail.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nReviewed-by: John Fastabend <john.fastabend@gmail.com>\nAcked-by: Alexei Starovoitov <ast@kernel.org>", "target": 0, "dataset": "other", "idx": 328523}
{"func": "\t\t\tBER_BVZERO( &mod_tmp->sml_type );\n\t\t\tmod_tmp->sml_numvals = 1;\n\t\t\tmod_tmp->sml_values = ( BerVarray )ch_malloc( 2 * sizeof( struct berval ) );\n\t\t\tber_dupbv( &mod_tmp->sml_values[0], &old_rdn[d_cnt]->la_value );\n\t\t\tmod_tmp->sml_values[1].bv_val = NULL;\n\t\t\tif( desc->ad_type->sat_equality->smr_normalize) {\n\t\t\t\tmod_tmp->sml_nvalues = ( BerVarray )ch_malloc( 2 * sizeof( struct berval ) );\n\t\t\t\t(void) (*desc->ad_type->sat_equality->smr_normalize)(\n\t\t\t\t\tSLAP_MR_EQUALITY|SLAP_MR_VALUE_OF_ASSERTION_SYNTAX,\n\t\t\t\t\tdesc->ad_type->sat_syntax,\n\t\t\t\t\tdesc->ad_type->sat_equality,", "project": "openldap", "hash": 98924267316692135884880271228017176283, "size": 162, "commit_id": "4c774220a752bf8e3284984890dc0931fe73165d", "message": "ITS#9370 check for equality rule on old_rdn\n\nJust skip normalization if there's no equality rule. We accept\nDNs without equality rules already.", "target": 1, "dataset": "other", "idx": 202129}
{"func": "\t\t\tBER_BVZERO( &mod_tmp->sml_type );\n\t\t\tmod_tmp->sml_numvals = 1;\n\t\t\tmod_tmp->sml_values = ( BerVarray )ch_malloc( 2 * sizeof( struct berval ) );\n\t\t\tber_dupbv( &mod_tmp->sml_values[0], &old_rdn[d_cnt]->la_value );\n\t\t\tmod_tmp->sml_values[1].bv_val = NULL;\n\t\t\tif( desc->ad_type->sat_equality && desc->ad_type->sat_equality->smr_normalize) {\n\t\t\t\tmod_tmp->sml_nvalues = ( BerVarray )ch_malloc( 2 * sizeof( struct berval ) );\n\t\t\t\t(void) (*desc->ad_type->sat_equality->smr_normalize)(\n\t\t\t\t\tSLAP_MR_EQUALITY|SLAP_MR_VALUE_OF_ASSERTION_SYNTAX,\n\t\t\t\t\tdesc->ad_type->sat_syntax,\n\t\t\t\t\tdesc->ad_type->sat_equality,", "project": "openldap", "hash": 23684533698009506860231461160337640809, "size": 162, "commit_id": "4c774220a752bf8e3284984890dc0931fe73165d", "message": "ITS#9370 check for equality rule on old_rdn\n\nJust skip normalization if there's no equality rule. We accept\nDNs without equality rules already.", "target": 0, "dataset": "other", "idx": 329989}
{"func": "      wave_image=DestroyImage(wave_image);\n      ThrowImageException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  for (i=0; i < (ssize_t) wave_image->columns; i++)\n    sine_map[i]=(float) fabs(amplitude)+amplitude*sin((double)\n      ((2.0*MagickPI*i)/wave_length));\n  /*\n    Wave image.\n  */\n  status=MagickTrue;\n  progress=0;", "project": "ImageMagick", "hash": 109616764411189885332816110905608229061, "size": 140, "commit_id": "94174beff065cb5683d09d79e992c3ebbdead311", "message": "https://github.com/ImageMagick/ImageMagick/issues/3296", "target": 1, "dataset": "other", "idx": 202140}
{"func": "      wave_image=DestroyImage(wave_image);\n      ThrowImageException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  for (i=0; i < (ssize_t) wave_image->columns; i++)\n    sine_map[i]=(float) fabs(amplitude)+amplitude*sin((double)\n      ((2.0*MagickPI*i)*PerceptibleReciprocal(wave_length)));\n  /*\n    Wave image.\n  */\n  status=MagickTrue;\n  progress=0;", "project": "ImageMagick", "hash": 295514983873945565860327311775528409590, "size": 140, "commit_id": "94174beff065cb5683d09d79e992c3ebbdead311", "message": "https://github.com/ImageMagick/ImageMagick/issues/3296", "target": 0, "dataset": "other", "idx": 330274}
{"func": "\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, netoff, hdrlen;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {", "project": "linux", "hash": 247138033746397759600375764218281666520, "size": 264, "commit_id": "acf69c946233259ab4d64f8869d4037a198c7f06", "message": "net/packet: fix overflow in tpacket_rcv\n\nUsing tp_reserve to calculate netoff can overflow as\ntp_reserve is unsigned int and netoff is unsigned short.\n\nThis may lead to macoff receving a smaller value then\nsizeof(struct virtio_net_hdr), and if po->has_vnet_hdr\nis set, an out-of-bounds write will occur when\ncalling virtio_net_hdr_from_skb.\n\nThe bug is fixed by converting netoff to unsigned int\nand checking if it exceeds USHRT_MAX.\n\nThis addresses CVE-2020-14386\n\nFixes: 8913336a7e8d (\"packet: add PACKET_RESERVE sockopt\")\nSigned-off-by: Or Cohen <orcohen@paloaltonetworks.com>\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 202143}
{"func": "\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {", "project": "linux", "hash": 321520857233827511931030116735694876634, "size": 269, "commit_id": "acf69c946233259ab4d64f8869d4037a198c7f06", "message": "net/packet: fix overflow in tpacket_rcv\n\nUsing tp_reserve to calculate netoff can overflow as\ntp_reserve is unsigned int and netoff is unsigned short.\n\nThis may lead to macoff receving a smaller value then\nsizeof(struct virtio_net_hdr), and if po->has_vnet_hdr\nis set, an out-of-bounds write will occur when\ncalling virtio_net_hdr_from_skb.\n\nThe bug is fixed by converting netoff to unsigned int\nand checking if it exceeds USHRT_MAX.\n\nThis addresses CVE-2020-14386\n\nFixes: 8913336a7e8d (\"packet: add PACKET_RESERVE sockopt\")\nSigned-off-by: Or Cohen <orcohen@paloaltonetworks.com>\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 330393}
{"func": "    default:\n        vnc_client_error(vs);\n        return;\n    }\n\n    vs->client_pf.rmax = red_max;\n    vs->client_pf.rbits = hweight_long(red_max);\n    vs->client_pf.rshift = red_shift;\n    vs->client_pf.rmask = red_max << red_shift;\n    vs->client_pf.gmax = green_max;\n    vs->client_pf.gbits = hweight_long(green_max);\n    vs->client_pf.gshift = green_shift;\n    vs->client_pf.gmask = green_max << green_shift;\n    vs->client_pf.bmax = blue_max;\n    vs->client_pf.bbits = hweight_long(blue_max);\n    vs->client_pf.bshift = blue_shift;\n    vs->client_pf.bmask = blue_max << blue_shift;\n    vs->client_pf.bits_per_pixel = bits_per_pixel;\n    vs->client_pf.bytes_per_pixel = bits_per_pixel / 8;", "project": "qemu", "hash": 209274626528158267666795461424033381853, "size": 43, "commit_id": "4c65fed8bdf96780735dbdb92a8bd0d6b6526cc3", "message": "ui: vnc: avoid floating point exception\n\nWhile sending 'SetPixelFormat' messages to a VNC server,\nthe client could set the 'red-max', 'green-max' and 'blue-max'\nvalues to be zero. This leads to a floating point exception in\nwrite_png_palette while doing frame buffer updates.\n\nReported-by: Lian Yihan <lianyihan@360.cn>\nSigned-off-by: Prasad J Pandit <pjp@fedoraproject.org>\nReviewed-by: Gerd Hoffmann <kraxel@redhat.com>\nSigned-off-by: Peter Maydell <peter.maydell@linaro.org>", "target": 1, "dataset": "other", "idx": 202144}
{"func": "    default:\n        vnc_client_error(vs);\n        return;\n    }\n\n    vs->client_pf.rmax = red_max ? red_max : 0xFF;\n    vs->client_pf.rbits = hweight_long(red_max);\n    vs->client_pf.rshift = red_shift;\n    vs->client_pf.rmask = red_max << red_shift;\n    vs->client_pf.gmax = green_max ? green_max : 0xFF;\n    vs->client_pf.gbits = hweight_long(green_max);\n    vs->client_pf.gshift = green_shift;\n    vs->client_pf.gmask = green_max << green_shift;\n    vs->client_pf.bmax = blue_max ? blue_max : 0xFF;\n    vs->client_pf.bbits = hweight_long(blue_max);\n    vs->client_pf.bshift = blue_shift;\n    vs->client_pf.bmask = blue_max << blue_shift;\n    vs->client_pf.bits_per_pixel = bits_per_pixel;\n    vs->client_pf.bytes_per_pixel = bits_per_pixel / 8;", "project": "qemu", "hash": 98211384367942618164926733929775680004, "size": 43, "commit_id": "4c65fed8bdf96780735dbdb92a8bd0d6b6526cc3", "message": "ui: vnc: avoid floating point exception\n\nWhile sending 'SetPixelFormat' messages to a VNC server,\nthe client could set the 'red-max', 'green-max' and 'blue-max'\nvalues to be zero. This leads to a floating point exception in\nwrite_png_palette while doing frame buffer updates.\n\nReported-by: Lian Yihan <lianyihan@360.cn>\nSigned-off-by: Prasad J Pandit <pjp@fedoraproject.org>\nReviewed-by: Gerd Hoffmann <kraxel@redhat.com>\nSigned-off-by: Peter Maydell <peter.maydell@linaro.org>", "target": 0, "dataset": "other", "idx": 330516}
{"func": "  len++;\n\n  cid = schematahash[h];\n  if (cid)\n    {\n      if (!memcmp(data->schemadata + data->schemata[cid], schema, len * sizeof(Id)))\n        return cid;\n      /* cache conflict, do a slow search */\n      for (cid = 1; cid < data->nschemata; cid++)\n        if (!memcmp(data->schemadata + data->schemata[cid], schema, len * sizeof(Id)))\n          return cid;\n    }\n  /* a new one */\n  if (!create)\n    return 0;", "project": "libsolv", "hash": 198011217878246148723542582906537124475, "size": 52, "commit_id": "fdb9c9c03508990e4583046b590c30d958f272da", "message": "repodata_schema2id: fix heap-buffer-overflow in memcmp\n\nWhen the length of last schema in data->schemadata is\nless than length of input schema, we got a read overflow\nin asan test.\n\nSigned-off-by: Zhipeng Xie <xiezhipeng1@huawei.com>", "target": 1, "dataset": "other", "idx": 202305}
{"func": "  len++;\n\n  cid = schematahash[h];\n  if (cid)\n    {\n      if ((data->schemata[cid] + len <= data->schemadatalen) &&\n\t\t\t  !memcmp(data->schemadata + data->schemata[cid], schema, len * sizeof(Id)))\n        return cid;\n      /* cache conflict, do a slow search */\n      for (cid = 1; cid < data->nschemata; cid++)\n        if ((data->schemata[cid] + len <= data->schemadatalen) &&\n\t\t\t\t!memcmp(data->schemadata + data->schemata[cid], schema, len * sizeof(Id)))\n          return cid;\n    }\n  /* a new one */\n  if (!create)\n    return 0;", "project": "libsolv", "hash": 40327825865103618095291848843313728372, "size": 54, "commit_id": "fdb9c9c03508990e4583046b590c30d958f272da", "message": "repodata_schema2id: fix heap-buffer-overflow in memcmp\n\nWhen the length of last schema in data->schemadata is\nless than length of input schema, we got a read overflow\nin asan test.\n\nSigned-off-by: Zhipeng Xie <xiezhipeng1@huawei.com>", "target": 0, "dataset": "other", "idx": 333146}
{"func": "\n    for (; state->y < state->ysize; state->y += rows_per_strip) {\n        img.row_offset = state->y;\n        rows_to_read = min(rows_per_strip, img.height - state->y);\n\n        if (TIFFRGBAImageGet(&img, (UINT32 *)state->buffer, img.width, rows_to_read) ==\n            -1) {\n            TRACE((\"Decode Error, y: %d\\n\", state->y));\n            state->errcode = IMAGING_CODEC_BROKEN;\n            goto decodeycbcr_err;\n        }\n", "project": "Pillow", "hash": 24226912125521825308253027036068382827, "size": 107, "commit_id": "3fee28eb9479bf7d59e0fa08068f9cc4a6e2f04c", "message": "Incorrect error code checking in TiffDecode.c\n\n* since Pillow 8.1.0\n* CVE-2021-25289", "target": 1, "dataset": "other", "idx": 202401}
{"func": "\n    for (; state->y < state->ysize; state->y += rows_per_strip) {\n        img.row_offset = state->y;\n        rows_to_read = min(rows_per_strip, img.height - state->y);\n\n        if (!TIFFRGBAImageGet(&img, (UINT32 *)state->buffer, img.width, rows_to_read)) {\n            TRACE((\"Decode Error, y: %d\\n\", state->y));\n            state->errcode = IMAGING_CODEC_BROKEN;\n            goto decodeycbcr_err;\n        }\n", "project": "Pillow", "hash": 337824209326848979984273145189268149899, "size": 106, "commit_id": "3fee28eb9479bf7d59e0fa08068f9cc4a6e2f04c", "message": "Incorrect error code checking in TiffDecode.c\n\n* since Pillow 8.1.0\n* CVE-2021-25289", "target": 0, "dataset": "other", "idx": 333841}
{"func": "\t\t\tbreak;\n#endif\n\t\t}\n\n\t\tif (!module) {\n\t\t\tvoid *iter = NULL;\n\t\t\twhile ((full_name = mono_dl_build_path (NULL, file_name, &iter))) {\n\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\t\"DllImport loading location: '%s'.\", full_name);\n\t\t\t\tmodule = cached_module_load (full_name, MONO_DL_LAZY, &error_msg);\n\t\t\t\tif (!module) {\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!module) {\n\t\t\tvoid *iter = NULL;\n\t\t\twhile ((full_name = mono_dl_build_path (\".\", file_name, &iter))) {\n\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\"DllImport loading library: '%s'.\", full_name);\n\t\t\t\tmodule = cached_module_load (full_name, MONO_DL_LAZY, &error_msg);\n\t\t\t\tif (!module) {\n\t\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\t\"DllImport error loading library '%s'.\",\n\t\t\t\t\t\terror_msg);\n\t\t\t\t\tg_free (error_msg);\n\t\t\t\t}\n\t\t\t\tg_free (full_name);\n\t\t\t\tif (module)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!module) {\n\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\"DllImport loading: '%s'.\", file_name);\n\t\t\tmodule = cached_module_load (file_name, MONO_DL_LAZY, &error_msg);\n\t\t\tif (!module) {\n\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,", "project": "mono", "hash": 320715786743187810347874367233438679401, "size": 276, "commit_id": "8e890a3bf80a4620e417814dc14886b1bbd17625", "message": "Search for dllimported shared libs in the base directory, not cwd.\n\n* loader.c: we don't search the current directory anymore for shared\nlibraries referenced in DllImport attributes, as it has a slight\nsecurity risk. We search in the same directory where the referencing\nimage was loaded from, instead. Fixes bug# 641915.", "target": 1, "dataset": "other", "idx": 202558}
{"func": "#endif\n\t\t}\n\n\t\tif (!module) {\n\t\t\tvoid *iter = NULL;\n\t\t\tchar *mdirname = g_path_get_dirname (image->name);\n\t\t\twhile ((full_name = mono_dl_build_path (mdirname, file_name, &iter))) {\n\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\"DllImport loading library: '%s'.\", full_name);\n\t\t\t\tmodule = cached_module_load (full_name, MONO_DL_LAZY, &error_msg);\n\t\t\t\tif (!module) {\n\t\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\t\"DllImport error loading library '%s'.\",\n\t\t\t\t\t\terror_msg);\n\t\t\t\t\tg_free (error_msg);\n\t\t\t\t}\n\t\t\t\tg_free (full_name);\n\t\t\t\tif (module)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tg_free (mdirname);\n\t\t}\n\n\t\tif (!module) {\n\t\t\tvoid *iter = NULL;\n\t\t\twhile ((full_name = mono_dl_build_path (NULL, file_name, &iter))) {\n\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\t\"DllImport loading location: '%s'.\", full_name);\n\t\t\t\tmodule = cached_module_load (full_name, MONO_DL_LAZY, &error_msg);\n\t\t\t\tif (!module) {\n\t\t\t\tif (module)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!module) {\n\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,\n\t\t\t\t\t\"DllImport loading: '%s'.\", file_name);\n\t\t\tmodule = cached_module_load (file_name, MONO_DL_LAZY, &error_msg);\n\t\t\tif (!module) {\n\t\t\t\tmono_trace (G_LOG_LEVEL_INFO, MONO_TRACE_DLLIMPORT,", "project": "mono", "hash": 23336029349961999853011559792852870024, "size": 278, "commit_id": "8e890a3bf80a4620e417814dc14886b1bbd17625", "message": "Search for dllimported shared libs in the base directory, not cwd.\n\n* loader.c: we don't search the current directory anymore for shared\nlibraries referenced in DllImport attributes, as it has a slight\nsecurity risk. We search in the same directory where the referencing\nimage was loaded from, instead. Fixes bug# 641915.", "target": 0, "dataset": "other", "idx": 334474}
{"func": "bool timerqueue_add(struct timerqueue_head *head, struct timerqueue_node *node)\n{\n\tstruct rb_node **p = &head->head.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct timerqueue_node  *ptr;\n\n\t/* Make sure we don't add nodes that are already added */\n\tWARN_ON_ONCE(!RB_EMPTY_NODE(&node->node));\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tptr = rb_entry(parent, struct timerqueue_node, node);\n\t\tif (node->expires < ptr->expires)\n\t\t\tp = &(*p)->rb_left;\n\t\telse\n\t\t\tp = &(*p)->rb_right;\n\t}\n\trb_link_node(&node->node, parent, p);\n\trb_insert_color(&node->node, &head->head);\n\n\tif (!head->next || node->expires < head->next->expires) {\n\t\thead->next = node;\n\t\treturn true;\n\t}\n\treturn false;\n}", "project": "tip", "hash": 295643821598703418675091892895767224857, "size": 26, "commit_id": "511885d7061eda3eb1faf3f57dcc936ff75863f1", "message": "lib/timerqueue: Rely on rbtree semantics for next timer\n\nSimplify the timerqueue code by using cached rbtrees and rely on the tree\nleftmost node semantics to get the timer with earliest expiration time.\nThis is a drop in conversion, and therefore semantics remain untouched.\n\nThe runtime overhead of cached rbtrees is be pretty much the same as the\ncurrent head->next method, noting that when removing the leftmost node,\na common operation for the timerqueue, the rb_next(leftmost) is O(1) as\nwell, so the next timer will either be the right node or its parent.\nTherefore no extra pointer chasing. Finally, the size of the struct\ntimerqueue_head remains the same.\n\nPasses several hours of rcutorture.\n\nSigned-off-by: Davidlohr Bueso <dbueso@suse.de>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nLink: https://lkml.kernel.org/r/20190724152323.bojciei3muvfxalm@linux-r8p5", "target": 1, "dataset": "other", "idx": 202574}
{"func": "bool timerqueue_add(struct timerqueue_head *head, struct timerqueue_node *node)\n{\n\tstruct rb_node **p = &head->rb_root.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct timerqueue_node *ptr;\n\tbool leftmost = true;\n\n\t/* Make sure we don't add nodes that are already added */\n\tWARN_ON_ONCE(!RB_EMPTY_NODE(&node->node));\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tptr = rb_entry(parent, struct timerqueue_node, node);\n\t\tif (node->expires < ptr->expires) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else {\n\t\t\tp = &(*p)->rb_right;\n\t\t\tleftmost = false;\n\t\t}\n\t}\n\trb_link_node(&node->node, parent, p);\n\trb_insert_color_cached(&node->node, &head->rb_root, leftmost);\n\n\treturn leftmost;\n}", "project": "tip", "hash": 121492852338608162479188929864215178822, "size": 25, "commit_id": "511885d7061eda3eb1faf3f57dcc936ff75863f1", "message": "lib/timerqueue: Rely on rbtree semantics for next timer\n\nSimplify the timerqueue code by using cached rbtrees and rely on the tree\nleftmost node semantics to get the timer with earliest expiration time.\nThis is a drop in conversion, and therefore semantics remain untouched.\n\nThe runtime overhead of cached rbtrees is be pretty much the same as the\ncurrent head->next method, noting that when removing the leftmost node,\na common operation for the timerqueue, the rb_next(leftmost) is O(1) as\nwell, so the next timer will either be the right node or its parent.\nTherefore no extra pointer chasing. Finally, the size of the struct\ntimerqueue_head remains the same.\n\nPasses several hours of rcutorture.\n\nSigned-off-by: Davidlohr Bueso <dbueso@suse.de>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nLink: https://lkml.kernel.org/r/20190724152323.bojciei3muvfxalm@linux-r8p5", "target": 0, "dataset": "other", "idx": 335107}
{"func": "\n      pinfo->fragmented = save_fragmented;\n    }\n    else\n    {\n      /* CRC error - throw away the data. */\n      next_tvb = NULL;\n    }\n  }\n\n  /* Set the length of the message */", "project": "wireshark", "hash": 308851302387044557192207962363054817597, "size": 277, "commit_id": "618661b22e34a59b21117db723d8ff91e064d4ba", "message": "dnp: plug a memory leak.\n\nIf we're throwing away the data, *throw away the data* - free it, as\nwe're not using it as the backing data for a tvbuff.", "target": 1, "dataset": "other", "idx": 202609}
{"func": "      pinfo->fragmented = save_fragmented;\n    }\n    else\n    {\n      /* CRC error - throw away the data. */\n      g_free(al_buffer);\n      next_tvb = NULL;\n    }\n  }\n\n  /* Set the length of the message */", "project": "wireshark", "hash": 125850200117559018768311602342424911021, "size": 278, "commit_id": "618661b22e34a59b21117db723d8ff91e064d4ba", "message": "dnp: plug a memory leak.\n\nIf we're throwing away the data, *throw away the data* - free it, as\nwe're not using it as the backing data for a tvbuff.", "target": 0, "dataset": "other", "idx": 335510}
{"func": "\n    if (!len) {\n        return NVME_SUCCESS;\n    }\n\n    trace_pci_nvme_map_addr(addr, len);\n\n    if (nvme_addr_is_cmb(n, addr)) {\n        cmb = true;\n    } else if (nvme_addr_is_pmr(n, addr)) {\n        pmr = true;", "project": "qemu", "hash": 161220796245487945523383117449990525230, "size": 49, "commit_id": "736b01642d85be832385063f278fe7cd4ffb5221", "message": "hw/nvme: fix CVE-2021-3929\n\nThis fixes CVE-2021-3929 \"locally\" by denying DMA to the iomem of the\ndevice itself. This still allows DMA to MMIO regions of other devices\n(e.g. doing P2P DMA to the controller memory buffer of another NVMe\ndevice).\n\nFixes: CVE-2021-3929\nReported-by: Qiuhao Li <Qiuhao.Li@outlook.com>\nReviewed-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Philippe Mathieu-Daud\u00e9 <f4bug@amsat.org>\nSigned-off-by: Klaus Jensen <k.jensen@samsung.com>", "target": 1, "dataset": "other", "idx": 202660}
{"func": "    if (!len) {\n        return NVME_SUCCESS;\n    }\n\n    trace_pci_nvme_map_addr(addr, len);\n\n    if (nvme_addr_is_iomem(n, addr)) {\n        return NVME_DATA_TRAS_ERROR;\n    }\n\n    if (nvme_addr_is_cmb(n, addr)) {\n        cmb = true;\n    } else if (nvme_addr_is_pmr(n, addr)) {\n        pmr = true;", "project": "qemu", "hash": 171394500595149274575594149292479412061, "size": 53, "commit_id": "736b01642d85be832385063f278fe7cd4ffb5221", "message": "hw/nvme: fix CVE-2021-3929\n\nThis fixes CVE-2021-3929 \"locally\" by denying DMA to the iomem of the\ndevice itself. This still allows DMA to MMIO regions of other devices\n(e.g. doing P2P DMA to the controller memory buffer of another NVMe\ndevice).\n\nFixes: CVE-2021-3929\nReported-by: Qiuhao Li <Qiuhao.Li@outlook.com>\nReviewed-by: Keith Busch <kbusch@kernel.org>\nReviewed-by: Philippe Mathieu-Daud\u00e9 <f4bug@amsat.org>\nSigned-off-by: Klaus Jensen <k.jensen@samsung.com>", "target": 0, "dataset": "other", "idx": 336154}
{"func": "\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t\tfull_check = 1;\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\terror = -ELOOP;\n\t\t\t\tif (ep_loop_check(ep, tf.file) != 0) {\n\t\t\t\t\tclear_tfile_check_list();\n\t\t\t\t\tgoto error_tgt_fput;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tget_file(tf.file);\n\t\t\t\tlist_add(&tf.file->f_tfile_llink,\n\t\t\t\t\t\t\t&tfile_check_list);\n\t\t\t}\n\t\t\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\t\t\tif (error) {\nout_del:\n\t\t\t\tlist_del(&tf.file->f_tfile_llink);\n\t\t\t\tif (!is_file_epoll(tf.file))\n\t\t\t\t\tfput(tf.file);\n\t\t\t\tgoto error_tgt_fput;\n\t\t\t}\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\ttep = tf.file->private_data;\n\t\t\t\terror = epoll_mutex_lock(&tep->mtx, 1, nonblock);\n\t\t\t\tif (error) {\n\t\t\t\t\tmutex_unlock(&ep->mtx);\n\t\t\t\t\tgoto out_del;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\t\tif (!epi) {\n\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\terror = ep_insert(ep, epds, tf.file, fd, full_check);\n\t\t} else\n\t\t\terror = -EEXIST;\n\t\tif (full_check)\n\t\t\tclear_tfile_check_list();\n\t\tbreak;\n\tcase EPOLL_CTL_DEL:\n\t\tif (epi)\n\t\t\terror = ep_remove(ep, epi);\n\t\telse\n\tif (tep != NULL)\n\t\tmutex_unlock(&tep->mtx);\n\tmutex_unlock(&ep->mtx);\n\nerror_tgt_fput:\n\tif (full_check)\n\t\tmutex_unlock(&epmutex);\n\n\tfdput(tf);\nerror_fput:\n\tfdput(f);\nerror_return:", "project": "linux", "hash": 278445563960181030958909173096615505008, "size": 162, "commit_id": "52c479697c9b73f628140dcdfcd39ea302d05482", "message": "do_epoll_ctl(): clean the failure exits up a bit\n\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>", "target": 1, "dataset": "other", "idx": 202665}
{"func": "\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t\tfull_check = 1;\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\terror = -ELOOP;\n\t\t\t\tif (ep_loop_check(ep, tf.file) != 0)\n\t\t\t\t\tgoto error_tgt_fput;\n\t\t\t} else {\n\t\t\t\tget_file(tf.file);\n\t\t\t\tlist_add(&tf.file->f_tfile_llink,\n\t\t\t\t\t\t\t&tfile_check_list);\n\t\t\t}\n\t\t\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\ttep = tf.file->private_data;\n\t\t\t\terror = epoll_mutex_lock(&tep->mtx, 1, nonblock);\n\t\t\t\tif (error) {\n\t\t\t\t\tmutex_unlock(&ep->mtx);\n\t\t\t\t\tgoto error_tgt_fput;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tcase EPOLL_CTL_ADD:\n\t\tif (!epi) {\n\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\terror = ep_insert(ep, epds, tf.file, fd, full_check);\n\t\t} else\n\t\t\terror = -EEXIST;\n\t\tbreak;\n\tcase EPOLL_CTL_DEL:\n\t\tif (epi)\n\t\t\terror = ep_remove(ep, epi);\n\t\telse\n\tif (tep != NULL)\n\t\tmutex_unlock(&tep->mtx);\n\tmutex_unlock(&ep->mtx);\n\nerror_tgt_fput:\n\tif (full_check) {\n\t\tclear_tfile_check_list();\n\t\tmutex_unlock(&epmutex);\n\t}\n\n\tfdput(tf);\nerror_fput:\n\tfdput(f);\nerror_return:", "project": "linux", "hash": 291400095598389893794119152689563159831, "size": 155, "commit_id": "52c479697c9b73f628140dcdfcd39ea302d05482", "message": "do_epoll_ctl(): clean the failure exits up a bit\n\nSigned-off-by: Al Viro <viro@zeniv.linux.org.uk>", "target": 0, "dataset": "other", "idx": 336269}
{"func": "            }\n        }\n    }\n\n    for (r = 0; r < retval->ext_size; ++r) {\n        /* set flag, which represent LYEXT_OPT_VALID */\n        if (retval->ext[r]->flags & LYEXT_OPT_VALID) {\n            retval->flags |= LYS_VALID_EXT;\n            if (retval->ext[r]->flags & LYEXT_OPT_VALID_SUBTREE) {\n                retval->flags |= LYS_VALID_EXT_SUBTREE;\n                break;\n            }", "project": "libyang", "hash": 55868768057507781729927570308714603647, "size": 217, "commit_id": "a3917d95d516e3de267d3cfa5d4d3715a90e8777", "message": "yin parser BUGFIX invalid memory access\n\n... in case there were some unresolved\nextensions.\nFixes #1454\nFixes #1455", "target": 1, "dataset": "other", "idx": 202684}
{"func": "            }\n        }\n    }\n\n    for (r = 0; r < retval->ext_size; ++r) {\n        /* extension instance may not yet be resolved */\n        if (retval->ext[r] && (retval->ext[r]->flags & LYEXT_OPT_VALID)) {\n             /* set flag, which represent LYEXT_OPT_VALID */\n            retval->flags |= LYS_VALID_EXT;\n            if (retval->ext[r]->flags & LYEXT_OPT_VALID_SUBTREE) {\n                retval->flags |= LYS_VALID_EXT_SUBTREE;\n                break;\n            }", "project": "libyang", "hash": 336173865557448454715868745869729551732, "size": 218, "commit_id": "a3917d95d516e3de267d3cfa5d4d3715a90e8777", "message": "yin parser BUGFIX invalid memory access\n\n... in case there were some unresolved\nextensions.\nFixes #1454\nFixes #1455", "target": 0, "dataset": "other", "idx": 336786}
{"func": "    VALUE w;\n    long width, len, flen = 1, fclen = 1;\n    VALUE res;\n    char *p;\n    const char *f = \" \";\n    long n, llen, rlen;\n    volatile VALUE pad;\n    int singlebyte = 1, cr;\n\n    rb_scan_args(argc, argv, \"11\", &w, &pad);\n    enc = STR_ENC_GET(str);\n    if (width < 0 || len >= width) return rb_str_dup(str);\n    n = width - len;\n    llen = (jflag == 'l') ? 0 : ((jflag == 'r') ? n : n/2);\n    rlen = n - llen;\n    cr = ENC_CODERANGE(str);\n    res = rb_str_new5(str, 0, RSTRING_LEN(str)+n*flen/fclen+2);\n    p = RSTRING_PTR(res);\n    while (llen) {\n\tif (flen <= 1) {\n\t    *p++ = *f;\n\t    llen--;\n\t}\n\telse if (llen > fclen) {\n\t    memcpy(p,f,flen);\n\t    p += flen;\n\t    llen -= fclen;\n\t}\n\telse {\n\t    char *fp = str_nth(f, f+flen, llen, enc, singlebyte);\n\t    n = fp - f;\n\t    memcpy(p,f,n);\n\t    p+=n;\n\t    break;\n\t}\n    }\n    memcpy(p, RSTRING_PTR(str), RSTRING_LEN(str));\n    p+=RSTRING_LEN(str);\n    while (rlen) {\n\tif (flen <= 1) {\n\t    *p++ = *f;\n\t    rlen--;\n\t}\n\telse if (rlen > fclen) {\n\t    memcpy(p,f,flen);\n\t    p += flen;\n\t    rlen -= fclen;\n\t}\n\telse {\n\t    char *fp = str_nth(f, f+flen, rlen, enc, singlebyte);\n\t    n = fp - f;\n\t    memcpy(p,f,n);\n\t    p+=n;\n\t    break;\n\t}\n    }\n    *p = '\\0';\n    STR_SET_LEN(res, p-RSTRING_PTR(res));\n    OBJ_INFECT(res, str);", "project": "ruby", "hash": 26901884064724764290314463604500539542, "size": 83, "commit_id": "1c2ef610358af33f9ded3086aa2d70aac03dcac5", "message": "* string.c (rb_str_justify): CVE-2009-4124.\n  Fixes a bug reported by \n  Emmanouel Kellinis <Emmanouel.Kellinis AT kpmg.co.uk>, KPMG London;\n  Patch by nobu.\n\n\ngit-svn-id: svn+ssh://ci.ruby-lang.org/ruby/trunk@26038 b2dd03c8-39d4-4d8f-98ff-823fe69b080e", "target": 1, "dataset": "other", "idx": 202689}
{"func": "    VALUE w;\n    long width, len, flen = 1, fclen = 1;\n    VALUE res;\n    char *p;\n    const char *f = \" \";\n    long n, size, llen, rlen, llen2 = 0, rlen2 = 0;\n    volatile VALUE pad;\n    int singlebyte = 1, cr;\n\n    rb_scan_args(argc, argv, \"11\", &w, &pad);\n    enc = STR_ENC_GET(str);\n    if (width < 0 || len >= width) return rb_str_dup(str);\n    n = width - len;\n    llen = (jflag == 'l') ? 0 : ((jflag == 'r') ? n : n/2);\n    rlen = n - llen;\n    cr = ENC_CODERANGE(str);\n    if (flen > 1) {\n       llen2 = str_offset(f, f + flen, llen % fclen, enc, singlebyte);\n       rlen2 = str_offset(f, f + flen, rlen % fclen, enc, singlebyte);\n    }\n    size = RSTRING_LEN(str);\n    if ((len = llen / fclen + rlen / fclen) >= LONG_MAX / flen ||\n       (len *= flen) >= LONG_MAX - llen2 - rlen2 ||\n       (len += llen2 + rlen2) >= LONG_MAX - size) {\n       rb_raise(rb_eArgError, \"argument too big\");\n    }\n    len += size;\n    res = rb_str_new5(str, 0, len);\n    p = RSTRING_PTR(res);\n    if (flen <= 1) {\n       memset(p, *f, llen);\n       p += llen;\n    }\n    else {\n       while (llen > fclen) {\n\t    memcpy(p,f,flen);\n\t    p += flen;\n\t    llen -= fclen;\n\t}\n       if (llen > 0) {\n           memcpy(p, f, llen2);\n           p += llen2;\n\t}\n    }\n    memcpy(p, RSTRING_PTR(str), size);\n    p += size;\n    if (flen <= 1) {\n       memset(p, *f, rlen);\n       p += rlen;\n    }\n    else {\n       while (rlen > fclen) {\n\t    memcpy(p,f,flen);\n\t    p += flen;\n\t    rlen -= fclen;\n\t}\n       if (rlen > 0) {\n           memcpy(p, f, rlen2);\n           p += rlen2;\n\t}\n    }\n    *p = '\\0';\n    STR_SET_LEN(res, p-RSTRING_PTR(res));\n    OBJ_INFECT(res, str);", "project": "ruby", "hash": 78565306356934170507378775330432968749, "size": 88, "commit_id": "1c2ef610358af33f9ded3086aa2d70aac03dcac5", "message": "* string.c (rb_str_justify): CVE-2009-4124.\n  Fixes a bug reported by \n  Emmanouel Kellinis <Emmanouel.Kellinis AT kpmg.co.uk>, KPMG London;\n  Patch by nobu.\n\n\ngit-svn-id: svn+ssh://ci.ruby-lang.org/ruby/trunk@26038 b2dd03c8-39d4-4d8f-98ff-823fe69b080e", "target": 0, "dataset": "other", "idx": 337027}
{"func": "  else if (option (OPTIMAPSERVERNOISE) && (ascii_strncasecmp (\"NO\", s, 2) == 0))\n  {\n    dprint (2, (debugfile, \"Handling untagged NO\\n\"));\n\n    /* Display the warning message from the server */\n    mutt_error (\"%s\", s+3);\n    mutt_sleep (2);\n  }\n\n  return 0;\n}", "project": "mutt", "hash": 50423773011428319367692617777736025326, "size": 101, "commit_id": "9347b5c01dc52682cb6be11539d9b7ebceae4416", "message": "Handle NO response without message properly", "target": 1, "dataset": "other", "idx": 202733}
{"func": "  else if (option (OPTIMAPSERVERNOISE) && (ascii_strncasecmp (\"NO\", s, 2) == 0))\n  {\n    dprint (2, (debugfile, \"Handling untagged NO\\n\"));\n\n    /* Display the warning message from the server */\n    mutt_error (\"%s\", s+2);\n    mutt_sleep (2);\n  }\n\n  return 0;\n}", "project": "mutt", "hash": 87163458428751244929158289758255212737, "size": 101, "commit_id": "9347b5c01dc52682cb6be11539d9b7ebceae4416", "message": "Handle NO response without message properly", "target": 0, "dataset": "other", "idx": 338014}
{"func": "      break;\n    prev_image->background_color.alpha_trait=BlendPixelTrait;\n    if ( disposals[i] == DelDispose ) {\n      size_t time = 0;\n      while ( disposals[i] == DelDispose ) {\n        time += curr->delay*1000/curr->ticks_per_second;\n        curr=GetNextImageInList(curr);\n        i++;\n      }\n      time += curr->delay*1000/curr->ticks_per_second;\n      prev_image->ticks_per_second = 100L;\n      prev_image->delay = time*prev_image->ticks_per_second/1000;\n    }\n    bgnd_image=CropImage(prev_image,&bounds[i],sans_exception);\n    prev_image=DestroyImage(prev_image);", "project": "ImageMagick", "hash": 140795675133758458513138893585385241253, "size": 449, "commit_id": "ef59bd764f88d893f1219fee8ba696a5d3f8c1c4", "message": "There is a Division by Zero in function OptimizeLayerFrames (#2743)\n\nin file MagickCore/layer.c. cur->ticks_per_seconds can be zero\r\nwith a crafted input argument *image. This is similar to\r\nCVE-2019-13454.", "target": 1, "dataset": "other", "idx": 202739}
{"func": "      break;\n    prev_image->background_color.alpha_trait=BlendPixelTrait;\n    if ( disposals[i] == DelDispose ) {\n      size_t time = 0;\n      while ( disposals[i] == DelDispose ) {\n        time +=(size_t) (curr->delay*1000*\n          PerceptibleReciprocal((double) curr->ticks_per_second));\n        curr=GetNextImageInList(curr);\n        i++;\n      }\n      time += (size_t)(curr->delay*1000*\n        PerceptibleReciprocal((double) curr->ticks_per_second));\n      prev_image->ticks_per_second = 100L;\n      prev_image->delay = time*prev_image->ticks_per_second/1000;\n    }\n    bgnd_image=CropImage(prev_image,&bounds[i],sans_exception);\n    prev_image=DestroyImage(prev_image);", "project": "ImageMagick", "hash": 137819104896390903763862580984210764323, "size": 451, "commit_id": "ef59bd764f88d893f1219fee8ba696a5d3f8c1c4", "message": "There is a Division by Zero in function OptimizeLayerFrames (#2743)\n\nin file MagickCore/layer.c. cur->ticks_per_seconds can be zero\r\nwith a crafted input argument *image. This is similar to\r\nCVE-2019-13454.", "target": 0, "dataset": "other", "idx": 338501}
{"func": "\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = pbuf->len;\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);", "project": "linux", "hash": 97053759323757789573775019463585321454, "size": 26, "commit_id": "d1f82808877bb10d3deee7cf3374a4eb3fb582db", "message": "io_uring: truncate lengths larger than MAX_RW_COUNT on provide buffers\n\nRead and write operations are capped to MAX_RW_COUNT. Some read ops rely on\nthat limit, and that is not guaranteed by the IORING_OP_PROVIDE_BUFFERS.\n\nTruncate those lengths when doing io_add_buffers, so buffer addresses still\nuse the uncapped length.\n\nAlso, take the chance and change struct io_buffer len member to __u32, so\nit matches struct io_provide_buffer len member.\n\nThis fixes CVE-2021-3491, also reported as ZDI-CAN-13546.\n\nFixes: ddf0322db79c (\"io_uring: add IORING_OP_PROVIDE_BUFFERS\")\nReported-by: Billy Jheng Bing-Jhong (@st424204)\nSigned-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>", "target": 1, "dataset": "other", "idx": 202741}
{"func": "\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = min_t(__u32, pbuf->len, MAX_RW_COUNT);\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);", "project": "linux", "hash": 66027508180468263129870084716869579887, "size": 26, "commit_id": "d1f82808877bb10d3deee7cf3374a4eb3fb582db", "message": "io_uring: truncate lengths larger than MAX_RW_COUNT on provide buffers\n\nRead and write operations are capped to MAX_RW_COUNT. Some read ops rely on\nthat limit, and that is not guaranteed by the IORING_OP_PROVIDE_BUFFERS.\n\nTruncate those lengths when doing io_add_buffers, so buffer addresses still\nuse the uncapped length.\n\nAlso, take the chance and change struct io_buffer len member to __u32, so\nit matches struct io_provide_buffer len member.\n\nThis fixes CVE-2021-3491, also reported as ZDI-CAN-13546.\n\nFixes: ddf0322db79c (\"io_uring: add IORING_OP_PROVIDE_BUFFERS\")\nReported-by: Billy Jheng Bing-Jhong (@st424204)\nSigned-off-by: Thadeu Lima de Souza Cascardo <cascardo@canonical.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>", "target": 0, "dataset": "other", "idx": 338639}
{"func": "\t\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_NONE)\n\t\t\treturn 0;\n\t\t/*\n\t\t * Indirect branch speculation is always disabled in strict\n\t\t * mode.\n\t\t */\n\t\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_STRICT ||\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT ||\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT_PREFERRED)\n\t\t\treturn -EPERM;\n\t\ttask_clear_spec_ib_disable(task);\n\t\ttask_update_spec_tif(task);\n\t\tbreak;\n\tcase PR_SPEC_DISABLE:", "project": "linux", "hash": 132412332318217215818390687833972356536, "size": 41, "commit_id": "4d8df8cbb9156b0a0ab3f802b80cb5db57acc0bf", "message": "x86/speculation: PR_SPEC_FORCE_DISABLE enforcement for indirect branches.\n\nCurrently, it is possible to enable indirect branch speculation even after\nit was force-disabled using the PR_SPEC_FORCE_DISABLE option. Moreover, the\nPR_GET_SPECULATION_CTRL command gives afterwards an incorrect result\n(force-disabled when it is in fact enabled). This also is inconsistent\nvs. STIBP and the documention which cleary states that\nPR_SPEC_FORCE_DISABLE cannot be undone.\n\nFix this by actually enforcing force-disabled indirect branch\nspeculation. PR_SPEC_ENABLE called after PR_SPEC_FORCE_DISABLE now fails\nwith -EPERM as described in the documentation.\n\nFixes: 9137bb27e60e (\"x86/speculation: Add prctl() control for indirect branch speculation\")\nSigned-off-by: Anthony Steinhauser <asteinhauser@google.com>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nCc: stable@vger.kernel.org", "target": 1, "dataset": "other", "idx": 202751}
{"func": "\t\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_NONE)\n\t\t\treturn 0;\n\t\t/*\n\t\t * Indirect branch speculation is always disabled in strict\n\t\t * mode. It can neither be enabled if it was force-disabled\n\t\t * by a  previous prctl call.\n\n\t\t */\n\t\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_STRICT ||\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT ||\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT_PREFERRED ||\n\t\t    task_spec_ib_force_disable(task))\n\t\t\treturn -EPERM;\n\t\ttask_clear_spec_ib_disable(task);\n\t\ttask_update_spec_tif(task);\n\t\tbreak;\n\tcase PR_SPEC_DISABLE:", "project": "linux", "hash": 11919961636460426634217932345711759228, "size": 44, "commit_id": "4d8df8cbb9156b0a0ab3f802b80cb5db57acc0bf", "message": "x86/speculation: PR_SPEC_FORCE_DISABLE enforcement for indirect branches.\n\nCurrently, it is possible to enable indirect branch speculation even after\nit was force-disabled using the PR_SPEC_FORCE_DISABLE option. Moreover, the\nPR_GET_SPECULATION_CTRL command gives afterwards an incorrect result\n(force-disabled when it is in fact enabled). This also is inconsistent\nvs. STIBP and the documention which cleary states that\nPR_SPEC_FORCE_DISABLE cannot be undone.\n\nFix this by actually enforcing force-disabled indirect branch\nspeculation. PR_SPEC_ENABLE called after PR_SPEC_FORCE_DISABLE now fails\nwith -EPERM as described in the documentation.\n\nFixes: 9137bb27e60e (\"x86/speculation: Add prctl() control for indirect branch speculation\")\nSigned-off-by: Anthony Steinhauser <asteinhauser@google.com>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nCc: stable@vger.kernel.org", "target": 0, "dataset": "other", "idx": 338780}
{"func": "\targs.out_args[0].value = &outarg;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tif (fuse_invalid_attr(&outarg.attr) ||\n\t\t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n\t\t\tmake_bad_inode(inode);\n\t\t\terr = -EIO;\n\t\t} else {\n\t\t\tfuse_change_attributes(inode, &outarg.attr,\n\t\t\t\t\t       attr_timeout(&outarg),\n\t\t\t\t\t       attr_version);", "project": "linux", "hash": 66305836614615397501498682867266125877, "size": 45, "commit_id": "5d069dbe8aaf2a197142558b6fb2978189ba3454", "message": "fuse: fix bad inode\n\nJan Kara's analysis of the syzbot report (edited):\n\n  The reproducer opens a directory on FUSE filesystem, it then attaches\n  dnotify mark to the open directory.  After that a fuse_do_getattr() call\n  finds that attributes returned by the server are inconsistent, and calls\n  make_bad_inode() which, among other things does:\n\n          inode->i_mode = S_IFREG;\n\n  This then confuses dnotify which doesn't tear down its structures\n  properly and eventually crashes.\n\nAvoid calling make_bad_inode() on a live inode: switch to a private flag on\nthe fuse inode.  Also add the test to ops which the bad_inode_ops would\nhave caught.\n\nThis bug goes back to the initial merge of fuse in 2.6.14...\n\nReported-by: syzbot+f427adf9324b92652ccc@syzkaller.appspotmail.com\nSigned-off-by: Miklos Szeredi <mszeredi@redhat.com>\nTested-by: Jan Kara <jack@suse.cz>\nCc: <stable@vger.kernel.org>", "target": 1, "dataset": "other", "idx": 202842}
{"func": "\targs.out_args[0].value = &outarg;\n\terr = fuse_simple_request(fm, &args);\n\tif (!err) {\n\t\tif (fuse_invalid_attr(&outarg.attr) ||\n\t\t    (inode->i_mode ^ outarg.attr.mode) & S_IFMT) {\n\t\t\tfuse_make_bad(inode);\n\t\t\terr = -EIO;\n\t\t} else {\n\t\t\tfuse_change_attributes(inode, &outarg.attr,\n\t\t\t\t\t       attr_timeout(&outarg),\n\t\t\t\t\t       attr_version);", "project": "linux", "hash": 207137391981469494967408362501142420848, "size": 45, "commit_id": "5d069dbe8aaf2a197142558b6fb2978189ba3454", "message": "fuse: fix bad inode\n\nJan Kara's analysis of the syzbot report (edited):\n\n  The reproducer opens a directory on FUSE filesystem, it then attaches\n  dnotify mark to the open directory.  After that a fuse_do_getattr() call\n  finds that attributes returned by the server are inconsistent, and calls\n  make_bad_inode() which, among other things does:\n\n          inode->i_mode = S_IFREG;\n\n  This then confuses dnotify which doesn't tear down its structures\n  properly and eventually crashes.\n\nAvoid calling make_bad_inode() on a live inode: switch to a private flag on\nthe fuse inode.  Also add the test to ops which the bad_inode_ops would\nhave caught.\n\nThis bug goes back to the initial merge of fuse in 2.6.14...\n\nReported-by: syzbot+f427adf9324b92652ccc@syzkaller.appspotmail.com\nSigned-off-by: Miklos Szeredi <mszeredi@redhat.com>\nTested-by: Jan Kara <jack@suse.cz>\nCc: <stable@vger.kernel.org>", "target": 0, "dataset": "other", "idx": 342120}
{"func": "                const byte *src = data + jbig2_huffman_offset(hs);\n                const int stride = (image->width >> 3) + ((image->width & 7) ? 1 : 0);\n                byte *dst = image->data;\n\n                /* SumatraPDF: prevent read access violation */\n                if (size - jbig2_huffman_offset(hs) < image->height * stride) {\n                    jbig2_error(ctx, JBIG2_SEVERITY_FATAL, segment->number, \"not enough data for decoding (%d/%d)\", image->height * stride,\n                                size - jbig2_huffman_offset(hs));\n                    jbig2_image_release(ctx, image);\n                    goto cleanup4;\n                }", "project": "ghostpdl", "hash": 151781195134694545420505614324781028208, "size": 568, "commit_id": "b184e783702246e154294326d03d9abda669fcfa", "message": "Bug 697703: Prevent integer overflow vulnerability.\n\nAdd extra check for the offset being greater than the size\nof the image and hence reading off the end of the buffer.\n\nThank you to Dai Ge for finding this issue and suggesting a patch.", "target": 1, "dataset": "other", "idx": 202890}
{"func": "                const byte *src = data + jbig2_huffman_offset(hs);\n                const int stride = (image->width >> 3) + ((image->width & 7) ? 1 : 0);\n                byte *dst = image->data;\n\n                /* SumatraPDF: prevent read access violation */\n                if ((size - jbig2_huffman_offset(hs) < image->height * stride) || (size < jbig2_huffman_offset(hs))) {\n                    jbig2_error(ctx, JBIG2_SEVERITY_FATAL, segment->number, \"not enough data for decoding (%d/%d)\", image->height * stride,\n                                size - jbig2_huffman_offset(hs));\n                    jbig2_image_release(ctx, image);\n                    goto cleanup4;\n                }", "project": "ghostpdl", "hash": 263544315846066824825119697673339734069, "size": 568, "commit_id": "b184e783702246e154294326d03d9abda669fcfa", "message": "Bug 697703: Prevent integer overflow vulnerability.\n\nAdd extra check for the offset being greater than the size\nof the image and hence reading off the end of the buffer.\n\nThank you to Dai Ge for finding this issue and suggesting a patch.", "target": 0, "dataset": "other", "idx": 343184}
{"func": "{\n\tstruct kvm_vcpu *vcpu;\n\tstruct kvm_vcpu_hv_synic *synic;\n\n\tvcpu = get_vcpu_by_vpidx(kvm, vpidx);\n\tif (!vcpu)\n\t\treturn NULL;\n\tsynic = to_hv_synic(vcpu);\n\treturn (synic->active) ? synic : NULL;\n}", "project": "linux", "hash": 276482777374283635842814384243075236447, "size": 11, "commit_id": "919f4ebc598701670e80e31573a58f1f2d2bf918", "message": "KVM: x86: hyper-v: Fix Hyper-V context null-ptr-deref\n\nReported by syzkaller:\n\n    KASAN: null-ptr-deref in range [0x0000000000000140-0x0000000000000147]\n    CPU: 1 PID: 8370 Comm: syz-executor859 Not tainted 5.11.0-syzkaller #0\n    RIP: 0010:synic_get arch/x86/kvm/hyperv.c:165 [inline]\n    RIP: 0010:kvm_hv_set_sint_gsi arch/x86/kvm/hyperv.c:475 [inline]\n    RIP: 0010:kvm_hv_irq_routing_update+0x230/0x460 arch/x86/kvm/hyperv.c:498\n    Call Trace:\n     kvm_set_irq_routing+0x69b/0x940 arch/x86/kvm/../../../virt/kvm/irqchip.c:223\n     kvm_vm_ioctl+0x12d0/0x2800 arch/x86/kvm/../../../virt/kvm/kvm_main.c:3959\n     vfs_ioctl fs/ioctl.c:48 [inline]\n     __do_sys_ioctl fs/ioctl.c:753 [inline]\n     __se_sys_ioctl fs/ioctl.c:739 [inline]\n     __x64_sys_ioctl+0x193/0x200 fs/ioctl.c:739\n     do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nHyper-V context is lazily allocated until Hyper-V specific MSRs are accessed\nor SynIC is enabled. However, the syzkaller testcase sets irq routing table\ndirectly w/o enabling SynIC. This results in null-ptr-deref when accessing\nSynIC Hyper-V context. This patch fixes it.\n\nsyzkaller source: https://syzkaller.appspot.com/x/repro.c?x=163342ccd00000\n\nReported-by: syzbot+6987f3b2dbd9eda95f12@syzkaller.appspotmail.com\nFixes: 8f014550dfb1 (\"KVM: x86: hyper-v: Make Hyper-V emulation enablement conditional\")\nSigned-off-by: Wanpeng Li <wanpengli@tencent.com>\nMessage-Id: <1614326399-5762-1-git-send-email-wanpengli@tencent.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 202898}
{"func": "{\n\tstruct kvm_vcpu *vcpu;\n\tstruct kvm_vcpu_hv_synic *synic;\n\n\tvcpu = get_vcpu_by_vpidx(kvm, vpidx);\n\tif (!vcpu || !to_hv_vcpu(vcpu))\n\t\treturn NULL;\n\tsynic = to_hv_synic(vcpu);\n\treturn (synic->active) ? synic : NULL;\n}", "project": "linux", "hash": 141367306392052616744625319702569801368, "size": 11, "commit_id": "919f4ebc598701670e80e31573a58f1f2d2bf918", "message": "KVM: x86: hyper-v: Fix Hyper-V context null-ptr-deref\n\nReported by syzkaller:\n\n    KASAN: null-ptr-deref in range [0x0000000000000140-0x0000000000000147]\n    CPU: 1 PID: 8370 Comm: syz-executor859 Not tainted 5.11.0-syzkaller #0\n    RIP: 0010:synic_get arch/x86/kvm/hyperv.c:165 [inline]\n    RIP: 0010:kvm_hv_set_sint_gsi arch/x86/kvm/hyperv.c:475 [inline]\n    RIP: 0010:kvm_hv_irq_routing_update+0x230/0x460 arch/x86/kvm/hyperv.c:498\n    Call Trace:\n     kvm_set_irq_routing+0x69b/0x940 arch/x86/kvm/../../../virt/kvm/irqchip.c:223\n     kvm_vm_ioctl+0x12d0/0x2800 arch/x86/kvm/../../../virt/kvm/kvm_main.c:3959\n     vfs_ioctl fs/ioctl.c:48 [inline]\n     __do_sys_ioctl fs/ioctl.c:753 [inline]\n     __se_sys_ioctl fs/ioctl.c:739 [inline]\n     __x64_sys_ioctl+0x193/0x200 fs/ioctl.c:739\n     do_syscall_64+0x2d/0x70 arch/x86/entry/common.c:46\n     entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nHyper-V context is lazily allocated until Hyper-V specific MSRs are accessed\nor SynIC is enabled. However, the syzkaller testcase sets irq routing table\ndirectly w/o enabling SynIC. This results in null-ptr-deref when accessing\nSynIC Hyper-V context. This patch fixes it.\n\nsyzkaller source: https://syzkaller.appspot.com/x/repro.c?x=163342ccd00000\n\nReported-by: syzbot+6987f3b2dbd9eda95f12@syzkaller.appspotmail.com\nFixes: 8f014550dfb1 (\"KVM: x86: hyper-v: Make Hyper-V emulation enablement conditional\")\nSigned-off-by: Wanpeng Li <wanpengli@tencent.com>\nMessage-Id: <1614326399-5762-1-git-send-email-wanpengli@tencent.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 343525}
{"func": "int ssh_buffer_add_data(struct ssh_buffer_struct *buffer, const void *data, uint32_t len)\n{\n  buffer_verify(buffer);\n\n  if (data == NULL) {\n      return -1;\n  }\n\n  if (buffer->used + len < len) {\n    return -1;\n  }\n\n  if (buffer->allocated < (buffer->used + len)) {\n    if(buffer->pos > 0)\n      buffer_shift(buffer);\n    if (realloc_buffer(buffer, buffer->used + len) < 0) {\n      return -1;\n    }\n  }\n\n  memcpy(buffer->data+buffer->used, data, len);\n  buffer->used+=len;\n  buffer_verify(buffer);\n  return 0;\n}", "project": "libssh-mirror", "hash": 245892501454401043574922830759233496776, "size": 25, "commit_id": "10b3ebbe61a7031a3dae97f05834442220447181", "message": "buffer: Reformat ssh_buffer_add_data()\n\nSigned-off-by: Andreas Schneider <asn@cryptomilk.org>\nReviewed-by: Anderson Toshiyuki Sasaki <ansasaki@redhat.com>\nReviewed-by: Jakub Jelen <jjelen@redhat.com>", "target": 1, "dataset": "other", "idx": 203616}
{"func": "void *ssh_buffer_allocate(struct ssh_buffer_struct *buffer, uint32_t len)\n{\n    void *ptr;\n    buffer_verify(buffer);\n\n    if (buffer->used + len < len) {\n        return NULL;\n    }\n\n    if (buffer->allocated < (buffer->used + len)) {\n        if (buffer->pos > 0) {\n            buffer_shift(buffer);\n        }\n\n        if (realloc_buffer(buffer, buffer->used + len) < 0) {\n            return NULL;\n        }\n    }\n\n    ptr = buffer->data + buffer->used;\n    buffer->used+=len;\n    buffer_verify(buffer);\n\n    return ptr;\n}", "project": "libssh-mirror", "hash": 213736684976097276551610086730047123660, "size": 25, "commit_id": "10b3ebbe61a7031a3dae97f05834442220447181", "message": "buffer: Reformat ssh_buffer_add_data()\n\nSigned-off-by: Andreas Schneider <asn@cryptomilk.org>\nReviewed-by: Anderson Toshiyuki Sasaki <ansasaki@redhat.com>\nReviewed-by: Jakub Jelen <jjelen@redhat.com>", "target": 0, "dataset": "other", "idx": 345161}
{"func": "static int usb_host_handle_control(USBHostDevice *s, USBPacket *p)\n{\n    struct usbdevfs_urb *urb;\n    AsyncURB *aurb;\n    int ret, value, index;\n\n    /* \n     * Process certain standard device requests.\n     * These are infrequent and are processed synchronously.\n     */\n    if (s->ctrl.req.bRequestType == 1 &&\n                  s->ctrl.req.bRequest == USB_REQ_SET_INTERFACE)\n        return usb_host_set_interface(s, index, value);\n\n    /* The rest are asynchronous */\n\n    aurb = async_alloc();\n    aurb->hdev   = s;\n    aurb->packet = p;\n\n    /* \n\n    urb->type     = USBDEVFS_URB_TYPE_CONTROL;\n    urb->endpoint = p->devep;\n\n    urb->buffer        = &s->ctrl.req;\n    urb->buffer_length = 8 + s->ctrl.len;\n\n    urb->usercontext = s;\n\n    ret = ioctl(s->fd, USBDEVFS_SUBMITURB, urb);\n", "project": "qemu", "hash": 312416163750000928776897162836846138475, "size": 73, "commit_id": "babd03fde68093482528010a5435c14ce9128e3f", "message": "usb-linux.c: fix buffer overflow\n\nIn usb-linux.c:usb_host_handle_control, we pass a 1024-byte buffer and\nlength to the kernel.  However, the length was provided by the caller\nof dev->handle_packet, and is not checked, so the kernel might provide\ntoo much data and overflow our buffer.\n\nFor example, hw/usb-uhci.c could set the length to 2047.\nhw/usb-ohci.c looks like it might go up to 4096 or 8192.\n\nThis causes a qemu crash, as reported here:\n  http://www.mail-archive.com/kvm@vger.kernel.org/msg18447.html\n\nThis patch increases the usb-linux.c buffer size to 2048 to fix the\nspecific device reported, and adds a check to avoid the overflow in\nany case.\n\nSigned-off-by: Jim Paris <jim@jtan.com>\nSigned-off-by: Anthony Liguori <aliguori@us.ibm.com>", "target": 1, "dataset": "other", "idx": 203887}
{"func": "static int usb_host_handle_control(USBHostDevice *s, USBPacket *p)\n{\n    struct usbdevfs_urb *urb;\n    AsyncURB *aurb;\n    int ret, value, index;\n    int buffer_len;\n\n    /* \n     * Process certain standard device requests.\n     * These are infrequent and are processed synchronously.\n     */\n                  s->ctrl.req.bRequest == USB_REQ_SET_INTERFACE)\n        return usb_host_set_interface(s, index, value);\n\n    /* The rest are asynchronous */\n\n    buffer_len = 8 + s->ctrl.len;\n    if (buffer_len > sizeof(s->ctrl.buffer)) {\n\t    fprintf(stderr, \"husb: ctrl buffer too small (%u > %lu)\\n\",\n\t\t    buffer_len, sizeof(s->ctrl.buffer));\n\t    return USB_RET_STALL;\n    }\n\n    aurb = async_alloc();\n    aurb->hdev   = s;\n    aurb->packet = p;\n\n    /* \n\n    urb->type     = USBDEVFS_URB_TYPE_CONTROL;\n    urb->endpoint = p->devep;\n\n    urb->buffer        = &s->ctrl.req;\n    urb->buffer_length = buffer_len;\n\n    urb->usercontext = s;\n\n    ret = ioctl(s->fd, USBDEVFS_SUBMITURB, urb);\n", "project": "qemu", "hash": 135771876040172573855391231658010318081, "size": 81, "commit_id": "babd03fde68093482528010a5435c14ce9128e3f", "message": "usb-linux.c: fix buffer overflow\n\nIn usb-linux.c:usb_host_handle_control, we pass a 1024-byte buffer and\nlength to the kernel.  However, the length was provided by the caller\nof dev->handle_packet, and is not checked, so the kernel might provide\ntoo much data and overflow our buffer.\n\nFor example, hw/usb-uhci.c could set the length to 2047.\nhw/usb-ohci.c looks like it might go up to 4096 or 8192.\n\nThis causes a qemu crash, as reported here:\n  http://www.mail-archive.com/kvm@vger.kernel.org/msg18447.html\n\nThis patch increases the usb-linux.c buffer size to 2048 to fix the\nspecific device reported, and adds a check to avoid the overflow in\nany case.\n\nSigned-off-by: Jim Paris <jim@jtan.com>\nSigned-off-by: Anthony Liguori <aliguori@us.ibm.com>", "target": 0, "dataset": "other", "idx": 346299}
{"func": "\tsize_t remain, to_alloc;\n\tint result = -1;\n\n\tEVBUFFER_LOCK(buf);\n\n\tif (buf->freeze_end) {\n\t\tgoto done;\n\t}\n\n\tchain = buf->last;\n\n\t\t\tgoto done;\n\t\tevbuffer_chain_insert(buf, chain);\n\t}\n\n\tif ((chain->flags & EVBUFFER_IMMUTABLE) == 0) {\n\t\tremain = (size_t)(chain->buffer_len - chain->misalign - chain->off);\n\t\tif (remain >= datlen) {\n\t\t\t/* there's enough space to hold all the data in the\n\t\t\t * current last chain */\n\t\t\tmemcpy(chain->buffer + chain->misalign + chain->off,\n\t\t\t    data, datlen);", "project": "libevent", "hash": 22227894411124508117434949505991372637, "size": 84, "commit_id": "20d6d4458bee5d88bda1511c225c25b2d3198d6c", "message": "Fix CVE-2014-6272 in Libevent 2.0\n\nFor this fix, we need to make sure that passing too-large inputs to\nthe evbuffer functions can't make us do bad things with the heap.\n\nAlso, lower the maximum chunk size to the lower of off_t, size_t maximum.\n\nThis is necessary since otherwise we could get into an infinite loop\nif we make a chunk that 'misalign' cannot index into.", "target": 1, "dataset": "other", "idx": 203893}
{"func": "\tint result = -1;\n\n\tEVBUFFER_LOCK(buf);\n\n\tif (buf->freeze_end) {\n\t\tgoto done;\n\t}\n\t/* Prevent buf->total_len overflow */\n\tif (datlen > EV_SIZE_MAX - buf->total_len) {\n\t\tgoto done;\n\t}\n\n\tchain = buf->last;\n\n\t\t\tgoto done;\n\t\tevbuffer_chain_insert(buf, chain);\n\t}\n\n\tif ((chain->flags & EVBUFFER_IMMUTABLE) == 0) {\n\t\t/* Always true for mutable buffers */\n\t\tEVUTIL_ASSERT(chain->misalign >= 0 &&\n\t\t    (ev_uint64_t)chain->misalign <= EVBUFFER_CHAIN_MAX);\n\t\tremain = chain->buffer_len - (size_t)chain->misalign - chain->off;\n\t\tif (remain >= datlen) {\n\t\t\t/* there's enough space to hold all the data in the\n\t\t\t * current last chain */\n\t\t\tmemcpy(chain->buffer + chain->misalign + chain->off,\n\t\t\t    data, datlen);", "project": "libevent", "hash": 52044498904590272412051991226803272284, "size": 91, "commit_id": "20d6d4458bee5d88bda1511c225c25b2d3198d6c", "message": "Fix CVE-2014-6272 in Libevent 2.0\n\nFor this fix, we need to make sure that passing too-large inputs to\nthe evbuffer functions can't make us do bad things with the heap.\n\nAlso, lower the maximum chunk size to the lower of off_t, size_t maximum.\n\nThis is necessary since otherwise we could get into an infinite loop\nif we make a chunk that 'misalign' cannot index into.", "target": 0, "dataset": "other", "idx": 346340}
{"func": "\t} else if (len >= 0x7fffffff) {\n\t\tp11_buffer_fail (buf);\n\t\treturn false;\n\t}\n\n\tif (buf->len < len || *offset > buf->len - len) {\n\t\tp11_buffer_fail (buf);\n\t\treturn false;\n\t}\n\n\tif (data)", "project": "p11-kit", "hash": 81431764540984656735179160726380243041, "size": 34, "commit_id": "69d751ca9df9ac101adfb1e5aa7e83e3358106ba", "message": "Fix bounds check in p11_rpc_buffer_get_byte_array\n\nThis bounds check should be using off, not *offset, because it has been\nadvanced four bytes from reading a uint32 earlier in the function.\nAdditionally, the pointer that is returned is computed using off, not\n*offset.", "target": 1, "dataset": "other", "idx": 204031}
{"func": "\t} else if (len >= 0x7fffffff) {\n\t\tp11_buffer_fail (buf);\n\t\treturn false;\n\t}\n\n\tif (buf->len < len || off > buf->len - len) {\n\t\tp11_buffer_fail (buf);\n\t\treturn false;\n\t}\n\n\tif (data)", "project": "p11-kit", "hash": 69269050861741048277937073880239113087, "size": 34, "commit_id": "69d751ca9df9ac101adfb1e5aa7e83e3358106ba", "message": "Fix bounds check in p11_rpc_buffer_get_byte_array\n\nThis bounds check should be using off, not *offset, because it has been\nadvanced four bytes from reading a uint32 earlier in the function.\nAdditionally, the pointer that is returned is computed using off, not\n*offset.", "target": 0, "dataset": "other", "idx": 349479}
{"func": "            pixEndianTwoByteSwap(pix);\n        LEPT_FREE(linebuf);\n    } else if (spp == 2 && bps == 8) {  /* gray plus alpha */\n        L_INFO(\"gray+alpha is not supported; converting to RGBA\\n\", procName);\n        pixSetSpp(pix, 4);\n        linebuf = (l_uint8 *)LEPT_CALLOC(tiffbpl + 1, sizeof(l_uint8));\n        pixdata = pixGetData(pix);\n        for (i = 0; i < h; i++) {\n            if (TIFFReadScanline(tif, linebuf, i, 0) < 0) {\n                LEPT_FREE(linebuf);\n                pixDestroy(&pix);", "project": "leptonica", "hash": 132046392038694907222335403052712221680, "size": 264, "commit_id": "5ba34b1fe741d69d43a6c8cf767756997eadd87c", "message": "Issue 23654 in oss-fuzz: Heap-buffer-overflow in pixReadFromTiffStream\n  * Increase scanline buffer for reading gray+alpha and converting to RGBA", "target": 1, "dataset": "other", "idx": 204059}
{"func": "            pixEndianTwoByteSwap(pix);\n        LEPT_FREE(linebuf);\n    } else if (spp == 2 && bps == 8) {  /* gray plus alpha */\n        L_INFO(\"gray+alpha is not supported; converting to RGBA\\n\", procName);\n        pixSetSpp(pix, 4);\n        linebuf = (l_uint8 *)LEPT_CALLOC(2 * tiffbpl + 1, sizeof(l_uint8));\n        pixdata = pixGetData(pix);\n        for (i = 0; i < h; i++) {\n            if (TIFFReadScanline(tif, linebuf, i, 0) < 0) {\n                LEPT_FREE(linebuf);\n                pixDestroy(&pix);", "project": "leptonica", "hash": 288150895440705810941702661710210262875, "size": 264, "commit_id": "5ba34b1fe741d69d43a6c8cf767756997eadd87c", "message": "Issue 23654 in oss-fuzz: Heap-buffer-overflow in pixReadFromTiffStream\n  * Increase scanline buffer for reading gray+alpha and converting to RGBA", "target": 0, "dataset": "other", "idx": 350367}
{"func": "\n\t/*\n\t * Simulate keyup event if keycode is not present\n\t * in the keymap anymore\n\t */\n\tif (test_bit(EV_KEY, dev->evbit) &&\n\t    !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&\n\t    __test_and_clear_bit(old_keycode, dev->key)) {\n\t\tstruct input_value vals[] =  {\n\t\t\t{ EV_KEY, old_keycode, 0 },\n\t\t\tinput_value_sync\n\t\t};\n", "project": "linux", "hash": 337446137803997904056500807170028096296, "size": 39, "commit_id": "cb222aed03d798fc074be55e59d9a112338ee784", "message": "Input: add safety guards to input_set_keycode()\n\nIf we happen to have a garbage in input device's keycode table with values\ntoo big we'll end up doing clear_bit() with offset way outside of our\nbitmaps, damaging other objects within an input device or even outside of\nit. Let's add sanity checks to the returned old keycodes.\n\nReported-by: syzbot+c769968809f9359b07aa@syzkaller.appspotmail.com\nReported-by: syzbot+76f3a30e88d256644c78@syzkaller.appspotmail.com\nLink: https://lore.kernel.org/r/20191207212757.GA245964@dtor-ws\nSigned-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>", "target": 1, "dataset": "other", "idx": 204147}
{"func": "\n\t/*\n\t * Simulate keyup event if keycode is not present\n\t * in the keymap anymore\n\t */\n\tif (old_keycode > KEY_MAX) {\n\t\tdev_warn(dev->dev.parent ?: &dev->dev,\n\t\t\t \"%s: got too big old keycode %#x\\n\",\n\t\t\t __func__, old_keycode);\n\t} else if (test_bit(EV_KEY, dev->evbit) &&\n\t\t   !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&\n\t\t   __test_and_clear_bit(old_keycode, dev->key)) {\n\t\tstruct input_value vals[] =  {\n\t\t\t{ EV_KEY, old_keycode, 0 },\n\t\t\tinput_value_sync\n\t\t};\n", "project": "linux", "hash": 250661356441538656426884342920937476021, "size": 43, "commit_id": "cb222aed03d798fc074be55e59d9a112338ee784", "message": "Input: add safety guards to input_set_keycode()\n\nIf we happen to have a garbage in input device's keycode table with values\ntoo big we'll end up doing clear_bit() with offset way outside of our\nbitmaps, damaging other objects within an input device or even outside of\nit. Let's add sanity checks to the returned old keycodes.\n\nReported-by: syzbot+c769968809f9359b07aa@syzkaller.appspotmail.com\nReported-by: syzbot+76f3a30e88d256644c78@syzkaller.appspotmail.com\nLink: https://lore.kernel.org/r/20191207212757.GA245964@dtor-ws\nSigned-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>", "target": 0, "dataset": "other", "idx": 353362}
{"func": "static int kvm_s390_get_cmma(struct kvm *kvm, struct kvm_s390_cmma_log *args,\n\t\t\t     u8 *res, unsigned long bufsize)\n{\n\tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *ms;\n\n\tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n\tms = gfn_to_memslot(kvm, cur_gfn);\n\targs->count = 0;\n\targs->start_gfn = cur_gfn;", "project": "linux", "hash": 69648874825348699945429887721025870530, "size": 47, "commit_id": "0774a964ef561b7170d8d1b1bfe6f88002b6d219", "message": "KVM: Fix out of range accesses to memslots\n\nReset the LRU slot if it becomes invalid when deleting a memslot to fix\nan out-of-bounds/use-after-free access when searching through memslots.\n\nExplicitly check for there being no used slots in search_memslots(), and\nin the caller of s390's approximation variant.\n\nFixes: 36947254e5f9 (\"KVM: Dynamically size memslot array based on number of used slots\")\nReported-by: Qian Cai <cai@lca.pw>\nCc: Peter Xu <peterx@redhat.com>\nSigned-off-by: Sean Christopherson <sean.j.christopherson@intel.com>\nMessage-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>\nAcked-by: Christian Borntraeger <borntraeger@de.ibm.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 204189}
{"func": "\t\t\t     u8 *res, unsigned long bufsize)\n{\n\tunsigned long mem_end, cur_gfn, next_gfn, hva, pgstev;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *ms;\n\n\tif (unlikely(!slots->used_slots))\n\t\treturn 0;\n\n\tcur_gfn = kvm_s390_next_dirty_cmma(slots, args->start_gfn);\n\tms = gfn_to_memslot(kvm, cur_gfn);\n\targs->count = 0;\n\targs->start_gfn = cur_gfn;", "project": "linux", "hash": 13699562725589835050195546967713430426, "size": 50, "commit_id": "0774a964ef561b7170d8d1b1bfe6f88002b6d219", "message": "KVM: Fix out of range accesses to memslots\n\nReset the LRU slot if it becomes invalid when deleting a memslot to fix\nan out-of-bounds/use-after-free access when searching through memslots.\n\nExplicitly check for there being no used slots in search_memslots(), and\nin the caller of s390's approximation variant.\n\nFixes: 36947254e5f9 (\"KVM: Dynamically size memslot array based on number of used slots\")\nReported-by: Qian Cai <cai@lca.pw>\nCc: Peter Xu <peterx@redhat.com>\nSigned-off-by: Sean Christopherson <sean.j.christopherson@intel.com>\nMessage-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>\nAcked-by: Christian Borntraeger <borntraeger@de.ibm.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 354690}
{"func": "\n\tif (WARN_ON(slots->id_to_index[memslot->id] == -1))\n\t\treturn;\n\n\tslots->used_slots--;\n\n\tfor (i = slots->id_to_index[memslot->id]; i < slots->used_slots; i++) {\n\t\tmslots[i] = mslots[i + 1];\n\t\tslots->id_to_index[mslots[i].id] = i;\n\t}\n\tmslots[i] = *memslot;", "project": "linux", "hash": 222311292455674309157254629171646173763, "size": 18, "commit_id": "0774a964ef561b7170d8d1b1bfe6f88002b6d219", "message": "KVM: Fix out of range accesses to memslots\n\nReset the LRU slot if it becomes invalid when deleting a memslot to fix\nan out-of-bounds/use-after-free access when searching through memslots.\n\nExplicitly check for there being no used slots in search_memslots(), and\nin the caller of s390's approximation variant.\n\nFixes: 36947254e5f9 (\"KVM: Dynamically size memslot array based on number of used slots\")\nReported-by: Qian Cai <cai@lca.pw>\nCc: Peter Xu <peterx@redhat.com>\nSigned-off-by: Sean Christopherson <sean.j.christopherson@intel.com>\nMessage-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>\nAcked-by: Christian Borntraeger <borntraeger@de.ibm.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 204190}
{"func": "\tif (WARN_ON(slots->id_to_index[memslot->id] == -1))\n\t\treturn;\n\n\tslots->used_slots--;\n\n\tif (atomic_read(&slots->lru_slot) >= slots->used_slots)\n\t\tatomic_set(&slots->lru_slot, 0);\n\n\tfor (i = slots->id_to_index[memslot->id]; i < slots->used_slots; i++) {\n\t\tmslots[i] = mslots[i + 1];\n\t\tslots->id_to_index[mslots[i].id] = i;\n\t}\n\tmslots[i] = *memslot;", "project": "linux", "hash": 281781368969257104938303693558751249981, "size": 21, "commit_id": "0774a964ef561b7170d8d1b1bfe6f88002b6d219", "message": "KVM: Fix out of range accesses to memslots\n\nReset the LRU slot if it becomes invalid when deleting a memslot to fix\nan out-of-bounds/use-after-free access when searching through memslots.\n\nExplicitly check for there being no used slots in search_memslots(), and\nin the caller of s390's approximation variant.\n\nFixes: 36947254e5f9 (\"KVM: Dynamically size memslot array based on number of used slots\")\nReported-by: Qian Cai <cai@lca.pw>\nCc: Peter Xu <peterx@redhat.com>\nSigned-off-by: Sean Christopherson <sean.j.christopherson@intel.com>\nMessage-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>\nAcked-by: Christian Borntraeger <borntraeger@de.ibm.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 354527}
{"func": "search_memslots(struct kvm_memslots *slots, gfn_t gfn)\n{\n\tint start = 0, end = slots->used_slots;\n\tint slot = atomic_read(&slots->lru_slot);\n\tstruct kvm_memory_slot *memslots = slots->memslots;\n\n\tif (gfn >= memslots[slot].base_gfn &&\n\t    gfn < memslots[slot].base_gfn + memslots[slot].npages)\n\t\treturn &memslots[slot];\n", "project": "linux", "hash": 30729153982995968815676335862978537577, "size": 27, "commit_id": "0774a964ef561b7170d8d1b1bfe6f88002b6d219", "message": "KVM: Fix out of range accesses to memslots\n\nReset the LRU slot if it becomes invalid when deleting a memslot to fix\nan out-of-bounds/use-after-free access when searching through memslots.\n\nExplicitly check for there being no used slots in search_memslots(), and\nin the caller of s390's approximation variant.\n\nFixes: 36947254e5f9 (\"KVM: Dynamically size memslot array based on number of used slots\")\nReported-by: Qian Cai <cai@lca.pw>\nCc: Peter Xu <peterx@redhat.com>\nSigned-off-by: Sean Christopherson <sean.j.christopherson@intel.com>\nMessage-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>\nAcked-by: Christian Borntraeger <borntraeger@de.ibm.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 204191}
{"func": "search_memslots(struct kvm_memslots *slots, gfn_t gfn)\n{\n\tint start = 0, end = slots->used_slots;\n\tint slot = atomic_read(&slots->lru_slot);\n\tstruct kvm_memory_slot *memslots = slots->memslots;\n\n\tif (unlikely(!slots->used_slots))\n\t\treturn NULL;\n\n\tif (gfn >= memslots[slot].base_gfn &&\n\t    gfn < memslots[slot].base_gfn + memslots[slot].npages)\n\t\treturn &memslots[slot];\n", "project": "linux", "hash": 235844977515726339339926851382064351419, "size": 30, "commit_id": "0774a964ef561b7170d8d1b1bfe6f88002b6d219", "message": "KVM: Fix out of range accesses to memslots\n\nReset the LRU slot if it becomes invalid when deleting a memslot to fix\nan out-of-bounds/use-after-free access when searching through memslots.\n\nExplicitly check for there being no used slots in search_memslots(), and\nin the caller of s390's approximation variant.\n\nFixes: 36947254e5f9 (\"KVM: Dynamically size memslot array based on number of used slots\")\nReported-by: Qian Cai <cai@lca.pw>\nCc: Peter Xu <peterx@redhat.com>\nSigned-off-by: Sean Christopherson <sean.j.christopherson@intel.com>\nMessage-Id: <20200320205546.2396-2-sean.j.christopherson@intel.com>\nAcked-by: Christian Borntraeger <borntraeger@de.ibm.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 354422}
{"func": "\t\t\t\tif (find_mount_flag(s, len, &on, &flag)) {\n\t\t\t\t\tif (on)\n\t\t\t\t\t\tflags |= flag;\n\t\t\t\t\telse\n\t\t\t\t\t\tflags  &= ~flag;\n\t\t\t\t} else {\n\t\t\t\t\tmemcpy(d, s, len);\n\t\t\t\t\td += len;\n\t\t\t\t\t*d++ = ',';\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ts += len;\n\t\tif (*s)", "project": "libfuse", "hash": 28697076378881475463957753142520709775, "size": 165, "commit_id": "5018a0c016495155ee598b7e0167b43d5d902414", "message": "fusermount: refuse unknown options\n\nBlacklists are notoriously fragile; especially if the kernel wishes to add\nsome security-critical mount option at a later date, all existing systems\nwith older versions of fusermount installed will suddenly have a security\nproblem.\nAdditionally, if the kernel's option parsing became a tiny bit laxer, the\nblacklist could probably be bypassed.\n\nWhitelist known-harmless flags instead, even if it's slightly more\ninconvenient.", "target": 1, "dataset": "other", "idx": 204216}
{"func": "\t\t\t\tif (find_mount_flag(s, len, &on, &flag)) {\n\t\t\t\t\tif (on)\n\t\t\t\t\t\tflags |= flag;\n\t\t\t\t\telse\n\t\t\t\t\t\tflags  &= ~flag;\n\t\t\t\t} else if (opt_eq(s, len, \"default_permissions\") ||\n\t\t\t\t\t   opt_eq(s, len, \"allow_other\") ||\n\t\t\t\t\t   begins_with(s, \"max_read=\") ||\n\t\t\t\t\t   begins_with(s, \"blksize=\")) {\n\t\t\t\t\tmemcpy(d, s, len);\n\t\t\t\t\td += len;\n\t\t\t\t\t*d++ = ',';\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"%s: unknown option '%.*s'\\n\", progname, len, s);\n\t\t\t\t\texit(1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ts += len;\n\t\tif (*s)", "project": "libfuse", "hash": 91910532426858602413944803061438595958, "size": 171, "commit_id": "5018a0c016495155ee598b7e0167b43d5d902414", "message": "fusermount: refuse unknown options\n\nBlacklists are notoriously fragile; especially if the kernel wishes to add\nsome security-critical mount option at a later date, all existing systems\nwith older versions of fusermount installed will suddenly have a security\nproblem.\nAdditionally, if the kernel's option parsing became a tiny bit laxer, the\nblacklist could probably be bypassed.\n\nWhitelist known-harmless flags instead, even if it's slightly more\ninconvenient.", "target": 0, "dataset": "other", "idx": 355497}
{"func": "            return -1;\n        }\n        if (ret == LZMA_DATA_ERROR) {\n            xz_error(state, LZMA_DATA_ERROR, \"compressed data error\");\n            return -1;\n        }\n    } while (strm->avail_out && ret != LZMA_STREAM_END);\n\n    /* update available output and crc check value */\n    state->have = had - strm->avail_out;\n    state->next = strm->next_out - state->have;", "project": "libxml2", "hash": 325103388462275924711924070031298132490, "size": 98, "commit_id": "f0709e3ca8f8947f2d91ed34e92e38a4c23eae63", "message": "CVE-2015-8035 Fix XZ compression support loop\n\nFor https://bugzilla.gnome.org/show_bug.cgi?id=757466\nDoS when parsing specially crafted XML document if XZ support\nis compiled in (which wasn't the case for 2.9.2 and master since\nNov 2013, fixed in next commit !)", "target": 1, "dataset": "other", "idx": 204281}
{"func": "        }\n        if (ret == LZMA_DATA_ERROR) {\n            xz_error(state, LZMA_DATA_ERROR, \"compressed data error\");\n            return -1;\n        }\n        if (ret == LZMA_PROG_ERROR) {\n            xz_error(state, LZMA_PROG_ERROR, \"compression error\");\n            return -1;\n        }\n    } while (strm->avail_out && ret != LZMA_STREAM_END);\n\n    /* update available output and crc check value */\n    state->have = had - strm->avail_out;\n    state->next = strm->next_out - state->have;", "project": "libxml2", "hash": 282138316863849492396180930649832436502, "size": 102, "commit_id": "f0709e3ca8f8947f2d91ed34e92e38a4c23eae63", "message": "CVE-2015-8035 Fix XZ compression support loop\n\nFor https://bugzilla.gnome.org/show_bug.cgi?id=757466\nDoS when parsing specially crafted XML document if XZ support\nis compiled in (which wasn't the case for 2.9.2 and master since\nNov 2013, fixed in next commit !)", "target": 0, "dataset": "other", "idx": 356273}
{"func": "    }\n  }\n\n  /* Process NATURAL keywords, and ON and USING clauses of joins.\n  */\n  if( db->mallocFailed || sqliteProcessJoin(pParse, p) ){\n    return WRC_Abort;\n  }\n\n  /* For every \"*\" that occurs in the column list, insert the names of\n  ** all columns in all tables.  And for every TABLE.* insert the names", "project": "sqlite", "hash": 306812494712022841209499043837047336891, "size": 276, "commit_id": "a6c1a71cde082e09750465d5675699062922e387", "message": "Do not attempt to unwind the WITH stack in the Parse object following an error. This fixes a separate case to [de6e6d68].\n\nFossilOrigin-Name: d29edef93451cc67a5d69c1cce1b1832d9ca8fff1f600afdd51338b74d077b92", "target": 1, "dataset": "other", "idx": 204285}
{"func": "    }\n  }\n\n  /* Process NATURAL keywords, and ON and USING clauses of joins.\n  */\n  if( pParse->nErr || db->mallocFailed || sqliteProcessJoin(pParse, p) ){\n    return WRC_Abort;\n  }\n\n  /* For every \"*\" that occurs in the column list, insert the names of\n  ** all columns in all tables.  And for every TABLE.* insert the names", "project": "sqlite", "hash": 27082898212798908738611756384964673964, "size": 276, "commit_id": "a6c1a71cde082e09750465d5675699062922e387", "message": "Do not attempt to unwind the WITH stack in the Parse object following an error. This fixes a separate case to [de6e6d68].\n\nFossilOrigin-Name: d29edef93451cc67a5d69c1cce1b1832d9ca8fff1f600afdd51338b74d077b92", "target": 0, "dataset": "other", "idx": 356309}
{"func": "sudoers_policy_deserialize_info(void *v)\n{\n    struct sudoers_open_info *info = v;\n    char * const *cur;\n    const char *p, *errstr, *groups = NULL;\n    const char *remhost = NULL;\n    int flags = 0;\n    debug_decl(sudoers_policy_deserialize_info, SUDOERS_DEBUG_PLUGIN);\n\n#define MATCHES(s, v)\t\\\n    (strncmp((s), (v), sizeof(v) - 1) == 0)\n\t    path_plugin_dir = *cur + sizeof(\"plugin_dir=\") - 1;\n\t    continue;\n\t}\n#endif\n    }\n\n    user_gid = (gid_t)-1;\n    user_sid = (pid_t)-1;\n    user_uid = (gid_t)-1;\n    user_umask = (mode_t)-1;\n    for (cur = info->user_info; *cur != NULL; cur++) {", "project": "sudo", "hash": 83391947378302188199193223394572343502, "size": 441, "commit_id": "c4d384082fdbc8406cf19e08d05db4cded920a55", "message": "Add sudoedit flag checks in plugin that are consistent with front-end.\nDon't assume the sudo front-end is sending reasonable mode flags.\nThese checks need to be kept consistent between the sudo front-end\nand the sudoers plugin.", "target": 1, "dataset": "other", "idx": 204323}
{"func": "sudoers_policy_deserialize_info(void *v)\n{\n    const int edit_mask = MODE_EDIT|MODE_IGNORE_TICKET|MODE_NONINTERACTIVE;\n    struct sudoers_open_info *info = v;\n    const char *p, *errstr, *groups = NULL;\n    const char *remhost = NULL;\n    char * const *cur;\n    int flags = 0;\n    debug_decl(sudoers_policy_deserialize_info, SUDOERS_DEBUG_PLUGIN);\n\n#define MATCHES(s, v)\t\\\n    (strncmp((s), (v), sizeof(v) - 1) == 0)\n\t    continue;\n\t}\n#endif\n    }\n\n    /* Sudo front-end should restrict mode flags for sudoedit. */\n    if (ISSET(flags, MODE_EDIT) && (flags & edit_mask) != flags) {\n\tsudo_warnx(U_(\"invalid mode flags from sudo front end: 0x%x\"), flags);\n\tgoto bad;\n    }\n\n    user_gid = (gid_t)-1;\n    user_sid = (pid_t)-1;\n    user_uid = (gid_t)-1;\n    user_umask = (mode_t)-1;\n    for (cur = info->user_info; *cur != NULL; cur++) {", "project": "sudo", "hash": 50210916584704883102461860878208331377, "size": 448, "commit_id": "c4d384082fdbc8406cf19e08d05db4cded920a55", "message": "Add sudoedit flag checks in plugin that are consistent with front-end.\nDon't assume the sudo front-end is sending reasonable mode flags.\nThese checks need to be kept consistent between the sudo front-end\nand the sudoers plugin.", "target": 0, "dataset": "other", "idx": 356720}
{"func": "      if (C_SslForceTls)\n        ans = MUTT_YES;\n      else if ((ans = query_quadoption(C_SslStarttls,\n                                       _(\"Secure connection with TLS?\"))) == MUTT_ABORT)\n      {\n        goto err_close_conn;\n      }\n      if (ans == MUTT_YES)\n      {\n        enum ImapExecResult rc = imap_exec(adata, \"STARTTLS\", IMAP_CMD_SINGLE);\n        // Clear any data after the STARTTLS acknowledgement\n        if (rc != IMAP_EXEC_ERROR)\n        {\n          if (mutt_ssl_starttls(adata->conn))\n          {\n            mutt_error(_(\"Could not negotiate TLS connection\"));\n            goto err_close_conn;\n          }\n          else\n          {\n            /* RFC2595 demands we recheck CAPABILITY after TLS completes. */\n            if (imap_exec(adata, \"CAPABILITY\", IMAP_CMD_NO_FLAGS))\n    }\n\n    if (C_SslForceTls && (adata->conn->ssf == 0))\n    {\n      mutt_error(_(\"Encrypted connection unavailable\"));\n      goto err_close_conn;\n    }\n#endif\n  }\n  else if (mutt_istr_startswith(adata->buf, \"* PREAUTH\"))\n  {\n     * decide whether to abort. Note that if using $tunnel and\n     * $tunnel_is_secure, adata->conn->ssf will be set to 1. */\n    if ((adata->conn->ssf == 0) && C_SslForceTls)\n    {\n      mutt_error(_(\"Encrypted connection unavailable\"));\n      goto err_close_conn;\n    }\n#endif\n\n    adata->state = IMAP_AUTHENTICATED;\n    if (check_capabilities(adata) != 0)\n    goto bail;\n  }\n\n  return 0;\n\n#ifdef USE_SSL\nerr_close_conn:\n  imap_close_connection(adata);\n#endif\nbail:\n  FREE(&adata->capstr);\n  return -1;\n}", "project": "neomutt", "hash": 116037772614219807053318355003276943446, "size": 101, "commit_id": "9c36717a3e2af1f2c1b7242035455ec8112b4b06", "message": "imap: close connection on all failures\n\nThanks to Gabriel Salles-Loustau for spotting the problem.\n\nCo-authored-by: Kevin McCarthy <kevin@8t8.us>", "target": 1, "dataset": "other", "idx": 204337}
{"func": "      if (C_SslForceTls)\n        ans = MUTT_YES;\n      else if ((ans = query_quadoption(C_SslStarttls,\n                                       _(\"Secure connection with TLS?\"))) == MUTT_ABORT)\n      {\n        goto bail;\n      }\n      if (ans == MUTT_YES)\n      {\n        enum ImapExecResult rc = imap_exec(adata, \"STARTTLS\", IMAP_CMD_SINGLE);\n        // Clear any data after the STARTTLS acknowledgement\n        if (rc != IMAP_EXEC_ERROR)\n        {\n          if (mutt_ssl_starttls(adata->conn))\n          {\n            mutt_error(_(\"Could not negotiate TLS connection\"));\n            goto bail;\n          }\n          else\n          {\n            /* RFC2595 demands we recheck CAPABILITY after TLS completes. */\n            if (imap_exec(adata, \"CAPABILITY\", IMAP_CMD_NO_FLAGS))\n    }\n\n    if (C_SslForceTls && (adata->conn->ssf == 0))\n    {\n      mutt_error(_(\"Encrypted connection unavailable\"));\n      goto bail;\n    }\n#endif\n  }\n  else if (mutt_istr_startswith(adata->buf, \"* PREAUTH\"))\n  {\n     * decide whether to abort. Note that if using $tunnel and\n     * $tunnel_is_secure, adata->conn->ssf will be set to 1. */\n    if ((adata->conn->ssf == 0) && C_SslForceTls)\n    {\n      mutt_error(_(\"Encrypted connection unavailable\"));\n      goto bail;\n    }\n#endif\n\n    adata->state = IMAP_AUTHENTICATED;\n    if (check_capabilities(adata) != 0)\n    goto bail;\n  }\n\n  return 0;\n\nbail:\n  imap_close_connection(adata);\n  FREE(&adata->capstr);\n  return -1;\n}", "project": "neomutt", "hash": 72296476132372139826321996354956312569, "size": 98, "commit_id": "9c36717a3e2af1f2c1b7242035455ec8112b4b06", "message": "imap: close connection on all failures\n\nThanks to Gabriel Salles-Loustau for spotting the problem.\n\nCo-authored-by: Kevin McCarthy <kevin@8t8.us>", "target": 0, "dataset": "other", "idx": 357032}
{"func": "                          \"Cannot encode tile: opj_tcd_marker_info_create() failed\\n\");\n            return OPJ_FALSE;\n        }\n    }\n\n    assert(l_remaining_data >\n           p_j2k->m_specific_param.m_encoder.m_reserved_bytes_for_PLT);\n    l_remaining_data -= p_j2k->m_specific_param.m_encoder.m_reserved_bytes_for_PLT;\n\n    if (! opj_tcd_encode_tile(p_tile_coder, p_j2k->m_current_tile_number,\n                              p_data + 2,\n                              p_data_written, l_remaining_data, l_cstr_info,", "project": "openjpeg", "hash": 2596444094136185670542568690370435768, "size": 137, "commit_id": "73fdf28342e4594019af26eb6a347a34eceb6296", "message": "opj_j2k_write_sod(): avoid potential heap buffer overflow (fixes #1299) (probably master only)", "target": 1, "dataset": "other", "idx": 204345}
{"func": "                          \"Cannot encode tile: opj_tcd_marker_info_create() failed\\n\");\n            return OPJ_FALSE;\n        }\n    }\n\n    if (l_remaining_data <\n            p_j2k->m_specific_param.m_encoder.m_reserved_bytes_for_PLT) {\n        opj_event_msg(p_manager, EVT_ERROR,\n                      \"Not enough bytes in output buffer to write SOD marker\\n\");\n        opj_tcd_marker_info_destroy(marker_info);\n        return OPJ_FALSE;\n    }\n    l_remaining_data -= p_j2k->m_specific_param.m_encoder.m_reserved_bytes_for_PLT;\n\n    if (! opj_tcd_encode_tile(p_tile_coder, p_j2k->m_current_tile_number,\n                              p_data + 2,\n                              p_data_written, l_remaining_data, l_cstr_info,", "project": "openjpeg", "hash": 110807645529220467738052209881275757752, "size": 142, "commit_id": "73fdf28342e4594019af26eb6a347a34eceb6296", "message": "opj_j2k_write_sod(): avoid potential heap buffer overflow (fixes #1299) (probably master only)", "target": 0, "dataset": "other", "idx": 357326}
{"func": "p11_rpc_buffer_get_attribute (p11_buffer *buffer,\n\t\t\t      size_t *offset,\n\t\t\t      CK_ATTRIBUTE *attr)\n{\n\tuint32_t type, length;\n\tunsigned char validity;\n\tp11_rpc_attribute_serializer *serializer;\n\tp11_rpc_value_type value_type;\n\n\t/* The attribute type */\n\tassert (value_type < ELEMS (p11_rpc_attribute_serializers));\n\tserializer = &p11_rpc_attribute_serializers[value_type];\n\tassert (serializer != NULL);\n\tif (!serializer->decode (buffer, offset, attr->pValue, &attr->ulValueLen))\n\t\treturn false;\n\tif (!attr->pValue)\n\t\tattr->ulValueLen = length;\n\tattr->type = type;\n\treturn true;\n}", "project": "p11-kit", "hash": 23421043781240478058626749820986768771, "size": 39, "commit_id": "2617f3ef888e103324a28811886b99ed0a56346d", "message": "Check attribute length against buffer size\n\nIf an attribute's length does not match the length of the byte array\ninside it, one length was used for allocation, and the other was used\nfor memcpy. This additional check will instead return an error on\nmalformed messages.", "target": 1, "dataset": "other", "idx": 204380}
{"func": "p11_rpc_buffer_get_attribute (p11_buffer *buffer,\n\t\t\t      size_t *offset,\n\t\t\t      CK_ATTRIBUTE *attr)\n{\n\tuint32_t type, length, decode_length;\n\tunsigned char validity;\n\tp11_rpc_attribute_serializer *serializer;\n\tp11_rpc_value_type value_type;\n\n\t/* The attribute type */\n\tassert (value_type < ELEMS (p11_rpc_attribute_serializers));\n\tserializer = &p11_rpc_attribute_serializers[value_type];\n\tassert (serializer != NULL);\n\tif (!serializer->decode (buffer, offset, attr->pValue, &attr->ulValueLen))\n\t\treturn false;\n\tif (!attr->pValue) {\n\t\tdecode_length = attr->ulValueLen;\n\t\tattr->ulValueLen = length;\n\t\tif (decode_length > length) {\n\t\t\treturn false;\n\t\t}\n\t}\n\tattr->type = type;\n\treturn true;\n}", "project": "p11-kit", "hash": 163590899753051294882112912981598223663, "size": 44, "commit_id": "2617f3ef888e103324a28811886b99ed0a56346d", "message": "Check attribute length against buffer size\n\nIf an attribute's length does not match the length of the byte array\ninside it, one length was used for allocation, and the other was used\nfor memcpy. This additional check will instead return an error on\nmalformed messages.", "target": 0, "dataset": "other", "idx": 358130}
{"func": "    OPJ_UINT32 l_data_size;\n\n    /* +1 is needed for https://github.com/uclouvain/openjpeg/issues/835 */\n    /* and actually +2 required for https://github.com/uclouvain/openjpeg/issues/982 */\n    /* and +7 for https://github.com/uclouvain/openjpeg/issues/1283 (-M 3) */\n    /* and +26 for https://github.com/uclouvain/openjpeg/issues/1283 (-M 7) */\n    /* TODO: is there a theoretical upper-bound for the compressed code */\n    /* block size ? */\n    l_data_size = 26 + (OPJ_UINT32)((p_code_block->x1 - p_code_block->x0) *\n                                   (p_code_block->y1 - p_code_block->y0) * (OPJ_INT32)sizeof(OPJ_UINT32));\n\n    if (l_data_size > p_code_block->data_size) {\n        if (p_code_block->data) {\n            /* We refer to data - 1 since below we incremented it */", "project": "openjpeg", "hash": 74332118889396083730693045237215462948, "size": 35, "commit_id": "15cf3d95814dc931ca0ecb132f81cb152e051bae", "message": "Encoder: grow again buffer size in opj_tcd_code_block_enc_allocate_data() (fixes #1283)", "target": 1, "dataset": "other", "idx": 204411}
{"func": "\n    /* +1 is needed for https://github.com/uclouvain/openjpeg/issues/835 */\n    /* and actually +2 required for https://github.com/uclouvain/openjpeg/issues/982 */\n    /* and +7 for https://github.com/uclouvain/openjpeg/issues/1283 (-M 3) */\n    /* and +26 for https://github.com/uclouvain/openjpeg/issues/1283 (-M 7) */\n    /* and +28 for https://github.com/uclouvain/openjpeg/issues/1283 (-M 44) */\n    /* TODO: is there a theoretical upper-bound for the compressed code */\n    /* block size ? */\n    l_data_size = 28 + (OPJ_UINT32)((p_code_block->x1 - p_code_block->x0) *\n                                   (p_code_block->y1 - p_code_block->y0) * (OPJ_INT32)sizeof(OPJ_UINT32));\n\n    if (l_data_size > p_code_block->data_size) {\n        if (p_code_block->data) {\n            /* We refer to data - 1 since below we incremented it */", "project": "openjpeg", "hash": 90613075038304196793521501973130942392, "size": 36, "commit_id": "15cf3d95814dc931ca0ecb132f81cb152e051bae", "message": "Encoder: grow again buffer size in opj_tcd_code_block_enc_allocate_data() (fixes #1283)", "target": 0, "dataset": "other", "idx": 359168}
{"func": "\n\t*ximg_ptr = xim;\n\n#if HAVE_XSHM\n\tshm->shmid = shmget(IPC_PRIVATE,\n\t    xim->bytes_per_line * xim->height, IPC_CREAT | 0777);\n\n\tif (shm->shmid == -1) {\n\t\trfbErr(\"shmget(%s) failed.\\n\", name);\n\t\trfbLogPerror(\"shmget\");\n", "project": "x11vnc", "hash": 101234356332723009809336080171659591661, "size": 126, "commit_id": "69eeb9f7baa14ca03b16c9de821f9876def7a36a", "message": "scan: limit access to shared memory segments to current user", "target": 1, "dataset": "other", "idx": 204491}
{"func": "\n\t*ximg_ptr = xim;\n\n#if HAVE_XSHM\n\tshm->shmid = shmget(IPC_PRIVATE,\n\t    xim->bytes_per_line * xim->height, IPC_CREAT | 0600);\n\n\tif (shm->shmid == -1) {\n\t\trfbErr(\"shmget(%s) failed.\\n\", name);\n\t\trfbLogPerror(\"shmget\");\n", "project": "x11vnc", "hash": 251759810483318878247162187332724697979, "size": 126, "commit_id": "69eeb9f7baa14ca03b16c9de821f9876def7a36a", "message": "scan: limit access to shared memory segments to current user", "target": 0, "dataset": "other", "idx": 360762}
{"func": "    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {", "project": "redis", "hash": 5416716427636361581435192994670558250, "size": 211, "commit_id": "394614a5f91d88380f480c4610926a865b5b0f16", "message": "Fix integer overflow in STRALGO LCS (CVE-2021-29477)\n\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\nthe STRALGO LCS command to corrupt the heap and potentially result with remote\ncode execution.\n\n(cherry picked from commit f0c5f920d0f88bd8aa376a2c05af4902789d1ef9)", "target": 1, "dataset": "other", "idx": 204533}
{"func": "    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((size_t)(alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {", "project": "redis", "hash": 327629069030382399823532872609259616525, "size": 211, "commit_id": "394614a5f91d88380f480c4610926a865b5b0f16", "message": "Fix integer overflow in STRALGO LCS (CVE-2021-29477)\n\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\nthe STRALGO LCS command to corrupt the heap and potentially result with remote\ncode execution.\n\n(cherry picked from commit f0c5f920d0f88bd8aa376a2c05af4902789d1ef9)", "target": 0, "dataset": "other", "idx": 361275}
{"func": "\t    int count = *in++;\n\t    inLength -= 2;\n\n\t    if (0 > (maxLength -= count + 1))\n\t\treturn 0;\n\n        memset(out, *(char*)in, count+1);\n        out += count+1;\n\n\t    in++;\n\t}", "project": "openexr", "hash": 304669280453166082193019580418525426392, "size": 40, "commit_id": "25259a84827234a283f6f9db72978198c7a3f268", "message": "detect buffer overflows in RleUncompress (#1036)\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>", "target": 1, "dataset": "other", "idx": 204639}
{"func": "\t    inLength -= 2;\n\n\t    if (0 > (maxLength -= count + 1))\n\t\treturn 0;\n\n        // check the input buffer is big enough to contain\n        // byte to be duplicated\n        if (inLength < 0)\n          return 0;\n\n        memset(out, *(char*)in, count+1);\n        out += count+1;\n\n\t    in++;\n\t}", "project": "openexr", "hash": 253476937124885143941445089513595709318, "size": 45, "commit_id": "25259a84827234a283f6f9db72978198c7a3f268", "message": "detect buffer overflows in RleUncompress (#1036)\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>", "target": 0, "dataset": "other", "idx": 362854}
{"func": "{\n\tbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\n\tunsigned int chunks, chunks_per_page;\n\tu64 addr = mr->addr, size = mr->len;\n\tint size_chk, err;\n\n\tif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\n\t\t/* Strictly speaking we could support this, if:\n\t\t * - huge pages, or*\n\t\t * - using an IOMMU, or\n\t\tchunks_per_page = PAGE_SIZE / chunk_size;\n\t\tif (chunks < chunks_per_page || chunks % chunks_per_page)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsize_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;\n\tif (size_chk < 0)\n\t\treturn -EINVAL;\n\n\tumem->address = (unsigned long)addr;\n\tumem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n\t\t\t\t\t    : ~((u64)chunk_size - 1);", "project": "linux", "hash": 269028637877237764370672455302491205384, "size": 91, "commit_id": "99e3a236dd43d06c65af0a2ef9cb44306aef6e02", "message": "xsk: Add missing check on user supplied headroom size\n\nAdd a check that the headroom cannot be larger than the available\nspace in the chunk. In the current code, a malicious user can set the\nheadroom to a value larger than the chunk size minus the fixed XDP\nheadroom. That way packets with a length larger than the supported\nsize in the umem could get accepted and result in an out-of-bounds\nwrite.\n\nFixes: c0c77d8fb787 (\"xsk: add user memory registration support sockopt\")\nReported-by: Bui Quang Minh <minhquangbui99@gmail.com>\nSigned-off-by: Magnus Karlsson <magnus.karlsson@intel.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://bugzilla.kernel.org/show_bug.cgi?id=207225\nLink: https://lore.kernel.org/bpf/1586849715-23490-1-git-send-email-magnus.karlsson@intel.com", "target": 1, "dataset": "other", "idx": 204723}
{"func": "{\n\tbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\n\tunsigned int chunks, chunks_per_page;\n\tu64 addr = mr->addr, size = mr->len;\n\tint err;\n\n\tif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\n\t\t/* Strictly speaking we could support this, if:\n\t\t * - huge pages, or*\n\t\t * - using an IOMMU, or\n\t\tchunks_per_page = PAGE_SIZE / chunk_size;\n\t\tif (chunks < chunks_per_page || chunks % chunks_per_page)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (headroom >= chunk_size - XDP_PACKET_HEADROOM)\n\t\treturn -EINVAL;\n\n\tumem->address = (unsigned long)addr;\n\tumem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n\t\t\t\t\t    : ~((u64)chunk_size - 1);", "project": "linux", "hash": 42054657955985470671897377670619516821, "size": 90, "commit_id": "99e3a236dd43d06c65af0a2ef9cb44306aef6e02", "message": "xsk: Add missing check on user supplied headroom size\n\nAdd a check that the headroom cannot be larger than the available\nspace in the chunk. In the current code, a malicious user can set the\nheadroom to a value larger than the chunk size minus the fixed XDP\nheadroom. That way packets with a length larger than the supported\nsize in the umem could get accepted and result in an out-of-bounds\nwrite.\n\nFixes: c0c77d8fb787 (\"xsk: add user memory registration support sockopt\")\nReported-by: Bui Quang Minh <minhquangbui99@gmail.com>\nSigned-off-by: Magnus Karlsson <magnus.karlsson@intel.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://bugzilla.kernel.org/show_bug.cgi?id=207225\nLink: https://lore.kernel.org/bpf/1586849715-23490-1-git-send-email-magnus.karlsson@intel.com", "target": 0, "dataset": "other", "idx": 364107}
{"func": "static inline int pri2fac(const int pri)\n{\n\tint fac = pri >> 3;\n\treturn (fac > 23) ? LOG_FAC_INVLD : fac;\n}", "project": "rsyslog", "hash": 38375841940809111922591785804510290465, "size": 5, "commit_id": "8a4ada405e98ed3470f2c5f54e52339e7263c258", "message": "prevent PRI underflow", "target": 1, "dataset": "other", "idx": 204724}
{"func": "static inline int pri2fac(const int pri)\n{\n\tunsigned fac = pri >> 3;\n\treturn (fac > 23) ? LOG_FAC_INVLD : fac;\n}", "project": "rsyslog", "hash": 132599495943349320656124963251558173960, "size": 5, "commit_id": "8a4ada405e98ed3470f2c5f54e52339e7263c258", "message": "prevent PRI underflow", "target": 0, "dataset": "other", "idx": 364120}
{"func": "void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct page *page)\n{\n\tspinlock_t *ptl;\n\tstruct mmu_notifier_range range;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\taddress & HPAGE_PMD_MASK,\n\t\t\t\t(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\t/*\n\t * If caller asks to setup a migration entries, we need a page to check\n\t * pmd against. Otherwise we can end up replacing wrong page.\n\t */\n\tVM_BUG_ON(freeze && !page);\n\tif (page && page != pmd_page(*pmd))\n\t        goto out;\n\n\tif (pmd_trans_huge(*pmd)) {\n\t\tpage = pmd_page(*pmd);\n\t\tif (PageMlocked(page))\n\t\t\tclear_page_mlock(page);\n\t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n\t\tgoto out;\n\t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\nout:\n\tspin_unlock(ptl);\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback.\n\t * They are 3 cases to consider inside __split_huge_pmd_locked():\n\t *  1) pmdp_huge_clear_flush_notify() call invalidate_range() obvious\n\t *  2) __split_huge_zero_page_pmd() read only zero page and any write", "project": "linux", "hash": 232560186172248161602253290668425073906, "size": 44, "commit_id": "c444eb564fb16645c172d550359cb3d75fe8a040", "message": "mm: thp: make the THP mapcount atomic against __split_huge_pmd_locked()\n\nWrite protect anon page faults require an accurate mapcount to decide\nif to break the COW or not. This is implemented in the THP path with\nreuse_swap_page() ->\npage_trans_huge_map_swapcount()/page_trans_huge_mapcount().\n\nIf the COW triggers while the other processes sharing the page are\nunder a huge pmd split, to do an accurate reading, we must ensure the\nmapcount isn't computed while it's being transferred from the head\npage to the tail pages.\n\nreuse_swap_cache() already runs serialized by the page lock, so it's\nenough to add the page lock around __split_huge_pmd_locked too, in\norder to add the missing serialization.\n\nNote: the commit in \"Fixes\" is just to facilitate the backporting,\nbecause the code before such commit didn't try to do an accurate THP\nmapcount calculation and it instead used the page_count() to decide if\nto COW or not. Both the page_count and the pin_count are THP-wide\nrefcounts, so they're inaccurate if used in\nreuse_swap_page(). Reverting such commit (besides the unrelated fix to\nthe local anon_vma assignment) would have also opened the window for\nmemory corruption side effects to certain workloads as documented in\nsuch commit header.\n\nSigned-off-by: Andrea Arcangeli <aarcange@redhat.com>\nSuggested-by: Jann Horn <jannh@google.com>\nReported-by: Jann Horn <jannh@google.com>\nAcked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nFixes: 6d0a07edd17c (\"mm: thp: calculate the mapcount correctly for THP pages during WP faults\")\nCc: stable@vger.kernel.org\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 204725}
{"func": "void __split_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,\n\t\tunsigned long address, bool freeze, struct page *page)\n{\n\tspinlock_t *ptl;\n\tstruct mmu_notifier_range range;\n\tbool was_locked = false;\n\tpmd_t _pmd;\n\n\tmmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, vma->vm_mm,\n\t\t\t\taddress & HPAGE_PMD_MASK,\n\t\t\t\t(address & HPAGE_PMD_MASK) + HPAGE_PMD_SIZE);\n\tmmu_notifier_invalidate_range_start(&range);\n\t/*\n\t * If caller asks to setup a migration entries, we need a page to check\n\t * pmd against. Otherwise we can end up replacing wrong page.\n\t */\n\tVM_BUG_ON(freeze && !page);\n\tif (page) {\n\t\tVM_WARN_ON_ONCE(!PageLocked(page));\n\t\twas_locked = true;\n\t\tif (page != pmd_page(*pmd))\n\t\t\tgoto out;\n\t}\n\nrepeat:\n\tif (pmd_trans_huge(*pmd)) {\n\t\tif (!page) {\n\t\t\tpage = pmd_page(*pmd);\n\t\t\tif (unlikely(!trylock_page(page))) {\n\t\t\t\tget_page(page);\n\t\t\t\t_pmd = *pmd;\n\t\t\t\tspin_unlock(ptl);\n\t\t\t\tlock_page(page);\n\t\t\t\tspin_lock(ptl);\n\t\t\t\tif (unlikely(!pmd_same(*pmd, _pmd))) {\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tput_page(page);\n\t\t\t\t\tpage = NULL;\n\t\t\t\t\tgoto repeat;\n\t\t\t\t}\n\t\t\t\tput_page(page);\n\t\t\t}\n\t\t}\n\t\tif (PageMlocked(page))\n\t\t\tclear_page_mlock(page);\n\t} else if (!(pmd_devmap(*pmd) || is_pmd_migration_entry(*pmd)))\n\t\tgoto out;\n\t__split_huge_pmd_locked(vma, pmd, range.start, freeze);\nout:\n\tspin_unlock(ptl);\n\tif (!was_locked && page)\n\t\tunlock_page(page);\n\t/*\n\t * No need to double call mmu_notifier->invalidate_range() callback.\n\t * They are 3 cases to consider inside __split_huge_pmd_locked():\n\t *  1) pmdp_huge_clear_flush_notify() call invalidate_range() obvious\n\t *  2) __split_huge_zero_page_pmd() read only zero page and any write", "project": "linux", "hash": 305184181549084038258030671746596105420, "size": 69, "commit_id": "c444eb564fb16645c172d550359cb3d75fe8a040", "message": "mm: thp: make the THP mapcount atomic against __split_huge_pmd_locked()\n\nWrite protect anon page faults require an accurate mapcount to decide\nif to break the COW or not. This is implemented in the THP path with\nreuse_swap_page() ->\npage_trans_huge_map_swapcount()/page_trans_huge_mapcount().\n\nIf the COW triggers while the other processes sharing the page are\nunder a huge pmd split, to do an accurate reading, we must ensure the\nmapcount isn't computed while it's being transferred from the head\npage to the tail pages.\n\nreuse_swap_cache() already runs serialized by the page lock, so it's\nenough to add the page lock around __split_huge_pmd_locked too, in\norder to add the missing serialization.\n\nNote: the commit in \"Fixes\" is just to facilitate the backporting,\nbecause the code before such commit didn't try to do an accurate THP\nmapcount calculation and it instead used the page_count() to decide if\nto COW or not. Both the page_count and the pin_count are THP-wide\nrefcounts, so they're inaccurate if used in\nreuse_swap_page(). Reverting such commit (besides the unrelated fix to\nthe local anon_vma assignment) would have also opened the window for\nmemory corruption side effects to certain workloads as documented in\nsuch commit header.\n\nSigned-off-by: Andrea Arcangeli <aarcange@redhat.com>\nSuggested-by: Jann Horn <jannh@google.com>\nReported-by: Jann Horn <jannh@google.com>\nAcked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>\nFixes: 6d0a07edd17c (\"mm: thp: calculate the mapcount correctly for THP pages during WP faults\")\nCc: stable@vger.kernel.org\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 364123}
{"func": "{\n\tstatic poptContext pc;\n\tchar *ref = lp_refuse_options(module_id);\n\tconst char *arg, **argv = *argv_p;\n\tint argc = *argc_p;\n\tint opt;\n\n\tif (ref && *ref)\n\t\tset_refuse_options(ref);\n\tif (am_daemon) {\n\t\tset_refuse_options(\"log-file*\");\n\t}\n#endif\n\n\tif (fuzzy_basis > 1)\n\t\tfuzzy_basis = basis_dir_cnt + 1;\n\n\tif (protect_args == 1 && am_server)\n\t\treturn 1;\n\n\t*argv_p = argv = poptGetArgs(pc);\n\t*argc_p = argc = count_args(argv);", "project": "rsync", "hash": 282111307716565365896500155152540425903, "size": 1062, "commit_id": "7706303828fcde524222babb2833864a4bd09e07", "message": "Ignore --protect-args when already sent by client\n\nIn parse_arguments when --protect-args is encountered the function exits\nearly. The caller is expected to check protect_args, and recall\nparse_arguments setting protect_args to 2. This patch prevents the\nclient from resetting protect_args during the second pass of\nparse_arguments. This prevents parse_arguments returning early the\nsecond time before it's able to sanitize the arguments it received.", "target": 1, "dataset": "other", "idx": 204749}
{"func": "\tstatic poptContext pc;\n\tchar *ref = lp_refuse_options(module_id);\n\tconst char *arg, **argv = *argv_p;\n\tint argc = *argc_p;\n\tint opt;\n\tint orig_protect_args = protect_args;\n\n\tif (ref && *ref)\n\t\tset_refuse_options(ref);\n\tif (am_daemon) {\n\t\tset_refuse_options(\"log-file*\");\n#endif\n\n\tif (fuzzy_basis > 1)\n\t\tfuzzy_basis = basis_dir_cnt + 1;\n\n\t/* Don't let the client reset protect_args if it was already processed */\n\tif (orig_protect_args == 2 && am_server)\n\t\tprotect_args = orig_protect_args;\n\n\tif (protect_args == 1 && am_server)\n\t\treturn 1;\n\n\t*argv_p = argv = poptGetArgs(pc);\n\t*argc_p = argc = count_args(argv);", "project": "rsync", "hash": 212765220547978151069862672879986367926, "size": 1067, "commit_id": "7706303828fcde524222babb2833864a4bd09e07", "message": "Ignore --protect-args when already sent by client\n\nIn parse_arguments when --protect-args is encountered the function exits\nearly. The caller is expected to check protect_args, and recall\nparse_arguments setting protect_args to 2. This patch prevents the\nclient from resetting protect_args during the second pass of\nparse_arguments. This prevents parse_arguments returning early the\nsecond time before it's able to sanitize the arguments it received.", "target": 0, "dataset": "other", "idx": 364670}
{"func": "static int tipc_nl_retrieve_key(struct nlattr **attrs,\n\t\t\t\tstruct tipc_aead_key **key)\n{\n\tstruct nlattr *attr = attrs[TIPC_NLA_NODE_KEY];\n\n\tif (!attr)\n\t\treturn -ENODATA;\n\n\t*key = (struct tipc_aead_key *)nla_data(attr);\n\tif (nla_len(attr) < tipc_aead_key_size(*key))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}", "project": "linux", "hash": 284003867985169166932353704253458867619, "size": 14, "commit_id": "0217ed2848e8538bcf9172d97ed2eeb4a26041bb", "message": "tipc: better validate user input in tipc_nl_retrieve_key()\n\nBefore calling tipc_aead_key_size(ptr), we need to ensure\nwe have enough data to dereference ptr->keylen.\n\nWe probably also want to make sure tipc_aead_key_size()\nwont overflow with malicious ptr->keylen values.\n\nSyzbot reported:\n\nBUG: KMSAN: uninit-value in __tipc_nl_node_set_key net/tipc/node.c:2971 [inline]\nBUG: KMSAN: uninit-value in tipc_nl_node_set_key+0x9bf/0x13b0 net/tipc/node.c:3023\nCPU: 0 PID: 21060 Comm: syz-executor.5 Not tainted 5.11.0-rc7-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:79 [inline]\n dump_stack+0x21c/0x280 lib/dump_stack.c:120\n kmsan_report+0xfb/0x1e0 mm/kmsan/kmsan_report.c:118\n __msan_warning+0x5f/0xa0 mm/kmsan/kmsan_instr.c:197\n __tipc_nl_node_set_key net/tipc/node.c:2971 [inline]\n tipc_nl_node_set_key+0x9bf/0x13b0 net/tipc/node.c:3023\n genl_family_rcv_msg_doit net/netlink/genetlink.c:739 [inline]\n genl_family_rcv_msg net/netlink/genetlink.c:783 [inline]\n genl_rcv_msg+0x1319/0x1610 net/netlink/genetlink.c:800\n netlink_rcv_skb+0x6fa/0x810 net/netlink/af_netlink.c:2494\n genl_rcv+0x63/0x80 net/netlink/genetlink.c:811\n netlink_unicast_kernel net/netlink/af_netlink.c:1304 [inline]\n netlink_unicast+0x11d6/0x14a0 net/netlink/af_netlink.c:1330\n netlink_sendmsg+0x1740/0x1840 net/netlink/af_netlink.c:1919\n sock_sendmsg_nosec net/socket.c:652 [inline]\n sock_sendmsg net/socket.c:672 [inline]\n ____sys_sendmsg+0xcfc/0x12f0 net/socket.c:2345\n ___sys_sendmsg net/socket.c:2399 [inline]\n __sys_sendmsg+0x714/0x830 net/socket.c:2432\n __compat_sys_sendmsg net/compat.c:347 [inline]\n __do_compat_sys_sendmsg net/compat.c:354 [inline]\n __se_compat_sys_sendmsg+0xa7/0xc0 net/compat.c:351\n __ia32_compat_sys_sendmsg+0x4a/0x70 net/compat.c:351\n do_syscall_32_irqs_on arch/x86/entry/common.c:79 [inline]\n __do_fast_syscall_32+0x102/0x160 arch/x86/entry/common.c:141\n do_fast_syscall_32+0x6a/0xc0 arch/x86/entry/common.c:166\n do_SYSENTER_32+0x73/0x90 arch/x86/entry/common.c:209\n entry_SYSENTER_compat_after_hwframe+0x4d/0x5c\nRIP: 0023:0xf7f60549\nCode: 03 74 c0 01 10 05 03 74 b8 01 10 06 03 74 b4 01 10 07 03 74 b0 01 10 08 03 74 d8 01 00 00 00 00 00 51 52 55 89 e5 0f 34 cd 80 <5d> 5a 59 c3 90 90 90 90 8d b4 26 00 00 00 00 8d b4 26 00 00 00 00\nRSP: 002b:00000000f555a5fc EFLAGS: 00000296 ORIG_RAX: 0000000000000172\nRAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000020000200\nRDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000\nRBP: 0000000000000000 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000\nR13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000\n\nUninit was created at:\n kmsan_save_stack_with_flags mm/kmsan/kmsan.c:121 [inline]\n kmsan_internal_poison_shadow+0x5c/0xf0 mm/kmsan/kmsan.c:104\n kmsan_slab_alloc+0x8d/0xe0 mm/kmsan/kmsan_hooks.c:76\n slab_alloc_node mm/slub.c:2907 [inline]\n __kmalloc_node_track_caller+0xa37/0x1430 mm/slub.c:4527\n __kmalloc_reserve net/core/skbuff.c:142 [inline]\n __alloc_skb+0x2f8/0xb30 net/core/skbuff.c:210\n alloc_skb include/linux/skbuff.h:1099 [inline]\n netlink_alloc_large_skb net/netlink/af_netlink.c:1176 [inline]\n netlink_sendmsg+0xdbc/0x1840 net/netlink/af_netlink.c:1894\n sock_sendmsg_nosec net/socket.c:652 [inline]\n sock_sendmsg net/socket.c:672 [inline]\n ____sys_sendmsg+0xcfc/0x12f0 net/socket.c:2345\n ___sys_sendmsg net/socket.c:2399 [inline]\n __sys_sendmsg+0x714/0x830 net/socket.c:2432\n __compat_sys_sendmsg net/compat.c:347 [inline]\n __do_compat_sys_sendmsg net/compat.c:354 [inline]\n __se_compat_sys_sendmsg+0xa7/0xc0 net/compat.c:351\n __ia32_compat_sys_sendmsg+0x4a/0x70 net/compat.c:351\n do_syscall_32_irqs_on arch/x86/entry/common.c:79 [inline]\n __do_fast_syscall_32+0x102/0x160 arch/x86/entry/common.c:141\n do_fast_syscall_32+0x6a/0xc0 arch/x86/entry/common.c:166\n do_SYSENTER_32+0x73/0x90 arch/x86/entry/common.c:209\n entry_SYSENTER_compat_after_hwframe+0x4d/0x5c\n\nFixes: e1f32190cf7d (\"tipc: add support for AEAD key setting via netlink\")\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: Tuong Lien <tuong.t.lien@dektech.com.au>\nCc: Jon Maloy <jmaloy@redhat.com>\nCc: Ying Xue <ying.xue@windriver.com>\nReported-by: syzbot <syzkaller@googlegroups.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 204764}
{"func": "static int tipc_nl_retrieve_key(struct nlattr **attrs,\n\t\t\t\tstruct tipc_aead_key **pkey)\n{\n\tstruct nlattr *attr = attrs[TIPC_NLA_NODE_KEY];\n\tstruct tipc_aead_key *key;\n\n\tif (!attr)\n\t\treturn -ENODATA;\n\n\tif (nla_len(attr) < sizeof(*key))\n\t\treturn -EINVAL;\n\tkey = (struct tipc_aead_key *)nla_data(attr);\n\tif (key->keylen > TIPC_AEAD_KEYLEN_MAX ||\n\t    nla_len(attr) < tipc_aead_key_size(key))\n\t\treturn -EINVAL;\n\n\t*pkey = key;\n\treturn 0;\n}", "project": "linux", "hash": 184504004987990882955216536013391680124, "size": 19, "commit_id": "0217ed2848e8538bcf9172d97ed2eeb4a26041bb", "message": "tipc: better validate user input in tipc_nl_retrieve_key()\n\nBefore calling tipc_aead_key_size(ptr), we need to ensure\nwe have enough data to dereference ptr->keylen.\n\nWe probably also want to make sure tipc_aead_key_size()\nwont overflow with malicious ptr->keylen values.\n\nSyzbot reported:\n\nBUG: KMSAN: uninit-value in __tipc_nl_node_set_key net/tipc/node.c:2971 [inline]\nBUG: KMSAN: uninit-value in tipc_nl_node_set_key+0x9bf/0x13b0 net/tipc/node.c:3023\nCPU: 0 PID: 21060 Comm: syz-executor.5 Not tainted 5.11.0-rc7-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:79 [inline]\n dump_stack+0x21c/0x280 lib/dump_stack.c:120\n kmsan_report+0xfb/0x1e0 mm/kmsan/kmsan_report.c:118\n __msan_warning+0x5f/0xa0 mm/kmsan/kmsan_instr.c:197\n __tipc_nl_node_set_key net/tipc/node.c:2971 [inline]\n tipc_nl_node_set_key+0x9bf/0x13b0 net/tipc/node.c:3023\n genl_family_rcv_msg_doit net/netlink/genetlink.c:739 [inline]\n genl_family_rcv_msg net/netlink/genetlink.c:783 [inline]\n genl_rcv_msg+0x1319/0x1610 net/netlink/genetlink.c:800\n netlink_rcv_skb+0x6fa/0x810 net/netlink/af_netlink.c:2494\n genl_rcv+0x63/0x80 net/netlink/genetlink.c:811\n netlink_unicast_kernel net/netlink/af_netlink.c:1304 [inline]\n netlink_unicast+0x11d6/0x14a0 net/netlink/af_netlink.c:1330\n netlink_sendmsg+0x1740/0x1840 net/netlink/af_netlink.c:1919\n sock_sendmsg_nosec net/socket.c:652 [inline]\n sock_sendmsg net/socket.c:672 [inline]\n ____sys_sendmsg+0xcfc/0x12f0 net/socket.c:2345\n ___sys_sendmsg net/socket.c:2399 [inline]\n __sys_sendmsg+0x714/0x830 net/socket.c:2432\n __compat_sys_sendmsg net/compat.c:347 [inline]\n __do_compat_sys_sendmsg net/compat.c:354 [inline]\n __se_compat_sys_sendmsg+0xa7/0xc0 net/compat.c:351\n __ia32_compat_sys_sendmsg+0x4a/0x70 net/compat.c:351\n do_syscall_32_irqs_on arch/x86/entry/common.c:79 [inline]\n __do_fast_syscall_32+0x102/0x160 arch/x86/entry/common.c:141\n do_fast_syscall_32+0x6a/0xc0 arch/x86/entry/common.c:166\n do_SYSENTER_32+0x73/0x90 arch/x86/entry/common.c:209\n entry_SYSENTER_compat_after_hwframe+0x4d/0x5c\nRIP: 0023:0xf7f60549\nCode: 03 74 c0 01 10 05 03 74 b8 01 10 06 03 74 b4 01 10 07 03 74 b0 01 10 08 03 74 d8 01 00 00 00 00 00 51 52 55 89 e5 0f 34 cd 80 <5d> 5a 59 c3 90 90 90 90 8d b4 26 00 00 00 00 8d b4 26 00 00 00 00\nRSP: 002b:00000000f555a5fc EFLAGS: 00000296 ORIG_RAX: 0000000000000172\nRAX: ffffffffffffffda RBX: 0000000000000003 RCX: 0000000020000200\nRDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000\nRBP: 0000000000000000 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000000 R12: 0000000000000000\nR13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000\n\nUninit was created at:\n kmsan_save_stack_with_flags mm/kmsan/kmsan.c:121 [inline]\n kmsan_internal_poison_shadow+0x5c/0xf0 mm/kmsan/kmsan.c:104\n kmsan_slab_alloc+0x8d/0xe0 mm/kmsan/kmsan_hooks.c:76\n slab_alloc_node mm/slub.c:2907 [inline]\n __kmalloc_node_track_caller+0xa37/0x1430 mm/slub.c:4527\n __kmalloc_reserve net/core/skbuff.c:142 [inline]\n __alloc_skb+0x2f8/0xb30 net/core/skbuff.c:210\n alloc_skb include/linux/skbuff.h:1099 [inline]\n netlink_alloc_large_skb net/netlink/af_netlink.c:1176 [inline]\n netlink_sendmsg+0xdbc/0x1840 net/netlink/af_netlink.c:1894\n sock_sendmsg_nosec net/socket.c:652 [inline]\n sock_sendmsg net/socket.c:672 [inline]\n ____sys_sendmsg+0xcfc/0x12f0 net/socket.c:2345\n ___sys_sendmsg net/socket.c:2399 [inline]\n __sys_sendmsg+0x714/0x830 net/socket.c:2432\n __compat_sys_sendmsg net/compat.c:347 [inline]\n __do_compat_sys_sendmsg net/compat.c:354 [inline]\n __se_compat_sys_sendmsg+0xa7/0xc0 net/compat.c:351\n __ia32_compat_sys_sendmsg+0x4a/0x70 net/compat.c:351\n do_syscall_32_irqs_on arch/x86/entry/common.c:79 [inline]\n __do_fast_syscall_32+0x102/0x160 arch/x86/entry/common.c:141\n do_fast_syscall_32+0x6a/0xc0 arch/x86/entry/common.c:166\n do_SYSENTER_32+0x73/0x90 arch/x86/entry/common.c:209\n entry_SYSENTER_compat_after_hwframe+0x4d/0x5c\n\nFixes: e1f32190cf7d (\"tipc: add support for AEAD key setting via netlink\")\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: Tuong Lien <tuong.t.lien@dektech.com.au>\nCc: Jon Maloy <jmaloy@redhat.com>\nCc: Ying Xue <ying.xue@windriver.com>\nReported-by: syzbot <syzkaller@googlegroups.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 364977}
{"func": "lyxml_parse_elem(struct ly_ctx *ctx, const char *data, unsigned int *len, struct lyxml_elem *parent, int options)\n{\n    const char *c = data, *start, *e;\n    const char *lws;    /* leading white space for handling mixed content */\n    int uc;\n    char *str;\n    struct lyxml_attr *attr;\n    unsigned int size;\n    int nons_flag = 0, closed_flag = 0;\n\n    *len = 0;\n\n    if (*c != '<') {\n        return NULL;\n    }\n\n    /* locate element name */\n                    child->content = elem->content;\n                    elem->content = NULL;\n                    lyxml_add_child(ctx, elem, child);\n                    elem->flags |= LYXML_ELEM_MIXED;\n                }\n                child = lyxml_parse_elem(ctx, c, &size, elem, options);\n                if (!child) {\n                    goto error;\n                }\n                c += size;      /* move after processed child element */\n            } else if (is_xmlws(*c)) {", "project": "libyang", "hash": 134258831294213793369395985834060205285, "size": 292, "commit_id": "298b30ea4ebee137226acf9bb38678bd82704582", "message": "common FEATURE add a hard limit for recursion\n\nFixes #1453", "target": 1, "dataset": "other", "idx": 204825}
{"func": "lyxml_parse_elem(struct ly_ctx *ctx, const char *data, unsigned int *len, struct lyxml_elem *parent, int options,\n                 int bt_count)\n{\n    const char *c = data, *start, *e;\n    const char *lws;    /* leading white space for handling mixed content */\n    int uc;\n    char *str;\n    unsigned int size;\n    int nons_flag = 0, closed_flag = 0;\n\n    *len = 0;\n\n    if (bt_count > LY_RECURSION_LIMIT) {\n        LOGVAL(ctx, LYE_XML_INVAL, LY_VLOG_NONE, NULL, \"Recursion limit %d reached\", LY_RECURSION_LIMIT);\n        return NULL;\n    }\n\n    if (*c != '<') {\n        return NULL;\n    }\n\n    /* locate element name */\n                    child->content = elem->content;\n                    elem->content = NULL;\n                    lyxml_add_child(ctx, elem, child);\n                    elem->flags |= LYXML_ELEM_MIXED;\n                }\n                child = lyxml_parse_elem(ctx, c, &size, elem, options, bt_count + 1);\n                if (!child) {\n                    goto error;\n                }\n                c += size;      /* move after processed child element */\n            } else if (is_xmlws(*c)) {", "project": "libyang", "hash": 212643853321618671175691086891444459949, "size": 298, "commit_id": "298b30ea4ebee137226acf9bb38678bd82704582", "message": "common FEATURE add a hard limit for recursion\n\nFixes #1453", "target": 0, "dataset": "other", "idx": 366009}
{"func": "            LOGVAL(ctx, LYE_XML_INCHAR, LY_VLOG_NONE, NULL, c);\n            goto error;\n        }\n    }\n\n    root = lyxml_parse_elem(ctx, c, &len, NULL, options);\n    if (!root) {\n        goto error;\n    } else if (!first) {\n        first = root;\n    } else {", "project": "libyang", "hash": 338248480891388136536572477802663278084, "size": 89, "commit_id": "298b30ea4ebee137226acf9bb38678bd82704582", "message": "common FEATURE add a hard limit for recursion\n\nFixes #1453", "target": 1, "dataset": "other", "idx": 204826}
{"func": "            LOGVAL(ctx, LYE_XML_INCHAR, LY_VLOG_NONE, NULL, c);\n            goto error;\n        }\n    }\n\n    root = lyxml_parse_elem(ctx, c, &len, NULL, options, 0);\n    if (!root) {\n        goto error;\n    } else if (!first) {\n        first = root;\n    } else {", "project": "libyang", "hash": 317835994028173949459715381216752210577, "size": 89, "commit_id": "298b30ea4ebee137226acf9bb38678bd82704582", "message": "common FEATURE add a hard limit for recursion\n\nFixes #1453", "target": 0, "dataset": "other", "idx": 366026}
{"func": "\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*", "project": "linux", "hash": 222498222544209649377028812404138990797, "size": 116, "commit_id": "aa9f7d5172fac9bf1f09e678c35e287a40a7b7dd", "message": "mm: mempolicy: require at least one nodeid for MPOL_PREFERRED\n\nUsing an empty (malformed) nodelist that is not caught during mount option\nparsing leads to a stack-out-of-bounds access.\n\nThe option string that was used was: \"mpol=prefer:,\".  However,\nMPOL_PREFERRED requires a single node number, which is not being provided\nhere.\n\nAdd a check that 'nodes' is not empty after parsing for MPOL_PREFERRED's\nnodeid.\n\nFixes: 095f1fc4ebf3 (\"mempolicy: rework shmem mpol parsing and display\")\nReported-by: Entropy Moe <3ntr0py1337@gmail.com>\nReported-by: syzbot+b055b1a6b2b958707a21@syzkaller.appspotmail.com\nSigned-off-by: Randy Dunlap <rdunlap@infradead.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nTested-by: syzbot+b055b1a6b2b958707a21@syzkaller.appspotmail.com\nCc: Lee Schermerhorn <lee.schermerhorn@hp.com>\nLink: http://lkml.kernel.org/r/89526377-7eb6-b662-e1d8-4430928abde9@infradead.org\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 204856}
{"func": "\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only, although later\n\t\t * we use first_node(nodes) to grab a single node, so here\n\t\t * nodelist (or nodes) cannot be empty.\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t\tif (nodes_empty(nodes))\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*", "project": "linux", "hash": 184911246501013902029875653873691151, "size": 120, "commit_id": "aa9f7d5172fac9bf1f09e678c35e287a40a7b7dd", "message": "mm: mempolicy: require at least one nodeid for MPOL_PREFERRED\n\nUsing an empty (malformed) nodelist that is not caught during mount option\nparsing leads to a stack-out-of-bounds access.\n\nThe option string that was used was: \"mpol=prefer:,\".  However,\nMPOL_PREFERRED requires a single node number, which is not being provided\nhere.\n\nAdd a check that 'nodes' is not empty after parsing for MPOL_PREFERRED's\nnodeid.\n\nFixes: 095f1fc4ebf3 (\"mempolicy: rework shmem mpol parsing and display\")\nReported-by: Entropy Moe <3ntr0py1337@gmail.com>\nReported-by: syzbot+b055b1a6b2b958707a21@syzkaller.appspotmail.com\nSigned-off-by: Randy Dunlap <rdunlap@infradead.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nTested-by: syzbot+b055b1a6b2b958707a21@syzkaller.appspotmail.com\nCc: Lee Schermerhorn <lee.schermerhorn@hp.com>\nLink: http://lkml.kernel.org/r/89526377-7eb6-b662-e1d8-4430928abde9@infradead.org\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 366756}
{"func": "    int format = (s->twoD_stretch >> 20) & 0x3;\n    int rop_mode = (s->twoD_control >> 15) & 0x1; /* 1 for rop2, else rop3 */\n    /* 1 if rop2 source is the pattern, otherwise the source is the bitmap */\n    int rop2_source_is_pattern = (s->twoD_control >> 14) & 0x1;\n    int rop = s->twoD_control & 0xFF;\n    int dst_x = (s->twoD_destination >> 16) & 0x01FFF;\n    int dst_y = s->twoD_destination & 0xFFFF;\n    int width = (s->twoD_dimension >> 16) & 0x1FFF;\n    int height = s->twoD_dimension & 0xFFFF;\n    uint32_t dst_base = s->twoD_destination_base & 0x03FFFFFF;\n    uint8_t *dst = s->local_mem + dst_base;\n    int dst_pitch = (s->twoD_pitch >> 16) & 0x1FFF;\n    int crt = (s->dc_crt_control & SM501_DC_CRT_CONTROL_SEL) ? 1 : 0;\n    int fb_len = get_width(s, crt) * get_height(s, crt) * get_bpp(s, crt);\n\n    if ((s->twoD_stretch >> 16) & 0xF) {\n        qemu_log_mask(LOG_UNIMP, \"sm501: only XY addressing is supported.\\n\");\n        return;\n    }\n\n    if (rop_mode == 0) {\n        if (rop != 0xcc) {\n            /* Anything other than plain copies are not supported */\n            qemu_log_mask(LOG_UNIMP, \"sm501: rop3 mode with rop %x is not \"\n                          \"supported.\\n\", rop);\n        }\n    } else {\n        if (rop2_source_is_pattern && rop != 0x5) {\n            /* For pattern source, we support only inverse dest */\n            qemu_log_mask(LOG_UNIMP, \"sm501: rop2 source being the pattern and \"\n                          \"rop %x is not supported.\\n\", rop);\n        } else {\n            if (rop != 0x5 && rop != 0xc) {\n                /* Anything other than plain copies or inverse dest is not\n                 * supported */\n                qemu_log_mask(LOG_UNIMP, \"sm501: rop mode %x is not \"\n                              \"supported.\\n\", rop);\n            }\n        }\n    }\n\n    if (s->twoD_source_base & BIT(27) || s->twoD_destination_base & BIT(27)) {\n        qemu_log_mask(LOG_UNIMP, \"sm501: only local memory is supported.\\n\");\n        return;\n    }\n\n    switch (cmd) {\n    case 0x00: /* copy area */\n    {\n        int src_x = (s->twoD_source >> 16) & 0x01FFF;\n        int src_y = s->twoD_source & 0xFFFF;\n        uint32_t src_base = s->twoD_source_base & 0x03FFFFFF;\n        uint8_t *src = s->local_mem + src_base;\n        int src_pitch = s->twoD_pitch & 0x1FFF;\n\n#define COPY_AREA(_bpp, _pixel_type, rtl) {                                   \\\n        int y, x, index_d, index_s;                                           \\\n        for (y = 0; y < height; y++) {                              \\\n            for (x = 0; x < width; x++) {                           \\\n                _pixel_type val;                                              \\\n                                                                              \\\n                if (rtl) {                                                    \\\n                    index_s = ((src_y - y) * src_pitch + src_x - x) * _bpp;   \\\n                    index_d = ((dst_y - y) * dst_pitch + dst_x - x) * _bpp;   \\\n                } else {                                                      \\\n                    index_s = ((src_y + y) * src_pitch + src_x + x) * _bpp;   \\\n                    index_d = ((dst_y + y) * dst_pitch + dst_x + x) * _bpp;   \\\n                }                                                             \\\n                if (rop_mode == 1 && rop == 5) {                              \\\n                    /* Invert dest */                                         \\\n                    val = ~*(_pixel_type *)&dst[index_d];                     \\\n                } else {                                                      \\\n                    val = *(_pixel_type *)&src[index_s];                      \\\n                }                                                             \\\n                *(_pixel_type *)&dst[index_d] = val;                          \\\n            }                                                                 \\\n        }                                                                     \\\n    }\n        switch (format) {\n        case 0:\n            COPY_AREA(1, uint8_t, rtl);\n            break;\n        case 1:\n            COPY_AREA(2, uint16_t, rtl);\n            break;\n        case 2:\n            COPY_AREA(4, uint32_t, rtl);\n            break;\n        }\n        break;\n    }\n    case 0x01: /* fill rectangle */\n    {\n        uint32_t color = s->twoD_foreground;\n\n#define FILL_RECT(_bpp, _pixel_type) {                                      \\\n        int y, x;                                                           \\\n        for (y = 0; y < height; y++) {                            \\\n            for (x = 0; x < width; x++) {                         \\\n                int index = ((dst_y + y) * dst_pitch + dst_x + x) * _bpp;   \\\n                *(_pixel_type *)&dst[index] = (_pixel_type)color;           \\\n            }                                                               \\\n        }                                                                   \\\n    }\n\n        switch (format) {\n        case 0:\n            FILL_RECT(1, uint8_t);\n            break;\n        case 1:\n            color = cpu_to_le16(color);\n            FILL_RECT(2, uint16_t);\n            break;\n        case 2:\n            color = cpu_to_le32(color);\n            FILL_RECT(4, uint32_t);\n            break;\n        }\n        break;\n    }\n    default:\n        qemu_log_mask(LOG_UNIMP, \"sm501: not implemented 2D operation: %d\\n\",\n                      cmd);", "project": "qemu", "hash": 231921026198244072541449258178431781416, "size": 139, "commit_id": "b15a22bbcbe6a78dc3d88fe3134985e4cdd87de4", "message": "sm501: Replace hand written implementation with pixman where possible\n\nBesides being faster this should also prevent malicious guests to\nabuse 2D engine to overwrite data or cause a crash.\n\nSigned-off-by: BALATON Zoltan <balaton@eik.bme.hu>\nMessage-id: 58666389b6cae256e4e972a32c05cf8aa51bffc0.1590089984.git.balaton@eik.bme.hu\nSigned-off-by: Gerd Hoffmann <kraxel@redhat.com>", "target": 1, "dataset": "other", "idx": 204867}
{"func": "    int format = (s->twoD_stretch >> 20) & 0x3;\n    int rop_mode = (s->twoD_control >> 15) & 0x1; /* 1 for rop2, else rop3 */\n    /* 1 if rop2 source is the pattern, otherwise the source is the bitmap */\n    int rop2_source_is_pattern = (s->twoD_control >> 14) & 0x1;\n    int rop = s->twoD_control & 0xFF;\n    unsigned int dst_x = (s->twoD_destination >> 16) & 0x01FFF;\n    unsigned int dst_y = s->twoD_destination & 0xFFFF;\n    unsigned int width = (s->twoD_dimension >> 16) & 0x1FFF;\n    unsigned int height = s->twoD_dimension & 0xFFFF;\n    uint32_t dst_base = s->twoD_destination_base & 0x03FFFFFF;\n    unsigned int dst_pitch = (s->twoD_pitch >> 16) & 0x1FFF;\n    int crt = (s->dc_crt_control & SM501_DC_CRT_CONTROL_SEL) ? 1 : 0;\n    int fb_len = get_width(s, crt) * get_height(s, crt) * get_bpp(s, crt);\n\n    if ((s->twoD_stretch >> 16) & 0xF) {\n        qemu_log_mask(LOG_UNIMP, \"sm501: only XY addressing is supported.\\n\");\n        return;\n    }\n\n    if (s->twoD_source_base & BIT(27) || s->twoD_destination_base & BIT(27)) {\n        qemu_log_mask(LOG_UNIMP, \"sm501: only local memory is supported.\\n\");\n        return;\n    }\n\n    if (!dst_pitch) {\n        qemu_log_mask(LOG_GUEST_ERROR, \"sm501: Zero dest pitch.\\n\");\n        return;\n    }\n\n    if (!width || !height) {\n        qemu_log_mask(LOG_GUEST_ERROR, \"sm501: Zero size 2D op.\\n\");\n        return;\n    }\n\n    if (rtl) {\n        dst_x -= width - 1;\n        dst_y -= height - 1;\n    }\n\n    if (dst_base >= get_local_mem_size(s) || dst_base +\n        (dst_x + width + (dst_y + height) * (dst_pitch + width)) *\n        (1 << format) >= get_local_mem_size(s)) {\n        qemu_log_mask(LOG_GUEST_ERROR, \"sm501: 2D op dest is outside vram.\\n\");\n        return;\n    }\n\n    switch (cmd) {\n    case 0: /* BitBlt */\n    {\n        unsigned int src_x = (s->twoD_source >> 16) & 0x01FFF;\n        unsigned int src_y = s->twoD_source & 0xFFFF;\n        uint32_t src_base = s->twoD_source_base & 0x03FFFFFF;\n        unsigned int src_pitch = s->twoD_pitch & 0x1FFF;\n\n        if (!src_pitch) {\n            qemu_log_mask(LOG_GUEST_ERROR, \"sm501: Zero src pitch.\\n\");\n            return;\n        }\n\n        if (rtl) {\n            src_x -= width - 1;\n            src_y -= height - 1;\n        }\n\n        if (src_base >= get_local_mem_size(s) || src_base +\n            (src_x + width + (src_y + height) * (src_pitch + width)) *\n            (1 << format) >= get_local_mem_size(s)) {\n            qemu_log_mask(LOG_GUEST_ERROR,\n                          \"sm501: 2D op src is outside vram.\\n\");\n            return;\n        }\n\n        if ((rop_mode && rop == 0x5) || (!rop_mode && rop == 0x55)) {\n            /* Invert dest, is there a way to do this with pixman? */\n            unsigned int x, y, i;\n            uint8_t *d = s->local_mem + dst_base;\n\n            for (y = 0; y < height; y++) {\n                i = (dst_x + (dst_y + y) * dst_pitch) * (1 << format);\n                for (x = 0; x < width; x++, i += (1 << format)) {\n                    switch (format) {\n                    case 0:\n                        d[i] = ~d[i];\n                        break;\n                    case 1:\n                        *(uint16_t *)&d[i] = ~*(uint16_t *)&d[i];\n                        break;\n                    case 2:\n                        *(uint32_t *)&d[i] = ~*(uint32_t *)&d[i];\n                        break;\n                    }\n                }\n            }\n        } else {\n            /* Do copy src for unimplemented ops, better than unpainted area */\n            if ((rop_mode && (rop != 0xc || rop2_source_is_pattern)) ||\n                (!rop_mode && rop != 0xcc)) {\n                qemu_log_mask(LOG_UNIMP,\n                              \"sm501: rop%d op %x%s not implemented\\n\",\n                              (rop_mode ? 2 : 3), rop,\n                              (rop2_source_is_pattern ?\n                                  \" with pattern source\" : \"\"));\n            }\n            /* Check for overlaps, this could be made more exact */\n            uint32_t sb, se, db, de;\n            sb = src_base + src_x + src_y * (width + src_pitch);\n            se = sb + width + height * (width + src_pitch);\n            db = dst_base + dst_x + dst_y * (width + dst_pitch);\n            de = db + width + height * (width + dst_pitch);\n            if (rtl && ((db >= sb && db <= se) || (de >= sb && de <= se))) {\n                /* regions may overlap: copy via temporary */\n                int llb = width * (1 << format);\n                int tmp_stride = DIV_ROUND_UP(llb, sizeof(uint32_t));\n                uint32_t *tmp = g_malloc(tmp_stride * sizeof(uint32_t) *\n                                         height);\n                pixman_blt((uint32_t *)&s->local_mem[src_base], tmp,\n                           src_pitch * (1 << format) / sizeof(uint32_t),\n                           tmp_stride, 8 * (1 << format), 8 * (1 << format),\n                           src_x, src_y, 0, 0, width, height);\n                pixman_blt(tmp, (uint32_t *)&s->local_mem[dst_base],\n                           tmp_stride,\n                           dst_pitch * (1 << format) / sizeof(uint32_t),\n                           8 * (1 << format), 8 * (1 << format),\n                           0, 0, dst_x, dst_y, width, height);\n                g_free(tmp);\n            } else {\n                pixman_blt((uint32_t *)&s->local_mem[src_base],\n                           (uint32_t *)&s->local_mem[dst_base],\n                           src_pitch * (1 << format) / sizeof(uint32_t),\n                           dst_pitch * (1 << format) / sizeof(uint32_t),\n                           8 * (1 << format), 8 * (1 << format),\n                           src_x, src_y, dst_x, dst_y, width, height);\n            }\n        }\n        break;\n    }\n    case 1: /* Rectangle Fill */\n    {\n        uint32_t color = s->twoD_foreground;\n\n        if (format == 2) {\n            color = cpu_to_le32(color);\n        } else if (format == 1) {\n            color = cpu_to_le16(color);\n        }\n\n        pixman_fill((uint32_t *)&s->local_mem[dst_base],\n                    dst_pitch * (1 << format) / sizeof(uint32_t),\n                    8 * (1 << format), dst_x, dst_y, width, height, color);\n        break;\n    }\n    default:\n        qemu_log_mask(LOG_UNIMP, \"sm501: not implemented 2D operation: %d\\n\",\n                      cmd);", "project": "qemu", "hash": 193599277263988750788636095730957842514, "size": 170, "commit_id": "b15a22bbcbe6a78dc3d88fe3134985e4cdd87de4", "message": "sm501: Replace hand written implementation with pixman where possible\n\nBesides being faster this should also prevent malicious guests to\nabuse 2D engine to overwrite data or cause a crash.\n\nSigned-off-by: BALATON Zoltan <balaton@eik.bme.hu>\nMessage-id: 58666389b6cae256e4e972a32c05cf8aa51bffc0.1590089984.git.balaton@eik.bme.hu\nSigned-off-by: Gerd Hoffmann <kraxel@redhat.com>", "target": 0, "dataset": "other", "idx": 367048}
{"func": "        }\n\n        if (newIdx < 0)\n            return 1;\n\n        p_->idx_ = static_cast<long>(newIdx);   //not very sure about this. need more test!!    - note by Shawn  fly2xj@gmail.com //TODO\n        p_->eof_ = false;\n        return 0;\n    }", "project": "exiv2", "hash": 299218834992083440493543625582483690654, "size": 23, "commit_id": "bd0afe0390439b2c424d881c8c6eb0c5624e31d9", "message": "Add bounds check to MemIo::seek(). (#944)\n\n- Regression test for missing bounds check in MemIo::seek()\r\n- Add bounds check to MemIo::seek(), this fixes CVE-2019-13504", "target": 1, "dataset": "other", "idx": 204868}
{"func": "        }\n\n        if (newIdx < 0)\n            return 1;\n\n        if (static_cast<size_t>(newIdx) > p_->size_) {\n            p_->eof_ = true;\n            return 1;\n        }\n\n        p_->idx_ = static_cast<size_t>(newIdx);\n        p_->eof_ = false;\n        return 0;\n    }", "project": "exiv2", "hash": 109191990002104936880753982885886945308, "size": 28, "commit_id": "bd0afe0390439b2c424d881c8c6eb0c5624e31d9", "message": "Add bounds check to MemIo::seek(). (#944)\n\n- Regression test for missing bounds check in MemIo::seek()\r\n- Add bounds check to MemIo::seek(), this fixes CVE-2019-13504", "target": 0, "dataset": "other", "idx": 367064}
{"func": "    decimateSum = 0;\n    decimateCount = 0;\n\n    // choose decimation factor so that result is approx. 1000 Hz\n    decimateBy = sampleRate / TARGET_SRATE;\n    assert(decimateBy > 0);\n    assert(INPUT_BLOCK_SIZE < decimateBy * DECIMATED_BLOCK_SIZE);\n\n    // Calculate window length & starting item according to desired min & max bpms\n    windowLen = (60 * sampleRate) / (decimateBy * MIN_BPM);\n    windowStart = (60 * sampleRate) / (decimateBy * MAX_BPM_RANGE);\n", "project": "soundtouch", "hash": 338493797798567657703708124487583988357, "size": 46, "commit_id": "a1c400eb2cff849c0e5f9d6916d69ffea3ad2c85", "message": "Fix issue CVE-2018-17096: Replace assert with runtime exception", "target": 1, "dataset": "other", "idx": 205562}
{"func": "    decimateSum = 0;\n    decimateCount = 0;\n\n    // choose decimation factor so that result is approx. 1000 Hz\n    decimateBy = sampleRate / TARGET_SRATE;\n    if ((decimateBy <= 0) || (decimateBy * DECIMATED_BLOCK_SIZE < INPUT_BLOCK_SIZE))\n    {\n        ST_THROW_RT_ERROR(\"Too small samplerate\");\n    }\n\n    // Calculate window length & starting item according to desired min & max bpms\n    windowLen = (60 * sampleRate) / (decimateBy * MIN_BPM);\n    windowStart = (60 * sampleRate) / (decimateBy * MAX_BPM_RANGE);\n", "project": "soundtouch", "hash": 9975483775436489168942805933636882922, "size": 48, "commit_id": "a1c400eb2cff849c0e5f9d6916d69ffea3ad2c85", "message": "Fix issue CVE-2018-17096: Replace assert with runtime exception", "target": 0, "dataset": "other", "idx": 368106}
{"func": "#pragma GCC diagnostic ignored \"-Wstrict-overflow\"\n      if (seg_len <= 4 || (seg_len & 3) != 0) {\n#pragma GCC diagnostic pop\n        if (is_root || !h->unsafe) {\n          SET_ERRNO (ENOTSUP,\n                     \"%s, the block at 0x%zx has invalid size %\" PRIu32\n                     \", bad registry\",\n                     filename, blkoff, le32toh (block->seg_len));\n          goto error;\n        } else {\n          DEBUG (2,\n                 \"%s: block at 0x%zx has invalid size %\" PRIu32 \", skipping\",\n                 filename, blkoff, le32toh (block->seg_len));\n          break;\n        }\n      }\n\n      if (h->msglvl >= 2) {\n        unsigned char *id = (unsigned char *) block->id;\n        int id0 = id[0], id1 = id[1];", "project": "hivex", "hash": 255254112256589566988814098457378736252, "size": 349, "commit_id": "8f1935733b10d974a1a4176d38dd151ed98cf381", "message": "lib/handle.c: Bounds check for block exceeding page length (CVE-2021-3504)\n\nHives are encoded as fixed-sized pages containing smaller variable-\nlength blocks:\n\n  +-------------------+-------------------+-------------------+--\n  | header            |[ blk ][blk][ blk ]|[blk][blk][blk]    |\n  +-------------------+-------------------+-------------------+--\n\nBlocks should not straddle a page boundary.  However because blocks\ncontain a 32 bit length field it is possible to construct an invalid\nhive where the last block in a page overlaps either the next page or\nthe end of the file:\n\n  +-------------------+-------------------+\n  | header            |[ blk ][blk][ blk ..... ]\n  +-------------------+-------------------+\n\nHivex lacked a bounds check and would process the registry.  Because\nthe rest of the code assumes this situation can never happen it was\npossible to have a block containing some field (eg. a registry key\nname) which would extend beyond the end of the file.  Hivex mmaps or\nmallocs the file, causing hivex to read memory beyond the end of the\nmapped region, resulting in reading other memory structures or a\ncrash.  (Writing beyond the end of the mapped region seems to be\nimpossible because we always allocate a new page before writing.)\n\nThis commit adds a check which rejects the malformed registry on\nhivex_open.\n\nCredit: Jeremy Galindo, Sr Security Engineer, Datto.com\nSigned-off-by: Richard W.M. Jones <rjones@redhat.com>\nFixes: CVE-2021-3504\nFixes: https://bugzilla.redhat.com/show_bug.cgi?id=1949687", "target": 1, "dataset": "other", "idx": 205584}
{"func": "#pragma GCC diagnostic ignored \"-Wstrict-overflow\"\n      if (seg_len <= 4 || (seg_len & 3) != 0) {\n#pragma GCC diagnostic pop\n        if (is_root || !h->unsafe) {\n          SET_ERRNO (ENOTSUP,\n                     \"%s, the block at 0x%zx size %\" PRIu32\n                     \" <= 4 or not a multiple of 4, bad registry\",\n                     filename, blkoff, le32toh (block->seg_len));\n          goto error;\n        } else {\n          DEBUG (2,\n                 \"%s: block at 0x%zx has invalid size %\" PRIu32 \", skipping\",\n                 filename, blkoff, le32toh (block->seg_len));\n          break;\n        }\n      }\n\n      if (blkoff + seg_len > off + page_size) {\n        SET_ERRNO (ENOTSUP,\n                   \"%s, the block at 0x%zx size %\" PRIu32\n                   \" extends beyond the current page, bad registry\",\n                   filename, blkoff, le32toh (block->seg_len));\n        goto error;\n      }\n\n      if (h->msglvl >= 2) {\n        unsigned char *id = (unsigned char *) block->id;\n        int id0 = id[0], id1 = id[1];", "project": "hivex", "hash": 191982106855348682819795174886766010383, "size": 357, "commit_id": "8f1935733b10d974a1a4176d38dd151ed98cf381", "message": "lib/handle.c: Bounds check for block exceeding page length (CVE-2021-3504)\n\nHives are encoded as fixed-sized pages containing smaller variable-\nlength blocks:\n\n  +-------------------+-------------------+-------------------+--\n  | header            |[ blk ][blk][ blk ]|[blk][blk][blk]    |\n  +-------------------+-------------------+-------------------+--\n\nBlocks should not straddle a page boundary.  However because blocks\ncontain a 32 bit length field it is possible to construct an invalid\nhive where the last block in a page overlaps either the next page or\nthe end of the file:\n\n  +-------------------+-------------------+\n  | header            |[ blk ][blk][ blk ..... ]\n  +-------------------+-------------------+\n\nHivex lacked a bounds check and would process the registry.  Because\nthe rest of the code assumes this situation can never happen it was\npossible to have a block containing some field (eg. a registry key\nname) which would extend beyond the end of the file.  Hivex mmaps or\nmallocs the file, causing hivex to read memory beyond the end of the\nmapped region, resulting in reading other memory structures or a\ncrash.  (Writing beyond the end of the mapped region seems to be\nimpossible because we always allocate a new page before writing.)\n\nThis commit adds a check which rejects the malformed registry on\nhivex_open.\n\nCredit: Jeremy Galindo, Sr Security Engineer, Datto.com\nSigned-off-by: Richard W.M. Jones <rjones@redhat.com>\nFixes: CVE-2021-3504\nFixes: https://bugzilla.redhat.com/show_bug.cgi?id=1949687", "target": 0, "dataset": "other", "idx": 368604}
{"func": "  assert(Z != (double *) NULL);\n  if (L > (CIEK*CIEEpsilon))\n    *Y=(double) pow((L+16.0)/116.0,3.0);\n  else\n    *Y=L/CIEK;\n  gamma=PerceptibleReciprocal((((52.0*L/(u+13.0*L*(4.0*D65X/(D65X+15.0*D65Y+\n    3.0*D65Z))))-1.0)/3.0)-(-1.0/3.0));\n  *X=gamma*((*Y*((39.0*L/(v+13.0*L*(9.0*D65Y/(D65X+15.0*D65Y+3.0*D65Z))))-5.0))+\n    5.0*(*Y));\n  *Z=(*X*(((52.0*L/(u+13.0*L*(4.0*D65X/(D65X+15.0*D65Y+3.0*D65Z))))-1.0)/3.0))-\n    5.0*(*Y);\n}", "project": "ImageMagick", "hash": 38417910825899276057542672543600594750, "size": 20, "commit_id": "a855d3ad660f307fdb071794351822f9ce878c4e", "message": "https://github.com/ImageMagick/ImageMagick/issues/3317", "target": 1, "dataset": "other", "idx": 205631}
{"func": "  assert(Z != (double *) NULL);\n  if (L > (CIEK*CIEEpsilon))\n    *Y=(double) pow((L+16.0)/116.0,3.0);\n  else\n    *Y=L/CIEK;\n  gamma=PerceptibleReciprocal((((52.0*L*PerceptibleReciprocal(u+13.0*L*\n    (4.0*D65X/(D65X+15.0*D65Y+3.0*D65Z))))-1.0)/3.0)-(-1.0/3.0));\n  *X=gamma*((*Y*((39.0*L*PerceptibleReciprocal(v+13.0*L*(9.0*D65Y/\n    (D65X+15.0*D65Y+3.0*D65Z))))-5.0))+5.0*(*Y));\n  *Z=(*X*(((52.0*L*PerceptibleReciprocal(u+13.0*L*(4.0*D65X/\n    (D65X+15.0*D65Y+3.0*D65Z))))-1.0)/3.0))-5.0*(*Y);\n}", "project": "ImageMagick", "hash": 187280963848298459107445613020860070554, "size": 20, "commit_id": "a855d3ad660f307fdb071794351822f9ce878c4e", "message": "https://github.com/ImageMagick/ImageMagick/issues/3317", "target": 0, "dataset": "other", "idx": 369443}
{"func": "                    str::stream() << \"$arrayToObject requires an array of key-value pairs, where \"\n                                     \"the key must be of type string. Found key type: \"\n                                  << typeName(valArray[0].getType()),\n                    (valArray[0].getType() == BSONType::String));\n\n            output[valArray[0].getString()] = valArray[1];\n\n        } else {\n            uassert(\n                40391,\n                str::stream() << \"$arrayToObject requires a consistent input format. Elements must\"\n                str::stream() << \"$arrayToObject requires an object with keys 'k' and 'v', where \"\n                                 \"the value of 'k' must be of type string. Found type: \"\n                              << typeName(key.getType()),\n                (key.getType() == BSONType::String));\n\n            output[key.getString()] = value;\n        }\n    }\n\n    return output.freezeToValue();\n}", "project": "mongo", "hash": 251402268363123730144063928657664987891, "size": 92, "commit_id": "1772b9a0393b55e6a280a35e8f0a1f75c014f301", "message": "SERVER-49404 Enforce additional checks in $arrayToObject", "target": 1, "dataset": "other", "idx": 205669}
{"func": "                    str::stream() << \"$arrayToObject requires an array of key-value pairs, where \"\n                                     \"the key must be of type string. Found key type: \"\n                                  << typeName(valArray[0].getType()),\n                    (valArray[0].getType() == BSONType::String));\n\n            auto keyName = valArray[0].getStringData();\n\n            uassert(4940400,\n                    \"Key field cannot contain an embedded null byte\",\n                    keyName.find('\\0') == std::string::npos);\n\n            output[keyName] = valArray[1];\n\n        } else {\n            uassert(\n                40391,\n                str::stream() << \"$arrayToObject requires a consistent input format. Elements must\"\n                str::stream() << \"$arrayToObject requires an object with keys 'k' and 'v', where \"\n                                 \"the value of 'k' must be of type string. Found type: \"\n                              << typeName(key.getType()),\n                (key.getType() == BSONType::String));\n\n            auto keyName = key.getStringData();\n\n            uassert(4940401,\n                    \"Key field cannot contain an embedded null byte\",\n                    keyName.find('\\0') == std::string::npos);\n\n            output[keyName] = value;\n        }\n    }\n\n    return output.freezeToValue();\n}", "project": "mongo", "hash": 32132725791897768420289240736903683617, "size": 104, "commit_id": "1772b9a0393b55e6a280a35e8f0a1f75c014f301", "message": "SERVER-49404 Enforce additional checks in $arrayToObject", "target": 0, "dataset": "other", "idx": 370060}
{"func": "static MSUSB_PIPE_DESCRIPTOR** msusb_mspipes_read(wStream* s, UINT32 NumberOfPipes)\n{\n\tUINT32 pnum;\n\tMSUSB_PIPE_DESCRIPTOR** MsPipes;\n\n\tif (Stream_GetRemainingCapacity(s) < 12 * NumberOfPipes)\n\t\treturn NULL;\n\n\tMsPipes = (MSUSB_PIPE_DESCRIPTOR**)calloc(NumberOfPipes, sizeof(MSUSB_PIPE_DESCRIPTOR*));\n\n\tif (!MsPipes)", "project": "FreeRDP", "hash": 43522950749896036127664697861930743417, "size": 43, "commit_id": "9f77fc3dd2394373e1be753952b00dafa1a9b7da", "message": "Fixed int overflow in msusb_mspipes_read\n\nThanks to hac425", "target": 1, "dataset": "other", "idx": 205671}
{"func": "static MSUSB_PIPE_DESCRIPTOR** msusb_mspipes_read(wStream* s, UINT32 NumberOfPipes)\n{\n\tUINT32 pnum;\n\tMSUSB_PIPE_DESCRIPTOR** MsPipes;\n\n\tif (Stream_GetRemainingCapacity(s) / 12 < NumberOfPipes)\n\t\treturn NULL;\n\n\tMsPipes = (MSUSB_PIPE_DESCRIPTOR**)calloc(NumberOfPipes, sizeof(MSUSB_PIPE_DESCRIPTOR*));\n\n\tif (!MsPipes)", "project": "FreeRDP", "hash": 327097256253608261746407166636662999864, "size": 43, "commit_id": "9f77fc3dd2394373e1be753952b00dafa1a9b7da", "message": "Fixed int overflow in msusb_mspipes_read\n\nThanks to hac425", "target": 0, "dataset": "other", "idx": 370198}
{"func": "    if (WebPPictureInit(&picture) == 0)\n      ThrowWriterException(ResourceLimitError,\"UnableToEncodeImageFile\");\n\n    WriteSingleWEBPImage(image_info, image, &picture, current, exception);\n\n    effective_delta = image->delay*1000/image->ticks_per_second;\n    if (effective_delta < 10)\n      effective_delta = 100; /* Consistent with gif2webp */\n    frame_timestamp+=effective_delta;\n\n    WebPAnimEncoderAdd(enc,&picture,(int) frame_timestamp,configure);", "project": "ImageMagick6", "hash": 88540247361010854619934050091886652539, "size": 68, "commit_id": "a78d92dc0f468e79c3d761aae9707042952cdaca", "message": "https://github.com/ImageMagick/ImageMagick/issues/3176", "target": 1, "dataset": "other", "idx": 205720}
{"func": "    if (WebPPictureInit(&picture) == 0)\n      ThrowWriterException(ResourceLimitError,\"UnableToEncodeImageFile\");\n\n    WriteSingleWEBPImage(image_info, image, &picture, current, exception);\n\n    effective_delta = image->delay*1000*PerceptibleReciprocal(\n      image->ticks_per_second);\n    if (effective_delta < 10)\n      effective_delta = 100; /* Consistent with gif2webp */\n    frame_timestamp+=effective_delta;\n\n    WebPAnimEncoderAdd(enc,&picture,(int) frame_timestamp,configure);", "project": "ImageMagick6", "hash": 184885542608773702111101964268936553139, "size": 69, "commit_id": "a78d92dc0f468e79c3d761aae9707042952cdaca", "message": "https://github.com/ImageMagick/ImageMagick/issues/3176", "target": 0, "dataset": "other", "idx": 370809}
{"func": "      for (i=0; alpha > poisson; i++)\n      {\n        beta=GetPseudoRandomValue(random_info);\n        alpha*=beta;\n      }\n      noise=(double) (QuantumRange*i/SigmaPoisson);\n      break;\n    }\n    case RandomNoise:\n    {\n      noise=(double) (QuantumRange*SigmaRandom*alpha);", "project": "ImageMagick6", "hash": 254287255275900171659283262606636567539, "size": 107, "commit_id": "90255f0834eead08d59f46b0bda7b1580451cc0f", "message": "https://github.com/ImageMagick/ImageMagick/issues/3077", "target": 1, "dataset": "other", "idx": 205775}
{"func": "      for (i=0; alpha > poisson; i++)\n      {\n        beta=GetPseudoRandomValue(random_info);\n        alpha*=beta;\n      }\n      noise=(double) (QuantumRange*i*PerceptibleReciprocal(SigmaPoisson));\n      break;\n    }\n    case RandomNoise:\n    {\n      noise=(double) (QuantumRange*SigmaRandom*alpha);", "project": "ImageMagick6", "hash": 58007218466345329691975839082535306927, "size": 107, "commit_id": "90255f0834eead08d59f46b0bda7b1580451cc0f", "message": "https://github.com/ImageMagick/ImageMagick/issues/3077", "target": 0, "dataset": "other", "idx": 371767}
{"func": "\tconst char\t*sp;\n\tchar\t\t*bp;\n\tint\t\t\t off;\n\ttime_t\t\t now;\n\ttime_t\t\t then;\n\n\t/* We had better go around this loop exactly twice! */\n\tthen = 0;\n\tfor (sp = stamp; *sp; sp++) {\n\t\tbp = strchr(SRS_TIME_BASECHARS, toupper(*sp));\n\t\tif (bp == NULL)", "project": "postsrsd", "hash": 230822056572576587686188533238108797740, "size": 27, "commit_id": "4733fb11f6bec6524bb8518c5e1a699288c26bac", "message": "SECURITY: Fix potential denial of service attack against PostSRSd\n\nI discovered that PostSRSd could be tricked into consuming a lot of CPU\ntime with an SRS address that has an excessively long time stamp tag,\ne.g.\n\nSRS0=HHHH=TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT=0@example.com", "target": 1, "dataset": "other", "idx": 205810}
{"func": "\tchar\t\t*bp;\n\tint\t\t\t off;\n\ttime_t\t\t now;\n\ttime_t\t\t then;\n\n\tif (strlen(stamp) != 2) return SRS_ETIMESTAMPOUTOFDATE;\n\t/* We had better go around this loop exactly twice! */\n\tthen = 0;\n\tfor (sp = stamp; *sp; sp++) {\n\t\tbp = strchr(SRS_TIME_BASECHARS, toupper(*sp));\n\t\tif (bp == NULL)", "project": "postsrsd", "hash": 179575258576723630835422814195992797694, "size": 28, "commit_id": "4733fb11f6bec6524bb8518c5e1a699288c26bac", "message": "SECURITY: Fix potential denial of service attack against PostSRSd\n\nI discovered that PostSRSd could be tricked into consuming a lot of CPU\ntime with an SRS address that has an excessively long time stamp tag,\ne.g.\n\nSRS0=HHHH=TTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT=0@example.com", "target": 0, "dataset": "other", "idx": 372380}
{"func": "static rsRetVal qAddDirect(qqueue_t *pThis, void* pUsr)\n{\n\tbatch_t singleBatch;\n\tbatch_obj_t batchObj;\n\tDEFiRet;\n\n\t//TODO: init batchObj (states _OK and new fields -- CHECK)\n\tASSERT(pThis != NULL);\n\n\tbatchObj.state = BATCH_STATE_RDY;\n\tbatchObj.pUsrp = (obj_t*) pUsr;\n\tbatchObj.bFilterOK = 1;\n\tsingleBatch.nElem = 1; /* there always is only one in direct mode */\n\tsingleBatch.pElem = &batchObj;\n\tiRet = pThis->pConsumer(pThis->pUsr, &singleBatch, &pThis->bShutdownImmediate);\n\tobjDestruct(pUsr);\n\n\tRETiRet;\n}", "project": "rsyslog", "hash": 246917057648162918663491604797767470769, "size": 29, "commit_id": "dfa88369d4ca4290db56b843f9eabdae1bfe0fd5", "message": "bugfix: memory leak when $RepeatedMsgReduction on was used\n\nbug tracker: http://bugzilla.adiscon.com/show_bug.cgi?id=225", "target": 1, "dataset": "other", "idx": 205840}
{"func": "static rsRetVal qAddDirect(qqueue_t *pThis, void* pUsr)\n{\n\tbatch_t singleBatch;\n\tbatch_obj_t batchObj;\n\tint i;\n\tDEFiRet;\n\n\t//TODO: init batchObj (states _OK and new fields -- CHECK)\n\tASSERT(pThis != NULL);\n\n\tbatchObj.pUsrp = (obj_t*) pUsr;\n\tbatchObj.bFilterOK = 1;\n\tsingleBatch.nElem = 1; /* there always is only one in direct mode */\n\tsingleBatch.pElem = &batchObj;\n\tiRet = pThis->pConsumer(pThis->pUsr, &singleBatch, &pThis->bShutdownImmediate);\n\t/* delete the batch string params: TODO: create its own \"class\" for this */\n\tfor(i = 0 ; i < CONF_OMOD_NUMSTRINGS_MAXSIZE ; ++i) {\n\t\tfree(batchObj.staticActStrings[i]);\n\t}\n\tobjDestruct(pUsr);\n\n\tRETiRet;\n}", "project": "rsyslog", "hash": 258491197826171869327416197774899637452, "size": 34, "commit_id": "dfa88369d4ca4290db56b843f9eabdae1bfe0fd5", "message": "bugfix: memory leak when $RepeatedMsgReduction on was used\n\nbug tracker: http://bugzilla.adiscon.com/show_bug.cgi?id=225", "target": 0, "dataset": "other", "idx": 373783}
{"func": "    int descr_type;\n    struct LeafDescriptor leaf;\n\n    GC_ASSERT(GC_explicit_typing_initialized);\n    descr_type = GC_make_array_descriptor((word)n, (word)lb, d, &simple_descr,\n                                          &complex_descr, &leaf);\n    switch(descr_type) {\n        case NO_MEM: return(0);\n        case SIMPLE: return(GC_malloc_explicitly_typed(n*lb, simple_descr));\n        case LEAF:\n            lb *= n;\n            lb += sizeof(struct LeafDescriptor) + TYPD_EXTRA_BYTES;\n            break;\n        case COMPLEX:\n            lb *= n;\n            lb += TYPD_EXTRA_BYTES;\n            break;\n    }\n    op = GC_malloc_kind(lb, GC_array_kind);\n    if (EXPECT(NULL == op, FALSE))", "project": "bdwgc", "hash": 129186795195986820153089371839787067250, "size": 61, "commit_id": "4e1a6f9d8f2a49403bbd00b8c8e5324048fb84d4", "message": "Fix calloc_explicitly_typed in case of lb*n overflow\n\n* typd_mlc.c: Include limits.h (for SIZE_MAX).\n* typd_mlc.c (GC_SIZE_MAX, GC_SQRT_SIZE_MAX): New macro (same as in\nmalloc.c).\n* typd_mlc.c (GC_calloc_explicitly_typed): Return NULL if lb * n\noverflows (same algorithm as in calloc defined in malloc.c); eliminate\nlb *= n code duplication.", "target": 1, "dataset": "other", "idx": 205872}
{"func": "    struct LeafDescriptor leaf;\n\n    GC_ASSERT(GC_explicit_typing_initialized);\n    descr_type = GC_make_array_descriptor((word)n, (word)lb, d, &simple_descr,\n                                          &complex_descr, &leaf);\n    if ((lb | n) > GC_SQRT_SIZE_MAX /* fast initial check */\n        && lb > 0 && n > GC_SIZE_MAX / lb)\n      return NULL; /* n*lb overflow */\n    lb *= n;\n    switch(descr_type) {\n        case NO_MEM: return(0);\n        case SIMPLE:\n            return GC_malloc_explicitly_typed(lb, simple_descr);\n        case LEAF:\n            lb += sizeof(struct LeafDescriptor) + TYPD_EXTRA_BYTES;\n            break;\n        case COMPLEX:\n            lb += TYPD_EXTRA_BYTES;\n            break;\n    }\n    op = GC_malloc_kind(lb, GC_array_kind);\n    if (EXPECT(NULL == op, FALSE))", "project": "bdwgc", "hash": 196754813537280026105640227782301143544, "size": 64, "commit_id": "4e1a6f9d8f2a49403bbd00b8c8e5324048fb84d4", "message": "Fix calloc_explicitly_typed in case of lb*n overflow\n\n* typd_mlc.c: Include limits.h (for SIZE_MAX).\n* typd_mlc.c (GC_SIZE_MAX, GC_SQRT_SIZE_MAX): New macro (same as in\nmalloc.c).\n* typd_mlc.c (GC_calloc_explicitly_typed): Return NULL if lb * n\noverflows (same algorithm as in calloc defined in malloc.c); eliminate\nlb *= n code duplication.", "target": 0, "dataset": "other", "idx": 374064}
{"func": "\tDEFAULT_0_PARAMS;\n\t\n\tres = estrdup(\"\");\n\tstr = &res;\n\n\ttrace = zend_read_property(default_exception_ce, getThis(), \"trace\", sizeof(\"trace\")-1, 1 TSRMLS_CC);\n\tzend_hash_apply_with_arguments(Z_ARRVAL_P(trace) TSRMLS_CC, (apply_func_args_t)_build_trace_string, 3, str, len, &num);\n\n\ts_tmp = emalloc(1 + MAX_LENGTH_OF_LONG + 7 + 1);\n\tsprintf(s_tmp, \"#%d {main}\", num);\n\tTRACE_APPEND_STRL(s_tmp, strlen(s_tmp));", "project": "php-src", "hash": 241319124076641660271307693135429377686, "size": 22, "commit_id": "a894a8155fab068d68a04bf181dbaddfa01ccbb0", "message": "More fixes for bug #69152", "target": 1, "dataset": "other", "idx": 205882}
{"func": "\t\n\tres = estrdup(\"\");\n\tstr = &res;\n\n\ttrace = zend_read_property(default_exception_ce, getThis(), \"trace\", sizeof(\"trace\")-1, 1 TSRMLS_CC);\n\tif(Z_TYPE_P(trace) != IS_ARRAY) {\n\t\tRETURN_FALSE;\n\t}\n\tzend_hash_apply_with_arguments(Z_ARRVAL_P(trace) TSRMLS_CC, (apply_func_args_t)_build_trace_string, 3, str, len, &num);\n\n\ts_tmp = emalloc(1 + MAX_LENGTH_OF_LONG + 7 + 1);\n\tsprintf(s_tmp, \"#%d {main}\", num);\n\tTRACE_APPEND_STRL(s_tmp, strlen(s_tmp));", "project": "php-src", "hash": 150063148970590872214374230082563770268, "size": 25, "commit_id": "a894a8155fab068d68a04bf181dbaddfa01ccbb0", "message": "More fixes for bug #69152", "target": 0, "dataset": "other", "idx": 374450}
{"func": "\tp[1] = \" \";\n\tAN(memcpy(p + 2, s->p, s->n * sizeof *s->p));\n\tst->n = s->n + 2;\n\tst->p = p;\n\n\tb = VRT_StrandsWS(ctx->ws, NULL, st);\n\n\thp = VRT_selecthttp(ctx, hdr->where);\n\thttp_SetHeader(hp, b);\n}", "project": "varnish-modules", "hash": 324979881010693657557959501891217334918, "size": 21, "commit_id": "2c120e576ebb73bc247790184702ba58dc0afc39", "message": "Check VRT_StrandsWS() return value\n\nFixes: VSV00006", "target": 1, "dataset": "other", "idx": 205887}
{"func": "\tAN(memcpy(p + 2, s->p, s->n * sizeof *s->p));\n\tst->n = s->n + 2;\n\tst->p = p;\n\n\tb = VRT_StrandsWS(ctx->ws, NULL, st);\n\tif (b == NULL) {\n\t\tVRT_fail(ctx, \"vmod_header: workspace allocation failure\");\n\t\treturn;\n\t}\n\n\thp = VRT_selecthttp(ctx, hdr->where);\n\thttp_SetHeader(hp, b);\n}", "project": "varnish-modules", "hash": 235596432137393318008443894497589238133, "size": 25, "commit_id": "2c120e576ebb73bc247790184702ba58dc0afc39", "message": "Check VRT_StrandsWS() return value\n\nFixes: VSV00006", "target": 0, "dataset": "other", "idx": 374508}
{"func": "unsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tbool use_siar = regs_use_siar(regs);\n\tunsigned long siar = mfspr(SPRN_SIAR);\n\n\tif (ppmu->flags & PPMU_P10_DD1) {\n\t\tif (siar)\n\t\t\treturn siar;\n\t\telse\n\t\t\treturn regs->nip;\n\t} else if (use_siar && siar_valid(regs))", "project": "linux", "hash": 8700491383079660474540574671645428931, "size": 17, "commit_id": "60b7ed54a41b550d50caf7f2418db4a7e75b5bdc", "message": "powerpc/perf: Fix crash in perf_instruction_pointer() when ppmu is not set\n\nOn systems without any specific PMU driver support registered, running\nperf record causes Oops.\n\nThe relevant portion from call trace:\n\n  BUG: Kernel NULL pointer dereference on read at 0x00000040\n  Faulting instruction address: 0xc0021f0c\n  Oops: Kernel access of bad area, sig: 11 [#1]\n  BE PAGE_SIZE=4K PREEMPT CMPCPRO\n  SAF3000 DIE NOTIFICATION\n  CPU: 0 PID: 442 Comm: null_syscall Not tainted 5.13.0-rc6-s3k-dev-01645-g7649ee3d2957 #5164\n  NIP:  c0021f0c LR: c00e8ad8 CTR: c00d8a5c\n  NIP perf_instruction_pointer+0x10/0x60\n  LR  perf_prepare_sample+0x344/0x674\n  Call Trace:\n    perf_prepare_sample+0x7c/0x674 (unreliable)\n    perf_event_output_forward+0x3c/0x94\n    __perf_event_overflow+0x74/0x14c\n    perf_swevent_hrtimer+0xf8/0x170\n    __hrtimer_run_queues.constprop.0+0x160/0x318\n    hrtimer_interrupt+0x148/0x3b0\n    timer_interrupt+0xc4/0x22c\n    Decrementer_virt+0xb8/0xbc\n\nDuring perf record session, perf_instruction_pointer() is called to\ncapture the sample IP. This function in core-book3s accesses\nppmu->flags. If a platform specific PMU driver is not registered, ppmu\nis set to NULL and accessing its members results in a crash. Fix this\ncrash by checking if ppmu is set.\n\nFixes: 2ca13a4cc56c (\"powerpc/perf: Use regs->nip when SIAR is zero\")\nCc: stable@vger.kernel.org # v5.11+\nReported-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nSigned-off-by: Athira Rajeev <atrajeev@linux.vnet.ibm.com>\nTested-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nSigned-off-by: Michael Ellerman <mpe@ellerman.id.au>\nLink: https://lore.kernel.org/r/1623952506-1431-1-git-send-email-atrajeev@linux.vnet.ibm.com", "target": 1, "dataset": "other", "idx": 205900}
{"func": "unsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tbool use_siar = regs_use_siar(regs);\n\tunsigned long siar = mfspr(SPRN_SIAR);\n\n\tif (ppmu && (ppmu->flags & PPMU_P10_DD1)) {\n\t\tif (siar)\n\t\t\treturn siar;\n\t\telse\n\t\t\treturn regs->nip;\n\t} else if (use_siar && siar_valid(regs))", "project": "linux", "hash": 187281309875844152562266298810867406643, "size": 17, "commit_id": "60b7ed54a41b550d50caf7f2418db4a7e75b5bdc", "message": "powerpc/perf: Fix crash in perf_instruction_pointer() when ppmu is not set\n\nOn systems without any specific PMU driver support registered, running\nperf record causes Oops.\n\nThe relevant portion from call trace:\n\n  BUG: Kernel NULL pointer dereference on read at 0x00000040\n  Faulting instruction address: 0xc0021f0c\n  Oops: Kernel access of bad area, sig: 11 [#1]\n  BE PAGE_SIZE=4K PREEMPT CMPCPRO\n  SAF3000 DIE NOTIFICATION\n  CPU: 0 PID: 442 Comm: null_syscall Not tainted 5.13.0-rc6-s3k-dev-01645-g7649ee3d2957 #5164\n  NIP:  c0021f0c LR: c00e8ad8 CTR: c00d8a5c\n  NIP perf_instruction_pointer+0x10/0x60\n  LR  perf_prepare_sample+0x344/0x674\n  Call Trace:\n    perf_prepare_sample+0x7c/0x674 (unreliable)\n    perf_event_output_forward+0x3c/0x94\n    __perf_event_overflow+0x74/0x14c\n    perf_swevent_hrtimer+0xf8/0x170\n    __hrtimer_run_queues.constprop.0+0x160/0x318\n    hrtimer_interrupt+0x148/0x3b0\n    timer_interrupt+0xc4/0x22c\n    Decrementer_virt+0xb8/0xbc\n\nDuring perf record session, perf_instruction_pointer() is called to\ncapture the sample IP. This function in core-book3s accesses\nppmu->flags. If a platform specific PMU driver is not registered, ppmu\nis set to NULL and accessing its members results in a crash. Fix this\ncrash by checking if ppmu is set.\n\nFixes: 2ca13a4cc56c (\"powerpc/perf: Use regs->nip when SIAR is zero\")\nCc: stable@vger.kernel.org # v5.11+\nReported-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nSigned-off-by: Athira Rajeev <atrajeev@linux.vnet.ibm.com>\nTested-by: Christophe Leroy <christophe.leroy@csgroup.eu>\nSigned-off-by: Michael Ellerman <mpe@ellerman.id.au>\nLink: https://lore.kernel.org/r/1623952506-1431-1-git-send-email-atrajeev@linux.vnet.ibm.com", "target": 0, "dataset": "other", "idx": 374683}
{"func": "\t\tif (val) { /* have a value */\n\t\t\tsize_t val_len;\n\t\t\tsize_t new_val_len;\n\n\t\t\t*val++ = '\\0';\n\t\t\tphp_url_decode(var, strlen(var));\n\t\t\tval_len = php_url_decode(val, strlen(val));\n\t\t\tval = estrndup(val, val_len);\n\t\t\tif (sapi_module.input_filter(arg, var, &val, val_len, &new_val_len)) {\n\t\t\t\tphp_register_variable_safe(var, val, new_val_len, &array);\n\t\t\t}\n\t\t\tefree(val);\n\t\t} else {\n\t\t\tsize_t val_len;\n\t\t\tsize_t new_val_len;\n\n\t\t\tphp_url_decode(var, strlen(var));\n\t\t\tval_len = 0;\n\t\t\tval = estrndup(\"\", val_len);\n\t\t\tif (sapi_module.input_filter(arg, var, &val, val_len, &new_val_len)) {\n\t\t\t\tphp_register_variable_safe(var, val, new_val_len, &array);\n\t\t\t}", "project": "php-src", "hash": 129611870495713562053024480121638591020, "size": 127, "commit_id": "6559fe912661ca5ce5f0eeeb591d928451428ed0", "message": "Do not decode cookie names anymore", "target": 1, "dataset": "other", "idx": 205909}
{"func": "\t\tif (val) { /* have a value */\n\t\t\tsize_t val_len;\n\t\t\tsize_t new_val_len;\n\n\t\t\t*val++ = '\\0';\n\t\t\tif (arg != PARSE_COOKIE) {\n\t\t\t\tphp_url_decode(var, strlen(var));\n\t\t\t}\n\t\t\tval_len = php_url_decode(val, strlen(val));\n\t\t\tval = estrndup(val, val_len);\n\t\t\tif (sapi_module.input_filter(arg, var, &val, val_len, &new_val_len)) {\n\t\t\t\tphp_register_variable_safe(var, val, new_val_len, &array);\n\t\t\t}\n\t\t\tefree(val);\n\t\t} else {\n\t\t\tsize_t val_len;\n\t\t\tsize_t new_val_len;\n\n\t\t\tif (arg != PARSE_COOKIE) {\n\t\t\t\tphp_url_decode(var, strlen(var));\n\t\t\t}\n\t\t\tval_len = 0;\n\t\t\tval = estrndup(\"\", val_len);\n\t\t\tif (sapi_module.input_filter(arg, var, &val, val_len, &new_val_len)) {\n\t\t\t\tphp_register_variable_safe(var, val, new_val_len, &array);\n\t\t\t}", "project": "php-src", "hash": 99217299303506150311767680298916812323, "size": 131, "commit_id": "6559fe912661ca5ce5f0eeeb591d928451428ed0", "message": "Do not decode cookie names anymore", "target": 0, "dataset": "other", "idx": 374974}
{"func": "\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != tsk->parent->self_exec_id)\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;", "project": "linux", "hash": 326403613636969146659682497730835154196, "size": 93, "commit_id": "d1e7fd6462ca9fc76650fbe6ca800e35b24267da", "message": "signal: Extend exec_id to 64bits\n\nReplace the 32bit exec_id with a 64bit exec_id to make it impossible\nto wrap the exec_id counter.  With care an attacker can cause exec_id\nwrap and send arbitrary signals to a newly exec'd parent.  This\nbypasses the signal sending checks if the parent changes their\ncredentials during exec.\n\nThe severity of this problem can been seen that in my limited testing\nof a 32bit exec_id it can take as little as 19s to exec 65536 times.\nWhich means that it can take as little as 14 days to wrap a 32bit\nexec_id.  Adam Zabrocki has succeeded wrapping the self_exe_id in 7\ndays.  Even my slower timing is in the uptime of a typical server.\nWhich means self_exec_id is simply a speed bump today, and if exec\ngets noticably faster self_exec_id won't even be a speed bump.\n\nExtending self_exec_id to 64bits introduces a problem on 32bit\narchitectures where reading self_exec_id is no longer atomic and can\ntake two read instructions.  Which means that is is possible to hit\na window where the read value of exec_id does not match the written\nvalue.  So with very lucky timing after this change this still\nremains expoiltable.\n\nI have updated the update of exec_id on exec to use WRITE_ONCE\nand the read of exec_id in do_notify_parent to use READ_ONCE\nto make it clear that there is no locking between these two\nlocations.\n\nLink: https://lore.kernel.org/kernel-hardening/20200324215049.GA3710@pi3.com.pl\nFixes: 2.3.23pre2\nCc: stable@vger.kernel.org\nSigned-off-by: \"Eric W. Biederman\" <ebiederm@xmission.com>", "target": 1, "dataset": "other", "idx": 205956}
{"func": "\tif (sig != SIGCHLD) {\n\t\t/*\n\t\t * This is only possible if parent == real_parent.\n\t\t * Check if it has changed security domain.\n\t\t */\n\t\tif (tsk->parent_exec_id != READ_ONCE(tsk->parent->self_exec_id))\n\t\t\tsig = SIGCHLD;\n\t}\n\n\tclear_siginfo(&info);\n\tinfo.si_signo = sig;", "project": "linux", "hash": 91267481540745002935349375046199958172, "size": 93, "commit_id": "d1e7fd6462ca9fc76650fbe6ca800e35b24267da", "message": "signal: Extend exec_id to 64bits\n\nReplace the 32bit exec_id with a 64bit exec_id to make it impossible\nto wrap the exec_id counter.  With care an attacker can cause exec_id\nwrap and send arbitrary signals to a newly exec'd parent.  This\nbypasses the signal sending checks if the parent changes their\ncredentials during exec.\n\nThe severity of this problem can been seen that in my limited testing\nof a 32bit exec_id it can take as little as 19s to exec 65536 times.\nWhich means that it can take as little as 14 days to wrap a 32bit\nexec_id.  Adam Zabrocki has succeeded wrapping the self_exe_id in 7\ndays.  Even my slower timing is in the uptime of a typical server.\nWhich means self_exec_id is simply a speed bump today, and if exec\ngets noticably faster self_exec_id won't even be a speed bump.\n\nExtending self_exec_id to 64bits introduces a problem on 32bit\narchitectures where reading self_exec_id is no longer atomic and can\ntake two read instructions.  Which means that is is possible to hit\na window where the read value of exec_id does not match the written\nvalue.  So with very lucky timing after this change this still\nremains expoiltable.\n\nI have updated the update of exec_id on exec to use WRITE_ONCE\nand the read of exec_id in do_notify_parent to use READ_ONCE\nto make it clear that there is no locking between these two\nlocations.\n\nLink: https://lore.kernel.org/kernel-hardening/20200324215049.GA3710@pi3.com.pl\nFixes: 2.3.23pre2\nCc: stable@vger.kernel.org\nSigned-off-by: \"Eric W. Biederman\" <ebiederm@xmission.com>", "target": 0, "dataset": "other", "idx": 375155}
{"func": "    /*\n     * Reassembly is complete; concatenate fragments.\n     */\n    q = fp->frag_link.next;\n    m = dtom(slirp, q);\n\n    q = (struct ipasfrag *)q->ipf_next;\n    while (q != (struct ipasfrag *)&fp->frag_link) {\n        struct mbuf *t = dtom(slirp, q);\n        q = (struct ipasfrag *)q->ipf_next;\n        m_cat(m, t);\n     * bigger than the total size of the fragment, then and\n     * m_ext buffer was alloced. But fp->ipq_next points to\n     * the old buffer (in the mbuf), so we must point ip\n     * into the new buffer.\n     */\n    if (m->m_flags & M_EXT) {\n        int delta = (char *)q - m->m_dat;\n        q = (struct ipasfrag *)(m->m_ext + delta);\n    }\n\n    ip = fragtoip(q);", "project": "libslirp", "hash": 138883903530681634140467642895512584556, "size": 149, "commit_id": "126c04acbabd7ad32c2b018fe10dfac2a3bc1210", "message": "Fix heap overflow in ip_reass on big packet input\n\nWhen the first fragment does not fit in the preallocated buffer, q will\nalready be pointing to the ext buffer, so we mustn't try to update it.\n\nSigned-off-by: Samuel Thibault <samuel.thibault@ens-lyon.org>", "target": 1, "dataset": "other", "idx": 205959}
{"func": "     * Reassembly is complete; concatenate fragments.\n     */\n    q = fp->frag_link.next;\n    m = dtom(slirp, q);\n\n    int was_ext = m->m_flags & M_EXT;\n\n    q = (struct ipasfrag *)q->ipf_next;\n    while (q != (struct ipasfrag *)&fp->frag_link) {\n        struct mbuf *t = dtom(slirp, q);\n        q = (struct ipasfrag *)q->ipf_next;\n        m_cat(m, t);\n     * bigger than the total size of the fragment, then and\n     * m_ext buffer was alloced. But fp->ipq_next points to\n     * the old buffer (in the mbuf), so we must point ip\n     * into the new buffer.\n     */\n    if (!was_ext && m->m_flags & M_EXT) {\n        int delta = (char *)q - m->m_dat;\n        q = (struct ipasfrag *)(m->m_ext + delta);\n    }\n\n    ip = fragtoip(q);", "project": "libslirp", "hash": 51858820435074331770642832081553618150, "size": 151, "commit_id": "126c04acbabd7ad32c2b018fe10dfac2a3bc1210", "message": "Fix heap overflow in ip_reass on big packet input\n\nWhen the first fragment does not fit in the preallocated buffer, q will\nalready be pointing to the ext buffer, so we mustn't try to update it.\n\nSigned-off-by: Samuel Thibault <samuel.thibault@ens-lyon.org>", "target": 0, "dataset": "other", "idx": 375358}
{"func": "        case TOK_PREPROC_QQ:\n        case TOK_PREPROC_SQQ:\n        {\n            size_t mlen = strlen(m->name);\n\t    size_t len;\n            char *p;\n\n            t->type = mstart->type;\n            if (t->type == TOK_LOCAL_MACRO) {\n\t\tconst char *psp; /* prefix start pointer */\n                const char *pep; /* prefix end pointer */\n\t\tpsp = tok_text(mstart);\n                get_ctx(psp, &pep);\n                plen = pep - psp;\n\n                len = mlen + plen;\n                p = nasm_malloc(len + 1);\n                p = mempcpy(p, psp, plen);\n            } else {\n                len = mlen;\n                p = nasm_malloc(len + 1);\n            }\n            p = mempcpy(p, m->name, mlen);\n            *p = '\\0';\n\t    set_text_free(t, p, len);\n\n            t->next = tline;\n            break;\n        }\n", "project": "nasm", "hash": 74552473003065677858003804288043584224, "size": 453, "commit_id": "7c88289e222dc5ef9f53f9e86ecaab1924744b88", "message": "BR3392711: preproc: fix memory corruption in expand_one_smacro\n\nThe mempcpy helper returns *last* byte pointer thus when\nwe call set_text_free we have to pass a pointer to the\nstart of the string.\n\nSigned-off-by: Cyrill Gorcunov <gorcunov@gmail.com>", "target": 1, "dataset": "other", "idx": 205976}
{"func": "        case TOK_PREPROC_QQ:\n        case TOK_PREPROC_SQQ:\n        {\n            size_t mlen = strlen(m->name);\n\t    size_t len;\n            char *p, *from;\n\n            t->type = mstart->type;\n            if (t->type == TOK_LOCAL_MACRO) {\n\t\tconst char *psp; /* prefix start pointer */\n                const char *pep; /* prefix end pointer */\n\t\tpsp = tok_text(mstart);\n                get_ctx(psp, &pep);\n                plen = pep - psp;\n\n                len = mlen + plen;\n                from = p = nasm_malloc(len + 1);\n                p = mempcpy(p, psp, plen);\n            } else {\n                len = mlen;\n                from = p = nasm_malloc(len + 1);\n            }\n            p = mempcpy(p, m->name, mlen);\n            *p = '\\0';\n\t    set_text_free(t, from, len);\n\n            t->next = tline;\n            break;\n        }\n", "project": "nasm", "hash": 103568277309255258992223520108689986234, "size": 453, "commit_id": "7c88289e222dc5ef9f53f9e86ecaab1924744b88", "message": "BR3392711: preproc: fix memory corruption in expand_one_smacro\n\nThe mempcpy helper returns *last* byte pointer thus when\nwe call set_text_free we have to pass a pointer to the\nstart of the string.\n\nSigned-off-by: Cyrill Gorcunov <gorcunov@gmail.com>", "target": 0, "dataset": "other", "idx": 375670}
{"func": "int url_is_local_not_ssh(const char *url)\n{\n\tconst char *colon = strchr(url, ':');\n\tconst char *slash = strchr(url, '/');\n\treturn !colon || (slash && slash < colon) ||\n\t\thas_dos_drive_prefix(url);\n}", "project": "git", "hash": 93761688030965830881707145604603054740, "size": 7, "commit_id": "f82a97eb9197c1e3768e72648f37ce0ca3233734", "message": "mingw: handle `subst`-ed \"DOS drives\"\n\nOver a decade ago, in 25fe217b86c (Windows: Treat Windows style path\nnames., 2008-03-05), Git was taught to handle absolute Windows paths,\ni.e. paths that start with a drive letter and a colon.\n\nUnbeknownst to us, while drive letters of physical drives are limited to\nletters of the English alphabet, there is a way to assign virtual drive\nletters to arbitrary directories, via the `subst` command, which is\n_not_ limited to English letters.\n\nIt is therefore possible to have absolute Windows paths of the form\n`1:\\what\\the\\hex.txt`. Even \"better\": pretty much arbitrary Unicode\nletters can also be used, e.g. `\u00e4:\\tschib\u00e4t.sch`.\n\nWhile it can be sensibly argued that users who set up such funny drive\nletters really seek adverse consequences, the Windows Operating System\nis known to be a platform where many users are at the mercy of\nadministrators who have their very own idea of what constitutes a\nreasonable setup.\n\nTherefore, let's just make sure that such funny paths are still\nconsidered absolute paths by Git, on Windows.\n\nIn addition to Unicode characters, pretty much any character is a valid\ndrive letter, as far as `subst` is concerned, even `:` and `\"` or even a\nspace character. While it is probably the opposite of smart to use them,\nlet's safeguard `is_dos_drive_prefix()` against all of them.\n\nNote: `[::1]:repo` is a valid URL, but not a valid path on Windows.\nAs `[` is now considered a valid drive letter, we need to be very\ncareful to avoid misinterpreting such a string as valid local path in\n`url_is_local_not_ssh()`. To do that, we use the just-introduced\nfunction `is_valid_path()` (which will label the string as invalid file\nname because of the colon characters).\n\nThis fixes CVE-2019-1351.\n\nReported-by: Nicolas Joly <Nicolas.Joly@microsoft.com>\nSigned-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>", "target": 1, "dataset": "other", "idx": 206023}
{"func": "int url_is_local_not_ssh(const char *url)\n{\n\tconst char *colon = strchr(url, ':');\n\tconst char *slash = strchr(url, '/');\n\treturn !colon || (slash && slash < colon) ||\n\t\t(has_dos_drive_prefix(url) && is_valid_path(url));\n}", "project": "git", "hash": 322711509457870301529370883808295524605, "size": 7, "commit_id": "f82a97eb9197c1e3768e72648f37ce0ca3233734", "message": "mingw: handle `subst`-ed \"DOS drives\"\n\nOver a decade ago, in 25fe217b86c (Windows: Treat Windows style path\nnames., 2008-03-05), Git was taught to handle absolute Windows paths,\ni.e. paths that start with a drive letter and a colon.\n\nUnbeknownst to us, while drive letters of physical drives are limited to\nletters of the English alphabet, there is a way to assign virtual drive\nletters to arbitrary directories, via the `subst` command, which is\n_not_ limited to English letters.\n\nIt is therefore possible to have absolute Windows paths of the form\n`1:\\what\\the\\hex.txt`. Even \"better\": pretty much arbitrary Unicode\nletters can also be used, e.g. `\u00e4:\\tschib\u00e4t.sch`.\n\nWhile it can be sensibly argued that users who set up such funny drive\nletters really seek adverse consequences, the Windows Operating System\nis known to be a platform where many users are at the mercy of\nadministrators who have their very own idea of what constitutes a\nreasonable setup.\n\nTherefore, let's just make sure that such funny paths are still\nconsidered absolute paths by Git, on Windows.\n\nIn addition to Unicode characters, pretty much any character is a valid\ndrive letter, as far as `subst` is concerned, even `:` and `\"` or even a\nspace character. While it is probably the opposite of smart to use them,\nlet's safeguard `is_dos_drive_prefix()` against all of them.\n\nNote: `[::1]:repo` is a valid URL, but not a valid path on Windows.\nAs `[` is now considered a valid drive letter, we need to be very\ncareful to avoid misinterpreting such a string as valid local path in\n`url_is_local_not_ssh()`. To do that, we use the just-introduced\nfunction `is_valid_path()` (which will label the string as invalid file\nname because of the colon characters).\n\nThis fixes CVE-2019-1351.\n\nReported-by: Nicolas Joly <Nicolas.Joly@microsoft.com>\nSigned-off-by: Johannes Schindelin <johannes.schindelin@gmx.de>", "target": 0, "dataset": "other", "idx": 376250}
{"func": "\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tif (!nested_vmcb_checks(svm, vmcb12)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;", "project": "linux", "hash": 141649806115592010502494780149120635226, "size": 102, "commit_id": "a58d9166a756a0f4a6618e4f593232593d6df134", "message": "KVM: SVM: load control fields from VMCB12 before checking them\n\nAvoid races between check and use of the nested VMCB controls.  This\nfor example ensures that the VMRUN intercept is always reflected to the\nnested hypervisor, instead of being processed by the host.  Without this\npatch, it is possible to end up with svm->nested.hsave pointing to\nthe MSR permission bitmap for nested guests.\n\nThis bug is CVE-2021-29657.\n\nReported-by: Felix Wilhelm <fwilhelm@google.com>\nCc: stable@vger.kernel.org\nFixes: 2fcf4876ada (\"KVM: nSVM: implement on demand allocation of the nested state\")\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 206039}
{"func": "\tvmcb12 = map.hva;\n\n\tif (WARN_ON_ONCE(!svm->nested.initialized))\n\t\treturn -EINVAL;\n\n\tload_nested_vmcb_control(svm, &vmcb12->control);\n\n\tif (!nested_vmcb_check_save(svm, vmcb12) ||\n\t    !nested_vmcb_check_controls(&svm->nested.ctl)) {\n\t\tvmcb12->control.exit_code    = SVM_EXIT_ERR;\n\t\tvmcb12->control.exit_code_hi = 0;\n\t\tvmcb12->control.exit_info_1  = 0;\n\t\tvmcb12->control.exit_info_2  = 0;\n\t\tgoto out;", "project": "linux", "hash": 86206897187959108781935741539715146259, "size": 105, "commit_id": "a58d9166a756a0f4a6618e4f593232593d6df134", "message": "KVM: SVM: load control fields from VMCB12 before checking them\n\nAvoid races between check and use of the nested VMCB controls.  This\nfor example ensures that the VMRUN intercept is always reflected to the\nnested hypervisor, instead of being processed by the host.  Without this\npatch, it is possible to end up with svm->nested.hsave pointing to\nthe MSR permission bitmap for nested guests.\n\nThis bug is CVE-2021-29657.\n\nReported-by: Felix Wilhelm <fwilhelm@google.com>\nCc: stable@vger.kernel.org\nFixes: 2fcf4876ada (\"KVM: nSVM: implement on demand allocation of the nested state\")\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 376647}
{"func": "            // This should be impossible\n            return AVIF_FALSE;\n        }\n        CHECK(avifROStreamReadU32(&s, &grid->outputWidth));  // unsigned int(FieldLength) output_width;\n        CHECK(avifROStreamReadU32(&s, &grid->outputHeight)); // unsigned int(FieldLength) output_height;\n    }\n    return AVIF_TRUE;\n}", "project": "libavif", "hash": 315972070308824825121763094549602004029, "size": 32, "commit_id": "0a8e7244d494ae98e9756355dfbfb6697ded2ff9", "message": "Set max image size to 16384 * 16384\n\nFix https://crbug.com/oss-fuzz/24728 and\nhttps://crbug.com/oss-fuzz/24734.", "target": 1, "dataset": "other", "idx": 206065}
{"func": "            return AVIF_FALSE;\n        }\n        CHECK(avifROStreamReadU32(&s, &grid->outputWidth));  // unsigned int(FieldLength) output_width;\n        CHECK(avifROStreamReadU32(&s, &grid->outputHeight)); // unsigned int(FieldLength) output_height;\n    }\n    if (grid->outputWidth > AVIF_MAX_IMAGE_SIZE / grid->outputHeight) {\n        return AVIF_FALSE;\n    }\n    return AVIF_TRUE;\n}", "project": "libavif", "hash": 264973754740216159561213879015400903826, "size": 35, "commit_id": "0a8e7244d494ae98e9756355dfbfb6697ded2ff9", "message": "Set max image size to 16384 * 16384\n\nFix https://crbug.com/oss-fuzz/24728 and\nhttps://crbug.com/oss-fuzz/24734.", "target": 0, "dataset": "other", "idx": 376801}
{"func": "  int isCommuted    /* The comparison has been commuted */\n){\n  int p5;\n  int addr;\n  CollSeq *p4;\n\n  if( isCommuted ){\n    p4 = sqlite3BinaryCompareCollSeq(pParse, pRight, pLeft);\n  }else{\n    p4 = sqlite3BinaryCompareCollSeq(pParse, pLeft, pRight);\n  }", "project": "sqlite", "hash": 208208245522129666857516610065395604656, "size": 25, "commit_id": "8654186b0236d556aa85528c2573ee0b6ab71be3", "message": "When an error occurs while rewriting the parser tree for window functions\nin the sqlite3WindowRewrite() routine, make sure that pParse->nErr is set,\nand make sure that this shuts down any subsequent code generation that might\ndepend on the transformations that were implemented.  This fixes a problem\ndiscovered by the Yongheng and Rui fuzzer.\n\nFossilOrigin-Name: e2bddcd4c55ba3cbe0130332679ff4b048630d0ced9a8899982edb5a3569ba7f", "target": 1, "dataset": "other", "idx": 206235}
{"func": "){\n  int p5;\n  int addr;\n  CollSeq *p4;\n\n  if( pParse->nErr ) return 0;\n  if( isCommuted ){\n    p4 = sqlite3BinaryCompareCollSeq(pParse, pRight, pLeft);\n  }else{\n    p4 = sqlite3BinaryCompareCollSeq(pParse, pLeft, pRight);\n  }", "project": "sqlite", "hash": 122085921151462389183406220419053423593, "size": 26, "commit_id": "8654186b0236d556aa85528c2573ee0b6ab71be3", "message": "When an error occurs while rewriting the parser tree for window functions\nin the sqlite3WindowRewrite() routine, make sure that pParse->nErr is set,\nand make sure that this shuts down any subsequent code generation that might\ndepend on the transformations that were implemented.  This fixes a problem\ndiscovered by the Yongheng and Rui fuzzer.\n\nFossilOrigin-Name: e2bddcd4c55ba3cbe0130332679ff4b048630d0ced9a8899982edb5a3569ba7f", "target": 0, "dataset": "other", "idx": 378666}
{"func": "static void vdbeVComment(Vdbe *p, const char *zFormat, va_list ap){\n  assert( p->nOp>0 || p->aOp==0 );\n  assert( p->aOp==0 || p->aOp[p->nOp-1].zComment==0 || p->db->mallocFailed );\n  if( p->nOp ){\n    assert( p->aOp );\n    sqlite3DbFree(p->db, p->aOp[p->nOp-1].zComment);\n    p->aOp[p->nOp-1].zComment = sqlite3VMPrintf(p->db, zFormat, ap);\n  }", "project": "sqlite", "hash": 297573905934146601478250262407134446540, "size": 9, "commit_id": "8654186b0236d556aa85528c2573ee0b6ab71be3", "message": "When an error occurs while rewriting the parser tree for window functions\nin the sqlite3WindowRewrite() routine, make sure that pParse->nErr is set,\nand make sure that this shuts down any subsequent code generation that might\ndepend on the transformations that were implemented.  This fixes a problem\ndiscovered by the Yongheng and Rui fuzzer.\n\nFossilOrigin-Name: e2bddcd4c55ba3cbe0130332679ff4b048630d0ced9a8899982edb5a3569ba7f", "target": 1, "dataset": "other", "idx": 206236}
{"func": "static void vdbeVComment(Vdbe *p, const char *zFormat, va_list ap){\n  assert( p->nOp>0 || p->aOp==0 );\n  assert( p->aOp==0 || p->aOp[p->nOp-1].zComment==0 || p->db->mallocFailed\n          || p->pParse->nErr>0 );\n  if( p->nOp ){\n    assert( p->aOp );\n    sqlite3DbFree(p->db, p->aOp[p->nOp-1].zComment);\n    p->aOp[p->nOp-1].zComment = sqlite3VMPrintf(p->db, zFormat, ap);\n  }", "project": "sqlite", "hash": 94525444203276371802552918193620025307, "size": 10, "commit_id": "8654186b0236d556aa85528c2573ee0b6ab71be3", "message": "When an error occurs while rewriting the parser tree for window functions\nin the sqlite3WindowRewrite() routine, make sure that pParse->nErr is set,\nand make sure that this shuts down any subsequent code generation that might\ndepend on the transformations that were implemented.  This fixes a problem\ndiscovered by the Yongheng and Rui fuzzer.\n\nFossilOrigin-Name: e2bddcd4c55ba3cbe0130332679ff4b048630d0ced9a8899982edb5a3569ba7f", "target": 0, "dataset": "other", "idx": 378537}
{"func": "    Window *pWin;                 /* Window object iterator */\n    Table *pTab;\n\n    pTab = sqlite3DbMallocZero(db, sizeof(Table));\n    if( pTab==0 ){\n      return SQLITE_NOMEM;\n    }\n\n    p->pSrc = 0;\n    p->pWhere = 0;\n    p->pGroupBy = 0;\n      sqlite3SelectDelete(db, pSub);\n    }\n    if( db->mallocFailed ) rc = SQLITE_NOMEM;\n    sqlite3DbFree(db, pTab);\n  }\n\n  return rc;\n}", "project": "sqlite", "hash": 244312120903084545062595033064722321765, "size": 126, "commit_id": "8654186b0236d556aa85528c2573ee0b6ab71be3", "message": "When an error occurs while rewriting the parser tree for window functions\nin the sqlite3WindowRewrite() routine, make sure that pParse->nErr is set,\nand make sure that this shuts down any subsequent code generation that might\ndepend on the transformations that were implemented.  This fixes a problem\ndiscovered by the Yongheng and Rui fuzzer.\n\nFossilOrigin-Name: e2bddcd4c55ba3cbe0130332679ff4b048630d0ced9a8899982edb5a3569ba7f", "target": 1, "dataset": "other", "idx": 206237}
{"func": "    Window *pWin;                 /* Window object iterator */\n    Table *pTab;\n\n    pTab = sqlite3DbMallocZero(db, sizeof(Table));\n    if( pTab==0 ){\n      return sqlite3ErrorToParser(db, SQLITE_NOMEM);\n    }\n\n    p->pSrc = 0;\n    p->pWhere = 0;\n    p->pGroupBy = 0;\n    }\n    if( db->mallocFailed ) rc = SQLITE_NOMEM;\n    sqlite3DbFree(db, pTab);\n  }\n\n  if( rc && pParse->nErr==0 ){\n    assert( pParse->db->mallocFailed );\n    return sqlite3ErrorToParser(pParse->db, SQLITE_NOMEM);\n  }\n  return rc;\n}", "project": "sqlite", "hash": 244558507838888566010387300747909731042, "size": 130, "commit_id": "8654186b0236d556aa85528c2573ee0b6ab71be3", "message": "When an error occurs while rewriting the parser tree for window functions\nin the sqlite3WindowRewrite() routine, make sure that pParse->nErr is set,\nand make sure that this shuts down any subsequent code generation that might\ndepend on the transformations that were implemented.  This fixes a problem\ndiscovered by the Yongheng and Rui fuzzer.\n\nFossilOrigin-Name: e2bddcd4c55ba3cbe0130332679ff4b048630d0ced9a8899982edb5a3569ba7f", "target": 0, "dataset": "other", "idx": 378646}
{"func": "\n  if (string == 0 || *string == '\\0')\n    return ((char *)NULL);\n\n#if defined (HANDLE_MULTIBYTE)\n  if (strstr (string, \"\\\\U\") != 0)\n    ret = (char *)xmalloc (6*len + 1);\n  else\n    ret = (char *)xmalloc (4*len + 1);\n#else\n  ret = (char *)xmalloc (2*len + 1);\t/* 2*len for possible CTLESC */\n#endif\n  for (r = ret, s = string; s && *s; )\n    {", "project": "bash", "hash": 165715042020473394274093045888014469618, "size": 155, "commit_id": "863d31ae775d56b785dc5b0105b6d251515d81d5", "message": "commit bash-20120224 snapshot", "target": 1, "dataset": "other", "idx": 206271}
{"func": "\n  if (string == 0 || *string == '\\0')\n    return ((char *)NULL);\n\n#if defined (HANDLE_MULTIBYTE)\n  ret = (char *)xmalloc (4*len + 1);\n#else\n  ret = (char *)xmalloc (2*len + 1);\t/* 2*len for possible CTLESC */\n#endif\n  for (r = ret, s = string; s && *s; )\n    {", "project": "bash", "hash": 69386439653475961833656724891203304786, "size": 152, "commit_id": "863d31ae775d56b785dc5b0105b6d251515d81d5", "message": "commit bash-20120224 snapshot", "target": 0, "dataset": "other", "idx": 379446}
{"func": "  ppm_source_ptr source = (ppm_source_ptr)sinfo;\n  register JSAMPROW ptr;\n  register U_CHAR *bufferptr;\n  register JSAMPLE *rescale = source->rescale;\n  JDIMENSION col;\n  unsigned int maxval = source->maxval;\n\n  if (!ReadOK(source->pub.input_file, source->iobuffer, source->buffer_width))\n    ERREXIT(cinfo, JERR_INPUT_EOF);\n  ptr = source->pub.buffer[0];\n  bufferptr = source->iobuffer;\n    register unsigned int temp;\n    temp  = UCH(*bufferptr++) << 8;\n    temp |= UCH(*bufferptr++);\n    if (temp > maxval)\n      ERREXIT(cinfo, JERR_PPM_OUTOFRANGE);\n    *ptr++ = rescale[temp];\n    temp  = UCH(*bufferptr++) << 8;\n    temp |= UCH(*bufferptr++);\n    if (temp > maxval)\n      ERREXIT(cinfo, JERR_PPM_OUTOFRANGE);\n    *ptr++ = rescale[temp];\n    temp  = UCH(*bufferptr++) << 8;\n    temp |= UCH(*bufferptr++);\n    if (temp > maxval)\n      ERREXIT(cinfo, JERR_PPM_OUTOFRANGE);\n    *ptr++ = rescale[temp];\n  }\n  return 1;\n}", "project": "libjpeg-turbo", "hash": 204167080044373314321748980723365143135, "size": 34, "commit_id": "f35fd27ec641c42d6b115bfa595e483ec58188d2", "message": "tjLoadImage: Fix issues w/loading 16-bit PPMs/PGMs\n\n- The PPM reader now throws an error rather than segfaulting (due to a\n  buffer overrun) if an application attempts to load a 16-bit PPM file\n  into a grayscale uncompressed image buffer.  No known applications\n  allowed that (not even the test applications in libjpeg-turbo),\n  because that mode of operation was never expected to work and did not\n  work under any circumstances.  (In fact, it was necessary to modify\n  TJBench in order to reproduce the issue outside of a fuzzing\n  environment.)  This was purely a matter of making the library bow out\n  gracefully rather than crash if an application tries to do something\n  really stupid.\n\n- The PPM reader now throws an error rather than generating incorrect\n  pixels if an application attempts to load a 16-bit PGM file into an\n  RGB uncompressed image buffer.\n\n- The PPM reader now correctly loads 16-bit PPM files into extended\n  RGB uncompressed image buffers.  (Previously it generated incorrect\n  pixels unless the input colorspace was JCS_RGB or JCS_EXT_RGB.)\n\nThe only way that users could have potentially encountered these issues\nwas through the tjLoadImage() function.  cjpeg and TJBench were\nunaffected.", "target": 1, "dataset": "other", "idx": 206275}
{"func": "  register JSAMPROW ptr;\n  register U_CHAR *bufferptr;\n  register JSAMPLE *rescale = source->rescale;\n  JDIMENSION col;\n  unsigned int maxval = source->maxval;\n  register int rindex = rgb_red[cinfo->in_color_space];\n  register int gindex = rgb_green[cinfo->in_color_space];\n  register int bindex = rgb_blue[cinfo->in_color_space];\n  register int aindex = alpha_index[cinfo->in_color_space];\n  register int ps = rgb_pixelsize[cinfo->in_color_space];\n\n  if (!ReadOK(source->pub.input_file, source->iobuffer, source->buffer_width))\n    ERREXIT(cinfo, JERR_INPUT_EOF);\n  ptr = source->pub.buffer[0];\n  bufferptr = source->iobuffer;\n    register unsigned int temp;\n    temp  = UCH(*bufferptr++) << 8;\n    temp |= UCH(*bufferptr++);\n    if (temp > maxval)\n      ERREXIT(cinfo, JERR_PPM_OUTOFRANGE);\n    ptr[rindex] = rescale[temp];\n    temp  = UCH(*bufferptr++) << 8;\n    temp |= UCH(*bufferptr++);\n    if (temp > maxval)\n      ERREXIT(cinfo, JERR_PPM_OUTOFRANGE);\n    ptr[gindex] = rescale[temp];\n    temp  = UCH(*bufferptr++) << 8;\n    temp |= UCH(*bufferptr++);\n    if (temp > maxval)\n      ERREXIT(cinfo, JERR_PPM_OUTOFRANGE);\n    ptr[bindex] = rescale[temp];\n    if (aindex >= 0)\n      ptr[aindex] = 0xFF;\n    ptr += ps;\n  }\n  return 1;\n}", "project": "libjpeg-turbo", "hash": 339407012057712144590227800123035459765, "size": 42, "commit_id": "f35fd27ec641c42d6b115bfa595e483ec58188d2", "message": "tjLoadImage: Fix issues w/loading 16-bit PPMs/PGMs\n\n- The PPM reader now throws an error rather than segfaulting (due to a\n  buffer overrun) if an application attempts to load a 16-bit PPM file\n  into a grayscale uncompressed image buffer.  No known applications\n  allowed that (not even the test applications in libjpeg-turbo),\n  because that mode of operation was never expected to work and did not\n  work under any circumstances.  (In fact, it was necessary to modify\n  TJBench in order to reproduce the issue outside of a fuzzing\n  environment.)  This was purely a matter of making the library bow out\n  gracefully rather than crash if an application tries to do something\n  really stupid.\n\n- The PPM reader now throws an error rather than generating incorrect\n  pixels if an application attempts to load a 16-bit PGM file into an\n  RGB uncompressed image buffer.\n\n- The PPM reader now correctly loads 16-bit PPM files into extended\n  RGB uncompressed image buffers.  (Previously it generated incorrect\n  pixels unless the input colorspace was JCS_RGB or JCS_EXT_RGB.)\n\nThe only way that users could have potentially encountered these issues\nwas through the tjLoadImage() function.  cjpeg and TJBench were\nunaffected.", "target": 0, "dataset": "other", "idx": 379756}
{"func": "\nstatic int\niscsi_if_recv_msg(struct sk_buff *skb, struct nlmsghdr *nlh, uint32_t *group)\n{\n\tint err = 0;\n\tu32 portid;\n\tstruct iscsi_uevent *ev = nlmsg_data(nlh);\n\tstruct iscsi_transport *transport = NULL;\n\tstruct iscsi_internal *priv;\n\tstruct iscsi_cls_session *session;\n\tstruct iscsi_cls_conn *conn;\n\t\tif (conn)\n\t\t\tiscsi_if_stop_conn(conn, ev->u.stop_conn.flag);\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\tcase ISCSI_UEVENT_SEND_PDU:\n\t\tconn = iscsi_conn_lookup(ev->u.send_pdu.sid, ev->u.send_pdu.cid);\n\t\tif (conn) {\n\t\t\tmutex_lock(&conn_mutex);\n\t\t\tev->r.retcode =\ttransport->send_pdu(conn,\n\t\t\t\t(struct iscsi_hdr*)((char*)ev + sizeof(*ev)),", "project": "linux", "hash": 165207676572058765527022757443722292191, "size": 225, "commit_id": "f9dbdf97a5bd92b1a49cee3d591b55b11fd7a6d5", "message": "scsi: iscsi: Verify lengths on passthrough PDUs\n\nOpen-iSCSI sends passthrough PDUs over netlink, but the kernel should be\nverifying that the provided PDU header and data lengths fall within the\nnetlink message to prevent accessing beyond that in memory.\n\nCc: stable@vger.kernel.org\nReported-by: Adam Nichols <adam@grimm-co.com>\nReviewed-by: Lee Duncan <lduncan@suse.com>\nReviewed-by: Mike Christie <michael.christie@oracle.com>\nSigned-off-by: Chris Leech <cleech@redhat.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>", "target": 1, "dataset": "other", "idx": 206293}
{"func": "static int\niscsi_if_recv_msg(struct sk_buff *skb, struct nlmsghdr *nlh, uint32_t *group)\n{\n\tint err = 0;\n\tu32 portid;\n\tu32 pdu_len;\n\tstruct iscsi_uevent *ev = nlmsg_data(nlh);\n\tstruct iscsi_transport *transport = NULL;\n\tstruct iscsi_internal *priv;\n\tstruct iscsi_cls_session *session;\n\tstruct iscsi_cls_conn *conn;\n\t\t\tiscsi_if_stop_conn(conn, ev->u.stop_conn.flag);\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\tcase ISCSI_UEVENT_SEND_PDU:\n\t\tpdu_len = nlh->nlmsg_len - sizeof(*nlh) - sizeof(*ev);\n\n\t\tif ((ev->u.send_pdu.hdr_size > pdu_len) ||\n\t\t    (ev->u.send_pdu.data_size > (pdu_len - ev->u.send_pdu.hdr_size))) {\n\t\t\terr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tconn = iscsi_conn_lookup(ev->u.send_pdu.sid, ev->u.send_pdu.cid);\n\t\tif (conn) {\n\t\t\tmutex_lock(&conn_mutex);\n\t\t\tev->r.retcode =\ttransport->send_pdu(conn,\n\t\t\t\t(struct iscsi_hdr*)((char*)ev + sizeof(*ev)),", "project": "linux", "hash": 290280226689808529758982123621457114455, "size": 234, "commit_id": "f9dbdf97a5bd92b1a49cee3d591b55b11fd7a6d5", "message": "scsi: iscsi: Verify lengths on passthrough PDUs\n\nOpen-iSCSI sends passthrough PDUs over netlink, but the kernel should be\nverifying that the provided PDU header and data lengths fall within the\nnetlink message to prevent accessing beyond that in memory.\n\nCc: stable@vger.kernel.org\nReported-by: Adam Nichols <adam@grimm-co.com>\nReviewed-by: Lee Duncan <lduncan@suse.com>\nReviewed-by: Mike Christie <michael.christie@oracle.com>\nSigned-off-by: Chris Leech <cleech@redhat.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>", "target": 0, "dataset": "other", "idx": 380003}
{"func": "\tprop = kzalloc(sizeof(*prop), GFP_KERNEL);\n\tif (!prop)\n\t\treturn NULL;\n\n\tname = (char *)ccwa + be32_to_cpu(ccwa->name_offset);\n\tprop->name = kstrdup(name, GFP_KERNEL);\n\n\tprop->length = be32_to_cpu(ccwa->prop_length);\n\tvalue = (char *)ccwa + be32_to_cpu(ccwa->prop_offset);\n\tprop->value = kmemdup(value, prop->length, GFP_KERNEL);\n\tif (!prop->value) {", "project": "linux", "hash": 112920879356861496445414674726809319128, "size": 23, "commit_id": "efa9ace68e487ddd29c2b4d6dd23242158f1f607", "message": "powerpc/pseries/dlpar: Fix a missing check in dlpar_parse_cc_property()\n\nIn dlpar_parse_cc_property(), 'prop->name' is allocated by kstrdup().\nkstrdup() may return NULL, so it should be checked and handle error.\nAnd prop should be freed if 'prop->name' is NULL.\n\nSigned-off-by: Gen Zhang <blackgod016574@gmail.com>\nSigned-off-by: Michael Ellerman <mpe@ellerman.id.au>", "target": 1, "dataset": "other", "idx": 206300}
{"func": "\tif (!prop)\n\t\treturn NULL;\n\n\tname = (char *)ccwa + be32_to_cpu(ccwa->name_offset);\n\tprop->name = kstrdup(name, GFP_KERNEL);\n\tif (!prop->name) {\n\t\tdlpar_free_cc_property(prop);\n\t\treturn NULL;\n\t}\n\n\tprop->length = be32_to_cpu(ccwa->prop_length);\n\tvalue = (char *)ccwa + be32_to_cpu(ccwa->prop_offset);\n\tprop->value = kmemdup(value, prop->length, GFP_KERNEL);\n\tif (!prop->value) {", "project": "linux", "hash": 307975532274501928757324716654129527375, "size": 27, "commit_id": "efa9ace68e487ddd29c2b4d6dd23242158f1f607", "message": "powerpc/pseries/dlpar: Fix a missing check in dlpar_parse_cc_property()\n\nIn dlpar_parse_cc_property(), 'prop->name' is allocated by kstrdup().\nkstrdup() may return NULL, so it should be checked and handle error.\nAnd prop should be freed if 'prop->name' is NULL.\n\nSigned-off-by: Gen Zhang <blackgod016574@gmail.com>\nSigned-off-by: Michael Ellerman <mpe@ellerman.id.au>", "target": 0, "dataset": "other", "idx": 380132}
{"func": "      decode_options->ignore_transformations=1;\n    }\n  else\n    (void) SetImageProperty(image,\"exif:Orientation\",\"1\");\n  error=heif_decode_image(image_handle,&heif_image,heif_colorspace_YCbCr,\n    heif_chroma_420,NULL);\n  if (IsHeifSuccess(&error,image) == MagickFalse)\n    {\n      heif_image_handle_release(image_handle);\n      heif_context_free(heif_context);\n      file_data=RelinquishMagickMemory(file_data);\n      return(DestroyImageList(image));\n    }\n  if (decode_options != (struct heif_decoding_options *) NULL)\n    {\n      /*\n        Correct the width and height of the image.\n      */\n      image->columns=(size_t) heif_image_get_width(heif_image,heif_channel_Y);\n      image->rows=(size_t) heif_image_get_height(heif_image,heif_channel_Y);\n      status=SetImageExtent(image,image->columns,image->rows);\n      heif_decoding_options_free(decode_options);\n      if (status == MagickFalse)\n        {\n          heif_image_release(heif_image);\n          heif_image_handle_release(image_handle);\n          heif_context_free(heif_context);\n          file_data=RelinquishMagickMemory(file_data);\n          return(DestroyImageList(image));\n        }\n    }\n  p_y=heif_image_get_plane_readonly(heif_image,heif_channel_Y,&stride_y);\n  p_cb=heif_image_get_plane_readonly(heif_image,heif_channel_Cb,&stride_cb);\n  p_cr=heif_image_get_plane_readonly(heif_image,heif_channel_Cr,&stride_cr);\n  for (y=0; y < (ssize_t) image->rows; y++)", "project": "ImageMagick6", "hash": 126205623246026879854304189328903106802, "size": 281, "commit_id": "3456724dff047db5adb32f8cf70c903c1b7d16d4", "message": "Always correct the width and height of the image.", "target": 1, "dataset": "other", "idx": 206422}
{"func": "      decode_options->ignore_transformations=1;\n    }\n  else\n    (void) SetImageProperty(image,\"exif:Orientation\",\"1\");\n  error=heif_decode_image(image_handle,&heif_image,heif_colorspace_YCbCr,\n    heif_chroma_420,decode_options);\n  if (decode_options != (struct heif_decoding_options *) NULL)\n    heif_decoding_options_free(decode_options);\n  if (IsHeifSuccess(&error,image) == MagickFalse)\n    {\n      heif_image_handle_release(image_handle);\n      heif_context_free(heif_context);\n      file_data=RelinquishMagickMemory(file_data);\n      return(DestroyImageList(image));\n    }\n  /*\n    Correct the width and height of the image.\n  */\n  image->columns=(size_t) heif_image_get_width(heif_image,heif_channel_Y);\n  image->rows=(size_t) heif_image_get_height(heif_image,heif_channel_Y);\n  status=SetImageExtent(image,image->columns,image->rows);\n  if (status == MagickFalse)\n    {\n      heif_image_release(heif_image);\n      heif_image_handle_release(image_handle);\n      heif_context_free(heif_context);\n      file_data=RelinquishMagickMemory(file_data);\n      return(DestroyImageList(image));\n    }\n  p_y=heif_image_get_plane_readonly(heif_image,heif_channel_Y,&stride_y);\n  p_cb=heif_image_get_plane_readonly(heif_image,heif_channel_Cb,&stride_cb);\n  p_cr=heif_image_get_plane_readonly(heif_image,heif_channel_Cr,&stride_cr);\n  for (y=0; y < (ssize_t) image->rows; y++)", "project": "ImageMagick6", "hash": 223412021839106218981699163268333438881, "size": 279, "commit_id": "3456724dff047db5adb32f8cf70c903c1b7d16d4", "message": "Always correct the width and height of the image.", "target": 0, "dataset": "other", "idx": 381036}
{"func": "int pci_piix3_xen_ide_unplug(DeviceState *dev)\n{\n    PCIIDEState *pci_ide;\n    DriveInfo *di;\n    int i;\n\n    pci_ide = PCI_IDE(dev);\n\n    for (i = 0; i < 4; i++) {\n        di = drive_get_by_index(IF_IDE, i);\n            BlockBackend *blk = blk_by_legacy_dinfo(di);\n            DeviceState *ds = blk_get_attached_dev(blk);\n            if (ds) {\n                blk_detach_dev(blk, ds);\n            }\n            pci_ide->bus[di->bus].ifs[di->unit].blk = NULL;\n            blk_unref(blk);\n        }\n    }\n    qdev_reset_all(DEVICE(dev));\n    return 0;", "project": "qemu", "hash": 24920103150571267488054332955141284957, "size": 23, "commit_id": "6cd387833d05e8ad31829d97e474dc420625aed9", "message": "Fix release_drive on unplugged devices (pci_piix3_xen_ide_unplug)\n\npci_piix3_xen_ide_unplug should completely unhook the unplugged\nIDEDevice from the corresponding BlockBackend, otherwise the next call\nto release_drive will try to detach the drive again.\n\nSuggested-by: Kevin Wolf <kwolf@redhat.com>\nSigned-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>", "target": 1, "dataset": "other", "idx": 206467}
{"func": "int pci_piix3_xen_ide_unplug(DeviceState *dev)\n{\n    PCIIDEState *pci_ide;\n    DriveInfo *di;\n    int i;\n    IDEDevice *idedev;\n\n    pci_ide = PCI_IDE(dev);\n\n    for (i = 0; i < 4; i++) {\n        di = drive_get_by_index(IF_IDE, i);\n            DeviceState *ds = blk_get_attached_dev(blk);\n            if (ds) {\n                blk_detach_dev(blk, ds);\n            }\n            pci_ide->bus[di->bus].ifs[di->unit].blk = NULL;\n            if (!(i % 2)) {\n                idedev = pci_ide->bus[di->bus].master;\n            } else {\n                idedev = pci_ide->bus[di->bus].slave;\n            }\n            idedev->conf.blk = NULL;\n            blk_unref(blk);\n        }\n    }\n    qdev_reset_all(DEVICE(dev));\n    return 0;", "project": "qemu", "hash": 50331044897901214962169569223095064355, "size": 30, "commit_id": "6cd387833d05e8ad31829d97e474dc420625aed9", "message": "Fix release_drive on unplugged devices (pci_piix3_xen_ide_unplug)\n\npci_piix3_xen_ide_unplug should completely unhook the unplugged\nIDEDevice from the corresponding BlockBackend, otherwise the next call\nto release_drive will try to detach the drive again.\n\nSuggested-by: Kevin Wolf <kwolf@redhat.com>\nSigned-off-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>", "target": 0, "dataset": "other", "idx": 381410}
{"func": "video_usercopy(struct file *file, unsigned int orig_cmd, unsigned long arg,\n\t       v4l2_kioctl func)\n{\n\tchar\tsbuf[128];\n\tvoid    *mbuf = NULL;\n\tvoid\t*parg = (void *)arg;\n\tlong\terr  = -EINVAL;\n\tbool\thas_array_args;\n\tbool\talways_copy = false;\n\tsize_t  array_size = 0;\n\tif (err < 0)\n\t\tgoto out;\n\thas_array_args = err;\n\n\tif (has_array_args) {\n\t\t/*\n\t\t * When adding new types of array args, make sure that the\n\t\t * parent argument to ioctl (which contains the pointer to the\n\t\t * array) fits into sbuf (so that mbuf will still remain\n\t\t * unused up to here).\n\t\t */\n\t\tmbuf = kvmalloc(array_size, GFP_KERNEL);\n\t\terr = -ENOMEM;\n\t\tif (NULL == mbuf)\n\t\t\tgoto out_array_args;\n\t\terr = -EFAULT;\n\t\tif (in_compat_syscall())\n\t\t\terr = v4l2_compat_get_array_args(file, mbuf, user_ptr,\n\t\t\t\t\t\t\t array_size, orig_cmd,\n\t\t\t\t\t\t\t parg);\n\t\telse\n\t\t\terr = copy_from_user(mbuf, user_ptr, array_size) ?\n\t\t\t\t\t\t\t\t-EFAULT : 0;\n\t\tif (err)\n\t\t\tgoto out_array_args;\n\t\t*kernel_ptr = mbuf;\n\t}\n\n\t/* Handles IOCTL */\n\terr = func(file, cmd, parg);\n\tif (err == -ENOTTY || err == -ENOIOCTLCMD) {\n\tif (has_array_args) {\n\t\t*kernel_ptr = (void __force *)user_ptr;\n\t\tif (in_compat_syscall()) {\n\t\t\tint put_err;\n\n\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr, mbuf,\n\t\t\t\t\t\t\t     array_size, orig_cmd,\n\t\t\t\t\t\t\t     parg);\n\t\t\tif (put_err)\n\t\t\t\terr = put_err;\n\t\t} else if (copy_to_user(user_ptr, mbuf, array_size)) {\n\t\t\terr = -EFAULT;\n\t\t}\n\t\tgoto out_array_args;\n\t}\n\t/*\n\t\tgoto out;\n\nout_array_args:\n\tif (video_put_user((void __user *)arg, parg, cmd, orig_cmd))\n\t\terr = -EFAULT;\nout:\n\tkvfree(mbuf);\n\treturn err;\n}", "project": "linux", "hash": 335168946814517336955071454206732269952, "size": 105, "commit_id": "fb18802a338b36f675a388fc03d2aa504a0d0899", "message": "media: v4l: ioctl: Fix memory leak in video_usercopy\n\nWhen an IOCTL with argument size larger than 128 that also used array\narguments were handled, two memory allocations were made but alas, only\nthe latter one of them was released. This happened because there was only\na single local variable to hold such a temporary allocation.\n\nFix this by adding separate variables to hold the pointers to the\ntemporary allocations.\n\nReported-by: Arnd Bergmann <arnd@kernel.org>\nReported-by: syzbot+1115e79c8df6472c612b@syzkaller.appspotmail.com\nFixes: d14e6d76ebf7 (\"[media] v4l: Add multi-planar ioctl handling code\")\nCc: stable@vger.kernel.org\nSigned-off-by: Sakari Ailus <sakari.ailus@linux.intel.com>\nAcked-by: Arnd Bergmann <arnd@arndb.de>\nAcked-by: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nReviewed-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 1, "dataset": "other", "idx": 206468}
{"func": "video_usercopy(struct file *file, unsigned int orig_cmd, unsigned long arg,\n\t       v4l2_kioctl func)\n{\n\tchar\tsbuf[128];\n\tvoid    *mbuf = NULL, *array_buf = NULL;\n\tvoid\t*parg = (void *)arg;\n\tlong\terr  = -EINVAL;\n\tbool\thas_array_args;\n\tbool\talways_copy = false;\n\tsize_t  array_size = 0;\n\tif (err < 0)\n\t\tgoto out;\n\thas_array_args = err;\n\n\tif (has_array_args) {\n\t\tarray_buf = kvmalloc(array_size, GFP_KERNEL);\n\t\terr = -ENOMEM;\n\t\tif (array_buf == NULL)\n\t\t\tgoto out_array_args;\n\t\terr = -EFAULT;\n\t\tif (in_compat_syscall())\n\t\t\terr = v4l2_compat_get_array_args(file, array_buf,\n\t\t\t\t\t\t\t user_ptr, array_size,\n\t\t\t\t\t\t\t orig_cmd, parg);\n\t\telse\n\t\t\terr = copy_from_user(array_buf, user_ptr, array_size) ?\n\t\t\t\t\t\t\t\t-EFAULT : 0;\n\t\tif (err)\n\t\t\tgoto out_array_args;\n\t\t*kernel_ptr = array_buf;\n\t}\n\n\t/* Handles IOCTL */\n\terr = func(file, cmd, parg);\n\tif (err == -ENOTTY || err == -ENOIOCTLCMD) {\n\tif (has_array_args) {\n\t\t*kernel_ptr = (void __force *)user_ptr;\n\t\tif (in_compat_syscall()) {\n\t\t\tint put_err;\n\n\t\t\tput_err = v4l2_compat_put_array_args(file, user_ptr,\n\t\t\t\t\t\t\t     array_buf,\n\t\t\t\t\t\t\t     array_size,\n\t\t\t\t\t\t\t     orig_cmd, parg);\n\t\t\tif (put_err)\n\t\t\t\terr = put_err;\n\t\t} else if (copy_to_user(user_ptr, array_buf, array_size)) {\n\t\t\terr = -EFAULT;\n\t\t}\n\t\tgoto out_array_args;\n\t}\n\t/*\n\nout_array_args:\n\tif (video_put_user((void __user *)arg, parg, cmd, orig_cmd))\n\t\terr = -EFAULT;\nout:\n\tkvfree(array_buf);\n\tkvfree(mbuf);\n\treturn err;\n}", "project": "linux", "hash": 111009062421503165995091383541572390268, "size": 101, "commit_id": "fb18802a338b36f675a388fc03d2aa504a0d0899", "message": "media: v4l: ioctl: Fix memory leak in video_usercopy\n\nWhen an IOCTL with argument size larger than 128 that also used array\narguments were handled, two memory allocations were made but alas, only\nthe latter one of them was released. This happened because there was only\na single local variable to hold such a temporary allocation.\n\nFix this by adding separate variables to hold the pointers to the\ntemporary allocations.\n\nReported-by: Arnd Bergmann <arnd@kernel.org>\nReported-by: syzbot+1115e79c8df6472c612b@syzkaller.appspotmail.com\nFixes: d14e6d76ebf7 (\"[media] v4l: Add multi-planar ioctl handling code\")\nCc: stable@vger.kernel.org\nSigned-off-by: Sakari Ailus <sakari.ailus@linux.intel.com>\nAcked-by: Arnd Bergmann <arnd@arndb.de>\nAcked-by: Hans Verkuil <hverkuil-cisco@xs4all.nl>\nReviewed-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 0, "dataset": "other", "idx": 381516}
{"func": "\n\tcase EVIOCGMTSLOTS(0):\n\t\treturn evdev_handle_mt_request(dev, size, ip);\n\n\tcase EVIOCGKEY(0):\n\t\treturn bits_to_user(dev->key, KEY_MAX, size, p, compat_mode);\n\n\tcase EVIOCGLED(0):\n\t\treturn bits_to_user(dev->led, LED_MAX, size, p, compat_mode);\n\n\tcase EVIOCGSND(0):\n\t\treturn bits_to_user(dev->snd, SND_MAX, size, p, compat_mode);\n\n\tcase EVIOCGSW(0):\n\t\treturn bits_to_user(dev->sw, SW_MAX, size, p, compat_mode);\n\n\tcase EVIOCGNAME(0):\n\t\treturn str_to_user(dev->name, size, p);\n\n\tcase EVIOCGPHYS(0):", "project": "linux", "hash": 58772209716728897872783601375559144047, "size": 191, "commit_id": "483180281f0ac60d1138710eb21f4b9961901294", "message": "Input: evdev - flush queues during EVIOCGKEY-like ioctls\n\nIf userspace requests current KEY-state, they very likely assume that no\nsuch events are pending in the output queue of the evdev device.\nOtherwise, they will parse events which they already handled via\nEVIOCGKEY(). For XKB applications this can cause irreversible keyboard\nstates if a modifier is locked multiple times because a CTRL-DOWN event is\nhandled once via EVIOCGKEY() and once from the queue via read(), even\nthough it should handle it only once.\n\nTherefore, lets do the only logical thing and flush the evdev queue\natomically during this ioctl. We only flush events that are affected by\nthe given ioctl.\n\nThis only affects boolean events like KEY, SND, SW and LED. ABS, REL and\nothers are not affected as duplicate events can be handled gracefully by\nuser-space.\n\nNote: This actually breaks semantics of the evdev ABI. However,\ninvestigations showed that userspace already expects the new semantics and\nwe end up fixing at least all XKB applications.\nAll applications that are aware of this race-condition mirror the KEY\nstate for each open-file and detect/drop duplicate events. Hence, they do\nnot care whether duplicates are posted or not and work fine with this fix.\n\nAlso note that we need proper locking to guarantee atomicity and avoid\ndead-locks. event_lock must be locked before queue_lock (see input-core).\nHowever, we can safely release event_lock while flushing the queue. This\nallows the input-core to proceed with pending events and only stop if it\nneeds our queue_lock to post new events.\nThis should guarantee that we don't block event-dispatching for too long\nwhile flushing a single event queue.\n\nSigned-off-by: David Herrmann <dh.herrmann@gmail.com>\nAcked-by: Peter Hutterer <peter.hutterer@who-t.net>\nSigned-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>", "target": 1, "dataset": "other", "idx": 206492}
{"func": "\n\tcase EVIOCGMTSLOTS(0):\n\t\treturn evdev_handle_mt_request(dev, size, ip);\n\n\tcase EVIOCGKEY(0):\n\t\treturn evdev_handle_get_val(client, dev, EV_KEY, dev->key,\n\t\t\t\t\t    KEY_MAX, size, p, compat_mode);\n\n\tcase EVIOCGLED(0):\n\t\treturn evdev_handle_get_val(client, dev, EV_LED, dev->led,\n\t\t\t\t\t    LED_MAX, size, p, compat_mode);\n\n\tcase EVIOCGSND(0):\n\t\treturn evdev_handle_get_val(client, dev, EV_SND, dev->snd,\n\t\t\t\t\t    SND_MAX, size, p, compat_mode);\n\n\tcase EVIOCGSW(0):\n\t\treturn evdev_handle_get_val(client, dev, EV_SW, dev->sw,\n\t\t\t\t\t    SW_MAX, size, p, compat_mode);\n\n\tcase EVIOCGNAME(0):\n\t\treturn str_to_user(dev->name, size, p);\n\n\tcase EVIOCGPHYS(0):", "project": "linux", "hash": 17942239882540888408053190104002056298, "size": 195, "commit_id": "483180281f0ac60d1138710eb21f4b9961901294", "message": "Input: evdev - flush queues during EVIOCGKEY-like ioctls\n\nIf userspace requests current KEY-state, they very likely assume that no\nsuch events are pending in the output queue of the evdev device.\nOtherwise, they will parse events which they already handled via\nEVIOCGKEY(). For XKB applications this can cause irreversible keyboard\nstates if a modifier is locked multiple times because a CTRL-DOWN event is\nhandled once via EVIOCGKEY() and once from the queue via read(), even\nthough it should handle it only once.\n\nTherefore, lets do the only logical thing and flush the evdev queue\natomically during this ioctl. We only flush events that are affected by\nthe given ioctl.\n\nThis only affects boolean events like KEY, SND, SW and LED. ABS, REL and\nothers are not affected as duplicate events can be handled gracefully by\nuser-space.\n\nNote: This actually breaks semantics of the evdev ABI. However,\ninvestigations showed that userspace already expects the new semantics and\nwe end up fixing at least all XKB applications.\nAll applications that are aware of this race-condition mirror the KEY\nstate for each open-file and detect/drop duplicate events. Hence, they do\nnot care whether duplicates are posted or not and work fine with this fix.\n\nAlso note that we need proper locking to guarantee atomicity and avoid\ndead-locks. event_lock must be locked before queue_lock (see input-core).\nHowever, we can safely release event_lock while flushing the queue. This\nallows the input-core to proceed with pending events and only stop if it\nneeds our queue_lock to post new events.\nThis should guarantee that we don't block event-dispatching for too long\nwhile flushing a single event queue.\n\nSigned-off-by: David Herrmann <dh.herrmann@gmail.com>\nAcked-by: Peter Hutterer <peter.hutterer@who-t.net>\nSigned-off-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>", "target": 0, "dataset": "other", "idx": 381731}
{"func": "{\n\tCNetChunk Packet;\n\tif(!pMsg)\n\t\treturn -1;\n\n\t// drop packet to dummy client\n\tif(0 <= ClientID && ClientID < MAX_CLIENTS && GameServer()->IsClientBot(ClientID))\n\t\treturn 0;\n\n\tmem_zero(&Packet, sizeof(CNetChunk));\n\tPacket.m_ClientID = ClientID;\n\tPacket.m_pData = pMsg->Data();", "project": "teeworlds", "hash": 145582132599251035090290965422339038739, "size": 42, "commit_id": "c68402fa7e279d42886d5951d1ea8ac2facc1ea5", "message": "changed a check", "target": 1, "dataset": "other", "idx": 206517}
{"func": "{\n\tCNetChunk Packet;\n\tif(!pMsg)\n\t\treturn -1;\n\n\t// drop invalid packet\n\tif(ClientID != -1 && (ClientID < 0 || ClientID >= MAX_CLIENTS || m_aClients[ClientID].m_State == CClient::STATE_EMPTY || m_aClients[ClientID].m_Quitting))\n\t\treturn 0;\n\n\tmem_zero(&Packet, sizeof(CNetChunk));\n\tPacket.m_ClientID = ClientID;\n\tPacket.m_pData = pMsg->Data();", "project": "teeworlds", "hash": 329487471777399300626475999293740358602, "size": 42, "commit_id": "c68402fa7e279d42886d5951d1ea8ac2facc1ea5", "message": "changed a check", "target": 0, "dataset": "other", "idx": 381986}
{"func": "\t\t\t\t\t\t\t * should be there as well; if it's there, we'll\n\t\t\t\t\t\t\t * also include that. If it's not, there isn't much\n\t\t\t\t\t\t\t * we can do at this point. */\n\t\t\t\t\t\t\tif (*(p1 + 1) == '=') {\n\t\t\t\t\t\t\t\t++p1;\n\t\t\t\t\t\t\t\t--str_left;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\terr = _php_iconv_appendl(pretval, encoded_word, (size_t)((p1 + 1) - encoded_word), cd_pl);\n\t\t\t\t\t\t\tif (err != PHP_ICONV_ERR_SUCCESS) {\n\t\t\t\t\t\t\t\tgoto out;", "project": "php-src", "hash": 252711754502941885197887607368897836371, "size": 549, "commit_id": "7cf7148a8f8f4f55fb04de2a517d740bb6253eac", "message": "Fix bug #78069 - Out-of-bounds read in iconv.c:_php_iconv_mime_decode() due to integer overflow", "target": 1, "dataset": "other", "idx": 206554}
{"func": "\t\t\t\t\t\t\t * should be there as well; if it's there, we'll\n\t\t\t\t\t\t\t * also include that. If it's not, there isn't much\n\t\t\t\t\t\t\t * we can do at this point. */\n\t\t\t\t\t\t\tif (*(p1 + 1) == '=') {\n\t\t\t\t\t\t\t\t++p1;\n\t\t\t\t\t\t\t\tif (str_left > 1) {\n\t\t\t\t\t\t\t\t\t--str_left;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\terr = _php_iconv_appendl(pretval, encoded_word, (size_t)((p1 + 1) - encoded_word), cd_pl);\n\t\t\t\t\t\t\tif (err != PHP_ICONV_ERR_SUCCESS) {\n\t\t\t\t\t\t\t\tgoto out;", "project": "php-src", "hash": 218325121010366491134849590402382438328, "size": 551, "commit_id": "7cf7148a8f8f4f55fb04de2a517d740bb6253eac", "message": "Fix bug #78069 - Out-of-bounds read in iconv.c:_php_iconv_mime_decode() due to integer overflow", "target": 0, "dataset": "other", "idx": 382780}
{"func": "        /* ======> COND_MUTEX */\n        pthread_mutex_lock(&t->cond_mutex);\n        while (t->query == NULL) {\n            /* wait for next query */\n            pthread_cond_wait(&t->cond_wakeup, &t->cond_mutex);\n            if (t->pool->shutdown) {\n                pthread_exit(NULL);\n            }\n        }\n\n        /* grab the query off the shared pointer */", "project": "gssproxy", "hash": 78285020008400258286650992522334327753, "size": 66, "commit_id": "cb761412e299ef907f22cd7c4146d50c8a792003", "message": "Unlock cond_mutex before pthread exit in gp_worker_main()\n\nSigned-off-by: GuiYao <guiyao@huawei.com>\n[rharwood@redhat.com: whitespace, tweak commit message]\nReviewed-by: Robbie Harwood <rharwood@redhat.com>", "target": 1, "dataset": "other", "idx": 206559}
{"func": "        pthread_mutex_lock(&t->cond_mutex);\n        while (t->query == NULL) {\n            /* wait for next query */\n            pthread_cond_wait(&t->cond_wakeup, &t->cond_mutex);\n            if (t->pool->shutdown) {\n                pthread_mutex_unlock(&t->cond_mutex);\n                pthread_exit(NULL);\n            }\n        }\n\n        /* grab the query off the shared pointer */", "project": "gssproxy", "hash": 4842314628500364921745841145573401402, "size": 67, "commit_id": "cb761412e299ef907f22cd7c4146d50c8a792003", "message": "Unlock cond_mutex before pthread exit in gp_worker_main()\n\nSigned-off-by: GuiYao <guiyao@huawei.com>\n[rharwood@redhat.com: whitespace, tweak commit message]\nReviewed-by: Robbie Harwood <rharwood@redhat.com>", "target": 0, "dataset": "other", "idx": 382874}
{"func": "        /* direct calculation as it needs to tile correctly\n         * for reversibility in a DePolar-Polar cycle */\n        fix_bounds = MagickFalse;\n        geometry.x = geometry.y = 0;\n        geometry.height = (size_t) ceil(coeff[0]-coeff[1]);\n        geometry.width = (size_t)\n                  ceil((coeff[0]-coeff[1])*(coeff[5]-coeff[4])*0.5);\n        /* correct scaling factors relative to new size */\n        coeff[6]=(coeff[5]-coeff[4])/geometry.width; /* changed width */\n        coeff[7]=(coeff[0]-coeff[1])/geometry.height; /* should be about 1.0 */\n        break;\n      }\n      case Cylinder2PlaneDistortion:\n      {\n        /* direct calculation so center of distortion is either a pixel", "project": "ImageMagick", "hash": 171104629152982557161712032261784795609, "size": 1159, "commit_id": "f8e8535bc821f24a30beee0030ff21ee3a2deedc", "message": "https://github.com/ImageMagick/ImageMagick/issues/3331", "target": 1, "dataset": "other", "idx": 206584}
{"func": "        /* direct calculation as it needs to tile correctly\n         * for reversibility in a DePolar-Polar cycle */\n        fix_bounds = MagickFalse;\n        geometry.x = geometry.y = 0;\n        geometry.height = (size_t) ceil(coeff[0]-coeff[1]);\n        geometry.width = (size_t) ceil((coeff[0]-coeff[1])*\n          (coeff[5]-coeff[4])*0.5);\n        /* correct scaling factors relative to new size */\n        coeff[6]=(coeff[5]-coeff[4])*PerceptibleReciprocal(geometry.width); /* changed width */\n        coeff[7]=(coeff[0]-coeff[1])*PerceptibleReciprocal(geometry.height); /* should be about 1.0 */\n        break;\n      }\n      case Cylinder2PlaneDistortion:\n      {\n        /* direct calculation so center of distortion is either a pixel", "project": "ImageMagick", "hash": 325010277995776734256210799163273736372, "size": 1159, "commit_id": "f8e8535bc821f24a30beee0030ff21ee3a2deedc", "message": "https://github.com/ImageMagick/ImageMagick/issues/3331", "target": 0, "dataset": "other", "idx": 383186}
{"func": "\tllcp_sock->service_name = kmemdup(llcp_addr.service_name,\n\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tret = -ENOMEM;\n\t\tgoto put_dev;\n\t}\n\tllcp_sock->ssap = nfc_llcp_get_sdp_ssap(local, llcp_sock);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tkfree(llcp_sock->service_name);\n\t\tllcp_sock->service_name = NULL;\n\t\tret = -EADDRINUSE;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;", "project": "linux", "hash": 318436817421404573454745290139407660076, "size": 82, "commit_id": "4ac06a1e013cf5fdd963317ffd3b968560f33bba", "message": "nfc: fix NULL ptr dereference in llcp_sock_getname() after failed connect\n\nIt's possible to trigger NULL pointer dereference by local unprivileged\nuser, when calling getsockname() after failed bind() (e.g. the bind\nfails because LLCP_SAP_MAX used as SAP):\n\n  BUG: kernel NULL pointer dereference, address: 0000000000000000\n  CPU: 1 PID: 426 Comm: llcp_sock_getna Not tainted 5.13.0-rc2-next-20210521+ #9\n  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.14.0-1 04/01/2014\n  Call Trace:\n   llcp_sock_getname+0xb1/0xe0\n   __sys_getpeername+0x95/0xc0\n   ? lockdep_hardirqs_on_prepare+0xd5/0x180\n   ? syscall_enter_from_user_mode+0x1c/0x40\n   __x64_sys_getpeername+0x11/0x20\n   do_syscall_64+0x36/0x70\n   entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nThis can be reproduced with Syzkaller C repro (bind followed by\ngetpeername):\nhttps://syzkaller.appspot.com/x/repro.c?x=14def446e00000\n\nCc: <stable@vger.kernel.org>\nFixes: d646960f7986 (\"NFC: Initial LLCP support\")\nReported-by: syzbot+80fb126e7f7d8b1a5914@syzkaller.appspotmail.com\nReported-by: butt3rflyh4ck <butterflyhuangxx@gmail.com>\nSigned-off-by: Krzysztof Kozlowski <krzysztof.kozlowski@canonical.com>\nLink: https://lore.kernel.org/r/20210531072138.5219-1-krzysztof.kozlowski@canonical.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>", "target": 1, "dataset": "other", "idx": 206589}
{"func": "\t\t\t\t\t  llcp_sock->service_name_len,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!llcp_sock->service_name) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tllcp_sock->dev = NULL;\n\t\tret = -ENOMEM;\n\t\tgoto put_dev;\n\t}\n\tllcp_sock->ssap = nfc_llcp_get_sdp_ssap(local, llcp_sock);\n\tif (llcp_sock->ssap == LLCP_SAP_MAX) {\n\t\tnfc_llcp_local_put(llcp_sock->local);\n\t\tllcp_sock->local = NULL;\n\t\tkfree(llcp_sock->service_name);\n\t\tllcp_sock->service_name = NULL;\n\t\tllcp_sock->dev = NULL;\n\t\tret = -EADDRINUSE;\n\t\tgoto put_dev;\n\t}\n\n\tllcp_sock->reserved_ssap = llcp_sock->ssap;", "project": "linux", "hash": 198293030880834049287684601516018080299, "size": 84, "commit_id": "4ac06a1e013cf5fdd963317ffd3b968560f33bba", "message": "nfc: fix NULL ptr dereference in llcp_sock_getname() after failed connect\n\nIt's possible to trigger NULL pointer dereference by local unprivileged\nuser, when calling getsockname() after failed bind() (e.g. the bind\nfails because LLCP_SAP_MAX used as SAP):\n\n  BUG: kernel NULL pointer dereference, address: 0000000000000000\n  CPU: 1 PID: 426 Comm: llcp_sock_getna Not tainted 5.13.0-rc2-next-20210521+ #9\n  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.14.0-1 04/01/2014\n  Call Trace:\n   llcp_sock_getname+0xb1/0xe0\n   __sys_getpeername+0x95/0xc0\n   ? lockdep_hardirqs_on_prepare+0xd5/0x180\n   ? syscall_enter_from_user_mode+0x1c/0x40\n   __x64_sys_getpeername+0x11/0x20\n   do_syscall_64+0x36/0x70\n   entry_SYSCALL_64_after_hwframe+0x44/0xae\n\nThis can be reproduced with Syzkaller C repro (bind followed by\ngetpeername):\nhttps://syzkaller.appspot.com/x/repro.c?x=14def446e00000\n\nCc: <stable@vger.kernel.org>\nFixes: d646960f7986 (\"NFC: Initial LLCP support\")\nReported-by: syzbot+80fb126e7f7d8b1a5914@syzkaller.appspotmail.com\nReported-by: butt3rflyh4ck <butterflyhuangxx@gmail.com>\nSigned-off-by: Krzysztof Kozlowski <krzysztof.kozlowski@canonical.com>\nLink: https://lore.kernel.org/r/20210531072138.5219-1-krzysztof.kozlowski@canonical.com\nSigned-off-by: Jakub Kicinski <kuba@kernel.org>", "target": 0, "dataset": "other", "idx": 383386}
{"func": "\n  /* Read and decipher Logical Screen Descriptor */\n  if (!ReadOK(source->pub.input_file, hdrbuf, 7))\n    ERREXIT(cinfo, JERR_INPUT_EOF);\n  width = LM_to_uint(hdrbuf, 0);\n  height = LM_to_uint(hdrbuf, 2);\n  /* we ignore the color resolution, sort flag, and background color index */\n  aspectRatio = UCH(hdrbuf[6]);\n  if (aspectRatio != 0 && aspectRatio != 49)\n    TRACEMS(cinfo, 1, JTRC_GIF_NONSQUARE);\n\n    /* Read and decipher Local Image Descriptor */\n    if (!ReadOK(source->pub.input_file, hdrbuf, 9))\n      ERREXIT(cinfo, JERR_INPUT_EOF);\n    /* we ignore top/left position info, also sort flag */\n    width = LM_to_uint(hdrbuf, 4);\n    height = LM_to_uint(hdrbuf, 6);\n    source->is_interlaced = (BitSet(hdrbuf[8], INTERLACE) != 0);\n\n    /* Read local colormap if header indicates it is present */\n    /* Note: if we wanted to support skipping images, */\n    /* we'd need to skip rather than read colormap for ignored images */", "project": "libjpeg-turbo", "hash": 201180578183398097843040932663132682779, "size": 144, "commit_id": "1719d12e51641cce5c77e259516649ba5ef6303c", "message": "cjpeg: Fix FPE when compressing 0-width GIF\n\nFixes #493", "target": 1, "dataset": "other", "idx": 206616}
{"func": "  /* Read and decipher Logical Screen Descriptor */\n  if (!ReadOK(source->pub.input_file, hdrbuf, 7))\n    ERREXIT(cinfo, JERR_INPUT_EOF);\n  width = LM_to_uint(hdrbuf, 0);\n  height = LM_to_uint(hdrbuf, 2);\n  if (width == 0 || height == 0)\n    ERREXIT(cinfo, JERR_GIF_EMPTY);\n  /* we ignore the color resolution, sort flag, and background color index */\n  aspectRatio = UCH(hdrbuf[6]);\n  if (aspectRatio != 0 && aspectRatio != 49)\n    TRACEMS(cinfo, 1, JTRC_GIF_NONSQUARE);\n\n    if (!ReadOK(source->pub.input_file, hdrbuf, 9))\n      ERREXIT(cinfo, JERR_INPUT_EOF);\n    /* we ignore top/left position info, also sort flag */\n    width = LM_to_uint(hdrbuf, 4);\n    height = LM_to_uint(hdrbuf, 6);\n    if (width == 0 || height == 0)\n      ERREXIT(cinfo, JERR_GIF_EMPTY);\n    source->is_interlaced = (BitSet(hdrbuf[8], INTERLACE) != 0);\n\n    /* Read local colormap if header indicates it is present */\n    /* Note: if we wanted to support skipping images, */\n    /* we'd need to skip rather than read colormap for ignored images */", "project": "libjpeg-turbo", "hash": 121557664873555763939551225581936916025, "size": 148, "commit_id": "1719d12e51641cce5c77e259516649ba5ef6303c", "message": "cjpeg: Fix FPE when compressing 0-width GIF\n\nFixes #493", "target": 0, "dataset": "other", "idx": 383920}
{"func": "\tcase EXIF_TAG_XP_KEYWORDS:\n\tcase EXIF_TAG_XP_SUBJECT:\n\t{\n\t\tunsigned char *utf16;\n\n\t\t/* Sanity check the size to prevent overflow */\n\t\tif (e->size+sizeof(uint16_t)+1 < e->size) break;\n\n\t\t/* The tag may not be U+0000-terminated , so make a local\n\t\t   U+0000-terminated copy before converting it */\n\t\tutf16 = exif_mem_alloc (e->priv->mem, e->size+sizeof(uint16_t)+1);\n\t\tif (!utf16) break;", "project": "libexif", "hash": 41736839540103750314907754428693931445, "size": 571, "commit_id": "9266d14b5ca4e29b970fa03272318e5f99386e06", "message": "fixed a incorrect overflow check that could be optimized away.\n\ninspired by:\nhttps://android.googlesource.com/platform/external/libexif/+/8e7345f3bc0bad06ac369d6cbc1124c8ceaf7d4b\n\nhttps://source.android.com/security/bulletin/2020-11-01\n\nCVE-2020-0452", "target": 1, "dataset": "other", "idx": 206645}
{"func": "\tcase EXIF_TAG_XP_KEYWORDS:\n\tcase EXIF_TAG_XP_SUBJECT:\n\t{\n\t\tunsigned char *utf16;\n\n\t\t/* Sanity check the size to prevent overflow. Note EXIF files are 64kb at most. */\n\t\tif (e->size >= 65536 - sizeof(uint16_t)*2) break;\n\n\t\t/* The tag may not be U+0000-terminated , so make a local\n\t\t   U+0000-terminated copy before converting it */\n\t\tutf16 = exif_mem_alloc (e->priv->mem, e->size+sizeof(uint16_t)+1);\n\t\tif (!utf16) break;", "project": "libexif", "hash": 96196385413825038531426685586673546332, "size": 571, "commit_id": "9266d14b5ca4e29b970fa03272318e5f99386e06", "message": "fixed a incorrect overflow check that could be optimized away.\n\ninspired by:\nhttps://android.googlesource.com/platform/external/libexif/+/8e7345f3bc0bad06ac369d6cbc1124c8ceaf7d4b\n\nhttps://source.android.com/security/bulletin/2020-11-01\n\nCVE-2020-0452", "target": 0, "dataset": "other", "idx": 384338}
{"func": "\t\t\titer = table;\n\t\t\tfor_each_sg(table, iter, sg_nents(table), i) {\n\t\t\t\tnew_page = sg_page(iter);\n\t\t\t\tif (new_page)\n\t\t\t\t\t__free_page(new_page);\n\t\t\t}\n\t\t\treturn NULL;\n\t\t}\n\t\talloc_size = min_t(int, size, PAGE_SIZE);\n\t\tsize -= PAGE_SIZE;\n\t\tsg_set_page(iter, new_page, alloc_size, 0);", "project": "linux", "hash": 155226500233023096269286137238523446812, "size": 31, "commit_id": "b4b814fec1a5a849383f7b3886b654a13abbda7d", "message": "iwlwifi: dbg_ini: fix memory leak in alloc_sgtable\n\nIn alloc_sgtable if alloc_page fails, the alocated table should be\nreleased.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Luca Coelho <luciano.coelho@intel.com>", "target": 1, "dataset": "other", "idx": 206661}
{"func": "\t\t\tfor_each_sg(table, iter, sg_nents(table), i) {\n\t\t\t\tnew_page = sg_page(iter);\n\t\t\t\tif (new_page)\n\t\t\t\t\t__free_page(new_page);\n\t\t\t}\n\t\t\tkfree(table);\n\t\t\treturn NULL;\n\t\t}\n\t\talloc_size = min_t(int, size, PAGE_SIZE);\n\t\tsize -= PAGE_SIZE;\n\t\tsg_set_page(iter, new_page, alloc_size, 0);", "project": "linux", "hash": 158159015289832357782514537392354870835, "size": 32, "commit_id": "b4b814fec1a5a849383f7b3886b654a13abbda7d", "message": "iwlwifi: dbg_ini: fix memory leak in alloc_sgtable\n\nIn alloc_sgtable if alloc_page fails, the alocated table should be\nreleased.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Luca Coelho <luciano.coelho@intel.com>", "target": 0, "dataset": "other", "idx": 384469}
{"func": "    body_remaining.size = max;\n\n    while (body_remaining.size && !parser.is_done()) {\n      boost::system::error_code ec;\n      http::async_read_some(stream, buffer, parser, yield[ec]);\n      if (ec == http::error::partial_message ||\n          ec == http::error::need_buffer) {\n        break;\n      }\n      if (ec) {\n        ldout(cct, 4) << \"failed to read body: \" << ec.message() << dendl;\n        throw rgw::io::Exception(ec.value(), std::system_category());", "project": "ceph", "hash": 239302242093544263117469919869262423971, "size": 20, "commit_id": "ff72c50a2c43c57aead933eb4903ad1ca6d1748a", "message": "rgw: improve beast\n\nAvoid leaking connections that had partially-consumed\nclient data on unexpected disconnect.\n\nResolves CVE-2020-1700 (moderate impact flaw).\n\nFixes: https://tracker.ceph.com/issues/42531\n\nSigned-off-by: Or Friedmann <ofriedma@redhat.com>\nSigned-off-by: Matt Benjamin <mbenjamin@redhat.com>", "target": 1, "dataset": "other", "idx": 206666}
{"func": "    body_remaining.size = max;\n\n    while (body_remaining.size && !parser.is_done()) {\n      boost::system::error_code ec;\n      http::async_read_some(stream, buffer, parser, yield[ec]);\n      if (ec == http::error::need_buffer) {\n        break;\n      }\n      if (ec) {\n        ldout(cct, 4) << \"failed to read body: \" << ec.message() << dendl;\n        throw rgw::io::Exception(ec.value(), std::system_category());", "project": "ceph", "hash": 297732611702424956920389637575646123237, "size": 19, "commit_id": "ff72c50a2c43c57aead933eb4903ad1ca6d1748a", "message": "rgw: improve beast\n\nAvoid leaking connections that had partially-consumed\nclient data on unexpected disconnect.\n\nResolves CVE-2020-1700 (moderate impact flaw).\n\nFixes: https://tracker.ceph.com/issues/42531\n\nSigned-off-by: Or Friedmann <ofriedma@redhat.com>\nSigned-off-by: Matt Benjamin <mbenjamin@redhat.com>", "target": 0, "dataset": "other", "idx": 384543}
{"func": "                item = proto_tree_add_uint(checksum_tree, hf_tcp_checksum_calculated, tvb,\n                                              offset + 16, 2, 0x0000);\n                PROTO_ITEM_SET_GENERATED(item);\n                /* XXX - What should this special status be? */\n                item = proto_tree_add_uint(checksum_tree, hf_tcp_checksum_status, tvb,\n                                              offset + 16, 0, 4);\n                PROTO_ITEM_SET_GENERATED(item);\n                expert_add_info(pinfo, item, &ei_tcp_checksum_ffff);\n\n                col_append_str(pinfo->cinfo, COL_INFO, \" [TCP CHECKSUM 0xFFFF]\");\n", "project": "wireshark", "hash": 267912446686612518591440218718898174428, "size": 807, "commit_id": "7f3fe6164a68b76d9988c4253b24d43f498f1753", "message": "TCP: do not use an unknown status when the checksum is 0xffff\n\nOtherwise it triggers an assert when adding the column as the field is\ndefined as BASE_NONE and not BASE_DEC or BASE_HEX. Thus an unknown value\n(not in proto_checksum_vals[)array) cannot be represented.\nMark the checksum as bad even if we process the packet.\nCloses #16816\n\nConflicts:\n\tepan/dissectors/packet-tcp.c", "target": 1, "dataset": "other", "idx": 206691}
{"func": "                item = proto_tree_add_uint(checksum_tree, hf_tcp_checksum_calculated, tvb,\n                                              offset + 16, 2, 0x0000);\n                PROTO_ITEM_SET_GENERATED(item);\n                /* XXX - What should this special status be? */\n                item = proto_tree_add_uint(checksum_tree, hf_tcp_checksum_status, tvb,\n                                              offset + 16, 0, PROTO_CHECKSUM_E_BAD);\n                PROTO_ITEM_SET_GENERATED(item);\n                expert_add_info(pinfo, item, &ei_tcp_checksum_ffff);\n\n                col_append_str(pinfo->cinfo, COL_INFO, \" [TCP CHECKSUM 0xFFFF]\");\n", "project": "wireshark", "hash": 314637810017431195334165172811945016897, "size": 807, "commit_id": "7f3fe6164a68b76d9988c4253b24d43f498f1753", "message": "TCP: do not use an unknown status when the checksum is 0xffff\n\nOtherwise it triggers an assert when adding the column as the field is\ndefined as BASE_NONE and not BASE_DEC or BASE_HEX. Thus an unknown value\n(not in proto_checksum_vals[)array) cannot be represented.\nMark the checksum as bad even if we process the packet.\nCloses #16816\n\nConflicts:\n\tepan/dissectors/packet-tcp.c", "target": 0, "dataset": "other", "idx": 385216}
{"func": "static void umd_cleanup(struct subprocess_info *info)\n{\n\tstruct umd_info *umd_info = info->data;\n\n\t/* cleanup if umh_setup() was successful but exec failed */\n\tif (info->retval) {\n\t\tfput(umd_info->pipe_to_umh);\n\t\tfput(umd_info->pipe_from_umh);\n\t\tput_pid(umd_info->tgid);\n\t\tumd_info->tgid = NULL;\n\t}\n}", "project": "linux", "hash": 202261761255865455882373619018707547027, "size": 12, "commit_id": "f60a85cad677c4f9bb4cadd764f1d106c38c7cf8", "message": "bpf: Fix umd memory leak in copy_process()\n\nThe syzbot reported a memleak as follows:\n\nBUG: memory leak\nunreferenced object 0xffff888101b41d00 (size 120):\n  comm \"kworker/u4:0\", pid 8, jiffies 4294944270 (age 12.780s)\n  backtrace:\n    [<ffffffff8125dc56>] alloc_pid+0x66/0x560\n    [<ffffffff81226405>] copy_process+0x1465/0x25e0\n    [<ffffffff81227943>] kernel_clone+0xf3/0x670\n    [<ffffffff812281a1>] kernel_thread+0x61/0x80\n    [<ffffffff81253464>] call_usermodehelper_exec_work\n    [<ffffffff81253464>] call_usermodehelper_exec_work+0xc4/0x120\n    [<ffffffff812591c9>] process_one_work+0x2c9/0x600\n    [<ffffffff81259ab9>] worker_thread+0x59/0x5d0\n    [<ffffffff812611c8>] kthread+0x178/0x1b0\n    [<ffffffff8100227f>] ret_from_fork+0x1f/0x30\n\nunreferenced object 0xffff888110ef5c00 (size 232):\n  comm \"kworker/u4:0\", pid 8414, jiffies 4294944270 (age 12.780s)\n  backtrace:\n    [<ffffffff8154a0cf>] kmem_cache_zalloc\n    [<ffffffff8154a0cf>] __alloc_file+0x1f/0xf0\n    [<ffffffff8154a809>] alloc_empty_file+0x69/0x120\n    [<ffffffff8154a8f3>] alloc_file+0x33/0x1b0\n    [<ffffffff8154ab22>] alloc_file_pseudo+0xb2/0x140\n    [<ffffffff81559218>] create_pipe_files+0x138/0x2e0\n    [<ffffffff8126c793>] umd_setup+0x33/0x220\n    [<ffffffff81253574>] call_usermodehelper_exec_async+0xb4/0x1b0\n    [<ffffffff8100227f>] ret_from_fork+0x1f/0x30\n\nAfter the UMD process exits, the pipe_to_umh/pipe_from_umh and\ntgid need to be released.\n\nFixes: d71fa5c9763c (\"bpf: Add kernel module with user mode driver that populates bpffs.\")\nReported-by: syzbot+44908bb56d2bfe56b28e@syzkaller.appspotmail.com\nSigned-off-by: Zqiang <qiang.zhang@windriver.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20210317030915.2865-1-qiang.zhang@windriver.com", "target": 1, "dataset": "other", "idx": 206697}
{"func": "static void umd_cleanup(struct subprocess_info *info)\n{\n\tstruct umd_info *umd_info = info->data;\n\n\t/* cleanup if umh_setup() was successful but exec failed */\n\tif (info->retval)\n\t\tumd_cleanup_helper(umd_info);\n}", "project": "linux", "hash": 164239646891437415065034800136492740579, "size": 8, "commit_id": "f60a85cad677c4f9bb4cadd764f1d106c38c7cf8", "message": "bpf: Fix umd memory leak in copy_process()\n\nThe syzbot reported a memleak as follows:\n\nBUG: memory leak\nunreferenced object 0xffff888101b41d00 (size 120):\n  comm \"kworker/u4:0\", pid 8, jiffies 4294944270 (age 12.780s)\n  backtrace:\n    [<ffffffff8125dc56>] alloc_pid+0x66/0x560\n    [<ffffffff81226405>] copy_process+0x1465/0x25e0\n    [<ffffffff81227943>] kernel_clone+0xf3/0x670\n    [<ffffffff812281a1>] kernel_thread+0x61/0x80\n    [<ffffffff81253464>] call_usermodehelper_exec_work\n    [<ffffffff81253464>] call_usermodehelper_exec_work+0xc4/0x120\n    [<ffffffff812591c9>] process_one_work+0x2c9/0x600\n    [<ffffffff81259ab9>] worker_thread+0x59/0x5d0\n    [<ffffffff812611c8>] kthread+0x178/0x1b0\n    [<ffffffff8100227f>] ret_from_fork+0x1f/0x30\n\nunreferenced object 0xffff888110ef5c00 (size 232):\n  comm \"kworker/u4:0\", pid 8414, jiffies 4294944270 (age 12.780s)\n  backtrace:\n    [<ffffffff8154a0cf>] kmem_cache_zalloc\n    [<ffffffff8154a0cf>] __alloc_file+0x1f/0xf0\n    [<ffffffff8154a809>] alloc_empty_file+0x69/0x120\n    [<ffffffff8154a8f3>] alloc_file+0x33/0x1b0\n    [<ffffffff8154ab22>] alloc_file_pseudo+0xb2/0x140\n    [<ffffffff81559218>] create_pipe_files+0x138/0x2e0\n    [<ffffffff8126c793>] umd_setup+0x33/0x220\n    [<ffffffff81253574>] call_usermodehelper_exec_async+0xb4/0x1b0\n    [<ffffffff8100227f>] ret_from_fork+0x1f/0x30\n\nAfter the UMD process exits, the pipe_to_umh/pipe_from_umh and\ntgid need to be released.\n\nFixes: d71fa5c9763c (\"bpf: Add kernel module with user mode driver that populates bpffs.\")\nReported-by: syzbot+44908bb56d2bfe56b28e@syzkaller.appspotmail.com\nSigned-off-by: Zqiang <qiang.zhang@windriver.com>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>\nLink: https://lore.kernel.org/bpf/20210317030915.2865-1-qiang.zhang@windriver.com", "target": 0, "dataset": "other", "idx": 385297}
{"func": "\t\t    contextNode) < 0) {\n\t\t    ctxt->error = XPATH_MEMORY_ERROR;\n\t\t    goto evaluation_exit;\n\t\t}\n\t    }\n\n            frame = xmlXPathSetFrame(ctxt);\n\t    valuePush(ctxt, contextObj);\n\t    res = xmlXPathCompOpEvalToBoolean(ctxt, exprOp, 1);\n            tmp = valuePop(ctxt);\n            xmlXPathPopFrame(ctxt, frame);\n\n\t    if ((ctxt->error != XPATH_EXPRESSION_OK) || (res == -1)) {\n                while (tmp != contextObj) {\n                    /*\n                     * Free up the result", "project": "libxml2", "hash": 173043750179367760971490701431316800176, "size": 198, "commit_id": "0f3b843b3534784ef57a4f9b874238aa1fda5a73", "message": "Fix XPath stack frame logic\n\nMove the calls to xmlXPathSetFrame and xmlXPathPopFrame around in\nxmlXPathCompOpEvalPositionalPredicate to make sure that the context\nobject on the stack is actually protected. Otherwise, memory corruption\ncan occur when calling sloppily coded XPath extension functions.\n\nFixes bug 783160.", "target": 1, "dataset": "other", "idx": 206698}
{"func": "\t\t    ctxt->error = XPATH_MEMORY_ERROR;\n\t\t    goto evaluation_exit;\n\t\t}\n\t    }\n\n\t    valuePush(ctxt, contextObj);\n            frame = xmlXPathSetFrame(ctxt);\n\t    res = xmlXPathCompOpEvalToBoolean(ctxt, exprOp, 1);\n            xmlXPathPopFrame(ctxt, frame);\n            tmp = valuePop(ctxt);\n\n\t    if ((ctxt->error != XPATH_EXPRESSION_OK) || (res == -1)) {\n                while (tmp != contextObj) {\n                    /*\n                     * Free up the result", "project": "libxml2", "hash": 87079765018061420144174234477625527081, "size": 198, "commit_id": "0f3b843b3534784ef57a4f9b874238aa1fda5a73", "message": "Fix XPath stack frame logic\n\nMove the calls to xmlXPathSetFrame and xmlXPathPopFrame around in\nxmlXPathCompOpEvalPositionalPredicate to make sure that the context\nobject on the stack is actually protected. Otherwise, memory corruption\ncan occur when calling sloppily coded XPath extension functions.\n\nFixes bug 783160.", "target": 0, "dataset": "other", "idx": 385345}
{"func": "void Utf8DecoderBase::WriteUtf16Slow(const uint8_t* stream,\n                                     uint16_t* data,\n                                     unsigned data_length) {\n  while (data_length != 0) {\n    unsigned cursor = 0;\n    uint32_t character = Utf8::ValueOf(stream, Utf8::kMaxEncodedSize, &cursor);\n    // There's a total lack of bounds checking for stream\n    // as it was already done in Reset.\n    stream += cursor;\n    if (character > unibrow::Utf16::kMaxNonSurrogateCharCode) {\n      *data++ = Utf16::LeadSurrogate(character);\n      *data++ = Utf16::TrailSurrogate(character);\n      DCHECK(data_length > 1);\n      data_length -= 2;\n    } else {\n      *data++ = character;\n      data_length -= 1;\n    }\n  }\n}", "project": "node", "hash": 252347364978872387181975652701666279871, "size": 20, "commit_id": "78b0e30954111cfaba0edbeee85450d8cbc6fdf6", "message": "deps: fix out-of-band write in utf8 decoder\n\nOriginally reported by: Kris Reeves <kris.re@bbhmedia.com>\n\nReviewed-By: Trevor Norris <trev.norris@gmail.com>", "target": 1, "dataset": "other", "idx": 206709}
{"func": "void Utf8DecoderBase::WriteUtf16Slow(const uint8_t* stream,\n                                     unsigned stream_length,\n                                     uint16_t* data,\n                                     unsigned data_length) {\n  while (data_length != 0) {\n    unsigned cursor = 0;\n\n    uint32_t character = Utf8::ValueOf(stream, stream_length, &cursor);\n    // There's a total lack of bounds checking for stream\n    // as it was already done in Reset.\n    stream += cursor;\n    stream_length -= cursor;\n    if (character > unibrow::Utf16::kMaxNonSurrogateCharCode) {\n      *data++ = Utf16::LeadSurrogate(character);\n      *data++ = Utf16::TrailSurrogate(character);\n      DCHECK(data_length > 1);\n      data_length -= 2;\n    } else {\n      *data++ = character;\n      data_length -= 1;\n    }\n  }\n  DCHECK(stream_length >= 0);\n}", "project": "node", "hash": 108599142078622494128331469487001444691, "size": 24, "commit_id": "78b0e30954111cfaba0edbeee85450d8cbc6fdf6", "message": "deps: fix out-of-band write in utf8 decoder\n\nOriginally reported by: Kris Reeves <kris.re@bbhmedia.com>\n\nReviewed-By: Trevor Norris <trev.norris@gmail.com>", "target": 0, "dataset": "other", "idx": 385616}
{"func": "      wave_image=DestroyImage(wave_image);\n      ThrowImageException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  for (i=0; i < (ssize_t) wave_image->columns; i++)\n    sine_map[i]=(float) fabs(amplitude)+amplitude*sin((double)\n      ((2.0*MagickPI*i)/wave_length));\n  /*\n    Wave image.\n  */\n  status=MagickTrue;\n  progress=0;", "project": "ImageMagick6", "hash": 151042824862097823118767733345259035764, "size": 136, "commit_id": "f1e68d22d1b35459421710587a0dcbab6900b51f", "message": "https://github.com/ImageMagick/ImageMagick/issues/3296", "target": 1, "dataset": "other", "idx": 206717}
{"func": "      wave_image=DestroyImage(wave_image);\n      ThrowImageException(ResourceLimitError,\"MemoryAllocationFailed\");\n    }\n  for (i=0; i < (ssize_t) wave_image->columns; i++)\n    sine_map[i]=(float) fabs(amplitude)+amplitude*sin((double)\n      ((2.0*MagickPI*i)*PerceptibleReciprocal(wave_length)));\n  /*\n    Wave image.\n  */\n  status=MagickTrue;\n  progress=0;", "project": "ImageMagick6", "hash": 101226025493037855922393203126658342941, "size": 136, "commit_id": "f1e68d22d1b35459421710587a0dcbab6900b51f", "message": "https://github.com/ImageMagick/ImageMagick/issues/3296", "target": 0, "dataset": "other", "idx": 385763}
{"func": "\t\tb = rtnl_dereference(head->table[th]);\n\t\tif (b) {\n\t\t\tfp = &b->ht[h];\n\t\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {\n\t\t\t\tif (pfp == f) {\n\t\t\t\t\t*fp = f->next;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "project": "linux", "hash": 231199128984425325302576009574271570955, "size": 95, "commit_id": "ef299cc3fa1a9e1288665a9fdc8bff55629fd359", "message": "net_sched: cls_route: remove the right filter from hashtable\n\nroute4_change() allocates a new filter and copies values from\nthe old one. After the new filter is inserted into the hash\ntable, the old filter should be removed and freed, as the final\nstep of the update.\n\nHowever, the current code mistakenly removes the new one. This\nlooks apparently wrong to me, and it causes double \"free\" and\nuse-after-free too, as reported by syzbot.\n\nReported-and-tested-by: syzbot+f9b32aaacd60305d9687@syzkaller.appspotmail.com\nReported-and-tested-by: syzbot+2f8c233f131943d6056d@syzkaller.appspotmail.com\nReported-and-tested-by: syzbot+9c2df9fd5e9445b74e01@syzkaller.appspotmail.com\nFixes: 1109c00547fc (\"net: sched: RCU cls_route\")\nCc: Jamal Hadi Salim <jhs@mojatatu.com>\nCc: Jiri Pirko <jiri@resnulli.us>\nCc: John Fastabend <john.fastabend@gmail.com>\nSigned-off-by: Cong Wang <xiyou.wangcong@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 206718}
{"func": "\t\tb = rtnl_dereference(head->table[th]);\n\t\tif (b) {\n\t\t\tfp = &b->ht[h];\n\t\t\tfor (pfp = rtnl_dereference(*fp); pfp;\n\t\t\t     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {\n\t\t\t\tif (pfp == fold) {\n\t\t\t\t\trcu_assign_pointer(*fp, fold->next);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}", "project": "linux", "hash": 37265442732800560811121149832465762697, "size": 95, "commit_id": "ef299cc3fa1a9e1288665a9fdc8bff55629fd359", "message": "net_sched: cls_route: remove the right filter from hashtable\n\nroute4_change() allocates a new filter and copies values from\nthe old one. After the new filter is inserted into the hash\ntable, the old filter should be removed and freed, as the final\nstep of the update.\n\nHowever, the current code mistakenly removes the new one. This\nlooks apparently wrong to me, and it causes double \"free\" and\nuse-after-free too, as reported by syzbot.\n\nReported-and-tested-by: syzbot+f9b32aaacd60305d9687@syzkaller.appspotmail.com\nReported-and-tested-by: syzbot+2f8c233f131943d6056d@syzkaller.appspotmail.com\nReported-and-tested-by: syzbot+9c2df9fd5e9445b74e01@syzkaller.appspotmail.com\nFixes: 1109c00547fc (\"net: sched: RCU cls_route\")\nCc: Jamal Hadi Salim <jhs@mojatatu.com>\nCc: Jiri Pirko <jiri@resnulli.us>\nCc: John Fastabend <john.fastabend@gmail.com>\nSigned-off-by: Cong Wang <xiyou.wangcong@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 385783}
{"func": "\n\tTSRMLS_FETCH();\n\n\tif ((map = CAST(struct magic_map *, ecalloc(1, sizeof(*map)))) == NULL) {\n\t\tfile_oomem(ms, sizeof(*map));\n\t\tefree(map);\n\t\tgoto error;\n\t}\n\n\tif (fn == NULL) {\n\t\tmap->p = (void *)&php_magic_database;\n\t\tgoto internal_loaded;", "project": "php-src", "hash": 251164010590167534593186953249875788485, "size": 149, "commit_id": "91aa340180eccfc15d4a143b54d47b8120f898be", "message": "Fixed bug #68827 Double free with disabled ZMM", "target": 1, "dataset": "other", "idx": 206725}
{"func": "\n\tTSRMLS_FETCH();\n\n\tif ((map = CAST(struct magic_map *, ecalloc(1, sizeof(*map)))) == NULL) {\n\t\tfile_oomem(ms, sizeof(*map));\n\t\treturn NULL;\n\t}\n\n\tif (fn == NULL) {\n\t\tmap->p = (void *)&php_magic_database;\n\t\tgoto internal_loaded;", "project": "php-src", "hash": 229425420476248340719725960925505400726, "size": 148, "commit_id": "91aa340180eccfc15d4a143b54d47b8120f898be", "message": "Fixed bug #68827 Double free with disabled ZMM", "target": 0, "dataset": "other", "idx": 385969}
{"func": "\t\t\t\thost->req->error = -ENOMEDIUM;\n\t\t} while (!err);\n\t}\n\tmutex_unlock(&host->host_mutex);\n\n\tmemstick_remove_host(msh);\n\tmemstick_free_host(msh);\n\n\t/* Balance possible unbalanced usage count\n\t * e.g. unconditional module removal\n\t */\n\tif (pm_runtime_active(ms_dev(host)))\n\t\tpm_runtime_put(ms_dev(host));\n\n\tpm_runtime_disable(ms_dev(host));\n\tplatform_set_drvdata(pdev, NULL);\n\n\tdev_dbg(ms_dev(host),\n\t\t\": Realtek USB Memstick controller has been removed\\n\");\n\n\treturn 0;\n}", "project": "linux", "hash": 251538097616710951131117275306805038352, "size": 40, "commit_id": "42933c8aa14be1caa9eda41f65cde8a3a95d3e39", "message": "memstick: rtsx_usb_ms: fix UAF\n\nThis patch fixes the following issues:\n1. memstick_free_host() will free the host, so the use of ms_dev(host) after\nit will be a problem. To fix this, move memstick_free_host() after when we\nare done with ms_dev(host).\n2. In rtsx_usb_ms_drv_remove(), pm need to be disabled before we remove\nand free host otherwise memstick_check will be called and UAF will\nhappen.\n\n[   11.351173] BUG: KASAN: use-after-free in rtsx_usb_ms_drv_remove+0x94/0x140 [rtsx_usb_ms]\n[   11.357077]  rtsx_usb_ms_drv_remove+0x94/0x140 [rtsx_usb_ms]\n[   11.357376]  platform_remove+0x2a/0x50\n[   11.367531] Freed by task 298:\n[   11.368537]  kfree+0xa4/0x2a0\n[   11.368711]  device_release+0x51/0xe0\n[   11.368905]  kobject_put+0xa2/0x120\n[   11.369090]  rtsx_usb_ms_drv_remove+0x8c/0x140 [rtsx_usb_ms]\n[   11.369386]  platform_remove+0x2a/0x50\n\n[   12.038408] BUG: KASAN: use-after-free in __mutex_lock.isra.0+0x3ec/0x7c0\n[   12.045432]  mutex_lock+0xc9/0xd0\n[   12.046080]  memstick_check+0x6a/0x578 [memstick]\n[   12.046509]  process_one_work+0x46d/0x750\n[   12.052107] Freed by task 297:\n[   12.053115]  kfree+0xa4/0x2a0\n[   12.053272]  device_release+0x51/0xe0\n[   12.053463]  kobject_put+0xa2/0x120\n[   12.053647]  rtsx_usb_ms_drv_remove+0xc4/0x140 [rtsx_usb_ms]\n[   12.053939]  platform_remove+0x2a/0x50\n\nSigned-off-by: Tong Zhang <ztong0001@gmail.com>\nCo-developed-by: Ulf Hansson <ulf.hansson@linaro.org>\nLink: https://lore.kernel.org/r/20210511163944.1233295-1-ztong0001@gmail.com\nSigned-off-by: Ulf Hansson <ulf.hansson@linaro.org>", "target": 1, "dataset": "other", "idx": 206735}
{"func": "\t\t\tif (!err)\n\t\t\t\thost->req->error = -ENOMEDIUM;\n\t\t} while (!err);\n\t}\n\tmutex_unlock(&host->host_mutex);\n\n\t/* Balance possible unbalanced usage count\n\t * e.g. unconditional module removal\n\t */\n\tif (pm_runtime_active(ms_dev(host)))\n\t\tpm_runtime_put(ms_dev(host));\n\n\tpm_runtime_disable(ms_dev(host));\n\tmemstick_remove_host(msh);\n\tdev_dbg(ms_dev(host),\n\t\t\": Realtek USB Memstick controller has been removed\\n\");\n\tmemstick_free_host(msh);\n\tplatform_set_drvdata(pdev, NULL);\n\n\treturn 0;\n}", "project": "linux", "hash": 221647974417543889059882407549025776248, "size": 38, "commit_id": "42933c8aa14be1caa9eda41f65cde8a3a95d3e39", "message": "memstick: rtsx_usb_ms: fix UAF\n\nThis patch fixes the following issues:\n1. memstick_free_host() will free the host, so the use of ms_dev(host) after\nit will be a problem. To fix this, move memstick_free_host() after when we\nare done with ms_dev(host).\n2. In rtsx_usb_ms_drv_remove(), pm need to be disabled before we remove\nand free host otherwise memstick_check will be called and UAF will\nhappen.\n\n[   11.351173] BUG: KASAN: use-after-free in rtsx_usb_ms_drv_remove+0x94/0x140 [rtsx_usb_ms]\n[   11.357077]  rtsx_usb_ms_drv_remove+0x94/0x140 [rtsx_usb_ms]\n[   11.357376]  platform_remove+0x2a/0x50\n[   11.367531] Freed by task 298:\n[   11.368537]  kfree+0xa4/0x2a0\n[   11.368711]  device_release+0x51/0xe0\n[   11.368905]  kobject_put+0xa2/0x120\n[   11.369090]  rtsx_usb_ms_drv_remove+0x8c/0x140 [rtsx_usb_ms]\n[   11.369386]  platform_remove+0x2a/0x50\n\n[   12.038408] BUG: KASAN: use-after-free in __mutex_lock.isra.0+0x3ec/0x7c0\n[   12.045432]  mutex_lock+0xc9/0xd0\n[   12.046080]  memstick_check+0x6a/0x578 [memstick]\n[   12.046509]  process_one_work+0x46d/0x750\n[   12.052107] Freed by task 297:\n[   12.053115]  kfree+0xa4/0x2a0\n[   12.053272]  device_release+0x51/0xe0\n[   12.053463]  kobject_put+0xa2/0x120\n[   12.053647]  rtsx_usb_ms_drv_remove+0xc4/0x140 [rtsx_usb_ms]\n[   12.053939]  platform_remove+0x2a/0x50\n\nSigned-off-by: Tong Zhang <ztong0001@gmail.com>\nCo-developed-by: Ulf Hansson <ulf.hansson@linaro.org>\nLink: https://lore.kernel.org/r/20210511163944.1233295-1-ztong0001@gmail.com\nSigned-off-by: Ulf Hansson <ulf.hansson@linaro.org>", "target": 0, "dataset": "other", "idx": 386074}
{"func": "            but lets leave it there for the moment to avoid\n            changing output.  */\n        if (line_context->lc_file_entry_count > 9) {\n            dwarfstring_append_printf_u(&m3,\n                \"  file[%2u] \",fiu);\n            dwarfstring_append_printf_s(&m3,\n                \"%-20s \",\n                (char *) fe->fi_file_name);\n            dwarfstring_append_printf_u(&m3,\n                \"(file-number: %u)\\n\",\n                filenum);\n        } else {\n            dwarfstring_append_printf_u(&m3,\n                \"  file[%u]  \", fiu);\n            dwarfstring_append_printf_s(&m3,\n                \"%-20s \",(char *)fe->fi_file_name);\n            dwarfstring_append_printf_u(&m3,\n                \"(file-number: %u)\\n\",filenum);\n        }\n        _dwarf_printf(dbg,dwarfstring_string(&m3));\n        dwarfstring_reset(&m3);\n        if (fe->fi_dir_index_present) {\n            Dwarf_Unsigned di = 0;\n            di = fe->fi_dir_index;", "project": "libdwarf-code", "hash": 305047332333399787689973899073262959948, "size": 86, "commit_id": "faf99408e3f9f706fc3809dd400e831f989778d3", "message": "modified:   libdwarf/dwarf_print_lines.c\n    * dwarf_print_lines.c: In case of corrupted\n      DWARF5 line header the fi_file_name field\n      for a file entry can be null. Now\n      we print a <no file name> string in that case\n      to avoid passing a null to dwarfstring_append.\n      Dwarfbug DW202010-003.\n      Also some lines longer than libdwarf standard\n      were shortened, but a few long lines really\n      must remain.", "target": 1, "dataset": "other", "idx": 206765}
{"func": "        /*  The space character at the end of line is silly,\n            but lets leave it there for the moment to avoid\n            changing output.  */\n        if (line_context->lc_file_entry_count > 9) {\n            dwarfstring_append_printf_u(&m3,\n                \"  file[%2u] \",fiu);\n        } else {\n            dwarfstring_append_printf_u(&m3,\n                \"  file[%u]  \", fiu);\n        }\n        /*  DWARF5 can have a null fi_file_name\n            if  the format code in the\n            line table header is unknown, such\n            as in a corrupt object file. */\n        dwarfstring_append_printf_s(&m3,\n            \"%-20s \",\n            fe->fi_file_name?\n            (char *) fe->fi_file_name:\n            \"<no file name>\");\n        dwarfstring_append_printf_u(&m3,\n            \"(file-number: %u)\\n\",\n            filenum);\n        _dwarf_printf(dbg,dwarfstring_string(&m3));\n        dwarfstring_reset(&m3);\n        if (fe->fi_dir_index_present) {\n            Dwarf_Unsigned di = 0;\n            di = fe->fi_dir_index;", "project": "libdwarf-code", "hash": 164570038693462174492969033994984555291, "size": 88, "commit_id": "faf99408e3f9f706fc3809dd400e831f989778d3", "message": "modified:   libdwarf/dwarf_print_lines.c\n    * dwarf_print_lines.c: In case of corrupted\n      DWARF5 line header the fi_file_name field\n      for a file entry can be null. Now\n      we print a <no file name> string in that case\n      to avoid passing a null to dwarfstring_append.\n      Dwarfbug DW202010-003.\n      Also some lines longer than libdwarf standard\n      were shortened, but a few long lines really\n      must remain.", "target": 0, "dataset": "other", "idx": 386452}
{"func": "static ssize_t acpi_table_aml_write(struct config_item *cfg,\n\t\t\t\t    const void *data, size_t size)\n{\n\tconst struct acpi_table_header *header = data;\n\tstruct acpi_table *table;\n\tint ret;\n\n\ttable = container_of(cfg, struct acpi_table, cfg);\n\n\tif (table->header) {\n\t\tpr_err(\"table already loaded\\n\");", "project": "linux", "hash": 178950288749024718434922419399845158436, "size": 38, "commit_id": "75b0cea7bf307f362057cc778efe89af4c615354", "message": "ACPI: configfs: Disallow loading ACPI tables when locked down\n\nLike other vectors already patched, this one here allows the root\nuser to load ACPI tables, which enables arbitrary physical address\nwrites, which in turn makes it possible to disable lockdown.\n\nPrevents this by checking the lockdown status before allowing a new\nACPI table to be installed. The link in the trailer shows a PoC of\nhow this might be used.\n\nLink: https://git.zx2c4.com/american-unsigned-language/tree/american-unsigned-language-2.sh\nCc: 5.4+ <stable@vger.kernel.org> # 5.4+\nSigned-off-by: Jason A. Donenfeld <Jason@zx2c4.com>\nSigned-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>", "target": 1, "dataset": "other", "idx": 206866}
{"func": "static ssize_t acpi_table_aml_write(struct config_item *cfg,\n\t\t\t\t    const void *data, size_t size)\n{\n\tconst struct acpi_table_header *header = data;\n\tstruct acpi_table *table;\n\tint ret = security_locked_down(LOCKDOWN_ACPI_TABLES);\n\n\tif (ret)\n\t\treturn ret;\n\n\ttable = container_of(cfg, struct acpi_table, cfg);\n\n\tif (table->header) {\n\t\tpr_err(\"table already loaded\\n\");", "project": "linux", "hash": 23089143507537986751432919005044809927, "size": 41, "commit_id": "75b0cea7bf307f362057cc778efe89af4c615354", "message": "ACPI: configfs: Disallow loading ACPI tables when locked down\n\nLike other vectors already patched, this one here allows the root\nuser to load ACPI tables, which enables arbitrary physical address\nwrites, which in turn makes it possible to disable lockdown.\n\nPrevents this by checking the lockdown status before allowing a new\nACPI table to be installed. The link in the trailer shows a PoC of\nhow this might be used.\n\nLink: https://git.zx2c4.com/american-unsigned-language/tree/american-unsigned-language-2.sh\nCc: 5.4+ <stable@vger.kernel.org> # 5.4+\nSigned-off-by: Jason A. Donenfeld <Jason@zx2c4.com>\nSigned-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>", "target": 0, "dataset": "other", "idx": 387888}
{"func": "    } else {\n        read_config(\"/etc/bwm-ng.conf\");\n#ifdef HAVE_GETPWUID    \n        pwd_entry=getpwuid(getuid());\n        if (pwd_entry!=NULL) {\n            str=(char*)malloc(strlen(pwd_entry->pw_dir)+14);\n            snprintf(str,strlen(pwd_entry->pw_dir)+14,\"%s/.bwm-ng.conf\",pwd_entry->pw_dir);\n            read_config(str);\n            free(str);\n        }\n#endif    ", "project": "bwm-ng", "hash": 190610676223568420413623069094924895723, "size": 227, "commit_id": "9774f23bf78a6e6d3ae4cfe3d73bad34f2fdcd17", "message": "Fix https://github.com/vgropp/bwm-ng/issues/26", "target": 1, "dataset": "other", "idx": 206867}
{"func": "        read_config(\"/etc/bwm-ng.conf\");\n#ifdef HAVE_GETPWUID    \n        pwd_entry=getpwuid(getuid());\n        if (pwd_entry!=NULL) {\n            str=(char*)malloc(strlen(pwd_entry->pw_dir)+14);\n            if(!str) {\n              printf(\"Fatal: failed to allocate %zu bytes.\\n\", strlen(pwd_entry->pw_dir)+14);\n              exit(EXIT_FAILURE);\n            }\n            snprintf(str,strlen(pwd_entry->pw_dir)+14,\"%s/.bwm-ng.conf\",pwd_entry->pw_dir);\n            read_config(str);\n            free(str);\n        }\n#endif    ", "project": "bwm-ng", "hash": 193898408646396083666542746077666107845, "size": 231, "commit_id": "9774f23bf78a6e6d3ae4cfe3d73bad34f2fdcd17", "message": "Fix https://github.com/vgropp/bwm-ng/issues/26", "target": 0, "dataset": "other", "idx": 387905}
{"func": "      */\n      width=(number_parameters >= 1) ? parameters[0] : 1.0;\n      center=(number_parameters >= 2) ? parameters[1] : 0.5;\n      range=(number_parameters >= 3) ? parameters[2] : 1.0;\n      bias=(number_parameters >= 4) ? parameters[3] : 0.5;\n      result=2.0/width*(QuantumScale*pixel-center);\n      if ( result <= -1.0 )\n        result=bias-range/2.0;\n      else\n        if (result >= 1.0)\n          result=bias+range/2.0;\n        else", "project": "ImageMagick", "hash": 144868375147363456336191166820888674353, "size": 97, "commit_id": "4717744e4bb27de8ea978e51c6d5bcddf62ffe49", "message": "https://github.com/ImageMagick/ImageMagick/issues/3332", "target": 1, "dataset": "other", "idx": 206873}
{"func": "      */\n      width=(number_parameters >= 1) ? parameters[0] : 1.0;\n      center=(number_parameters >= 2) ? parameters[1] : 0.5;\n      range=(number_parameters >= 3) ? parameters[2] : 1.0;\n      bias=(number_parameters >= 4) ? parameters[3] : 0.5;\n      result=2.0*PerceptibleReciprocal(width)*(QuantumScale*pixel-center);\n      if (result <= -1.0)\n        result=bias-range/2.0;\n      else\n        if (result >= 1.0)\n          result=bias+range/2.0;\n        else", "project": "ImageMagick", "hash": 67328351938312049905670604818711921794, "size": 97, "commit_id": "4717744e4bb27de8ea978e51c6d5bcddf62ffe49", "message": "https://github.com/ImageMagick/ImageMagick/issues/3332", "target": 0, "dataset": "other", "idx": 388298}
{"func": "\tsize_t position;\n\tBOOL asciiNames;\n\tint formatNameLength;\n\tchar* szFormatName;\n\tWCHAR* wszFormatName;\n\tUINT32 dataLen = formatList->dataLen;\n\tCLIPRDR_FORMAT* formats = NULL;\n\tUINT error = CHANNEL_RC_OK;\n\n\tasciiNames = (formatList->msgFlags & CB_ASCII_NAMES) ? TRUE : FALSE;\n\n\tindex = 0;\n\tformatList->numFormats = 0;\n\tposition = Stream_GetPosition(s);\n\n\tif (!formatList->dataLen)\n\t{\n\t\t/* empty format list */\n\t\tformatList->formats = NULL;\n\t\tformatList->numFormats = 0;\n\t}\n\telse if (!useLongFormatNames)\n\t{\n\t\tformatList->numFormats = (dataLen / 36);\n\n\t\tif ((formatList->numFormats * 36) != dataLen)\n\t\t{\n\t\t\tWLog_ERR(TAG, \"Invalid short format list length: %\" PRIu32 \"\", dataLen);\n\t\t\treturn ERROR_INTERNAL_ERROR;\n\t\t}\n\n\t\tif (formatList->numFormats)\n\t\t\tformats = (CLIPRDR_FORMAT*)calloc(formatList->numFormats, sizeof(CLIPRDR_FORMAT));\n\t\t\treturn CHANNEL_RC_NO_MEMORY;\n\t\t}\n\n\t\tformatList->formats = formats;\n\n\t\twhile (dataLen)\n\t\t{\n\t\t\tStream_Read_UINT32(s, formats[index].formatId); /* formatId (4 bytes) */\n\t\t\tdataLen -= 4;\n\n\t\t\tformats[index].formatName = NULL;\n\n\t\t\t/* According to MS-RDPECLIP 2.2.3.1.1.1 formatName is \"a 32-byte block containing\n\t\t\t * the *null-terminated* name assigned to the Clipboard Format: (32 ASCII 8 characters\n\t\t\t * or 16 Unicode characters)\"\n\t\t\t * However, both Windows RDSH and mstsc violate this specs as seen in the following\n\t\t\t * example of a transferred short format name string: [R.i.c.h. .T.e.x.t. .F.o.r.m.a.t.]\n\t\t\t * These are 16 unicode charaters - *without* terminating null !\n\t\t\t */\n\n\t\t\tif (asciiNames)\n\t\t\t{\n\t\t\t\tszFormatName = (char*)Stream_Pointer(s);\n\n\t\t\t\tif (szFormatName[0])\n\t\t\t\t{\n\t\t\t\t\t/* ensure null termination */\n\t\t\t\t\tformats[index].formatName = (char*)malloc(32 + 1);\n\t\t\t\t\tif (!formats[index].formatName)\n\t\t\t\t\tformats[index].formatName[32] = '\\0';\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\twszFormatName = (WCHAR*)Stream_Pointer(s);\n\n\t\t\t\tif (wszFormatName[0])\n\t\t\t\t{\n\t\t\t\t\t/* ConvertFromUnicode always returns a null-terminated\n\t\t\t\t\t * string on success, even if the source string isn't.\n\t\t\t\t\t */\n\t\t\t\t\t\tgoto error_out;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tStream_Seek(s, 32);\n\t\t\tdataLen -= 32;\n\t\t\tindex++;\n\t\t}\n\t}\n\telse\n\t{\n\t\twhile (dataLen)\n\t\t{\n\t\t\tStream_Seek(s, 4); /* formatId (4 bytes) */\n\t\t\tdataLen -= 4;\n\n\t\t\twszFormatName = (WCHAR*)Stream_Pointer(s);\n\n\t\t\tif (!wszFormatName[0])\n\t\t\t\tformatNameLength = 0;\n\t\t\telse\n\t\t\t\tformatNameLength = _wcslen(wszFormatName);\n\n\t\t\tStream_Seek(s, (formatNameLength + 1) * 2);\n\t\t\tdataLen -= ((formatNameLength + 1) * 2);\n\n\t\t\tformatList->numFormats++;\n\t\t}\n\n\t\tdataLen = formatList->dataLen;\n\t\tStream_SetPosition(s, position);\n\n\t\tif (formatList->numFormats)\n\t\t\tformats = (CLIPRDR_FORMAT*)calloc(formatList->numFormats, sizeof(CLIPRDR_FORMAT));\n\n\t\tif (!formats)\n\t\t\treturn CHANNEL_RC_NO_MEMORY;\n\t\t}\n\n\t\tformatList->formats = formats;\n\n\t\twhile (dataLen)\n\t\t{\n\t\t\tStream_Read_UINT32(s, formats[index].formatId); /* formatId (4 bytes) */\n\t\t\tdataLen -= 4;\n\n\t\t\tformats[index].formatName = NULL;\n\n\t\t\twszFormatName = (WCHAR*)Stream_Pointer(s);\n\n\t\t\tif (!wszFormatName[0])\n\t\t\t\tformatNameLength = 0;\n\t\t\telse\n\t\t\t\tformatNameLength = _wcslen(wszFormatName);\n\n\t\t\tif (formatNameLength)\n\t\t\t{\n\t\t\t\tif (ConvertFromUnicode(CP_UTF8, 0, wszFormatName, -1, &(formats[index].formatName),\n\t\t\t\t                       0, NULL, NULL) < 1)\n\t\t\t\t{\n\t\t\t\t\tWLog_ERR(TAG, \"failed to convert long clipboard format name\");\n\t\t\t\t\terror = ERROR_INTERNAL_ERROR;\n\t\t\t\t\tgoto error_out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tStream_Seek(s, (formatNameLength + 1) * 2);\n\t\t\tdataLen -= ((formatNameLength + 1) * 2);\n\n\t\t\tindex++;\n\t\t}\n\t}\n", "project": "FreeRDP", "hash": 291591512452233208574467037688769410743, "size": 174, "commit_id": "b73143cf7ee5fe4cdabcbf56908aa15d8a883821", "message": "Fixed oob read in cliprdr_read_format_list", "target": 1, "dataset": "other", "idx": 206874}
{"func": "\tsize_t position;\n\tBOOL asciiNames;\n\tint formatNameLength;\n\tchar* szFormatName;\n\tWCHAR* wszFormatName;\n\twStream sub1, sub2;\n\tCLIPRDR_FORMAT* formats = NULL;\n\tUINT error = CHANNEL_RC_OK;\n\n\tasciiNames = (formatList->msgFlags & CB_ASCII_NAMES) ? TRUE : FALSE;\n\n\tindex = 0;\n\t/* empty format list */\n\tformatList->formats = NULL;\n\tformatList->numFormats = 0;\n\n\tStream_StaticInit(&sub1, Stream_Pointer(s), formatList->dataLen);\n\tif (!Stream_SafeSeek(s, formatList->dataLen))\n\t\treturn ERROR_INVALID_DATA;\n\n\tif (!formatList->dataLen)\n\t{\n\t}\n\telse if (!useLongFormatNames)\n\t{\n\t\tconst size_t cap = Stream_Capacity(&sub1);\n\t\tformatList->numFormats = (cap / 36);\n\n\t\tif ((formatList->numFormats * 36) != cap)\n\t\t{\n\t\t\tWLog_ERR(TAG, \"Invalid short format list length: %\" PRIuz \"\", cap);\n\t\t\treturn ERROR_INTERNAL_ERROR;\n\t\t}\n\n\t\tif (formatList->numFormats)\n\t\t\tformats = (CLIPRDR_FORMAT*)calloc(formatList->numFormats, sizeof(CLIPRDR_FORMAT));\n\t\t\treturn CHANNEL_RC_NO_MEMORY;\n\t\t}\n\n\t\tformatList->formats = formats;\n\n\t\twhile (Stream_GetRemainingLength(&sub1) >= 4)\n\t\t{\n\t\t\tStream_Read_UINT32(&sub1, formats[index].formatId); /* formatId (4 bytes) */\n\n\t\t\tformats[index].formatName = NULL;\n\n\t\t\t/* According to MS-RDPECLIP 2.2.3.1.1.1 formatName is \"a 32-byte block containing\n\t\t\t * the *null-terminated* name assigned to the Clipboard Format: (32 ASCII 8 characters\n\t\t\t * However, both Windows RDSH and mstsc violate this specs as seen in the following\n\t\t\t * example of a transferred short format name string: [R.i.c.h. .T.e.x.t. .F.o.r.m.a.t.]\n\t\t\t * These are 16 unicode charaters - *without* terminating null !\n\t\t\t */\n\n\t\t\tszFormatName = (char*)Stream_Pointer(&sub1);\n\t\t\twszFormatName = (WCHAR*)Stream_Pointer(&sub1);\n\t\t\tif (!Stream_SafeSeek(&sub1, 32))\n\t\t\t\tgoto error_out;\n\t\t\tif (asciiNames)\n\t\t\t{\n\t\t\t\tif (szFormatName[0])\n\t\t\t\t{\n\t\t\t\t\t/* ensure null termination */\n\t\t\t\t\tformats[index].formatName = (char*)malloc(32 + 1);\n\t\t\t\t\tif (!formats[index].formatName)\n\t\t\t\t\tCopyMemory(formats[index].formatName, szFormatName, 32);\n\t\t\t\t\tformats[index].formatName[32] = '\\0';\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tif (wszFormatName[0])\n\t\t\t\t{\n\t\t\t\t\t/* ConvertFromUnicode always returns a null-terminated\n\t\t\t\t\t * string on success, even if the source string isn't.\n\t\t\t\t\t */\n\t\t\t\t\t\terror = ERROR_INTERNAL_ERROR;\n\t\t\t\t\t\tgoto error_out;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tindex++;\n\t\t}\n\t}\n\telse\n\t{\n\t\tsub2 = sub1;\n\t\twhile (Stream_GetRemainingLength(&sub1) > 0)\n\t\t{\n\t\t\tsize_t rest;\n\t\t\tif (!Stream_SafeSeek(&sub1, 4)) /* formatId (4 bytes) */\n\t\t\t\tgoto error_out;\n\n\t\t\twszFormatName = (WCHAR*)Stream_Pointer(&sub1);\n\t\t\trest = Stream_GetRemainingLength(&sub1);\n\t\t\tformatNameLength = _wcsnlen(wszFormatName, rest / sizeof(WCHAR));\n\n\t\t\tif (!Stream_SafeSeek(&sub1, (formatNameLength + 1) * sizeof(WCHAR)))\n\t\t\t\tgoto error_out;\n\t\t\tformatList->numFormats++;\n\t\t}\n\n\t\tif (formatList->numFormats)\n\t\t\tformats = (CLIPRDR_FORMAT*)calloc(formatList->numFormats, sizeof(CLIPRDR_FORMAT));\n\n\t\tif (!formats)\n\t\t\treturn CHANNEL_RC_NO_MEMORY;\n\t\t}\n\n\t\tformatList->formats = formats;\n\n\t\twhile (Stream_GetRemainingLength(&sub2) >= 4)\n\t\t{\n\t\t\tsize_t rest;\n\t\t\tStream_Read_UINT32(&sub2, formats[index].formatId); /* formatId (4 bytes) */\n\n\t\t\tformats[index].formatName = NULL;\n\n\t\t\twszFormatName = (WCHAR*)Stream_Pointer(&sub2);\n\t\t\trest = Stream_GetRemainingLength(&sub2);\n\t\t\tformatNameLength = _wcsnlen(wszFormatName, rest / sizeof(WCHAR));\n\t\t\tif (!Stream_SafeSeek(&sub2, (formatNameLength + 1) * sizeof(WCHAR)))\n\t\t\t\tgoto error_out;\n\n\t\t\tif (formatNameLength)\n\t\t\t{\n\t\t\t\tif (ConvertFromUnicode(CP_UTF8, 0, wszFormatName, formatNameLength,\n\t\t\t\t                       &(formats[index].formatName), 0, NULL, NULL) < 1)\n\t\t\t\t{\n\t\t\t\t\tWLog_ERR(TAG, \"failed to convert long clipboard format name\");\n\t\t\t\t\terror = ERROR_INTERNAL_ERROR;\n\t\t\t\t\tgoto error_out;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tindex++;\n\t\t}\n\t}\n", "project": "FreeRDP", "hash": 237858437120074239124496021317037679825, "size": 165, "commit_id": "b73143cf7ee5fe4cdabcbf56908aa15d8a883821", "message": "Fixed oob read in cliprdr_read_format_list", "target": 0, "dataset": "other", "idx": 388319}
{"func": "static int identity_count(void *v, const char *key, const char *val)\n{\n    int *count = v;\n    *count += strlen(key) * 3 + strlen(val) * 3 + 1;\n    return 1;\n}", "project": "httpd", "hash": 50330208498269000547705518476088709988, "size": 6, "commit_id": "7e09dd714fc62c08c5b0319ed7b9702594faf49b", "message": "mod_session: account for the '&' in identity_concat().\n\ngit-svn-id: https://svn.apache.org/repos/asf/httpd/httpd/trunk@1887052 13f79535-47bb-0310-9956-ffa450edef68", "target": 1, "dataset": "other", "idx": 206917}
{"func": "static int identity_count(void *v, const char *key, const char *val)\n{\n    int *count = v;\n    *count += strlen(key) * 3 + strlen(val) * 3 + 2;\n    return 1;\n}", "project": "httpd", "hash": 243767006619414530688134707731592870569, "size": 6, "commit_id": "7e09dd714fc62c08c5b0319ed7b9702594faf49b", "message": "mod_session: account for the '&' in identity_concat().\n\ngit-svn-id: https://svn.apache.org/repos/asf/httpd/httpd/trunk@1887052 13f79535-47bb-0310-9956-ffa450edef68", "target": 0, "dataset": "other", "idx": 389228}
{"func": "      (void) remove_utf8(clone_info->filename);\n    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "project": "ImageMagick", "hash": 213822569694002162486182145935420282110, "size": 512, "commit_id": "a6802e21d824e786d1e2a8440cf749a6e1a8d95f", "message": "https://github.com/ImageMagick/ImageMagick/issues/587", "target": 1, "dataset": "other", "idx": 206930}
{"func": "    }\n    DestroyImageInfo(clone_info);\n    clone_info = NULL;\n  }\n  if (logging) (void)LogMagickEvent(CoderEvent,GetMagickModule(),\"return\");\n  if ((image != image2) && (image2 != (Image *) NULL))\n    image2=DestroyImage(image2);\n  if(image==NULL)\n    ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n  return (image);\n}", "project": "ImageMagick", "hash": 17837560180019284785462531344102032169, "size": 514, "commit_id": "a6802e21d824e786d1e2a8440cf749a6e1a8d95f", "message": "https://github.com/ImageMagick/ImageMagick/issues/587", "target": 0, "dataset": "other", "idx": 389327}
{"func": "{\n\tstruct printer_dev *dev = func_to_printer(f);\n\tstruct f_printer_opts *opts;\n\n\topts = container_of(f->fi, struct f_printer_opts, func_inst);\n\tkfree(dev);\n\tmutex_lock(&opts->lock);\n\t--opts->refcnt;\n\tmutex_unlock(&opts->lock);\n}", "project": "linux", "hash": 139503561102920286146443985238426195363, "size": 11, "commit_id": "e8d5f92b8d30bb4ade76494490c3c065e12411b1", "message": "usb: gadget: function: printer: fix use-after-free in __lock_acquire\n\nFix this by increase object reference count.\n\nBUG: KASAN: use-after-free in __lock_acquire+0x3fd4/0x4180\nkernel/locking/lockdep.c:3831\nRead of size 8 at addr ffff8880683b0018 by task syz-executor.0/3377\n\nCPU: 1 PID: 3377 Comm: syz-executor.0 Not tainted 5.6.11 #1\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0xce/0x128 lib/dump_stack.c:118\n print_address_description.constprop.4+0x21/0x3c0 mm/kasan/report.c:374\n __kasan_report+0x131/0x1b0 mm/kasan/report.c:506\n kasan_report+0x12/0x20 mm/kasan/common.c:641\n __asan_report_load8_noabort+0x14/0x20 mm/kasan/generic_report.c:135\n __lock_acquire+0x3fd4/0x4180 kernel/locking/lockdep.c:3831\n lock_acquire+0x127/0x350 kernel/locking/lockdep.c:4488\n __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:110 [inline]\n _raw_spin_lock_irqsave+0x35/0x50 kernel/locking/spinlock.c:159\n printer_ioctl+0x4a/0x110 drivers/usb/gadget/function/f_printer.c:723\n vfs_ioctl fs/ioctl.c:47 [inline]\n ksys_ioctl+0xfb/0x130 fs/ioctl.c:763\n __do_sys_ioctl fs/ioctl.c:772 [inline]\n __se_sys_ioctl fs/ioctl.c:770 [inline]\n __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:770\n do_syscall_64+0x9e/0x510 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\nRIP: 0033:0x4531a9\nCode: ed 60 fc ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48\n89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d\n01 f0 ff ff 0f 83 bb 60 fc ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007fd14ad72c78 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\nRAX: ffffffffffffffda RBX: 000000000073bfa8 RCX: 00000000004531a9\nRDX: fffffffffffffff9 RSI: 000000000000009e RDI: 0000000000000003\nRBP: 0000000000000003 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 00000000004bbd61\nR13: 00000000004d0a98 R14: 00007fd14ad736d4 R15: 00000000ffffffff\n\nAllocated by task 2393:\n save_stack+0x21/0x90 mm/kasan/common.c:72\n set_track mm/kasan/common.c:80 [inline]\n __kasan_kmalloc.constprop.3+0xa7/0xd0 mm/kasan/common.c:515\n kasan_kmalloc+0x9/0x10 mm/kasan/common.c:529\n kmem_cache_alloc_trace+0xfa/0x2d0 mm/slub.c:2813\n kmalloc include/linux/slab.h:555 [inline]\n kzalloc include/linux/slab.h:669 [inline]\n gprinter_alloc+0xa1/0x870 drivers/usb/gadget/function/f_printer.c:1416\n usb_get_function+0x58/0xc0 drivers/usb/gadget/functions.c:61\n config_usb_cfg_link+0x1ed/0x3e0 drivers/usb/gadget/configfs.c:444\n configfs_symlink+0x527/0x11d0 fs/configfs/symlink.c:202\n vfs_symlink+0x33d/0x5b0 fs/namei.c:4201\n do_symlinkat+0x11b/0x1d0 fs/namei.c:4228\n __do_sys_symlinkat fs/namei.c:4242 [inline]\n __se_sys_symlinkat fs/namei.c:4239 [inline]\n __x64_sys_symlinkat+0x73/0xb0 fs/namei.c:4239\n do_syscall_64+0x9e/0x510 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\nFreed by task 3368:\n save_stack+0x21/0x90 mm/kasan/common.c:72\n set_track mm/kasan/common.c:80 [inline]\n kasan_set_free_info mm/kasan/common.c:337 [inline]\n __kasan_slab_free+0x135/0x190 mm/kasan/common.c:476\n kasan_slab_free+0xe/0x10 mm/kasan/common.c:485\n slab_free_hook mm/slub.c:1444 [inline]\n slab_free_freelist_hook mm/slub.c:1477 [inline]\n slab_free mm/slub.c:3034 [inline]\n kfree+0xf7/0x410 mm/slub.c:3995\n gprinter_free+0x49/0xd0 drivers/usb/gadget/function/f_printer.c:1353\n usb_put_function+0x38/0x50 drivers/usb/gadget/functions.c:87\n config_usb_cfg_unlink+0x2db/0x3b0 drivers/usb/gadget/configfs.c:485\n configfs_unlink+0x3b9/0x7f0 fs/configfs/symlink.c:250\n vfs_unlink+0x287/0x570 fs/namei.c:4073\n do_unlinkat+0x4f9/0x620 fs/namei.c:4137\n __do_sys_unlink fs/namei.c:4184 [inline]\n __se_sys_unlink fs/namei.c:4182 [inline]\n __x64_sys_unlink+0x42/0x50 fs/namei.c:4182\n do_syscall_64+0x9e/0x510 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\nThe buggy address belongs to the object at ffff8880683b0000\n which belongs to the cache kmalloc-1k of size 1024\nThe buggy address is located 24 bytes inside of\n 1024-byte region [ffff8880683b0000, ffff8880683b0400)\nThe buggy address belongs to the page:\npage:ffffea0001a0ec00 refcount:1 mapcount:0 mapping:ffff88806c00e300\nindex:0xffff8880683b1800 compound_mapcount: 0\nflags: 0x100000000010200(slab|head)\nraw: 0100000000010200 0000000000000000 0000000600000001 ffff88806c00e300\nraw: ffff8880683b1800 000000008010000a 00000001ffffffff 0000000000000000\npage dumped because: kasan: bad access detected\n\nReported-by: Kyungtae Kim <kt0755@gmail.com>\nSigned-off-by: Zqiang <qiang.zhang@windriver.com>\nSigned-off-by: Felipe Balbi <balbi@kernel.org>", "target": 1, "dataset": "other", "idx": 206932}
{"func": "{\n\tstruct printer_dev *dev = func_to_printer(f);\n\tstruct f_printer_opts *opts;\n\n\topts = container_of(f->fi, struct f_printer_opts, func_inst);\n\n\tkref_put(&dev->kref, printer_dev_free);\n\tmutex_lock(&opts->lock);\n\t--opts->refcnt;\n\tmutex_unlock(&opts->lock);\n}", "project": "linux", "hash": 27438058775959452498290599893423336582, "size": 12, "commit_id": "e8d5f92b8d30bb4ade76494490c3c065e12411b1", "message": "usb: gadget: function: printer: fix use-after-free in __lock_acquire\n\nFix this by increase object reference count.\n\nBUG: KASAN: use-after-free in __lock_acquire+0x3fd4/0x4180\nkernel/locking/lockdep.c:3831\nRead of size 8 at addr ffff8880683b0018 by task syz-executor.0/3377\n\nCPU: 1 PID: 3377 Comm: syz-executor.0 Not tainted 5.6.11 #1\nHardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011\nCall Trace:\n __dump_stack lib/dump_stack.c:77 [inline]\n dump_stack+0xce/0x128 lib/dump_stack.c:118\n print_address_description.constprop.4+0x21/0x3c0 mm/kasan/report.c:374\n __kasan_report+0x131/0x1b0 mm/kasan/report.c:506\n kasan_report+0x12/0x20 mm/kasan/common.c:641\n __asan_report_load8_noabort+0x14/0x20 mm/kasan/generic_report.c:135\n __lock_acquire+0x3fd4/0x4180 kernel/locking/lockdep.c:3831\n lock_acquire+0x127/0x350 kernel/locking/lockdep.c:4488\n __raw_spin_lock_irqsave include/linux/spinlock_api_smp.h:110 [inline]\n _raw_spin_lock_irqsave+0x35/0x50 kernel/locking/spinlock.c:159\n printer_ioctl+0x4a/0x110 drivers/usb/gadget/function/f_printer.c:723\n vfs_ioctl fs/ioctl.c:47 [inline]\n ksys_ioctl+0xfb/0x130 fs/ioctl.c:763\n __do_sys_ioctl fs/ioctl.c:772 [inline]\n __se_sys_ioctl fs/ioctl.c:770 [inline]\n __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:770\n do_syscall_64+0x9e/0x510 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\nRIP: 0033:0x4531a9\nCode: ed 60 fc ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48\n89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d\n01 f0 ff ff 0f 83 bb 60 fc ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007fd14ad72c78 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\nRAX: ffffffffffffffda RBX: 000000000073bfa8 RCX: 00000000004531a9\nRDX: fffffffffffffff9 RSI: 000000000000009e RDI: 0000000000000003\nRBP: 0000000000000003 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 00000000004bbd61\nR13: 00000000004d0a98 R14: 00007fd14ad736d4 R15: 00000000ffffffff\n\nAllocated by task 2393:\n save_stack+0x21/0x90 mm/kasan/common.c:72\n set_track mm/kasan/common.c:80 [inline]\n __kasan_kmalloc.constprop.3+0xa7/0xd0 mm/kasan/common.c:515\n kasan_kmalloc+0x9/0x10 mm/kasan/common.c:529\n kmem_cache_alloc_trace+0xfa/0x2d0 mm/slub.c:2813\n kmalloc include/linux/slab.h:555 [inline]\n kzalloc include/linux/slab.h:669 [inline]\n gprinter_alloc+0xa1/0x870 drivers/usb/gadget/function/f_printer.c:1416\n usb_get_function+0x58/0xc0 drivers/usb/gadget/functions.c:61\n config_usb_cfg_link+0x1ed/0x3e0 drivers/usb/gadget/configfs.c:444\n configfs_symlink+0x527/0x11d0 fs/configfs/symlink.c:202\n vfs_symlink+0x33d/0x5b0 fs/namei.c:4201\n do_symlinkat+0x11b/0x1d0 fs/namei.c:4228\n __do_sys_symlinkat fs/namei.c:4242 [inline]\n __se_sys_symlinkat fs/namei.c:4239 [inline]\n __x64_sys_symlinkat+0x73/0xb0 fs/namei.c:4239\n do_syscall_64+0x9e/0x510 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\nFreed by task 3368:\n save_stack+0x21/0x90 mm/kasan/common.c:72\n set_track mm/kasan/common.c:80 [inline]\n kasan_set_free_info mm/kasan/common.c:337 [inline]\n __kasan_slab_free+0x135/0x190 mm/kasan/common.c:476\n kasan_slab_free+0xe/0x10 mm/kasan/common.c:485\n slab_free_hook mm/slub.c:1444 [inline]\n slab_free_freelist_hook mm/slub.c:1477 [inline]\n slab_free mm/slub.c:3034 [inline]\n kfree+0xf7/0x410 mm/slub.c:3995\n gprinter_free+0x49/0xd0 drivers/usb/gadget/function/f_printer.c:1353\n usb_put_function+0x38/0x50 drivers/usb/gadget/functions.c:87\n config_usb_cfg_unlink+0x2db/0x3b0 drivers/usb/gadget/configfs.c:485\n configfs_unlink+0x3b9/0x7f0 fs/configfs/symlink.c:250\n vfs_unlink+0x287/0x570 fs/namei.c:4073\n do_unlinkat+0x4f9/0x620 fs/namei.c:4137\n __do_sys_unlink fs/namei.c:4184 [inline]\n __se_sys_unlink fs/namei.c:4182 [inline]\n __x64_sys_unlink+0x42/0x50 fs/namei.c:4182\n do_syscall_64+0x9e/0x510 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\n\nThe buggy address belongs to the object at ffff8880683b0000\n which belongs to the cache kmalloc-1k of size 1024\nThe buggy address is located 24 bytes inside of\n 1024-byte region [ffff8880683b0000, ffff8880683b0400)\nThe buggy address belongs to the page:\npage:ffffea0001a0ec00 refcount:1 mapcount:0 mapping:ffff88806c00e300\nindex:0xffff8880683b1800 compound_mapcount: 0\nflags: 0x100000000010200(slab|head)\nraw: 0100000000010200 0000000000000000 0000000600000001 ffff88806c00e300\nraw: ffff8880683b1800 000000008010000a 00000001ffffffff 0000000000000000\npage dumped because: kasan: bad access detected\n\nReported-by: Kyungtae Kim <kt0755@gmail.com>\nSigned-off-by: Zqiang <qiang.zhang@windriver.com>\nSigned-off-by: Felipe Balbi <balbi@kernel.org>", "target": 0, "dataset": "other", "idx": 389362}
{"func": "\n    /* Write the page data. */\n    {\n        int y;\n        int size = gdev_prn_raster(pdev);\n        byte *data = gs_alloc_bytes(pdev->memory, size, \"tiff12_print_page\");\n\n        if (data == 0)\n            return_error(gs_error_VMerror);\n\n        memset(data, 0, size);\n\n        for (y = 0; y < pdev->height; ++y) {\n            const byte *src;\n            byte *dest;\n            int x;", "project": "ghostpdl", "hash": 112158423848618498504949276444897095461, "size": 57, "commit_id": "714e8995cd582d418276915cbbec3c70711fb19e", "message": "Bug 701807: avoid buffer overflow in tiff12_print_page().\n\nFixes:\n    ./sanbin/gs -r650 -sOutputFile=tmp -sDEVICE=tiff12nc ../bug-701807.pdf", "target": 1, "dataset": "other", "idx": 206948}
{"func": "\n    /* Write the page data. */\n    {\n        int y;\n        int size = gdev_prn_raster(pdev);\n\n        /* We allocate an extra 5 bytes to avoid buffer overflow when accessing\n        src[5] below, if size if not multiple of 6. This fixes bug-701807. */\n        int size_alloc = size + 5;\n        byte *data = gs_alloc_bytes(pdev->memory, size_alloc, \"tiff12_print_page\");\n\n        if (data == 0)\n            return_error(gs_error_VMerror);\n\n        memset(data, 0, size_alloc);\n\n        for (y = 0; y < pdev->height; ++y) {\n            const byte *src;\n            byte *dest;\n            int x;", "project": "ghostpdl", "hash": 199567541113132283956086938291295983290, "size": 61, "commit_id": "714e8995cd582d418276915cbbec3c70711fb19e", "message": "Bug 701807: avoid buffer overflow in tiff12_print_page().\n\nFixes:\n    ./sanbin/gs -r650 -sOutputFile=tmp -sDEVICE=tiff12nc ../bug-701807.pdf", "target": 0, "dataset": "other", "idx": 389796}
{"func": "\n    if (!result || !expected)\n        return 1;\n    switch (hash) {\n    case XAR_CKSUM_SHA1:\n        len = SHA1_HASH_SIZE;\n        break;\n    case XAR_CKSUM_MD5:\n        len = CLI_HASH_MD5;\n        break;\n    case XAR_CKSUM_OTHER:\n    case XAR_CKSUM_NONE:\n    default:\n        return 1;", "project": "clamav-devel", "hash": 16464750489424958099095161220741826057, "size": 20, "commit_id": "d96a6b8bcc7439fa7e3876207aa0a8e79c8451b6", "message": "bb11588 - fix out of bounds read.", "target": 1, "dataset": "other", "idx": 206998}
{"func": "\n    if (!result || !expected)\n        return 1;\n    switch (hash) {\n    case XAR_CKSUM_SHA1:\n        len = CLI_HASHLEN_SHA1;\n        break;\n    case XAR_CKSUM_MD5:\n        len = CLI_HASHLEN_MD5;\n        break;\n    case XAR_CKSUM_OTHER:\n    case XAR_CKSUM_NONE:\n    default:\n        return 1;", "project": "clamav-devel", "hash": 298399301874156136344056006861919253944, "size": 20, "commit_id": "d96a6b8bcc7439fa7e3876207aa0a8e79c8451b6", "message": "bb11588 - fix out of bounds read.", "target": 0, "dataset": "other", "idx": 390690}
{"func": "    for (col=0; col < width; col+=256) {\n      pred[0] = pred[1] = 0;\n      len = MIN (256, width-col);\n      ret = kodak_65000_decode (buf, len);\n      for (i=0; i < len; i++)\n\tif ((RAW(row,col+i) =\tcurve[ret ? buf[i] :\n\t\t(pred[i & 1] += buf[i])]) >> 12) derror();\n    }\n  }\n}", "project": "LibRaw", "hash": 313345098145828817127235037429126165072, "size": 20, "commit_id": "d13e8f6d1e987b7491182040a188c16a395f1d21", "message": "CVE-2017-1438 credits; fix for Kodak 65000 out of bounds access", "target": 1, "dataset": "other", "idx": 207059}
{"func": "    for (col=0; col < width; col+=256) {\n      pred[0] = pred[1] = 0;\n      len = MIN (256, width-col);\n      ret = kodak_65000_decode (buf, len);\n      for (i=0; i < len; i++)\n      {\n\tint idx = ret ? buf[i] : (pred[i & 1] += buf[i]);\n\tif(idx >=0 && idx <= 0xffff)\n\t {\n\t   if ((RAW(row,col+i) = curve[idx]) >> 12) derror();\n         }\n\t else\n\t   derror();\n      }\n    }\n  }\n}", "project": "LibRaw", "hash": 281742020088408334913030368393690018605, "size": 27, "commit_id": "d13e8f6d1e987b7491182040a188c16a395f1d21", "message": "CVE-2017-1438 credits; fix for Kodak 65000 out of bounds access", "target": 0, "dataset": "other", "idx": 391457}
{"func": "static int propagateConstantExprRewrite(Walker *pWalker, Expr *pExpr){\n  int i;\n  WhereConst *pConst;\n  if( pExpr->op!=TK_COLUMN ) return WRC_Continue;\n  if( ExprHasProperty(pExpr, EP_FixedCol) ) return WRC_Continue;\n  pConst = pWalker->u.pConst;\n  for(i=0; i<pConst->nConst; i++){\n    Expr *pColumn = pConst->apExpr[i*2];\n    if( pColumn==pExpr ) continue;\n    if( pColumn->iTable!=pExpr->iTable ) continue;", "project": "sqlite", "hash": 236002876375750538692909015588787193480, "size": 21, "commit_id": "39df24a3f02495e5ef6bb5ea8ce029a2c1e377e6", "message": "Do not allow the constant-propagation optimization to apple to ON/USING clause\nterms as it does not help and it might cause downstream problems.\n\nFossilOrigin-Name: 1bc783da63d58b05c690468b569cb2787846357b63c1100d11777666c5787bf4", "target": 1, "dataset": "other", "idx": 207148}
{"func": "static int propagateConstantExprRewrite(Walker *pWalker, Expr *pExpr){\n  int i;\n  WhereConst *pConst;\n  if( pExpr->op!=TK_COLUMN ) return WRC_Continue;\n  if( ExprHasProperty(pExpr, EP_FixedCol|EP_FromJoin) ) return WRC_Continue;\n  pConst = pWalker->u.pConst;\n  for(i=0; i<pConst->nConst; i++){\n    Expr *pColumn = pConst->apExpr[i*2];\n    if( pColumn==pExpr ) continue;\n    if( pColumn->iTable!=pExpr->iTable ) continue;", "project": "sqlite", "hash": 138100391746403533081790549455832608266, "size": 21, "commit_id": "39df24a3f02495e5ef6bb5ea8ce029a2c1e377e6", "message": "Do not allow the constant-propagation optimization to apple to ON/USING clause\nterms as it does not help and it might cause downstream problems.\n\nFossilOrigin-Name: 1bc783da63d58b05c690468b569cb2787846357b63c1100d11777666c5787bf4", "target": 0, "dataset": "other", "idx": 393392}
{"func": "        assert(record);\n\n        if (!lr_yum_repomd_record_enabled(handle, record->type, repomd->records))\n            continue;\n\n        char *location_href = record->location_href;\n        gboolean is_zchunk = FALSE;\n        #ifdef WITH_ZCHUNK\n        if (handle->cachedir && record->header_checksum)\n            is_zchunk = TRUE;\n        #endif /* WITH_ZCHUNK */", "project": "librepo", "hash": 130004621016105399242097539963842119282, "size": 108, "commit_id": "7daea2a2429a54dad68b1de9b37a5f65c5cf2600", "message": "Validate path read from repomd.xml (RhBug:1868639)\n\n= changelog =\nmsg: Validate path read from repomd.xml\ntype: security\nresolves: https://bugzilla.redhat.com/show_bug.cgi?id=1868639", "target": 1, "dataset": "other", "idx": 207158}
{"func": "\n        if (!lr_yum_repomd_record_enabled(handle, record->type, repomd->records))\n            continue;\n\n        char *location_href = record->location_href;\n\n        char *dest_dir = realpath(handle->destdir, NULL);\n        path = lr_pathconcat(handle->destdir, record->location_href, NULL);\n        char *requested_dir = realpath(dirname(path), NULL);\n        lr_free(path);\n        if (!g_str_has_prefix(requested_dir, dest_dir)) {\n            g_debug(\"%s: Invalid path: %s\", __func__, location_href);\n            g_set_error(err, LR_YUM_ERROR, LRE_IO, \"Invalid path: %s\", location_href);\n            g_slist_free_full(*targets, (GDestroyNotify) lr_downloadtarget_free);\n            free(requested_dir);\n            free(dest_dir);\n            return FALSE;\n        }\n        free(requested_dir);\n        free(dest_dir);\n\n        gboolean is_zchunk = FALSE;\n        #ifdef WITH_ZCHUNK\n        if (handle->cachedir && record->header_checksum)\n            is_zchunk = TRUE;\n        #endif /* WITH_ZCHUNK */", "project": "librepo", "hash": 107802422713710840025599174526096987290, "size": 124, "commit_id": "7daea2a2429a54dad68b1de9b37a5f65c5cf2600", "message": "Validate path read from repomd.xml (RhBug:1868639)\n\n= changelog =\nmsg: Validate path read from repomd.xml\ntype: security\nresolves: https://bugzilla.redhat.com/show_bug.cgi?id=1868639", "target": 0, "dataset": "other", "idx": 393607}
{"func": "\tint htileno;\n\tint vtileno;\n\tjpc_dec_cmpt_t *cmpt;\n\tsize_t size;\n\tsize_t num_samples;\n\tsize_t num_samples_delta;\n\n\tsize_t tile_samples;\n\tif (!jas_safe_size_mul(siz->tilewidth, siz->tileheight, &tile_samples) ||\n\t    (dec->max_samples > 0 && tile_samples > dec->max_samples)) {\n\t\tjas_eprintf(\"tile too large\\n\");", "project": "jasper", "hash": 20009022757512388200600144044584043479, "size": 155, "commit_id": "1b1c591306817e46e1e6a3300f714992b32f972b", "message": "jpc_dec: fix another integer overflow in SIZ\n\nValidate the width and height values first, before doing anything\nelse.  This prevents integer overflows in the `numhtiles` /\n`numvtiles` calculation below, triggering assertion failures.\n\nFixes CVE-2017-13750\n\nCloses https://github.com/mdadams/jasper/issues/165\nCloses https://github.com/mdadams/jasper/issues/174", "target": 1, "dataset": "other", "idx": 207163}
{"func": "\tint vtileno;\n\tjpc_dec_cmpt_t *cmpt;\n\tsize_t size;\n\tsize_t num_samples;\n\tsize_t num_samples_delta;\n\n\tsize_t total_samples;\n\tif (!jas_safe_size_mul(siz->width, siz->height, &total_samples) ||\n\t    (dec->max_samples > 0 && total_samples > dec->max_samples)) {\n\t\tjas_eprintf(\"image too large\\n\");\n\t\treturn -1;\n\t}\n\n\tsize_t tile_samples;\n\tif (!jas_safe_size_mul(siz->tilewidth, siz->tileheight, &tile_samples) ||\n\t    (dec->max_samples > 0 && tile_samples > dec->max_samples)) {\n\t\tjas_eprintf(\"tile too large\\n\");", "project": "jasper", "hash": 131934528969260506628731666992318104628, "size": 162, "commit_id": "1b1c591306817e46e1e6a3300f714992b32f972b", "message": "jpc_dec: fix another integer overflow in SIZ\n\nValidate the width and height values first, before doing anything\nelse.  This prevents integer overflows in the `numhtiles` /\n`numvtiles` calculation below, triggering assertion failures.\n\nFixes CVE-2017-13750\n\nCloses https://github.com/mdadams/jasper/issues/165\nCloses https://github.com/mdadams/jasper/issues/174", "target": 0, "dataset": "other", "idx": 393685}
{"func": "  unsigned i, nWB;\n  unsigned MasterKey, SRF2Key, RawDataKey;\n  INT64 srf_offset, tag_offset, tag_data, tag_dataoffset;\n  int tag_dataunitlen;\n  uchar *srf_buf;\n  short entries;\n  unsigned tag_id, tag_type, tag_datalen;\n\n  srf_buf = (uchar *)malloc(len);\n  fread(srf_buf, len, 1, ifp);\n\n  offset += srf_buf[offset] << 2;\n\n#define CHECKBUFFER_SGET4(offset)                                              \\\n  while (entries--) {\n    if (tiff_sget (save, srf_buf, len,\n                   &tag_offset, &tag_id, &tag_type, &tag_dataoffset,\n                   &tag_datalen, &tag_dataunitlen) == 0) {\n      if (tag_id == 0x0000) {\n        SRF2Key = sget4(srf_buf + tag_dataoffset);\n      } else if (tag_id == 0x0001) {\n        RawDataKey = sget4(srf_buf + tag_dataoffset);\n      }\n    } else goto restore_after_parseSonySRF;\n  }\n  offset = tag_offset;\n\n    goto restore_after_parseSonySRF;\n  offset = srf_offset + 2;\n  tag_offset = offset;\n\n  while (entries--) {\n    if (tiff_sget (save, srf_buf, len,\n                   &tag_offset, &tag_id, &tag_type, &tag_dataoffset,\n                   &tag_datalen, &tag_dataunitlen) == 0) {\n      if ((tag_id >= 0x00c0) && (tag_id <= 0x00ce)) {\n        i = (tag_id - 0x00c0) % 3;\n        nWB = (tag_id - 0x00c0) / 3;\n        icWBC[Sony_SRF_wb_list[nWB]][i] = sget4(srf_buf + tag_dataoffset);\n        if (i == 1) {\n          icWBC[Sony_SRF_wb_list[nWB]][3] =\n            icWBC[Sony_SRF_wb_list[nWB]][i];\n        }\n      } else if ((tag_id >= 0x00d0) && (tag_id <= 0x00d2)) {\n        i = (tag_id - 0x00d0) % 3;\n        cam_mul[i] = sget4(srf_buf + tag_dataoffset);\n        if (i == 1) {\n          cam_mul[3] = cam_mul[i];\n        }\n      } else switch (tag_id) {\n        /*\n        0x0003  SRFDataOffset (?)\n        0x0004  RawDataOffset\n        0x0005  RawDataLength\n        */\n      case 0x0043:\n        ilm.MaxAp4MaxFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      case 0x0044:\n         ilm.MaxAp4MinFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      case 0x0045:\n        ilm.MinFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      case 0x0046:\n        ilm.MaxFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      }\n    } else goto restore_after_parseSonySRF;\n  }\n  offset = tag_offset;", "project": "LibRaw", "hash": 133935611544622211790149768901407187953, "size": 150, "commit_id": "c243f4539233053466c1309bde606815351bee81", "message": "additional checks in parseSonySRF\n\nparseSonySR2: buffer size check", "target": 1, "dataset": "other", "idx": 207218}
{"func": "  unsigned i, nWB;\n  unsigned MasterKey, SRF2Key, RawDataKey;\n  INT64 srf_offset, tag_offset, tag_data, tag_dataoffset;\n  int tag_dataunitlen;\n  uchar *srf_buf;\n  ushort entries;\n  unsigned tag_id, tag_type, tag_datalen;\n\n  srf_buf = (uchar *)malloc(len+64);\n  fread(srf_buf, len, 1, ifp);\n\n  offset += srf_buf[offset] << 2;\n\n#define CHECKBUFFER_SGET4(offset)                                              \\\n  while (entries--) {\n    if (tiff_sget (save, srf_buf, len,\n                   &tag_offset, &tag_id, &tag_type, &tag_dataoffset,\n                   &tag_datalen, &tag_dataunitlen) == 0) {\n      if (tag_id == 0x0000) {\n\t\t  CHECKBUFFER_SGET4(tag_dataoffset);\n\t\t  SRF2Key = sget4(srf_buf + tag_dataoffset);\n      } else if (tag_id == 0x0001) {\n\t\t  CHECKBUFFER_SGET4(tag_dataoffset);\n\t\t  RawDataKey = sget4(srf_buf + tag_dataoffset);\n      }\n    } else goto restore_after_parseSonySRF;\n  }\n  offset = tag_offset;\n\n    goto restore_after_parseSonySRF;\n  offset = srf_offset + 2;\n  tag_offset = offset;\n\n  while (entries--) {\n\t  if (tiff_sget(save, srf_buf, len,\n                   &tag_offset, &tag_id, &tag_type, &tag_dataoffset,\n                   &tag_datalen, &tag_dataunitlen) == 0) {\n      if ((tag_id >= 0x00c0) && (tag_id <= 0x00ce)) {\n        i = (tag_id - 0x00c0) % 3;\n        nWB = (tag_id - 0x00c0) / 3;\n\t\tCHECKBUFFER_SGET4(tag_dataoffset);\n\t\ticWBC[Sony_SRF_wb_list[nWB]][i] = sget4(srf_buf + tag_dataoffset);\n        if (i == 1) {\n          icWBC[Sony_SRF_wb_list[nWB]][3] =\n            icWBC[Sony_SRF_wb_list[nWB]][i];\n        }\n      } else if ((tag_id >= 0x00d0) && (tag_id <= 0x00d2)) {\n        i = (tag_id - 0x00d0) % 3;\n\t\tCHECKBUFFER_SGET4(tag_dataoffset);\n\t\tcam_mul[i] = sget4(srf_buf + tag_dataoffset);\n        if (i == 1) {\n          cam_mul[3] = cam_mul[i];\n        }\n      } else switch (tag_id) {\n        /*\n        0x0003  SRFDataOffset (?)\n        0x0004  RawDataOffset\n        0x0005  RawDataLength\n        */\n      case 0x0043:\n\t\t  CHECKBUFFER_SGET4(tag_dataoffset); // need to add extra space\n\t\t  ilm.MaxAp4MaxFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      case 0x0044:\n\t\t  CHECKBUFFER_SGET4(tag_dataoffset);\n\t\t  ilm.MaxAp4MinFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      case 0x0045:\n\t\t  CHECKBUFFER_SGET4(tag_dataoffset);\n\t\t  ilm.MinFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      case 0x0046:\n\t\t  CHECKBUFFER_SGET4(tag_dataoffset);\n\t\t  ilm.MaxFocal = sgetreal(tag_type, srf_buf + tag_dataoffset);\n        break;\n      }\n    } else goto restore_after_parseSonySRF;\n  }\n  offset = tag_offset;", "project": "LibRaw", "hash": 211422748523486439603542502209147525104, "size": 158, "commit_id": "c243f4539233053466c1309bde606815351bee81", "message": "additional checks in parseSonySRF\n\nparseSonySR2: buffer size check", "target": 0, "dataset": "other", "idx": 394100}
{"func": "\terr = nla_parse_nested_deprecated(tb, CTA_TUPLE_MAX, cda[type],\n\t\t\t\t\t  tuple_nla_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\n\ttuple->src.l3num = l3num;\n\n\tif (flags & CTA_FILTER_FLAG(CTA_IP_DST) ||\n\t    flags & CTA_FILTER_FLAG(CTA_IP_SRC)) {\n\t\tif (!tb[CTA_TUPLE_IP])", "project": "linux", "hash": 305449011109180351247584082573465785790, "size": 58, "commit_id": "1cc5ef91d2ff94d2bf2de3b3585423e8a1051cb6", "message": "netfilter: ctnetlink: add a range check for l3/l4 protonum\n\nThe indexes to the nf_nat_l[34]protos arrays come from userspace. So\ncheck the tuple's family, e.g. l3num, when creating the conntrack in\norder to prevent an OOB memory access during setup.  Here is an example\nkernel panic on 4.14.180 when userspace passes in an index greater than\nNFPROTO_NUMPROTO.\n\nInternal error: Oops - BUG: 0 [#1] PREEMPT SMP\nModules linked in:...\nProcess poc (pid: 5614, stack limit = 0x00000000a3933121)\nCPU: 4 PID: 5614 Comm: poc Tainted: G S      W  O    4.14.180-g051355490483\nHardware name: Qualcomm Technologies, Inc. SM8150 V2 PM8150 Google Inc. MSM\ntask: 000000002a3dfffe task.stack: 00000000a3933121\npc : __cfi_check_fail+0x1c/0x24\nlr : __cfi_check_fail+0x1c/0x24\n...\nCall trace:\n__cfi_check_fail+0x1c/0x24\nname_to_dev_t+0x0/0x468\nnfnetlink_parse_nat_setup+0x234/0x258\nctnetlink_parse_nat_setup+0x4c/0x228\nctnetlink_new_conntrack+0x590/0xc40\nnfnetlink_rcv_msg+0x31c/0x4d4\nnetlink_rcv_skb+0x100/0x184\nnfnetlink_rcv+0xf4/0x180\nnetlink_unicast+0x360/0x770\nnetlink_sendmsg+0x5a0/0x6a4\n___sys_sendmsg+0x314/0x46c\nSyS_sendmsg+0xb4/0x108\nel0_svc_naked+0x34/0x38\n\nThis crash is not happening since 5.4+, however, ctnetlink still\nallows for creating entries with unsupported layer 3 protocol number.\n\nFixes: c1d10adb4a521 (\"[NETFILTER]: Add ctnetlink port for nf_conntrack\")\nSigned-off-by: Will McVicker <willmcvicker@google.com>\n[pablo@netfilter.org: rebased original patch on top of nf.git]\nSigned-off-by: Pablo Neira Ayuso <pablo@netfilter.org>", "target": 1, "dataset": "other", "idx": 207223}
{"func": "\terr = nla_parse_nested_deprecated(tb, CTA_TUPLE_MAX, cda[type],\n\t\t\t\t\t  tuple_nla_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (l3num != NFPROTO_IPV4 && l3num != NFPROTO_IPV6)\n\t\treturn -EOPNOTSUPP;\n\ttuple->src.l3num = l3num;\n\n\tif (flags & CTA_FILTER_FLAG(CTA_IP_DST) ||\n\t    flags & CTA_FILTER_FLAG(CTA_IP_SRC)) {\n\t\tif (!tb[CTA_TUPLE_IP])", "project": "linux", "hash": 16152469972975795768075689172273398454, "size": 59, "commit_id": "1cc5ef91d2ff94d2bf2de3b3585423e8a1051cb6", "message": "netfilter: ctnetlink: add a range check for l3/l4 protonum\n\nThe indexes to the nf_nat_l[34]protos arrays come from userspace. So\ncheck the tuple's family, e.g. l3num, when creating the conntrack in\norder to prevent an OOB memory access during setup.  Here is an example\nkernel panic on 4.14.180 when userspace passes in an index greater than\nNFPROTO_NUMPROTO.\n\nInternal error: Oops - BUG: 0 [#1] PREEMPT SMP\nModules linked in:...\nProcess poc (pid: 5614, stack limit = 0x00000000a3933121)\nCPU: 4 PID: 5614 Comm: poc Tainted: G S      W  O    4.14.180-g051355490483\nHardware name: Qualcomm Technologies, Inc. SM8150 V2 PM8150 Google Inc. MSM\ntask: 000000002a3dfffe task.stack: 00000000a3933121\npc : __cfi_check_fail+0x1c/0x24\nlr : __cfi_check_fail+0x1c/0x24\n...\nCall trace:\n__cfi_check_fail+0x1c/0x24\nname_to_dev_t+0x0/0x468\nnfnetlink_parse_nat_setup+0x234/0x258\nctnetlink_parse_nat_setup+0x4c/0x228\nctnetlink_new_conntrack+0x590/0xc40\nnfnetlink_rcv_msg+0x31c/0x4d4\nnetlink_rcv_skb+0x100/0x184\nnfnetlink_rcv+0xf4/0x180\nnetlink_unicast+0x360/0x770\nnetlink_sendmsg+0x5a0/0x6a4\n___sys_sendmsg+0x314/0x46c\nSyS_sendmsg+0xb4/0x108\nel0_svc_naked+0x34/0x38\n\nThis crash is not happening since 5.4+, however, ctnetlink still\nallows for creating entries with unsupported layer 3 protocol number.\n\nFixes: c1d10adb4a521 (\"[NETFILTER]: Add ctnetlink port for nf_conntrack\")\nSigned-off-by: Will McVicker <willmcvicker@google.com>\n[pablo@netfilter.org: rebased original patch on top of nf.git]\nSigned-off-by: Pablo Neira Ayuso <pablo@netfilter.org>", "target": 0, "dataset": "other", "idx": 394260}
{"func": "LJ_NOINLINE void lj_err_run(lua_State *L)\n{\n  ptrdiff_t ef = finderrfunc(L);\n  if (ef) {\n    TValue *errfunc = restorestack(L, ef);\n    TValue *top = L->top;\n    lj_trace_abort(G(L));\n    if (!tvisfunc(errfunc) || L->status == LUA_ERRERR) {\n      setstrV(L, top-1, lj_err_str(L, LJ_ERR_ERRERR));\n      lj_err_throw(L, LUA_ERRERR);\n    }\n    L->status = LUA_ERRERR;\n    copyTV(L, top, top-1);\n    copyTV(L, top-1, errfunc);\n    L->top = top+1;\n    lj_vm_call(L, top, 1+1);  /* Stack: |errfunc|msg| -> |msg| */\n  }\n  lj_err_throw(L, LUA_ERRRUN);\n}", "project": "LuaJIT", "hash": 295611232396518332386836316299494710511, "size": 19, "commit_id": "e296f56b825c688c3530a981dc6b495d972f3d01", "message": "Call error function on rethrow after trace exit.", "target": 1, "dataset": "other", "idx": 207262}
{"func": "LJ_NOINLINE void LJ_FASTCALL lj_err_run(lua_State *L)\n{\n  ptrdiff_t ef = finderrfunc(L);\n  if (ef) {\n    TValue *errfunc = restorestack(L, ef);\n    TValue *top = L->top;\n    lj_trace_abort(G(L));\n    if (!tvisfunc(errfunc) || L->status == LUA_ERRERR) {\n      setstrV(L, top-1, lj_err_str(L, LJ_ERR_ERRERR));\n      lj_err_throw(L, LUA_ERRERR);\n    }\n    L->status = LUA_ERRERR;\n    copyTV(L, top, top-1);\n    copyTV(L, top-1, errfunc);\n    L->top = top+1;\n    lj_vm_call(L, top, 1+1);  /* Stack: |errfunc|msg| -> |msg| */\n  }\n  lj_err_throw(L, LUA_ERRRUN);\n}", "project": "LuaJIT", "hash": 38911403279138630648125077204690901074, "size": 19, "commit_id": "e296f56b825c688c3530a981dc6b495d972f3d01", "message": "Call error function on rethrow after trace exit.", "target": 0, "dataset": "other", "idx": 394651}
{"func": "      break;\n    case 0x9205:\n      imgdata.lens.EXIF_MaxAp = libraw_powf64l(2.0f, (getreal(type) / 2.0f));\n      break;\n    case 0x829a: // 33434\n      tiff_ifd[tiff_nifds - 1].t_shutter = shutter = getreal(type);\n      break;\n    case 0x829d: // 33437, FNumber\n      aperture = getreal(type);\n      break;\n    case 0x8827: // 34855\n    case 0x9003: // 36867\n    case 0x9004: // 36868\n      get_timestamp(0);\n      break;\n    case 0x9201: // 37377\n      if ((expo = -getreal(type)) < 128 && shutter == 0.)\n        tiff_ifd[tiff_nifds - 1].t_shutter = shutter =\n            libraw_powf64l(2.0, expo);\n      break;\n    case 0x9202: // 37378 ApertureValue\n      if ((fabs(ape = getreal(type)) < 256.0) && (!aperture))\n        aperture = libraw_powf64l(2.0, ape / 2);\n      break;", "project": "LibRaw", "hash": 182171441535201305274163229980614035603, "size": 245, "commit_id": "55f0a0c08974b8b79ebfa7762b555a1704b25fb2", "message": "possible buffer underrun in exif parser", "target": 1, "dataset": "other", "idx": 207309}
{"func": "      break;\n    case 0x9205:\n      imgdata.lens.EXIF_MaxAp = libraw_powf64l(2.0f, (getreal(type) / 2.0f));\n      break;\n    case 0x829a: // 33434\n      shutter = getreal(type);\n      if (tiff_nifds > 0 && tiff_nifds <= LIBRAW_IFD_MAXCOUNT)\n          tiff_ifd[tiff_nifds - 1].t_shutter = shutter;\n      break;\n    case 0x829d: // 33437, FNumber\n      aperture = getreal(type);\n      break;\n    case 0x8827: // 34855\n    case 0x9003: // 36867\n    case 0x9004: // 36868\n      get_timestamp(0);\n      break;\n    case 0x9201: // 37377\n       if ((expo = -getreal(type)) < 128 && shutter == 0.)\n       {\n            shutter = libraw_powf64l(2.0, expo);\n            if (tiff_nifds > 0 && tiff_nifds <= LIBRAW_IFD_MAXCOUNT)\n              tiff_ifd[tiff_nifds - 1].t_shutter = shutter;\n       }\n      break;\n    case 0x9202: // 37378 ApertureValue\n      if ((fabs(ape = getreal(type)) < 256.0) && (!aperture))\n        aperture = libraw_powf64l(2.0, ape / 2);\n      break;", "project": "LibRaw", "hash": 124973419188282141476335511350835218609, "size": 250, "commit_id": "55f0a0c08974b8b79ebfa7762b555a1704b25fb2", "message": "possible buffer underrun in exif parser", "target": 0, "dataset": "other", "idx": 395532}
{"func": "    Open image file.\n  */\n  image=AcquireImage(image_info);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    return((Image *) NULL);\n  /*\n    Get user defaults from X resource database.\n  */\n  client_name=GetClientName();\n  resource_database=XGetResourceDatabase(display,client_name);", "project": "ImageMagick6", "hash": 290877276966701211376434314093754867321, "size": 436, "commit_id": "ebe38274941908892c32b7244fa4e0fe7497e528", "message": "...", "target": 1, "dataset": "other", "idx": 207312}
{"func": "    Open image file.\n  */\n  image=AcquireImage(image_info);\n  status=OpenBlob(image_info,image,ReadBinaryBlobMode,exception);\n  if (status == MagickFalse)\n    {\n      image=DestroyImageList(image);\n      return((Image *) NULL);\n    }\n  /*\n    Get user defaults from X resource database.\n  */\n  client_name=GetClientName();\n  resource_database=XGetResourceDatabase(display,client_name);", "project": "ImageMagick6", "hash": 82893322187814403256007653034729823397, "size": 439, "commit_id": "ebe38274941908892c32b7244fa4e0fe7497e528", "message": "...", "target": 0, "dataset": "other", "idx": 395543}
{"func": "\t\t\t\tlprintf(LOG_ERR, \"It is likely that the channel in use \"\n\t\t\t\t\t\"does not support sessions\");\n\t\t}\n\t\telse\n\t\t{\n\t\t\tmemcpy(&session_info,  rsp->data, rsp->data_len);\n\t\t\tprint_session_info(&session_info, rsp->data_len);\n\t\t}\n\t\tbreak;\n\t\t\n\tcase IPMI_SESSION_REQUEST_ALL:\n\t\treq.msg.data_len = 1;\n\t\t\t{\n\t\t\t\tretval = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmemcpy(&session_info,  rsp->data, rsp->data_len);\n\t\t\tprint_session_info(&session_info, rsp->data_len);\n\t\t\t\n\t\t} while (i <= session_info.session_slot_count);\n\t\tbreak;\n\t}\n", "project": "ipmitool", "hash": 224137137758231083307007282464002439869, "size": 109, "commit_id": "41d7026946fafbd4d1ec0bcaca3ea30a6e8eed22", "message": "session: Fix buffer overflow in ipmi_get_session_info\n\nPartial fix for CVE-2020-5208, see\nhttps://github.com/ipmitool/ipmitool/security/advisories/GHSA-g659-9qxw-p7cp\n\nThe `ipmi_get_session_info` function does not properly check the\nresponse `data_len`, which is used as a copy size, allowing stack buffer\noverflow.", "target": 1, "dataset": "other", "idx": 207463}
{"func": "\t\t\t\tlprintf(LOG_ERR, \"It is likely that the channel in use \"\n\t\t\t\t\t\"does not support sessions\");\n\t\t}\n\t\telse\n\t\t{\n\t\t\tmemcpy(&session_info,  rsp->data,\n\t\t\t       __min(rsp->data_len, sizeof(session_info)));\n\t\t\tprint_session_info(&session_info,\n\t\t\t                   __min(rsp->data_len, sizeof(session_info)));\n\t\t}\n\t\tbreak;\n\t\t\n\tcase IPMI_SESSION_REQUEST_ALL:\n\t\treq.msg.data_len = 1;\n\t\t\t{\n\t\t\t\tretval = -1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmemcpy(&session_info,  rsp->data,\n\t\t\t       __min(rsp->data_len, sizeof(session_info)));\n\t\t\tprint_session_info(&session_info,\n\t\t\t                   __min(rsp->data_len, sizeof(session_info)));\n\t\t\t\n\t\t} while (i <= session_info.session_slot_count);\n\t\tbreak;\n\t}\n", "project": "ipmitool", "hash": 242323179109677020584476187087776996309, "size": 113, "commit_id": "41d7026946fafbd4d1ec0bcaca3ea30a6e8eed22", "message": "session: Fix buffer overflow in ipmi_get_session_info\n\nPartial fix for CVE-2020-5208, see\nhttps://github.com/ipmitool/ipmitool/security/advisories/GHSA-g659-9qxw-p7cp\n\nThe `ipmi_get_session_info` function does not properly check the\nresponse `data_len`, which is used as a copy size, allowing stack buffer\noverflow.", "target": 0, "dataset": "other", "idx": 398021}
{"func": "\t */\n\tif (sp->ep == NULL)\n\t\treturn;\n\n\tif (sp->do_auto_asconf) {\n\t\tsp->do_auto_asconf = 0;\n\t\tlist_del(&sp->auto_asconf_list);\n\t}\n\tsctp_endpoint_free(sp->ep);\n\tlocal_bh_disable();\n\tsk_sockets_allocated_dec(sk);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);", "project": "linux", "hash": 16718687577270921329602634345512729479, "size": 24, "commit_id": "b166a20b07382b8bc1dcee2a448715c9c2c81b5b", "message": "net/sctp: fix race condition in sctp_destroy_sock\n\nIf sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock\nheld and sp->do_auto_asconf is true, then an element is removed\nfrom the auto_asconf_splist without any proper locking.\n\nThis can happen in the following functions:\n1. In sctp_accept, if sctp_sock_migrate fails.\n2. In inet_create or inet6_create, if there is a bpf program\n   attached to BPF_CGROUP_INET_SOCK_CREATE which denies\n   creation of the sctp socket.\n\nThe bug is fixed by acquiring addr_wq_lock in sctp_destroy_sock\ninstead of sctp_close.\n\nThis addresses CVE-2021-23133.\n\nReported-by: Or Cohen <orcohen@paloaltonetworks.com>\nReviewed-by: Xin Long <lucien.xin@gmail.com>\nFixes: 610236587600 (\"bpf: Add new cgroup attach type to enable sock modifications\")\nSigned-off-by: Or Cohen <orcohen@paloaltonetworks.com>\nAcked-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 207478}
{"func": "\tif (sp->ep == NULL)\n\t\treturn;\n\n\tif (sp->do_auto_asconf) {\n\t\tsp->do_auto_asconf = 0;\n\t\tspin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\t\tlist_del(&sp->auto_asconf_list);\n\t\tspin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);\n\t}\n\tsctp_endpoint_free(sp->ep);\n\tlocal_bh_disable();\n\tsk_sockets_allocated_dec(sk);\n\tsock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);", "project": "linux", "hash": 169454299650799904813060257343861693714, "size": 26, "commit_id": "b166a20b07382b8bc1dcee2a448715c9c2c81b5b", "message": "net/sctp: fix race condition in sctp_destroy_sock\n\nIf sctp_destroy_sock is called without sock_net(sk)->sctp.addr_wq_lock\nheld and sp->do_auto_asconf is true, then an element is removed\nfrom the auto_asconf_splist without any proper locking.\n\nThis can happen in the following functions:\n1. In sctp_accept, if sctp_sock_migrate fails.\n2. In inet_create or inet6_create, if there is a bpf program\n   attached to BPF_CGROUP_INET_SOCK_CREATE which denies\n   creation of the sctp socket.\n\nThe bug is fixed by acquiring addr_wq_lock in sctp_destroy_sock\ninstead of sctp_close.\n\nThis addresses CVE-2021-23133.\n\nReported-by: Or Cohen <orcohen@paloaltonetworks.com>\nReviewed-by: Xin Long <lucien.xin@gmail.com>\nFixes: 610236587600 (\"bpf: Add new cgroup attach type to enable sock modifications\")\nSigned-off-by: Or Cohen <orcohen@paloaltonetworks.com>\nAcked-by: Marcelo Ricardo Leitner <marcelo.leitner@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 398128}
{"func": "\n\tusb_set_intfdata(intf, NULL);\n\n\tnetdev_info(priv->netdev, \"device disconnected\\n\");\n\n\tunregister_candev(priv->netdev);\n\tfree_candev(priv->netdev);\n\n\tmcba_urb_unlink(priv);\n}", "project": "linux", "hash": 331613299342883456089163439463915430023, "size": 13, "commit_id": "4d6636498c41891d0482a914dd570343a838ad79", "message": "can: mcba_usb: fix use-after-free on disconnect\n\nThe driver was accessing its driver data after having freed it.\n\nFixes: 51f3baad7de9 (\"can: mcba_usb: Add support for Microchip CAN BUS Analyzer\")\nCc: stable <stable@vger.kernel.org>     # 4.12\nCc: Remigiusz Ko\u0142\u0142\u0105taj <remigiusz.kollataj@mobica.com>\nReported-by: syzbot+e29b17e5042bbc56fae9@syzkaller.appspotmail.com\nSigned-off-by: Johan Hovold <johan@kernel.org>\nSigned-off-by: Marc Kleine-Budde <mkl@pengutronix.de>", "target": 1, "dataset": "other", "idx": 207488}
{"func": "\tusb_set_intfdata(intf, NULL);\n\n\tnetdev_info(priv->netdev, \"device disconnected\\n\");\n\n\tunregister_candev(priv->netdev);\n\tmcba_urb_unlink(priv);\n\tfree_candev(priv->netdev);\n}", "project": "linux", "hash": 111040647052107229552079151990945826263, "size": 12, "commit_id": "4d6636498c41891d0482a914dd570343a838ad79", "message": "can: mcba_usb: fix use-after-free on disconnect\n\nThe driver was accessing its driver data after having freed it.\n\nFixes: 51f3baad7de9 (\"can: mcba_usb: Add support for Microchip CAN BUS Analyzer\")\nCc: stable <stable@vger.kernel.org>     # 4.12\nCc: Remigiusz Ko\u0142\u0142\u0105taj <remigiusz.kollataj@mobica.com>\nReported-by: syzbot+e29b17e5042bbc56fae9@syzkaller.appspotmail.com\nSigned-off-by: Johan Hovold <johan@kernel.org>\nSigned-off-by: Marc Kleine-Budde <mkl@pengutronix.de>", "target": 0, "dataset": "other", "idx": 398308}
{"func": "\t\tgoto out_free_blob;\n\n\tlicenseStream = Stream_New(calBlob->data, calBlob->length);\n\tif (!licenseStream)\n\t\tgoto out_free_blob;\n\n\tStream_Read_UINT16(licenseStream, os_minor);\n\tStream_Read_UINT16(licenseStream, os_major);\n\n\t/* Scope */\n\tStream_Read_UINT32(licenseStream, cbScope);\n\tWLog_DBG(TAG, \"Scope:\");\n\twinpr_HexDump(TAG, WLOG_DEBUG, Stream_Pointer(licenseStream), cbScope);\n#endif\n\tStream_Seek(licenseStream, cbScope);\n\n\t/* CompanyName */\n\tStream_Read_UINT32(licenseStream, cbCompanyName);\n\tif (Stream_GetRemainingLength(licenseStream) < cbCompanyName)\n\t\tgoto out_free_stream;\n#ifdef WITH_DEBUG_LICENSE\n\tWLog_DBG(TAG, \"Company name:\");\n\twinpr_HexDump(TAG, WLOG_DEBUG, Stream_Pointer(licenseStream), cbCompanyName);\n#endif\n\tStream_Seek(licenseStream, cbCompanyName);\n\n\t/* productId */\n\tStream_Read_UINT32(licenseStream, cbProductId);\n\tif (Stream_GetRemainingLength(licenseStream) < cbProductId)\n\t\tgoto out_free_stream;\n#ifdef WITH_DEBUG_LICENSE\n\tWLog_DBG(TAG, \"Product id:\");\n\twinpr_HexDump(TAG, WLOG_DEBUG, Stream_Pointer(licenseStream), cbProductId);\n#endif\n\tStream_Seek(licenseStream, cbProductId);\n\n\t/* licenseInfo */\n\tStream_Read_UINT32(licenseStream, cbLicenseInfo);\n\tif (Stream_GetRemainingLength(licenseStream) < cbLicenseInfo)\n\t\tgoto out_free_stream;\n\n\tlicense->state = LICENSE_STATE_COMPLETED;", "project": "FreeRDP", "hash": 315612488608223251182245385146208820876, "size": 91, "commit_id": "6ade7b4cbfd71c54b3d724e8f2d6ac76a58e879a", "message": "Fixed OOB Read in license_read_new_or_upgrade_license_packet\n\nCVE-2020-11099 thanks to @antonio-morales for finding this.", "target": 1, "dataset": "other", "idx": 207567}
{"func": "\n\tlicenseStream = Stream_New(calBlob->data, calBlob->length);\n\tif (!licenseStream)\n\t\tgoto out_free_blob;\n\n\tif (Stream_GetRemainingLength(licenseStream) < 8)\n\t\tgoto out_free_stream;\n\n\tStream_Read_UINT16(licenseStream, os_minor);\n\tStream_Read_UINT16(licenseStream, os_major);\n\n\t/* Scope */\n\tStream_Read_UINT32(licenseStream, cbScope);\n\twinpr_HexDump(TAG, WLOG_DEBUG, Stream_Pointer(licenseStream), cbScope);\n#endif\n\tStream_Seek(licenseStream, cbScope);\n\n\t/* CompanyName */\n\tif (Stream_GetRemainingLength(licenseStream) < 4)\n\t\tgoto out_free_stream;\n\tStream_Read_UINT32(licenseStream, cbCompanyName);\n\tif (Stream_GetRemainingLength(licenseStream) < cbCompanyName)\n\t\tgoto out_free_stream;\n#ifdef WITH_DEBUG_LICENSE\n\tWLog_DBG(TAG, \"Company name:\");\n\twinpr_HexDump(TAG, WLOG_DEBUG, Stream_Pointer(licenseStream), cbCompanyName);\n#endif\n\tStream_Seek(licenseStream, cbCompanyName);\n\n\t/* productId */\n\tif (Stream_GetRemainingLength(licenseStream) < 4)\n\t\tgoto out_free_stream;\n\tStream_Read_UINT32(licenseStream, cbProductId);\n\tif (Stream_GetRemainingLength(licenseStream) < cbProductId)\n\t\tgoto out_free_stream;\n#ifdef WITH_DEBUG_LICENSE\n\tWLog_DBG(TAG, \"Product id:\");\n\twinpr_HexDump(TAG, WLOG_DEBUG, Stream_Pointer(licenseStream), cbProductId);\n#endif\n\tStream_Seek(licenseStream, cbProductId);\n\n\t/* licenseInfo */\n\tif (Stream_GetRemainingLength(licenseStream) < 4)\n\t\tgoto out_free_stream;\n\tStream_Read_UINT32(licenseStream, cbLicenseInfo);\n\tif (Stream_GetRemainingLength(licenseStream) < cbLicenseInfo)\n\t\tgoto out_free_stream;\n\n\tlicense->state = LICENSE_STATE_COMPLETED;", "project": "FreeRDP", "hash": 119587276096900182263474845173512880982, "size": 100, "commit_id": "6ade7b4cbfd71c54b3d724e8f2d6ac76a58e879a", "message": "Fixed OOB Read in license_read_new_or_upgrade_license_packet\n\nCVE-2020-11099 thanks to @antonio-morales for finding this.", "target": 0, "dataset": "other", "idx": 399253}
{"func": "  {\n    iter->substr_cur = iter->substr_end;\n    if (iter->substr_cur == iter->eostr)\n      return 1;\n\n    while (!*(iter->substr_cur))\n      iter->substr_cur++;\n    iter->substr_end = strchr(iter->substr_cur, ',');\n    if (!iter->substr_end)\n      iter->substr_end = iter->eostr;\n    else\n      *(iter->substr_end) = '\\0';\n\n    char *range_sep = strchr(iter->substr_cur, ':');\n    if (range_sep)\n      *range_sep++ = '\\0';\n", "project": "neomutt", "hash": 255563423373473490710136560651315420142, "size": 54, "commit_id": "fa1db5785e5cfd9d3cd27b7571b9fe268d2ec2dc", "message": "Fix seqset iterator when it ends in a comma\n\nIf the seqset ended with a comma, the substr_end marker would be just\nbefore the trailing nul.  In the next call, the loop to skip the\nmarker would iterate right past the end of string too.\n\nThe fix is simple: place the substr_end marker and skip past it\nimmediately.", "target": 1, "dataset": "other", "idx": 207568}
{"func": "  if (!iter->in_range)\n  {\n    iter->substr_cur = iter->substr_end;\n    if (iter->substr_cur == iter->eostr)\n      return 1;\n\n    iter->substr_end = strchr(iter->substr_cur, ',');\n    if (!iter->substr_end)\n      iter->substr_end = iter->eostr;\n    else\n      *(iter->substr_end++) = '\\0';\n\n    char *range_sep = strchr(iter->substr_cur, ':');\n    if (range_sep)\n      *range_sep++ = '\\0';\n", "project": "neomutt", "hash": 234210525400853698338379777365300845795, "size": 52, "commit_id": "fa1db5785e5cfd9d3cd27b7571b9fe268d2ec2dc", "message": "Fix seqset iterator when it ends in a comma\n\nIf the seqset ended with a comma, the substr_end marker would be just\nbefore the trailing nul.  In the next call, the loop to skip the\nmarker would iterate right past the end of string too.\n\nThe fix is simple: place the substr_end marker and skip past it\nimmediately.", "target": 0, "dataset": "other", "idx": 399279}
{"func": "\t\tdesc_addrs[i] = vhost_iova_to_vva(dev, vq,\n\t\t\t\t\t\t  descs[avail_idx + i].addr,\n\t\t\t\t\t\t  &lens[i],\n\t\t\t\t\t\t  VHOST_ACCESS_RW);\n\n\tvhost_for_each_try_unroll(i, 0, PACKED_BATCH_SIZE) {\n\t\tif (unlikely(lens[i] != descs[avail_idx + i].len))\n\t\t\treturn -1;\n\t}\n\n\tvhost_for_each_try_unroll(i, 0, PACKED_BATCH_SIZE) {", "project": "dpdk", "hash": 141015319348152109183918601798829151336, "size": 78, "commit_id": "97ecc1c85c95c13bc66a87435758e93406c35c48", "message": "vhost: fix translated address not checked\n\nMalicious guest can construct desc with invalid address and zero buffer\nlength. That will request vhost to check both translated address and\ntranslated data length. This patch will add missed address check.\n\nCVE-2020-10725\nFixes: 75ed51697820 (\"vhost: add packed ring batch dequeue\")\nFixes: ef861692c398 (\"vhost: add packed ring batch enqueue\")\nCc: stable@dpdk.org\n\nSigned-off-by: Marvin Liu <yong.liu@intel.com>\nReviewed-by: Maxime Coquelin <maxime.coquelin@redhat.com>", "target": 1, "dataset": "other", "idx": 207646}
{"func": "\t\t\t\t\t\t  descs[avail_idx + i].addr,\n\t\t\t\t\t\t  &lens[i],\n\t\t\t\t\t\t  VHOST_ACCESS_RW);\n\n\tvhost_for_each_try_unroll(i, 0, PACKED_BATCH_SIZE) {\n\t\tif (unlikely(!desc_addrs[i]))\n\t\t\treturn -1;\n\t\tif (unlikely(lens[i] != descs[avail_idx + i].len))\n\t\t\treturn -1;\n\t}\n\n\tvhost_for_each_try_unroll(i, 0, PACKED_BATCH_SIZE) {", "project": "dpdk", "hash": 198754053444601304377848158125427004349, "size": 80, "commit_id": "97ecc1c85c95c13bc66a87435758e93406c35c48", "message": "vhost: fix translated address not checked\n\nMalicious guest can construct desc with invalid address and zero buffer\nlength. That will request vhost to check both translated address and\ntranslated data length. This patch will add missed address check.\n\nCVE-2020-10725\nFixes: 75ed51697820 (\"vhost: add packed ring batch dequeue\")\nFixes: ef861692c398 (\"vhost: add packed ring batch enqueue\")\nCc: stable@dpdk.org\n\nSigned-off-by: Marvin Liu <yong.liu@intel.com>\nReviewed-by: Maxime Coquelin <maxime.coquelin@redhat.com>", "target": 0, "dataset": "other", "idx": 399931}
{"func": "    u_int *secondsp, int *confirmp, char **sk_providerp)\n{\n\tu_char ctype;\n\tint r;\n\tu_int seconds, maxsign = 0;\n\tchar *ext_name = NULL;\n\tstruct sshbuf *b = NULL;\n\n\twhile (sshbuf_len(m)) {\n\t\tif ((r = sshbuf_get_u8(m, &ctype)) != 0) {\n\t\t\terror_fr(r, \"parse constraint type\");\n\t\t\tgoto err;\n\t\t}\n\t\tswitch (ctype) {\n\t\tcase SSH_AGENT_CONSTRAIN_LIFETIME:\n\t\t\tif (*deathp != 0) {\n\t\t\t\terror_f(\"lifetime already set\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif ((r = sshbuf_get_u32(m, &seconds)) != 0) {\n\t\t\t\terror_fr(r, \"parse lifetime constraint\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\t*deathp = monotime() + seconds;\n\t\t\t*secondsp = seconds;\n\t\t\tbreak;\n\t\tcase SSH_AGENT_CONSTRAIN_CONFIRM:\n\t\t\tif (*confirmp != 0) {\n\t\t\t\terror_f(\"confirm already set\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\t*confirmp = 1;\n\t\t\tbreak;\n\t\tcase SSH_AGENT_CONSTRAIN_MAXSIGN:\n\t\t\tif (k == NULL) {\n\t\t\t\terror_f(\"maxsign not valid here\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (maxsign != 0) {\n\t\t\t\terror_f(\"maxsign already set\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif ((r = sshbuf_get_u32(m, &maxsign)) != 0) {\n\t\t\t\terror_fr(r, \"parse maxsign constraint\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif ((r = sshkey_enable_maxsign(k, maxsign)) != 0) {\n\t\t\t\terror_fr(r, \"enable maxsign\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SSH_AGENT_CONSTRAIN_EXTENSION:\n\t\t\tif ((r = sshbuf_get_cstring(m, &ext_name, NULL)) != 0) {\n\t\t\t\terror_fr(r, \"parse constraint extension\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tdebug_f(\"constraint ext %s\", ext_name);\n\t\t\tif (strcmp(ext_name, \"sk-provider@openssh.com\") == 0) {\n\t\t\t\tif (sk_providerp == NULL) {\n\t\t\t\t\terror_f(\"%s not valid here\", ext_name);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t\tif (*sk_providerp != NULL) {\n\t\t\t\t\terror_f(\"%s already set\", ext_name);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t\tif ((r = sshbuf_get_cstring(m,\n\t\t\t\t    sk_providerp, NULL)) != 0) {\n\t\t\t\t\terror_fr(r, \"parse %s\", ext_name);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\terror_f(\"unsupported constraint \\\"%s\\\"\",\n\t\t\t\t    ext_name);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tfree(ext_name);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terror_f(\"Unknown constraint %d\", ctype);\n err:\n\t\t\tfree(ext_name);\n\t\t\tsshbuf_free(b);\n\t\t\treturn -1;\n\t\t}\n\t}\n\t/* success */\n\treturn 0;\n}", "project": "openssh-portable", "hash": 68211468467426422101137350916636802558, "size": 90, "commit_id": "e04fd6dde16de1cdc5a4d9946397ff60d96568db", "message": "upstream: factor SSH_AGENT_CONSTRAIN_EXTENSION parsing into its own\n\nfunction and remove an unused variable; ok dtucker@\n\nOpenBSD-Commit-ID: e1a938657fbf7ef0ba5e73b30365734a0cc96559", "target": 1, "dataset": "other", "idx": 207709}
{"func": "parse_key_constraints(struct sshbuf *m, struct sshkey *k, time_t *deathp,\n    u_int *secondsp, int *confirmp, char **sk_providerp)\n{\n\tu_char ctype;\n\tint r;\n\tu_int seconds, maxsign = 0;\n\n\twhile (sshbuf_len(m)) {\n\t\tif ((r = sshbuf_get_u8(m, &ctype)) != 0) {\n\t\t\terror_fr(r, \"parse constraint type\");\n\t\t\tgoto out;\n\t\t}\n\t\tswitch (ctype) {\n\t\tcase SSH_AGENT_CONSTRAIN_LIFETIME:\n\t\t\tif (*deathp != 0) {\n\t\t\t\terror_f(\"lifetime already set\");\n\t\t\t\tr = SSH_ERR_INVALID_FORMAT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif ((r = sshbuf_get_u32(m, &seconds)) != 0) {\n\t\t\t\terror_fr(r, \"parse lifetime constraint\");\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t*deathp = monotime() + seconds;\n\t\t\t*secondsp = seconds;\n\t\t\tbreak;\n\t\tcase SSH_AGENT_CONSTRAIN_CONFIRM:\n\t\t\tif (*confirmp != 0) {\n\t\t\t\terror_f(\"confirm already set\");\n\t\t\t\tr = SSH_ERR_INVALID_FORMAT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t*confirmp = 1;\n\t\t\tbreak;\n\t\tcase SSH_AGENT_CONSTRAIN_MAXSIGN:\n\t\t\tif (k == NULL) {\n\t\t\t\terror_f(\"maxsign not valid here\");\n\t\t\t\tr = SSH_ERR_INVALID_FORMAT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (maxsign != 0) {\n\t\t\t\terror_f(\"maxsign already set\");\n\t\t\t\tr = SSH_ERR_INVALID_FORMAT;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif ((r = sshbuf_get_u32(m, &maxsign)) != 0) {\n\t\t\t\terror_fr(r, \"parse maxsign constraint\");\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif ((r = sshkey_enable_maxsign(k, maxsign)) != 0) {\n\t\t\t\terror_fr(r, \"enable maxsign\");\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase SSH_AGENT_CONSTRAIN_EXTENSION:\n\t\t\tif ((r = parse_key_constraint_extension(m,\n\t\t\t    sk_providerp)) != 0)\n\t\t\t\tgoto out; /* error already logged */\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terror_f(\"Unknown constraint %d\", ctype);\n\t\t\tr = SSH_ERR_FEATURE_UNSUPPORTED;\n\t\t\tgoto out;\n\t\t}\n\t}\n\t/* success */\n\tr = 0;\n out:\n\treturn r;\n}", "project": "openssh-portable", "hash": 302547089865800792591833484855608026826, "size": 70, "commit_id": "e04fd6dde16de1cdc5a4d9946397ff60d96568db", "message": "upstream: factor SSH_AGENT_CONSTRAIN_EXTENSION parsing into its own\n\nfunction and remove an unused variable; ok dtucker@\n\nOpenBSD-Commit-ID: e1a938657fbf7ef0ba5e73b30365734a0cc96559", "target": 0, "dataset": "other", "idx": 400219}
{"func": "       global decoder data and store it under the .jbig2globalctx key\n     */\n    s_jbig2decode_set_global_data((stream_state*)&state, NULL);\n    if (r_has_type(op, t_dictionary)) {\n        check_dict_read(*op);\n        if ( dict_find_string(op, \".jbig2globalctx\", &sop) > 0) {\n            gref = r_ptr(sop, s_jbig2_global_data_t);\n            s_jbig2decode_set_global_data((stream_state*)&state, gref);\n        }\n    }\n", "project": "ghostpdl", "hash": 196694238274389223071484648318752658727, "size": 27, "commit_id": "ef252e7dc214bcbd9a2539216aab9202848602bb", "message": "Bug #700168 - add a type check\n\nBug #700168 \"Type confusion in JBIG2Decode\"\n\nThe code was assuming that .jbig2globalctx was a structure allocated\nby the graphics library, without checking.\n\nAdd a check to see that it is a structure and that its the correct\ntype of structure.", "target": 1, "dataset": "other", "idx": 207760}
{"func": "     */\n    s_jbig2decode_set_global_data((stream_state*)&state, NULL);\n    if (r_has_type(op, t_dictionary)) {\n        check_dict_read(*op);\n        if ( dict_find_string(op, \".jbig2globalctx\", &sop) > 0) {\n            if (!r_is_struct(sop) || !r_has_stype(sop, imemory, st_jbig2_global_data_t))\n                return_error(gs_error_typecheck);\n            gref = r_ptr(sop, s_jbig2_global_data_t);\n            s_jbig2decode_set_global_data((stream_state*)&state, gref);\n        }\n    }\n", "project": "ghostpdl", "hash": 88125760258340032082692592349055443353, "size": 29, "commit_id": "ef252e7dc214bcbd9a2539216aab9202848602bb", "message": "Bug #700168 - add a type check\n\nBug #700168 \"Type confusion in JBIG2Decode\"\n\nThe code was assuming that .jbig2globalctx was a structure allocated\nby the graphics library, without checking.\n\nAdd a check to see that it is a structure and that its the correct\ntype of structure.", "target": 0, "dataset": "other", "idx": 400829}
{"func": "ReadFromRFBServer(rfbClient* client, char *out, unsigned int n)\n{\n#undef DEBUG_READ_EXACT\n#ifdef DEBUG_READ_EXACT\n\tchar* oout=out;\n\tunsigned int nn=n;\n\trfbClientLog(\"ReadFromRFBServer %d bytes\\n\",n);\n      }\n#endif\n  \n      if (i <= 0) {\n\tif (i < 0) {\n\t  if (errno == EWOULDBLOCK || errno == EAGAIN) {\n\t    /* TODO:\n\t       ProcessXtEvents();\n\t    */\n\t    WaitForMessage(client, 100000);\n\t    i = 0;\n\t  } else {\n\t    rfbClientErr(\"read (%d: %s)\\n\",errno,strerror(errno));\n\t    return FALSE;\n\t  }\n      if (i <= 0) {\n\tif (i < 0) {\n#ifdef WIN32\n\t  errno=WSAGetLastError();\n#endif\n\t  if (errno == EWOULDBLOCK || errno == EAGAIN) {\n\t    /* TODO:\n\t       ProcessXtEvents();\n\t    */\n\t    WaitForMessage(client, 100000);\n\t    i = 0;\n\t  } else {\n\t    rfbClientErr(\"read (%s)\\n\",strerror(errno));\n\t    return FALSE;\n\t  }", "project": "libvncserver", "hash": 56627314386826006638045179713369898227, "size": 166, "commit_id": "57433015f856cc12753378254ce4f1c78f5d9c7b", "message": "libvncclient: handle half-open TCP connections\n\nWhen a connection is not reset properly at the TCP level (e.g. sudden\npower loss or process crash) the TCP connection becomes half-open and\nread() always returns -1 with errno = EAGAIN while select() always\nreturns 0. This leads to an infinite loop and can be fixed by closing\nthe connection after a certain number of retries (based on a timeout)\nhas been exceeded.", "target": 1, "dataset": "other", "idx": 207769}
{"func": "ReadFromRFBServer(rfbClient* client, char *out, unsigned int n)\n{\n  const int USECS_WAIT_PER_RETRY = 100000;\n  int retries = 0;\n#undef DEBUG_READ_EXACT\n#ifdef DEBUG_READ_EXACT\n\tchar* oout=out;\n\tunsigned int nn=n;\n\trfbClientLog(\"ReadFromRFBServer %d bytes\\n\",n);\n#endif\n  \n      if (i <= 0) {\n\tif (i < 0) {\n\t  if (errno == EWOULDBLOCK || errno == EAGAIN) {\n\t    if (client->readTimeout > 0 &&\n\t\t++retries > (client->readTimeout * 1000 * 1000 / USECS_WAIT_PER_RETRY))\n\t    {\n\t      rfbClientLog(\"Connection timed out\\n\");\n\t      return FALSE;\n\t    }\n\t    /* TODO:\n\t       ProcessXtEvents();\n\t    */\n\t    WaitForMessage(client, USECS_WAIT_PER_RETRY);\n\t    i = 0;\n\t  } else {\n\t    rfbClientErr(\"read (%d: %s)\\n\",errno,strerror(errno));\n\t    return FALSE;\n\t  }\n\tif (i < 0) {\n#ifdef WIN32\n\t  errno=WSAGetLastError();\n#endif\n\t  if (errno == EWOULDBLOCK || errno == EAGAIN) {\n\t    if (client->readTimeout > 0 &&\n\t\t++retries > (client->readTimeout * 1000 * 1000 / USECS_WAIT_PER_RETRY))\n\t    {\n\t\trfbClientLog(\"Connection timed out\\n\");\n\t\treturn FALSE;\n\t    }\n\t    /* TODO:\n\t       ProcessXtEvents();\n\t    */\n\t    WaitForMessage(client, USECS_WAIT_PER_RETRY);\n\t    i = 0;\n\t  } else {\n\t    rfbClientErr(\"read (%s)\\n\",strerror(errno));\n\t    return FALSE;\n\t  }", "project": "libvncserver", "hash": 96987461285267687729476498662054608847, "size": 180, "commit_id": "57433015f856cc12753378254ce4f1c78f5d9c7b", "message": "libvncclient: handle half-open TCP connections\n\nWhen a connection is not reset properly at the TCP level (e.g. sudden\npower loss or process crash) the TCP connection becomes half-open and\nread() always returns -1 with errno = EAGAIN while select() always\nreturns 0. This leads to an infinite loop and can be fixed by closing\nthe connection after a certain number of retries (based on a timeout)\nhas been exceeded.", "target": 0, "dataset": "other", "idx": 400906}
{"func": "\t\t\tcase MEGA_MEGA_SET_FG_RUN:\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\n\t\t\t\tif (code == LITE_SET_FG_FG_RUN || code == MEGA_MEGA_SET_FG_RUN)\n\t\t\t\t{\n\t\t\t\t\tSRCREADPIXEL(fgPel, pbSrc);\n\t\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\t}\n\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength))\n\n\t\t\t/* Handle Dithered Run Orders. */\n\t\t\tcase LITE_DITHERED_RUN:\n\t\t\tcase MEGA_MEGA_DITHERED_RUN:\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\t\t\t\tSRCREADPIXEL(pixelA, pbSrc);\n\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\tSRCREADPIXEL(pixelB, pbSrc);\n\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength * 2))\n\t\t\t\t\treturn FALSE;\n\n\t\t\t/* Handle Color Run Orders. */\n\t\t\tcase REGULAR_COLOR_RUN:\n\t\t\tcase MEGA_MEGA_COLOR_RUN:\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\t\t\t\tSRCREADPIXEL(pixelA, pbSrc);\n\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength))\n\t\t\t\t\treturn FALSE;\n\t\t\tcase MEGA_MEGA_FGBG_IMAGE:\n\t\t\tcase LITE_SET_FG_FGBG_IMAGE:\n\t\t\tcase MEGA_MEGA_SET_FGBG_IMAGE:\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\n\t\t\t\tif (code == LITE_SET_FG_FGBG_IMAGE || code == MEGA_MEGA_SET_FGBG_IMAGE)\n\t\t\t\t{\n\t\t\t\t\tSRCREADPIXEL(fgPel, pbSrc);\n\t\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\t}\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength))\n\t\t\t\t\treturn FALSE;\n\n\t\t\t\tUNROLL(runLength, {\n\t\t\t\t\tSRCREADPIXEL(temp, pbSrc);\n\t\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\t\tDESTWRITEPIXEL(pbDest, temp);\n\t\t\t\t\tDESTNEXTPIXEL(pbDest);\n\t\t\t\t});", "project": "FreeRDP", "hash": 9826210098596619583118666328681710851, "size": 329, "commit_id": "0a98c450c58ec150e44781c89aa6f8e7e0f571f5", "message": "Fixed out of bound read in RLEDECOMPRESS\n\nCVE-2020-4033 thanks to @antonio-morales for finding this.", "target": 1, "dataset": "other", "idx": 207836}
{"func": "\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\n\t\t\t\tif (code == LITE_SET_FG_FG_RUN || code == MEGA_MEGA_SET_FG_RUN)\n\t\t\t\t{\n\t\t\t\t\tif (pbSrc >= pbEnd)\n\t\t\t\t\t\treturn FALSE;\n\t\t\t\t\tSRCREADPIXEL(fgPel, pbSrc);\n\t\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\t}\n\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength))\n\t\t\t/* Handle Dithered Run Orders. */\n\t\t\tcase LITE_DITHERED_RUN:\n\t\t\tcase MEGA_MEGA_DITHERED_RUN:\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\t\t\t\tif (pbSrc >= pbEnd)\n\t\t\t\t\treturn FALSE;\n\t\t\t\tSRCREADPIXEL(pixelA, pbSrc);\n\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\tif (pbSrc >= pbEnd)\n\t\t\t\t\treturn FALSE;\n\t\t\t\tSRCREADPIXEL(pixelB, pbSrc);\n\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength * 2))\n\t\t\t\t\treturn FALSE;\n\t\t\t/* Handle Color Run Orders. */\n\t\t\tcase REGULAR_COLOR_RUN:\n\t\t\tcase MEGA_MEGA_COLOR_RUN:\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\t\t\t\tif (pbSrc >= pbEnd)\n\t\t\t\t\treturn FALSE;\n\t\t\t\tSRCREADPIXEL(pixelA, pbSrc);\n\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength))\n\t\t\t\t\treturn FALSE;\n\t\t\tcase LITE_SET_FG_FGBG_IMAGE:\n\t\t\tcase MEGA_MEGA_SET_FGBG_IMAGE:\n\t\t\t\trunLength = ExtractRunLength(code, pbSrc, &advance);\n\t\t\t\tpbSrc = pbSrc + advance;\n\n\t\t\t\tif (pbSrc >= pbEnd)\n\t\t\t\t\treturn FALSE;\n\t\t\t\tif (code == LITE_SET_FG_FGBG_IMAGE || code == MEGA_MEGA_SET_FGBG_IMAGE)\n\t\t\t\t{\n\t\t\t\t\tSRCREADPIXEL(fgPel, pbSrc);\n\t\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\t}\n\t\t\t\tpbSrc = pbSrc + advance;\n\t\t\t\tif (!ENSURE_CAPACITY(pbDest, pbDestEnd, runLength))\n\t\t\t\t\treturn FALSE;\n\n\t\t\t\tUNROLL(runLength, {\n\t\t\t\t\tif (pbSrc >= pbEnd)\n\t\t\t\t\t\treturn FALSE;\n\t\t\t\t\tSRCREADPIXEL(temp, pbSrc);\n\t\t\t\t\tSRCNEXTPIXEL(pbSrc);\n\t\t\t\t\tDESTWRITEPIXEL(pbDest, temp);\n\t\t\t\t\tDESTNEXTPIXEL(pbDest);\n\t\t\t\t});", "project": "FreeRDP", "hash": 37817883328068271668101554775017217633, "size": 341, "commit_id": "0a98c450c58ec150e44781c89aa6f8e7e0f571f5", "message": "Fixed out of bound read in RLEDECOMPRESS\n\nCVE-2020-4033 thanks to @antonio-morales for finding this.", "target": 0, "dataset": "other", "idx": 402135}
{"func": "        if (align_mem(mem) < 0) {\n            hts_log_error(\"Memory allocation failure at %s:%\"PRIhts_pos, bcf_seqname_safe(h,v), v->pos+1);\n            v->errcode |= BCF_ERR_LIMITS;\n            return -1;\n        }\n        f->offset = mem->l;\n\n        // Limit the total memory to ~2Gb per VCF row.  This should mean\n        // malformed VCF data is less likely to take excessive memory and/or\n        // time.\n        if (v->n_sample * (uint64_t)f->size > INT_MAX) {\n            hts_log_error(\"Excessive memory required by FORMAT fields at %s:%\"PRIhts_pos, bcf_seqname_safe(h,v), v->pos+1);\n            v->errcode |= BCF_ERR_LIMITS;\n            return -1;\n        }\n        if (ks_resize(mem, mem->l + v->n_sample * (size_t)f->size) < 0) {\n            hts_log_error(\"Memory allocation failure at %s:%\"PRIhts_pos, bcf_seqname_safe(h,v), v->pos+1);\n            v->errcode |= BCF_ERR_LIMITS;\n            return -1;\n        }", "project": "htslib", "hash": 315712016709704795623581796512547762288, "size": 371, "commit_id": "dcd4b7304941a8832fba2d0fc4c1e716e7a4e72c", "message": "Fix check for VCF record size\n\nThe check for excessive record size in vcf_parse_format() only\nlooked at individual fields.  It was therefore possible to\nexceed the limit and overflow fmt_aux_t::offset by having\nmultiple fields with a combined size that went over INT_MAX.\nFix by including the amount of memory used so far in the check.\n\nCredit to OSS-Fuzz\nFixes oss-fuzz 24097", "target": 1, "dataset": "other", "idx": 207837}
{"func": "        }\n        if (align_mem(mem) < 0) {\n            hts_log_error(\"Memory allocation failure at %s:%\"PRIhts_pos, bcf_seqname_safe(h,v), v->pos+1);\n            v->errcode |= BCF_ERR_LIMITS;\n            return -1;\n        }\n\n        // Limit the total memory to ~2Gb per VCF row.  This should mean\n        // malformed VCF data is less likely to take excessive memory and/or\n        // time.\n        if ((uint64_t) mem->l + v->n_sample * (uint64_t)f->size > INT_MAX) {\n            hts_log_error(\"Excessive memory required by FORMAT fields at %s:%\"PRIhts_pos, bcf_seqname_safe(h,v), v->pos+1);\n            v->errcode |= BCF_ERR_LIMITS;\n            return -1;\n        }\n\n        f->offset = mem->l;\n        if (ks_resize(mem, mem->l + v->n_sample * (size_t)f->size) < 0) {\n            hts_log_error(\"Memory allocation failure at %s:%\"PRIhts_pos, bcf_seqname_safe(h,v), v->pos+1);\n            v->errcode |= BCF_ERR_LIMITS;\n            return -1;\n        }", "project": "htslib", "hash": 92847516526430128297643351147009305704, "size": 372, "commit_id": "dcd4b7304941a8832fba2d0fc4c1e716e7a4e72c", "message": "Fix check for VCF record size\n\nThe check for excessive record size in vcf_parse_format() only\nlooked at individual fields.  It was therefore possible to\nexceed the limit and overflow fmt_aux_t::offset by having\nmultiple fields with a combined size that went over INT_MAX.\nFix by including the amount of memory used so far in the check.\n\nCredit to OSS-Fuzz\nFixes oss-fuzz 24097", "target": 0, "dataset": "other", "idx": 402146}
{"func": "\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\tif (!test_bit(HCI_UP, &hdev->flags))\n\t\treturn -ENETDOWN;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}", "project": "linux", "hash": 119058947373563095041334918661267933841, "size": 16, "commit_id": "e2cb6b891ad2b8caa9131e3be70f45243df82a80", "message": "bluetooth: eliminate the potential race condition when removing the HCI controller\n\nThere is a possible race condition vulnerability between issuing a HCI\ncommand and removing the cont.  Specifically, functions hci_req_sync()\nand hci_dev_do_close() can race each other like below:\n\nthread-A in hci_req_sync()      |   thread-B in hci_dev_do_close()\n                                |   hci_req_sync_lock(hdev);\ntest_bit(HCI_UP, &hdev->flags); |\n...                             |   test_and_clear_bit(HCI_UP, &hdev->flags)\nhci_req_sync_lock(hdev);        |\n                                |\nIn this commit we alter the sequence in function hci_req_sync(). Hence,\nthe thread-A cannot issue th.\n\nSigned-off-by: Lin Ma <linma@zju.edu.cn>\nCc: Marcel Holtmann <marcel@holtmann.org>\nFixes: 7c6a329e4447 (\"[Bluetooth] Fix regression from using default link policy\")\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 1, "dataset": "other", "idx": 207859}
{"func": "int hci_req_sync(struct hci_dev *hdev, int (*req)(struct hci_request *req,\n\t\t\t\t\t\t  unsigned long opt),\n\t\t unsigned long opt, u32 timeout, u8 *hci_status)\n{\n\tint ret;\n\n\t/* Serialize all requests */\n\thci_req_sync_lock(hdev);\n\t/* check the state after obtaing the lock to protect the HCI_UP\n\t * against any races from hci_dev_do_close when the controller\n\t * gets removed.\n\t */\n\tif (test_bit(HCI_UP, &hdev->flags))\n\t\tret = __hci_req_sync(hdev, req, opt, timeout, hci_status);\n\telse\n\t\tret = -ENETDOWN;\n\thci_req_sync_unlock(hdev);\n\n\treturn ret;\n}", "project": "linux", "hash": 18084705760266005754007910834066501149, "size": 20, "commit_id": "e2cb6b891ad2b8caa9131e3be70f45243df82a80", "message": "bluetooth: eliminate the potential race condition when removing the HCI controller\n\nThere is a possible race condition vulnerability between issuing a HCI\ncommand and removing the cont.  Specifically, functions hci_req_sync()\nand hci_dev_do_close() can race each other like below:\n\nthread-A in hci_req_sync()      |   thread-B in hci_dev_do_close()\n                                |   hci_req_sync_lock(hdev);\ntest_bit(HCI_UP, &hdev->flags); |\n...                             |   test_and_clear_bit(HCI_UP, &hdev->flags)\nhci_req_sync_lock(hdev);        |\n                                |\nIn this commit we alter the sequence in function hci_req_sync(). Hence,\nthe thread-A cannot issue th.\n\nSigned-off-by: Lin Ma <linma@zju.edu.cn>\nCc: Marcel Holtmann <marcel@holtmann.org>\nFixes: 7c6a329e4447 (\"[Bluetooth] Fix regression from using default link policy\")\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 0, "dataset": "other", "idx": 402494}
{"func": "static UINT parallel_process_irp_create(PARALLEL_DEVICE* parallel, IRP* irp)\n{\n\tchar* path = NULL;\n\tint status;\n\tUINT32 PathLength;\n\tStream_Seek(irp->input, 28);\n\t/* DesiredAccess(4) AllocationSize(8), FileAttributes(4) */\n\t/* SharedAccess(4) CreateDisposition(4), CreateOptions(4) */\n\tStream_Read_UINT32(irp->input, PathLength);\n\tstatus = ConvertFromUnicode(CP_UTF8, 0, (WCHAR*)Stream_Pointer(irp->input), PathLength / 2,\n\t                            &path, 0, NULL, NULL);\n\n\tif (status < 1)\n\t\tif (!(path = (char*)calloc(1, 1)))\n\t\t{\n\t\t\tWLog_ERR(TAG, \"calloc failed!\");", "project": "FreeRDP", "hash": 91654068438972054959889615095747233707, "size": 40, "commit_id": "795842f4096501fcefc1a7f535ccc8132feb31d7", "message": "Fixed oob read in parallel_process_irp_create", "target": 1, "dataset": "other", "idx": 207960}
{"func": "static UINT parallel_process_irp_create(PARALLEL_DEVICE* parallel, IRP* irp)\n{\n\tchar* path = NULL;\n\tint status;\n\tWCHAR* ptr;\n\tUINT32 PathLength;\n\tif (!Stream_SafeSeek(irp->input, 28))\n\t\treturn ERROR_INVALID_DATA;\n\t/* DesiredAccess(4) AllocationSize(8), FileAttributes(4) */\n\t/* SharedAccess(4) CreateDisposition(4), CreateOptions(4) */\n\tif (Stream_GetRemainingLength(irp->input) < 4)\n\t\treturn ERROR_INVALID_DATA;\n\tStream_Read_UINT32(irp->input, PathLength);\n\tptr = (WCHAR*)Stream_Pointer(irp->input);\n\tif (!Stream_SafeSeek(irp->input, PathLength))\n\t\treturn ERROR_INVALID_DATA;\n\tstatus = ConvertFromUnicode(CP_UTF8, 0, ptr, PathLength / 2, &path, 0, NULL, NULL);\n\n\tif (status < 1)\n\t\tif (!(path = (char*)calloc(1, 1)))\n\t\t{\n\t\t\tWLog_ERR(TAG, \"calloc failed!\");", "project": "FreeRDP", "hash": 213401734041482676778583216895571372326, "size": 46, "commit_id": "795842f4096501fcefc1a7f535ccc8132feb31d7", "message": "Fixed oob read in parallel_process_irp_create", "target": 0, "dataset": "other", "idx": 403469}
{"func": "\ttorture_suite_add_1smb2_test(suite, \"rmdir2\",\n\t\t\t\t     torture_smb2_notify_rmdir2);\n\ttorture_suite_add_2smb2_test(suite, \"rmdir3\",\n\t\t\t\t     torture_smb2_notify_rmdir3);\n\ttorture_suite_add_2smb2_test(suite, \"rmdir4\",\n\t\t\t\t     torture_smb2_notify_rmdir4);\n\n\tsuite->description = talloc_strdup(suite, \"SMB2-NOTIFY tests\");\n\n\treturn suite;\n}", "project": "samba", "hash": 270007973625551949002175375598998346622, "size": 35, "commit_id": "f100bd2f2e4f047942002a992c99104227a17f81", "message": "s4: torture: Add smb2.notify.handle-permissions test.\n\nAdd knownfail entry.\n\nCVE-2020-14318\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=14434\n\nSigned-off-by: Jeremy Allison <jra@samba.org>", "target": 1, "dataset": "other", "idx": 207975}
{"func": "\t\t\t\t     torture_smb2_notify_rmdir2);\n\ttorture_suite_add_2smb2_test(suite, \"rmdir3\",\n\t\t\t\t     torture_smb2_notify_rmdir3);\n\ttorture_suite_add_2smb2_test(suite, \"rmdir4\",\n\t\t\t\t     torture_smb2_notify_rmdir4);\n\ttorture_suite_add_1smb2_test(suite,\n\t\t\t\t    \"handle-permissions\",\n\t\t\t\t    torture_smb2_notify_handle_permissions);\n\n\tsuite->description = talloc_strdup(suite, \"SMB2-NOTIFY tests\");\n\n\treturn suite;\n}", "project": "samba", "hash": 19139210488759600268757183606179666736, "size": 38, "commit_id": "f100bd2f2e4f047942002a992c99104227a17f81", "message": "s4: torture: Add smb2.notify.handle-permissions test.\n\nAdd knownfail entry.\n\nCVE-2020-14318\n\nBUG: https://bugzilla.samba.org/show_bug.cgi?id=14434\n\nSigned-off-by: Jeremy Allison <jra@samba.org>", "target": 0, "dataset": "other", "idx": 403809}
{"func": "\t/* Sanity checks */\n\tif (o >= ds) {\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\", \"Bogus thumbnail offset (%u).\", o);\n\t\treturn;\n\t}\n\tif (s > ds - o) {\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\", \"Bogus thumbnail size (%u), max would be %u.\", s, ds-o);\n\t\treturn;\n\t}\n\tif (data->data) \n\t\texif_mem_free (data->priv->mem, data->data);", "project": "libexif", "hash": 204041437380593833360882392912459353328, "size": 22, "commit_id": "ce03ad7ef4e8aeefce79192bf5b6f69fae396f0c", "message": "fixed another unsigned integer overflow\n\nfirst fixed by google in android fork,\nhttps://android.googlesource.com/platform/external/libexif/+/1e187b62682ffab5003c702657d6d725b4278f16%5E%21/#F0\n\n(use a more generic overflow check method, also check second overflow instance.)\n\nhttps://security-tracker.debian.org/tracker/CVE-2020-0198", "target": 1, "dataset": "other", "idx": 207982}
{"func": "\t/* Sanity checks */\n\tif (o >= ds) {\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\", \"Bogus thumbnail offset (%u).\", o);\n\t\treturn;\n\t}\n\tif (CHECKOVERFLOW(o,ds,s)) {\n\t\texif_log (data->priv->log, EXIF_LOG_CODE_DEBUG, \"ExifData\", \"Bogus thumbnail size (%u), max would be %u.\", s, ds-o);\n\t\treturn;\n\t}\n\tif (data->data) \n\t\texif_mem_free (data->priv->mem, data->data);", "project": "libexif", "hash": 93334110242984400786615733042597988830, "size": 22, "commit_id": "ce03ad7ef4e8aeefce79192bf5b6f69fae396f0c", "message": "fixed another unsigned integer overflow\n\nfirst fixed by google in android fork,\nhttps://android.googlesource.com/platform/external/libexif/+/1e187b62682ffab5003c702657d6d725b4278f16%5E%21/#F0\n\n(use a more generic overflow check method, also check second overflow instance.)\n\nhttps://security-tracker.debian.org/tracker/CVE-2020-0198", "target": 0, "dataset": "other", "idx": 404043}
{"func": "\t * set, but the kvm_get_pfn/kvm_release_pfn_clean pair will\n\t * simply do nothing for reserved pfns.\n\t *\n\t * Whoever called remap_pfn_range is also going to call e.g.\n\t * unmap_mapping_range before the underlying pages are freed,\n\t * causing a call to our MMU notifier.\n\t */ \n\tkvm_get_pfn(pfn);\n\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\t*p_pfn = pfn;\n\treturn 0;\n}", "project": "linux", "hash": 260764785426289038088174052083851102831, "size": 57, "commit_id": "f8be156be163a052a067306417cd0ff679068c97", "message": "KVM: do not allow mapping valid but non-reference-counted pages\n\nIt's possible to create a region which maps valid but non-refcounted\npages (e.g., tail pages of non-compound higher order allocations). These\nhost pages can then be returned by gfn_to_page, gfn_to_pfn, etc., family\nof APIs, which take a reference to the page, which takes it from 0 to 1.\nWhen the reference is dropped, this will free the page incorrectly.\n\nFix this by only taking a reference on valid pages if it was non-zero,\nwhich indicates it is participating in normal refcounting (and can be\nreleased with put_page).\n\nThis addresses CVE-2021-22543.\n\nSigned-off-by: Nicholas Piggin <npiggin@gmail.com>\nTested-by: Paolo Bonzini <pbonzini@redhat.com>\nCc: stable@vger.kernel.org\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 207984}
{"func": "\t * simply do nothing for reserved pfns.\n\t *\n\t * Whoever called remap_pfn_range is also going to call e.g.\n\t * unmap_mapping_range before the underlying pages are freed,\n\t * causing a call to our MMU notifier.\n\t *\n\t * Certain IO or PFNMAP mappings can be backed with valid\n\t * struct pages, but be allocated without refcounting e.g.,\n\t * tail pages of non-compound higher order allocations, which\n\t * would then underflow the refcount when the caller does the\n\t * required put_page. Don't allow those pages here.\n\t */ \n\tif (!kvm_try_get_pfn(pfn))\n\t\tr = -EFAULT;\n\nout:\n\tpte_unmap_unlock(ptep, ptl);\n\t*p_pfn = pfn;\n\n\treturn r;\n}", "project": "linux", "hash": 119627960290880640791677244710201346582, "size": 65, "commit_id": "f8be156be163a052a067306417cd0ff679068c97", "message": "KVM: do not allow mapping valid but non-reference-counted pages\n\nIt's possible to create a region which maps valid but non-refcounted\npages (e.g., tail pages of non-compound higher order allocations). These\nhost pages can then be returned by gfn_to_page, gfn_to_pfn, etc., family\nof APIs, which take a reference to the page, which takes it from 0 to 1.\nWhen the reference is dropped, this will free the page incorrectly.\n\nFix this by only taking a reference on valid pages if it was non-zero,\nwhich indicates it is participating in normal refcounting (and can be\nreleased with put_page).\n\nThis addresses CVE-2021-22543.\n\nSigned-off-by: Nicholas Piggin <npiggin@gmail.com>\nTested-by: Paolo Bonzini <pbonzini@redhat.com>\nCc: stable@vger.kernel.org\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 404071}
{"func": "{\n  int\t\ti;\t\t\t/* Looping var */\n  cache_t\t*wc;\t\t\t/* Current cache file */\n\n\n  for (i = web_files, wc = web_cache; i > 0; i --, wc ++)\n    if (!strcmp(wc->name, filename))\n      return (wc->url);\n\n  return (filename);\n}", "project": "htmldoc", "hash": 37144065587419517647394917709524696999, "size": 12, "commit_id": "369b2ea1fd0d0537ba707f20a2f047b6afd2fbdc", "message": "Fix JPEG error handling (Issue #415)", "target": 1, "dataset": "other", "idx": 207988}
{"func": "  int\t\ti;\t\t\t/* Looping var */\n  cache_t\t*wc;\t\t\t/* Current cache file */\n\n\n  for (i = web_files, wc = web_cache; i > 0; i --, wc ++)\n  {\n    if (!strcmp(wc->name, filename))\n    {\n      if (!strncmp(wc->url, \"data:\", 5))\n        return (\"data URL\");\n      else\n        return (wc->url);\n    }\n  }\n\n  return (filename);\n}", "project": "htmldoc", "hash": 5027088153078274175236805576153910500, "size": 19, "commit_id": "369b2ea1fd0d0537ba707f20a2f047b6afd2fbdc", "message": "Fix JPEG error handling (Issue #415)", "target": 0, "dataset": "other", "idx": 404189}
{"func": "  message[n]='\\0';\n  if (n > 0)\n    {\n      svg_info->parser=xmlCreatePushParserCtxt(sax_handler,svg_info,(char *)\n        message,n,image->filename);\n      option=GetImageOption(image_info,\"svg:xml-parse-huge\");\n      if ((option != (char *) NULL) && (IsStringTrue(option) != MagickFalse))\n        (void) xmlCtxtUseOptions(svg_info->parser,XML_PARSE_HUGE);\n      while ((n=ReadBlob(image,MagickPathExtent-1,message)) != 0)\n      {\n        message[n]='\\0';\n        status=xmlParseChunk(svg_info->parser,(char *) message,(int) n,0);\n        if (status != 0)\n          break;\n      }\n    }\n  (void) xmlParseChunk(svg_info->parser,(char *) message,0,1);\n  SVGEndDocument(svg_info);\n  if (svg_info->parser->myDoc != (xmlDocPtr) NULL)\n    xmlFreeDoc(svg_info->parser->myDoc);", "project": "ImageMagick", "hash": 25217158321971555438423291073333068574, "size": 492, "commit_id": "43dfb1894761c4929d5d5c98dc80ba4e59a0d114", "message": "Handle null pointer return from call to xmlCreatePushParserCtxt (#2624).", "target": 1, "dataset": "other", "idx": 208034}
{"func": "  message[n]='\\0';\n  if (n > 0)\n    {\n      svg_info->parser=xmlCreatePushParserCtxt(sax_handler,svg_info,(char *)\n        message,n,image->filename);\n      if (svg_info->parser != (xmlParserCtxtPtr) NULL)\n        {\n          option=GetImageOption(image_info,\"svg:xml-parse-huge\");\n          if ((option != (char *) NULL) && (IsStringTrue(option) != MagickFalse))\n            (void) xmlCtxtUseOptions(svg_info->parser,XML_PARSE_HUGE);\n          while ((n=ReadBlob(image,MagickPathExtent-1,message)) != 0)\n          {\n            message[n]='\\0';\n            status=xmlParseChunk(svg_info->parser,(char *) message,(int) n,0);\n            if (status != 0)\n              break;\n          }\n        }\n    }\n  if (svg_info->parser == (xmlParserCtxtPtr) NULL)\n    {\n      svg_info=DestroySVGInfo(svg_info);\n      (void) RelinquishUniqueFileResource(filename);\n      image=DestroyImage(image);\n      return((Image *) NULL);\n    }\n  (void) xmlParseChunk(svg_info->parser,(char *) message,0,1);\n  SVGEndDocument(svg_info);\n  if (svg_info->parser->myDoc != (xmlDocPtr) NULL)\n    xmlFreeDoc(svg_info->parser->myDoc);", "project": "ImageMagick", "hash": 69475010542433184360573770927608777665, "size": 502, "commit_id": "43dfb1894761c4929d5d5c98dc80ba4e59a0d114", "message": "Handle null pointer return from call to xmlCreatePushParserCtxt (#2624).", "target": 0, "dataset": "other", "idx": 404419}
{"func": "    EVP_PKEY              *pkey = NULL;\n    EVP_PKEY_CTX          *ctx = NULL;\n    const EVP_MD          *md = NULL;\n    const char            *digestname;\n    size_t                 outlen;\n    unsigned char         *tmp = NULL;\n\n    // Make sure that the necessary parameters are provided\n    pAssert(cIn != NULL && dOut != NULL && key != NULL);\n    // Size is checked to make sure that the encrypted value is the right size\n    if(cIn->size != key->publicArea.unique.rsa.t.size)\n\t  default:\n            ERROR_RETURN(TPM_RC_SCHEME);\n            break;\n\t}\n\n    outlen = cIn->size;\n    if (EVP_PKEY_decrypt(ctx, dOut->buffer, &outlen,\n                         cIn->buffer, cIn->size) <= 0)\n        ERROR_RETURN(TPM_RC_FAILURE);\n\n    dOut->size = outlen;\n\n    retVal = TPM_RC_SUCCESS;\n\n Exit:", "project": "libtpms", "hash": 336056029318474660269537136446607856601, "size": 85, "commit_id": "40cfe134c017d3aeaaed05ce71eaf9bfbe556b16", "message": "tpm2: Fix output buffer parameter and size for RSA decryption\n\nFor the RSA decryption we have to use an output buffer of the size of the\n(largest possible) RSA key for the decryption to always work.\n\nThis fixes a stack corruption bug that caused a SIGBUS and termination of\n'swtpm'.\n\nSigned-off-by: Stefan Berger <stefanb@linux.ibm.com>", "target": 1, "dataset": "other", "idx": 208069}
{"func": "    EVP_PKEY_CTX          *ctx = NULL;\n    const EVP_MD          *md = NULL;\n    const char            *digestname;\n    size_t                 outlen;\n    unsigned char         *tmp = NULL;\n    unsigned char          buffer[MAX_RSA_KEY_BYTES];\n\n    // Make sure that the necessary parameters are provided\n    pAssert(cIn != NULL && dOut != NULL && key != NULL);\n    // Size is checked to make sure that the encrypted value is the right size\n    if(cIn->size != key->publicArea.unique.rsa.t.size)\n\t  default:\n            ERROR_RETURN(TPM_RC_SCHEME);\n            break;\n\t}\n\n    /* cannot use cOut->buffer */\n    outlen = sizeof(buffer);\n    if (EVP_PKEY_decrypt(ctx, buffer, &outlen,\n                         cIn->buffer, cIn->size) <= 0)\n        ERROR_RETURN(TPM_RC_FAILURE);\n\n    if (outlen > dOut->size)\n        ERROR_RETURN(TPM_RC_FAILURE);\n\n    memcpy(dOut->buffer, buffer, outlen);\n    dOut->size = outlen;\n\n    retVal = TPM_RC_SUCCESS;\n\n Exit:", "project": "libtpms", "hash": 84935986072655399087103363215779446823, "size": 91, "commit_id": "40cfe134c017d3aeaaed05ce71eaf9bfbe556b16", "message": "tpm2: Fix output buffer parameter and size for RSA decryption\n\nFor the RSA decryption we have to use an output buffer of the size of the\n(largest possible) RSA key for the decryption to always work.\n\nThis fixes a stack corruption bug that caused a SIGBUS and termination of\n'swtpm'.\n\nSigned-off-by: Stefan Berger <stefanb@linux.ibm.com>", "target": 0, "dataset": "other", "idx": 404492}
{"func": "    if (!convert) {\n#ifdef notdef\n        return (Imaging)ImagingError_ValueError(\"conversion not supported\");\n#else\n        static char buf[100];\n        sprintf(buf, \"conversion from %.10s to %.10s not supported\", imIn->mode, mode);\n        return (Imaging)ImagingError_ValueError(buf);\n#endif\n    }\n\n    imOut = ImagingNew2Dirty(mode, imOut, imIn);", "project": "Pillow", "hash": 106666609759821496258963236330025048119, "size": 72, "commit_id": "518ee3722a99d7f7d890db82a20bd81c1c0327fb", "message": "Use snprintf instead of sprintf", "target": 1, "dataset": "other", "idx": 208176}
{"func": "    if (!convert) {\n#ifdef notdef\n        return (Imaging)ImagingError_ValueError(\"conversion not supported\");\n#else\n        static char buf[100];\n        snprintf(buf, 100, \"conversion from %.10s to %.10s not supported\", imIn->mode, mode);\n        return (Imaging)ImagingError_ValueError(buf);\n#endif\n    }\n\n    imOut = ImagingNew2Dirty(mode, imOut, imIn);", "project": "Pillow", "hash": 250426672546487471044540466681713327260, "size": 72, "commit_id": "518ee3722a99d7f7d890db82a20bd81c1c0327fb", "message": "Use snprintf instead of sprintf", "target": 0, "dataset": "other", "idx": 406417}
{"func": "\n  /* Scale ellipse formula to directly index the Filter Lookup Table */\n  { double scale;\n#if FILTER_LUT\n    /* scale so that F = WLUT_WIDTH; -- hardcoded */\n    scale = (double)WLUT_WIDTH/F;\n#else\n    /* scale so that F = resample_filter->F (support^2) */\n    scale = resample_filter->F/F;\n#endif\n    resample_filter->A = A*scale;\n    resample_filter->B = B*scale;\n    resample_filter->C = C*scale;\n  }", "project": "ImageMagick", "hash": 231019287981565352051359680228225257165, "size": 175, "commit_id": "8d25d94a363b104acd6ff23df7470aeedb806c51", "message": "https://github.com/ImageMagick/ImageMagick/issues/3195", "target": 1, "dataset": "other", "idx": 208186}
{"func": "\n  /* Scale ellipse formula to directly index the Filter Lookup Table */\n  { double scale;\n#if FILTER_LUT\n    /* scale so that F = WLUT_WIDTH; -- hardcoded */\n    scale=(double) WLUT_WIDTH*PerceptibleReciprocal(F);\n#else\n    /* scale so that F = resample_filter->F (support^2) */\n    scale=resample_filter->F*PerceptibleReciprocal(F);\n#endif\n    resample_filter->A = A*scale;\n    resample_filter->B = B*scale;\n    resample_filter->C = C*scale;\n  }", "project": "ImageMagick", "hash": 159696077506715638875467992554706158674, "size": 175, "commit_id": "8d25d94a363b104acd6ff23df7470aeedb806c51", "message": "https://github.com/ImageMagick/ImageMagick/issues/3195", "target": 0, "dataset": "other", "idx": 406508}
{"func": "      char *empty[] = { NULL };\n      env = g_strdupv (empty);\n    }\n  else\n    env = g_get_environ ();\n\n  n_envs = g_variant_n_children (arg_envs);\n  for (i = 0; i < n_envs; i++)\n    {\n      const char *var = NULL;\n      const char *val = NULL;\n      g_variant_get_child (arg_envs, i, \"{&s&s}\", &var, &val);\n\n      env = g_environ_setenv (env, var, val, TRUE);\n    }\n\n  g_ptr_array_add (flatpak_argv, g_strdup (\"flatpak\"));\n  g_ptr_array_add (flatpak_argv, g_strdup (\"run\"));\n", "project": "flatpak", "hash": 337708073948946729047618626634843221247, "size": 608, "commit_id": "cc1401043c075268ecc652eac557ef8076b5eaba", "message": "portal: Do not use caller-supplied variables in environment\n\nIf the caller specifies a variable that can be used to inject arbitrary\ncode into processes, we must not allow it to enter the environment\nblock used to run `flatpak run`, which runs unsandboxed.\n\nThis change requires the previous commit \"context: Add --env-fd option\",\nwhich adds infrastructure used here.\n\nTo be secure, this change also requires the previous commit\n\"run: Convert all environment variables into bwrap arguments\", which\nprotects a non-setuid bwrap(1) from the same attack.\n\nSigned-off-by: Simon McVittie <smcv@collabora.com>\nPart-of: https://github.com/flatpak/flatpak/security/advisories/GHSA-4ppf-fxf6-vxg2", "target": 1, "dataset": "other", "idx": 208226}
{"func": "      env = g_strdupv (empty);\n    }\n  else\n    env = g_get_environ ();\n\n  /* Let the environment variables given by the caller override the ones\n   * from extra_args. Don't add them to @env, because they are controlled\n   * by our caller, which might be trying to use them to inject code into\n   * flatpak(1); add them to the environment block instead.\n   *\n   * We don't use --env= here, so that if the values are something that\n   * should not be exposed to other uids, they can remain confidential. */\n  n_envs = g_variant_n_children (arg_envs);\n  for (i = 0; i < n_envs; i++)\n    {\n      const char *var = NULL;\n      const char *val = NULL;\n      g_variant_get_child (arg_envs, i, \"{&s&s}\", &var, &val);\n\n      if (var[0] == '\\0')\n        {\n          g_dbus_method_invocation_return_error (invocation, G_DBUS_ERROR,\n                                                 G_DBUS_ERROR_INVALID_ARGS,\n                                                 \"Environment variable cannot have empty name\");\n          return G_DBUS_METHOD_INVOCATION_HANDLED;\n        }\n\n      if (strchr (var, '=') != NULL)\n        {\n          g_dbus_method_invocation_return_error (invocation, G_DBUS_ERROR,\n                                                 G_DBUS_ERROR_INVALID_ARGS,\n                                                 \"Environment variable name cannot contain '='\");\n          return G_DBUS_METHOD_INVOCATION_HANDLED;\n        }\n\n      g_string_append (env_string, var);\n      g_string_append_c (env_string, '=');\n      g_string_append (env_string, val);\n      g_string_append_c (env_string, '\\0');\n    }\n\n  g_ptr_array_add (flatpak_argv, g_strdup (\"flatpak\"));\n  g_ptr_array_add (flatpak_argv, g_strdup (\"run\"));\n", "project": "flatpak", "hash": 232713244041032616005904953820748692526, "size": 634, "commit_id": "cc1401043c075268ecc652eac557ef8076b5eaba", "message": "portal: Do not use caller-supplied variables in environment\n\nIf the caller specifies a variable that can be used to inject arbitrary\ncode into processes, we must not allow it to enter the environment\nblock used to run `flatpak run`, which runs unsandboxed.\n\nThis change requires the previous commit \"context: Add --env-fd option\",\nwhich adds infrastructure used here.\n\nTo be secure, this change also requires the previous commit\n\"run: Convert all environment variables into bwrap arguments\", which\nprotects a non-setuid bwrap(1) from the same attack.\n\nSigned-off-by: Simon McVittie <smcv@collabora.com>\nPart-of: https://github.com/flatpak/flatpak/security/advisories/GHSA-4ppf-fxf6-vxg2", "target": 0, "dataset": "other", "idx": 406983}
{"func": "         * This is very basic way to send packets. Ideally there should be\n         * a FIFO and packets should be sent out from FIFO only when\n         * R_CFG1 bit 0 is set.\n         */\n        if (s->regs[R_CFG1] & R_CFG1_LB_EN_MASK) {\n            nc->info->receive(nc, buf, size);\n        } else {\n            qemu_send_packet(nc, buf, size);\n        }\n        d.pktsize |= EMPTY_MASK;\n        emac_store_desc(s, &d, desc);", "project": "qemu", "hash": 266996506783650672252406401947191343006, "size": 46, "commit_id": "26194a58f4eb83c5bdf4061a1628508084450ba1", "message": "msf2-mac: switch to use qemu_receive_packet() for loopback\n\nThis patch switches to use qemu_receive_packet() which can detect\nreentrancy and return early.\n\nThis is intended to address CVE-2021-3416.\n\nCc: Prasad J Pandit <ppandit@redhat.com>\nCc: qemu-stable@nongnu.org\nReviewed-by: Philippe Mathieu-Daud\u00e9 <philmd@redhat.com>\nSigned-off-by: Jason Wang <jasowang@redhat.com>", "target": 1, "dataset": "other", "idx": 208328}
{"func": "         * This is very basic way to send packets. Ideally there should be\n         * a FIFO and packets should be sent out from FIFO only when\n         * R_CFG1 bit 0 is set.\n         */\n        if (s->regs[R_CFG1] & R_CFG1_LB_EN_MASK) {\n            qemu_receive_packet(nc, buf, size);\n        } else {\n            qemu_send_packet(nc, buf, size);\n        }\n        d.pktsize |= EMPTY_MASK;\n        emac_store_desc(s, &d, desc);", "project": "qemu", "hash": 673024745665237607007576976494892674, "size": 46, "commit_id": "26194a58f4eb83c5bdf4061a1628508084450ba1", "message": "msf2-mac: switch to use qemu_receive_packet() for loopback\n\nThis patch switches to use qemu_receive_packet() which can detect\nreentrancy and return early.\n\nThis is intended to address CVE-2021-3416.\n\nCc: Prasad J Pandit <ppandit@redhat.com>\nCc: qemu-stable@nongnu.org\nReviewed-by: Philippe Mathieu-Daud\u00e9 <philmd@redhat.com>\nSigned-off-by: Jason Wang <jasowang@redhat.com>", "target": 0, "dataset": "other", "idx": 408536}
{"func": "\tpdata = kmalloc(sizeof(*pdata), GFP_KERNEL);\n\tif (!pdata) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tret = ca8210_get_platform_data(priv->spi, pdata);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_get_platform_data failed\\n\");\n\t\tgoto error;\n\t}\n\tpriv->spi->dev.platform_data = pdata;\n\n\tret = ca8210_dev_com_init(priv);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_dev_com_init failed\\n\");\n\t\tgoto error;", "project": "linux", "hash": 140536448521281187745613931545799203059, "size": 113, "commit_id": "6402939ec86eaf226c8b8ae00ed983936b164908", "message": "ieee802154: ca8210: prevent memory leak\n\nIn ca8210_probe the allocated pdata needs to be assigned to\nspi_device->dev.platform_data before calling ca8210_get_platform_data.\nOthrwise when ca8210_get_platform_data fails pdata cannot be released.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nLink: https://lore.kernel.org/r/20190917224713.26371-1-navid.emamdoost@gmail.com\nSigned-off-by: Stefan Schmidt <stefan@datenfreihafen.org>", "target": 1, "dataset": "other", "idx": 208360}
{"func": "\tif (!pdata) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tpriv->spi->dev.platform_data = pdata;\n\tret = ca8210_get_platform_data(priv->spi, pdata);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_get_platform_data failed\\n\");\n\t\tgoto error;\n\t}\n\n\tret = ca8210_dev_com_init(priv);\n\tif (ret) {\n\t\tdev_crit(&spi_device->dev, \"ca8210_dev_com_init failed\\n\");\n\t\tgoto error;", "project": "linux", "hash": 188473089036627924087079919084020542406, "size": 113, "commit_id": "6402939ec86eaf226c8b8ae00ed983936b164908", "message": "ieee802154: ca8210: prevent memory leak\n\nIn ca8210_probe the allocated pdata needs to be assigned to\nspi_device->dev.platform_data before calling ca8210_get_platform_data.\nOthrwise when ca8210_get_platform_data fails pdata cannot be released.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nLink: https://lore.kernel.org/r/20190917224713.26371-1-navid.emamdoost@gmail.com\nSigned-off-by: Stefan Schmidt <stefan@datenfreihafen.org>", "target": 0, "dataset": "other", "idx": 408790}
{"func": "    void Jp2Image::encodeJp2Header(const DataBuf& boxBuf,DataBuf& outBuf)\n    {\n        DataBuf output(boxBuf.size_ + iccProfile_.size_ + 100); // allocate sufficient space\n        int     outlen = sizeof(Jp2BoxHeader) ; // now many bytes have we written to output?\n        int      inlen = sizeof(Jp2BoxHeader) ; // how many bytes have we read from boxBuf?\n        Jp2BoxHeader* pBox   = (Jp2BoxHeader*) boxBuf.pData_;\n        int32_t       length = getLong((byte*)&pBox->length, bigEndian);\n        int32_t       count  = sizeof (Jp2BoxHeader);\n        char*         p      = (char*) boxBuf.pData_;\n        bool          bWroteColor = false ;\n\n        while ( count < length || !bWroteColor ) {\n            Jp2BoxHeader* pSubBox = (Jp2BoxHeader*) (p+count) ;\n            if ( count < length ) {\n                subBox.length = getLong((byte*)&subBox.length, bigEndian);\n                subBox.type   = getLong((byte*)&subBox.type  , bigEndian);\n#ifdef EXIV2_DEBUG_MESSAGES\n                std::cout << \"Jp2Image::encodeJp2Header subbox: \"<< toAscii(subBox.type) << \" length = \" << subBox.length << std::endl;\n#endif\n                count        += subBox.length;\n                newBox.type   = subBox.type;\n            } else {\n                subBox.length=0;\n                newBox.type = kJp2BoxTypeColorHeader;\n                count = length;\n            }\n\n            int32_t newlen = subBox.length;\n            if ( newBox.type == kJp2BoxTypeColorHeader ) {\n                bWroteColor = true ;\n                if ( ! iccProfileDefined() ) {\n                    const char* pad   = \"\\x01\\x00\\x00\\x00\\x00\\x00\\x10\\x00\\x00\\x05\\x1cuuid\";\n                    uint32_t    psize = 15;\n                    newlen            = sizeof(newBox) + psize ;\n                    ul2Data((byte*)&newBox.length,psize      ,bigEndian);\n                    ul2Data((byte*)&newBox.type  ,newBox.type,bigEndian);\n                    ::memcpy(output.pData_+outlen                     ,&newBox            ,sizeof(newBox));\n                    ::memcpy(output.pData_+outlen+sizeof(newBox)      ,pad                ,psize         );\n                } else {\n                    const char* pad   = \"\\x02\\x00\\x00\";\n                    uint32_t    psize = 3;\n                    newlen            = sizeof(newBox) + psize + iccProfile_.size_;\n                    ul2Data((byte*)&newBox.length,newlen,bigEndian);\n                    ul2Data((byte*)&newBox.type,newBox.type,bigEndian);\n                    ::memcpy(output.pData_+outlen                     ,&newBox            ,sizeof(newBox)  );\n                    ::memcpy(output.pData_+outlen+sizeof(newBox)      , pad               ,psize           );\n                    ::memcpy(output.pData_+outlen+sizeof(newBox)+psize,iccProfile_.pData_,iccProfile_.size_);\n                }\n            } else {\n                ::memcpy(output.pData_+outlen,boxBuf.pData_+inlen,subBox.length);\n            }\n\n            outlen += newlen;\n            inlen  += subBox.length;", "project": "exiv2", "hash": 3024404238179680571348817945424819382, "size": 68, "commit_id": "f9308839198aca5e68a65194f151a1de92398f54", "message": "Better bounds checking in Jp2Image::encodeJp2Header()", "target": 1, "dataset": "other", "idx": 208379}
{"func": "    void Jp2Image::encodeJp2Header(const DataBuf& boxBuf,DataBuf& outBuf)\n    {\n        DataBuf output(boxBuf.size_ + iccProfile_.size_ + 100); // allocate sufficient space\n        long    outlen = sizeof(Jp2BoxHeader) ; // now many bytes have we written to output?\n        long    inlen = sizeof(Jp2BoxHeader) ; // how many bytes have we read from boxBuf?\n        Jp2BoxHeader* pBox   = (Jp2BoxHeader*) boxBuf.pData_;\n        uint32_t      length = getLong((byte*)&pBox->length, bigEndian);\n        uint32_t      count  = sizeof (Jp2BoxHeader);\n        char*         p      = (char*) boxBuf.pData_;\n        bool          bWroteColor = false ;\n\n        while ( count < length || !bWroteColor ) {\n            Jp2BoxHeader* pSubBox = (Jp2BoxHeader*) (p+count) ;\n                subBox.length = getLong((byte*)&subBox.length, bigEndian);\n                subBox.type   = getLong((byte*)&subBox.type  , bigEndian);\n#ifdef EXIV2_DEBUG_MESSAGES\n                std::cout << \"Jp2Image::encodeJp2Header subbox: \"<< toAscii(subBox.type) << \" length = \" << subBox.length << std::endl;\n#endif\n                enforce(subBox.length <= length - count, Exiv2::kerCorruptedMetadata);\n                count        += subBox.length;\n                newBox.type   = subBox.type;\n            } else {\n                subBox.length=0;\n                newBox.type = kJp2BoxTypeColorHeader;\n                count = length;\n            }\n\n            uint32_t newlen = subBox.length;\n            if ( newBox.type == kJp2BoxTypeColorHeader ) {\n                bWroteColor = true ;\n                if ( ! iccProfileDefined() ) {\n                    const char* pad   = \"\\x01\\x00\\x00\\x00\\x00\\x00\\x10\\x00\\x00\\x05\\x1cuuid\";\n                    uint32_t    psize = 15;\n                    newlen            = sizeof(newBox) + psize ;\n                    enforce(newlen <= output.size_ - outlen, Exiv2::kerCorruptedMetadata);\n                    ul2Data((byte*)&newBox.length,psize      ,bigEndian);\n                    ul2Data((byte*)&newBox.type  ,newBox.type,bigEndian);\n                    ::memcpy(output.pData_+outlen                     ,&newBox            ,sizeof(newBox));\n                    ::memcpy(output.pData_+outlen+sizeof(newBox)      ,pad                ,psize         );\n                } else {\n                    const char* pad   = \"\\x02\\x00\\x00\";\n                    uint32_t    psize = 3;\n                    newlen            = sizeof(newBox) + psize + iccProfile_.size_;\n                    enforce(newlen <= output.size_ - outlen, Exiv2::kerCorruptedMetadata);\n                    ul2Data((byte*)&newBox.length,newlen,bigEndian);\n                    ul2Data((byte*)&newBox.type,newBox.type,bigEndian);\n                    ::memcpy(output.pData_+outlen                     ,&newBox            ,sizeof(newBox)  );\n                    ::memcpy(output.pData_+outlen+sizeof(newBox)      , pad               ,psize           );\n                    ::memcpy(output.pData_+outlen+sizeof(newBox)+psize,iccProfile_.pData_,iccProfile_.size_);\n                }\n            } else {\n                enforce(newlen <= output.size_ - outlen, Exiv2::kerCorruptedMetadata);\n                ::memcpy(output.pData_+outlen,boxBuf.pData_+inlen,subBox.length);\n            }\n\n            outlen += newlen;\n            inlen  += subBox.length;", "project": "exiv2", "hash": 152635762460183118135898337348969828226, "size": 72, "commit_id": "f9308839198aca5e68a65194f151a1de92398f54", "message": "Better bounds checking in Jp2Image::encodeJp2Header()", "target": 0, "dataset": "other", "idx": 409137}
{"func": "exif_mnote_data_canon_load (ExifMnoteData *ne,\n\tconst unsigned char *buf, unsigned int buf_size)\n{\n\tExifMnoteDataCanon *n = (ExifMnoteDataCanon *) ne;\n\tExifShort c;\n\tsize_t i, tcount, o, datao;\n\n\tif (!n || !buf || !buf_size) {\n\t\texif_log (ne->log, EXIF_LOG_CODE_CORRUPT_DATA,\n\t\t\t  \"ExifMnoteCanon\", \"Short MakerNote\");\n\t\treturn;\n\t\t\t\tEXIF_LOG_NO_MEMORY(ne->log, \"ExifMnoteCanon\", s);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmemcpy (n->entries[tcount].data, buf + dataofs, s);\n\t\t}\n\n\t\t/* Tag was successfully parsed */\n\t\t++tcount;\n\t}\n\t/* Store the count of successfully parsed tags */\n\tn->count = tcount;", "project": "libexif", "hash": 192830977113316409961824397562079867910, "size": 103, "commit_id": "e6a38a1a23ba94d139b1fa2cd4519fdcfe3c9bab", "message": "Add a failsafe on the maximum number of Canon MakerNote subtags.\n\nA malicious file could be crafted to cause extremely large values in some\ntags without tripping any buffer range checks.  This is bad with the libexif\nrepresentation of Canon MakerNotes because some arrays are turned into\nindividual tags that the application must loop around.\n\nThe largest value I've seen for failsafe_size in a (very small) sample of valid\nCanon files is <5000.  The limit is set two orders of magnitude larger to avoid\ntripping up falsely in case some models use much larger values.\n\nPatch from Google.\n\nCVE-2020-13114", "target": 1, "dataset": "other", "idx": 208385}
{"func": "\tconst unsigned char *buf, unsigned int buf_size)\n{\n\tExifMnoteDataCanon *n = (ExifMnoteDataCanon *) ne;\n\tExifShort c;\n\tsize_t i, tcount, o, datao;\n\tlong failsafe_size = 0;\n\n\tif (!n || !buf || !buf_size) {\n\t\texif_log (ne->log, EXIF_LOG_CODE_CORRUPT_DATA,\n\t\t\t  \"ExifMnoteCanon\", \"Short MakerNote\");\n\t\treturn;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmemcpy (n->entries[tcount].data, buf + dataofs, s);\n\t\t}\n\n\t\t/* Track the size of decoded tag data. A malicious file could\n\t\t * be crafted to cause extremely large values here without\n\t\t * tripping any buffer range checks.  This is especially bad\n\t\t * with the libexif representation of Canon MakerNotes because\n\t\t * some arrays are turned into individual tags that the\n\t\t * application must loop around. */\n\t\tfailsafe_size += mnote_canon_entry_count_values(&n->entries[tcount]);\n\n\t\tif (failsafe_size > FAILSAFE_SIZE_MAX) {\n\t\t\t/* Abort if the total size of the data in the tags extraordinarily large, */\n\t\t\texif_mem_free (ne->mem, n->entries[tcount].data);\n\t\t\texif_log (ne->log, EXIF_LOG_CODE_CORRUPT_DATA,\n\t\t\t\t\t  \"ExifMnoteCanon\", \"Failsafe tag size overflow (%lu > %ld)\",\n\t\t\t\t\t  failsafe_size, FAILSAFE_SIZE_MAX);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Tag was successfully parsed */\n\t\t++tcount;\n\t}\n\t/* Store the count of successfully parsed tags */\n\tn->count = tcount;", "project": "libexif", "hash": 215870267647801111255387237724645875655, "size": 121, "commit_id": "e6a38a1a23ba94d139b1fa2cd4519fdcfe3c9bab", "message": "Add a failsafe on the maximum number of Canon MakerNote subtags.\n\nA malicious file could be crafted to cause extremely large values in some\ntags without tripping any buffer range checks.  This is bad with the libexif\nrepresentation of Canon MakerNotes because some arrays are turned into\nindividual tags that the application must loop around.\n\nThe largest value I've seen for failsafe_size in a (very small) sample of valid\nCanon files is <5000.  The limit is set two orders of magnitude larger to avoid\ntripping up falsely in case some models use much larger values.\n\nPatch from Google.\n\nCVE-2020-13114", "target": 0, "dataset": "other", "idx": 409181}
{"func": "\n      /*--- Now the selectors ---*/\n      GET_BITS(BZ_X_SELECTOR_1, nGroups, 3);\n      if (nGroups < 2 || nGroups > 6) RETURN(BZ_DATA_ERROR);\n      GET_BITS(BZ_X_SELECTOR_2, nSelectors, 15);\n      if (nSelectors < 1) RETURN(BZ_DATA_ERROR);\n      for (i = 0; i < nSelectors; i++) {\n         j = 0;\n         while (True) {\n            GET_BIT(BZ_X_SELECTOR_3, uc);\n            if (uc == 0) break;", "project": "bzip2", "hash": 165979335088151361311447542269597699517, "size": 536, "commit_id": "74de1e2e6ffc9d51ef9824db71a8ffee5962cdbc", "message": "Make sure nSelectors is not out of range\n\nnSelectors is used in a loop from 0 to nSelectors to access selectorMtf\nwhich is\n\tUChar    selectorMtf[BZ_MAX_SELECTORS];\nso if nSelectors is bigger than BZ_MAX_SELECTORS it'll do an invalid memory\naccess\n\nFixes out of bounds access discovered while fuzzying karchive", "target": 1, "dataset": "other", "idx": 208386}
{"func": "\n      /*--- Now the selectors ---*/\n      GET_BITS(BZ_X_SELECTOR_1, nGroups, 3);\n      if (nGroups < 2 || nGroups > 6) RETURN(BZ_DATA_ERROR);\n      GET_BITS(BZ_X_SELECTOR_2, nSelectors, 15);\n      if (nSelectors < 1 || nSelectors > BZ_MAX_SELECTORS) RETURN(BZ_DATA_ERROR);\n      for (i = 0; i < nSelectors; i++) {\n         j = 0;\n         while (True) {\n            GET_BIT(BZ_X_SELECTOR_3, uc);\n            if (uc == 0) break;", "project": "bzip2", "hash": 108046436633071473316758216167773719100, "size": 536, "commit_id": "74de1e2e6ffc9d51ef9824db71a8ffee5962cdbc", "message": "Make sure nSelectors is not out of range\n\nnSelectors is used in a loop from 0 to nSelectors to access selectorMtf\nwhich is\n\tUChar    selectorMtf[BZ_MAX_SELECTORS];\nso if nSelectors is bigger than BZ_MAX_SELECTORS it'll do an invalid memory\naccess\n\nFixes out of bounds access discovered while fuzzying karchive", "target": 0, "dataset": "other", "idx": 409183}
{"func": "\t\tforward = uvc_entity_by_reference(chain->dev, entity->id,\n\t\t\tforward);\n\t\tif (forward == NULL)\n\t\t\tbreak;\n\t\tif (forward == prev)\n\t\t\tcontinue;\n\n\t\tswitch (UVC_ENTITY_TYPE(forward)) {\n\t\tcase UVC_VC_EXTENSION_UNIT:\n\t\t\tif (forward->bNrInPins != 1) {\n\t\t\t\tuvc_trace(UVC_TRACE_DESCR, \"Extension unit %d \"", "project": "linux", "hash": 40134444787400205515449543582828290804, "size": 63, "commit_id": "68035c80e129c4cfec659aac4180354530b26527", "message": "media: uvcvideo: Avoid cyclic entity chains due to malformed USB descriptors\n\nWay back in 2017, fuzzing the 4.14-rc2 USB stack with syzkaller kicked\nup the following WARNING from the UVC chain scanning code:\n\n  | list_add double add: new=ffff880069084010, prev=ffff880069084010,\n  | next=ffff880067d22298.\n  | ------------[ cut here ]------------\n  | WARNING: CPU: 1 PID: 1846 at lib/list_debug.c:31 __list_add_valid+0xbd/0xf0\n  | Modules linked in:\n  | CPU: 1 PID: 1846 Comm: kworker/1:2 Not tainted\n  | 4.14.0-rc2-42613-g1488251d1a98 #238\n  | Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011\n  | Workqueue: usb_hub_wq hub_event\n  | task: ffff88006b01ca40 task.stack: ffff880064358000\n  | RIP: 0010:__list_add_valid+0xbd/0xf0 lib/list_debug.c:29\n  | RSP: 0018:ffff88006435ddd0 EFLAGS: 00010286\n  | RAX: 0000000000000058 RBX: ffff880067d22298 RCX: 0000000000000000\n  | RDX: 0000000000000058 RSI: ffffffff85a58800 RDI: ffffed000c86bbac\n  | RBP: ffff88006435dde8 R08: 1ffff1000c86ba52 R09: 0000000000000000\n  | R10: 0000000000000002 R11: 0000000000000000 R12: ffff880069084010\n  | R13: ffff880067d22298 R14: ffff880069084010 R15: ffff880067d222a0\n  | FS:  0000000000000000(0000) GS:ffff88006c900000(0000) knlGS:0000000000000000\n  | CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  | CR2: 0000000020004ff2 CR3: 000000006b447000 CR4: 00000000000006e0\n  | Call Trace:\n  |  __list_add ./include/linux/list.h:59\n  |  list_add_tail+0x8c/0x1b0 ./include/linux/list.h:92\n  |  uvc_scan_chain_forward.isra.8+0x373/0x416\n  | drivers/media/usb/uvc/uvc_driver.c:1471\n  |  uvc_scan_chain drivers/media/usb/uvc/uvc_driver.c:1585\n  |  uvc_scan_device drivers/media/usb/uvc/uvc_driver.c:1769\n  |  uvc_probe+0x77f2/0x8f00 drivers/media/usb/uvc/uvc_driver.c:2104\n\nLooking into the output from usbmon, the interesting part is the\nfollowing data packet:\n\n  ffff880069c63e00 30710169 C Ci:1:002:0 0 143 = 09028f00 01030080\n  00090403 00000e01 00000924 03000103 7c003328 010204db\n\nIf we drop the lead configuration and interface descriptors, we're left\nwith an output terminal descriptor describing a generic display:\n\n  /* Output terminal descriptor */\n  buf[0]\t09\n  buf[1]\t24\n  buf[2]\t03\t/* UVC_VC_OUTPUT_TERMINAL */\n  buf[3]\t00\t/* ID */\n  buf[4]\t01\t/* type == 0x0301 (UVC_OTT_DISPLAY) */\n  buf[5]\t03\n  buf[6]\t7c\n  buf[7]\t00\t/* source ID refers to self! */\n  buf[8]\t33\n\nThe problem with this descriptor is that it is self-referential: the\nsource ID of 0 matches itself! This causes the 'struct uvc_entity'\nrepresenting the display to be added to its chain list twice during\n'uvc_scan_chain()': once via 'uvc_scan_chain_entity()' when it is\nprocessed directly from the 'dev->entities' list and then again\nimmediately afterwards when trying to follow the source ID in\n'uvc_scan_chain_forward()'\n\nAdd a check before adding an entity to a chain list to ensure that the\nentity is not already part of a chain.\n\nLink: https://lore.kernel.org/linux-media/CAAeHK+z+Si69jUR+N-SjN9q4O+o5KFiNManqEa-PjUta7EOb7A@mail.gmail.com/\n\nCc: <stable@vger.kernel.org>\nFixes: c0efd232929c (\"V4L/DVB (8145a): USB Video Class driver\")\nReported-by: Andrey Konovalov <andreyknvl@google.com>\nSigned-off-by: Will Deacon <will@kernel.org>\nSigned-off-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 1, "dataset": "other", "idx": 208387}
{"func": "\t\t\tforward);\n\t\tif (forward == NULL)\n\t\t\tbreak;\n\t\tif (forward == prev)\n\t\t\tcontinue;\n\t\tif (forward->chain.next || forward->chain.prev) {\n\t\t\tuvc_trace(UVC_TRACE_DESCR, \"Found reference to \"\n\t\t\t\t\"entity %d already in chain.\\n\", forward->id);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tswitch (UVC_ENTITY_TYPE(forward)) {\n\t\tcase UVC_VC_EXTENSION_UNIT:\n\t\t\tif (forward->bNrInPins != 1) {\n\t\t\t\tuvc_trace(UVC_TRACE_DESCR, \"Extension unit %d \"", "project": "linux", "hash": 295353061660172152390372845804699304944, "size": 68, "commit_id": "68035c80e129c4cfec659aac4180354530b26527", "message": "media: uvcvideo: Avoid cyclic entity chains due to malformed USB descriptors\n\nWay back in 2017, fuzzing the 4.14-rc2 USB stack with syzkaller kicked\nup the following WARNING from the UVC chain scanning code:\n\n  | list_add double add: new=ffff880069084010, prev=ffff880069084010,\n  | next=ffff880067d22298.\n  | ------------[ cut here ]------------\n  | WARNING: CPU: 1 PID: 1846 at lib/list_debug.c:31 __list_add_valid+0xbd/0xf0\n  | Modules linked in:\n  | CPU: 1 PID: 1846 Comm: kworker/1:2 Not tainted\n  | 4.14.0-rc2-42613-g1488251d1a98 #238\n  | Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011\n  | Workqueue: usb_hub_wq hub_event\n  | task: ffff88006b01ca40 task.stack: ffff880064358000\n  | RIP: 0010:__list_add_valid+0xbd/0xf0 lib/list_debug.c:29\n  | RSP: 0018:ffff88006435ddd0 EFLAGS: 00010286\n  | RAX: 0000000000000058 RBX: ffff880067d22298 RCX: 0000000000000000\n  | RDX: 0000000000000058 RSI: ffffffff85a58800 RDI: ffffed000c86bbac\n  | RBP: ffff88006435dde8 R08: 1ffff1000c86ba52 R09: 0000000000000000\n  | R10: 0000000000000002 R11: 0000000000000000 R12: ffff880069084010\n  | R13: ffff880067d22298 R14: ffff880069084010 R15: ffff880067d222a0\n  | FS:  0000000000000000(0000) GS:ffff88006c900000(0000) knlGS:0000000000000000\n  | CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  | CR2: 0000000020004ff2 CR3: 000000006b447000 CR4: 00000000000006e0\n  | Call Trace:\n  |  __list_add ./include/linux/list.h:59\n  |  list_add_tail+0x8c/0x1b0 ./include/linux/list.h:92\n  |  uvc_scan_chain_forward.isra.8+0x373/0x416\n  | drivers/media/usb/uvc/uvc_driver.c:1471\n  |  uvc_scan_chain drivers/media/usb/uvc/uvc_driver.c:1585\n  |  uvc_scan_device drivers/media/usb/uvc/uvc_driver.c:1769\n  |  uvc_probe+0x77f2/0x8f00 drivers/media/usb/uvc/uvc_driver.c:2104\n\nLooking into the output from usbmon, the interesting part is the\nfollowing data packet:\n\n  ffff880069c63e00 30710169 C Ci:1:002:0 0 143 = 09028f00 01030080\n  00090403 00000e01 00000924 03000103 7c003328 010204db\n\nIf we drop the lead configuration and interface descriptors, we're left\nwith an output terminal descriptor describing a generic display:\n\n  /* Output terminal descriptor */\n  buf[0]\t09\n  buf[1]\t24\n  buf[2]\t03\t/* UVC_VC_OUTPUT_TERMINAL */\n  buf[3]\t00\t/* ID */\n  buf[4]\t01\t/* type == 0x0301 (UVC_OTT_DISPLAY) */\n  buf[5]\t03\n  buf[6]\t7c\n  buf[7]\t00\t/* source ID refers to self! */\n  buf[8]\t33\n\nThe problem with this descriptor is that it is self-referential: the\nsource ID of 0 matches itself! This causes the 'struct uvc_entity'\nrepresenting the display to be added to its chain list twice during\n'uvc_scan_chain()': once via 'uvc_scan_chain_entity()' when it is\nprocessed directly from the 'dev->entities' list and then again\nimmediately afterwards when trying to follow the source ID in\n'uvc_scan_chain_forward()'\n\nAdd a check before adding an entity to a chain list to ensure that the\nentity is not already part of a chain.\n\nLink: https://lore.kernel.org/linux-media/CAAeHK+z+Si69jUR+N-SjN9q4O+o5KFiNManqEa-PjUta7EOb7A@mail.gmail.com/\n\nCc: <stable@vger.kernel.org>\nFixes: c0efd232929c (\"V4L/DVB (8145a): USB Video Class driver\")\nReported-by: Andrey Konovalov <andreyknvl@google.com>\nSigned-off-by: Will Deacon <will@kernel.org>\nSigned-off-by: Laurent Pinchart <laurent.pinchart@ideasonboard.com>\nSigned-off-by: Mauro Carvalho Chehab <mchehab+huawei@kernel.org>", "target": 0, "dataset": "other", "idx": 409209}
{"func": "static BOOL rdp_read_font_capability_set(wStream* s, UINT16 length, rdpSettings* settings)\n{\n\tWINPR_UNUSED(settings);\n\tif (length > 4)\n\t\tStream_Seek_UINT16(s); /* fontSupportFlags (2 bytes) */\n\n\tif (length > 6)\n\t\tStream_Seek_UINT16(s); /* pad2Octets (2 bytes) */\n\n\treturn TRUE;\n}", "project": "FreeRDP", "hash": 63945765978373457675419959857261610616, "size": 11, "commit_id": "3627aaf7d289315b614a584afb388f04abfb5bbf", "message": "Fixed #6011: Bounds check in rdp_read_font_capability_set", "target": 1, "dataset": "other", "idx": 208417}
{"func": "static BOOL rdp_read_font_capability_set(wStream* s, UINT16 length, rdpSettings* settings)\n{\n\tWINPR_UNUSED(settings);\n\tif (length > 5)\n\t\tStream_Seek_UINT16(s); /* fontSupportFlags (2 bytes) */\n\n\tif (length > 7)\n\t\tStream_Seek_UINT16(s); /* pad2Octets (2 bytes) */\n\n\treturn TRUE;\n}", "project": "FreeRDP", "hash": 92561895110037279576315503246460434637, "size": 11, "commit_id": "3627aaf7d289315b614a584afb388f04abfb5bbf", "message": "Fixed #6011: Bounds check in rdp_read_font_capability_set", "target": 0, "dataset": "other", "idx": 409646}
{"func": "delete_buff_tail(buffheader_T *buf, int slen)\n{\n    int len = (int)STRLEN(buf->bh_curr->b_str);\n\n    if (len >= slen)\n    {\n\tbuf->bh_curr->b_str[len - slen] = NUL;\n\tbuf->bh_space += slen;\n    }", "project": "vim", "hash": 320320646444324926020830996441995496926, "size": 10, "commit_id": "a4bc2dd7cccf5a4a9f78b58b6f35a45d17164323", "message": "patch 8.2.4233: crash when recording and using Select mode\n\nProblem:    Crash when recording and using Select mode.\nSolution:   When deleting the last recorded character check there is something\n            to delete.", "target": 1, "dataset": "other", "idx": 208442}
{"func": "delete_buff_tail(buffheader_T *buf, int slen)\n{\n    int len;\n\n    if (buf->bh_curr == NULL || buf->bh_curr->b_str == NULL)\n\treturn;  // nothing to delete\n    len = (int)STRLEN(buf->bh_curr->b_str);\n    if (len >= slen)\n    {\n\tbuf->bh_curr->b_str[len - slen] = NUL;\n\tbuf->bh_space += slen;\n    }", "project": "vim", "hash": 67321294593976323375988048434039618369, "size": 13, "commit_id": "a4bc2dd7cccf5a4a9f78b58b6f35a45d17164323", "message": "patch 8.2.4233: crash when recording and using Select mode\n\nProblem:    Crash when recording and using Select mode.\nSolution:   When deleting the last recorded character check there is something\n            to delete.", "target": 0, "dataset": "other", "idx": 410380}
{"func": "static int is_fuse_usermount(struct libmnt_context *cxt, int *errsv)\n{\n\tstruct libmnt_ns *ns_old;\n\tconst char *type = mnt_fs_get_fstype(cxt->fs);\n\tconst char *optstr;\n\tchar *user_id = NULL;\n\tsize_t sz;\n\tuid_t uid;\n\tchar uidstr[sizeof(stringify_value(ULONG_MAX))];\n\n\t*errsv = 0;\n\n\tif (!type)\n\t\treturn 0;\n\n\t/* get user_id= from mount table */\n\toptstr = mnt_fs_get_fs_options(cxt->fs);\n\tif (!optstr)\n\t\treturn 0;\n\n\tif (mnt_optstr_get_option(optstr, \"user_id\", &user_id, &sz) != 0)\n\t\treturn 0;\n\n\tif (sz == 0 || user_id == NULL)\n\t\treturn 0;\n\n\t/* get current user */\n\tns_old = mnt_context_switch_origin_ns(cxt);\n\tif (!ns_old) {\n\tif (!mnt_context_switch_ns(cxt, ns_old)) {\n\t\t*errsv = -MNT_ERR_NAMESPACE;\n\t\treturn 0;\n\t}\n\n\tsnprintf(uidstr, sizeof(uidstr), \"%lu\", (unsigned long) uid);\n\treturn strncmp(user_id, uidstr, sz) == 0;\n}", "project": "util-linux", "hash": 210310102417734751354249196559456080186, "size": 49, "commit_id": "57202f5713afa2af20ffbb6ab5331481d0396f8d", "message": "libmount: fix UID check for FUSE umount [CVE-2021-3995]\n\nImproper UID check allows an unprivileged user to unmount FUSE\nfilesystems of users with similar UID.\n\nSigned-off-by: Karel Zak <kzak@redhat.com>", "target": 1, "dataset": "other", "idx": 208443}
{"func": "static int is_fuse_usermount(struct libmnt_context *cxt, int *errsv)\n{\n\tstruct libmnt_ns *ns_old;\n\tconst char *type = mnt_fs_get_fstype(cxt->fs);\n\tconst char *optstr;\n\tuid_t uid, entry_uid;\n\n\t*errsv = 0;\n\n\tif (!type)\n\t\treturn 0;\n\n\t/* get user_id= from mount table */\n\toptstr = mnt_fs_get_fs_options(cxt->fs);\n\tif (!optstr)\n\t\treturn 0;\n\tif (mnt_optstr_get_uid(optstr, \"user_id\", &entry_uid) != 0)\n\t\treturn 0;\n\n\t/* get current user */\n\tns_old = mnt_context_switch_origin_ns(cxt);\n\tif (!ns_old) {\n\tif (!mnt_context_switch_ns(cxt, ns_old)) {\n\t\t*errsv = -MNT_ERR_NAMESPACE;\n\t\treturn 0;\n\t}\n\n\treturn uid == entry_uid;\n}", "project": "util-linux", "hash": 183698717299577400492179801479311774546, "size": 41, "commit_id": "57202f5713afa2af20ffbb6ab5331481d0396f8d", "message": "libmount: fix UID check for FUSE umount [CVE-2021-3995]\n\nImproper UID check allows an unprivileged user to unmount FUSE\nfilesystems of users with similar UID.\n\nSigned-off-by: Karel Zak <kzak@redhat.com>", "target": 0, "dataset": "other", "idx": 410433}
{"func": "static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)\n{\n\tif (sk->sk_send_head == skb_unlinked)\n\t\tsk->sk_send_head = NULL;\n}", "project": "linux", "hash": 305735579556355500541582008604019854778, "size": 5, "commit_id": "bb1fceca22492109be12640d49f5ea5a544c6bb4", "message": "tcp: fix use after free in tcp_xmit_retransmit_queue()\n\nWhen tcp_sendmsg() allocates a fresh and empty skb, it puts it at the\ntail of the write queue using tcp_add_write_queue_tail()\n\nThen it attempts to copy user data into this fresh skb.\n\nIf the copy fails, we undo the work and remove the fresh skb.\n\nUnfortunately, this undo lacks the change done to tp->highest_sack and\nwe can leave a dangling pointer (to a freed skb)\n\nLater, tcp_xmit_retransmit_queue() can dereference this pointer and\naccess freed memory. For regular kernels where memory is not unmapped,\nthis might cause SACK bugs because tcp_highest_sack_seq() is buggy,\nreturning garbage instead of tp->snd_nxt, but with various debug\nfeatures like CONFIG_DEBUG_PAGEALLOC, this can crash the kernel.\n\nThis bug was found by Marco Grassi thanks to syzkaller.\n\nFixes: 6859d49475d4 (\"[TCP]: Abstract tp->highest_sack accessing & point to next skb\")\nReported-by: Marco Grassi <marco.gra@gmail.com>\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: Ilpo J\u00e4rvinen <ilpo.jarvinen@helsinki.fi>\nCc: Yuchung Cheng <ycheng@google.com>\nCc: Neal Cardwell <ncardwell@google.com>\nAcked-by: Neal Cardwell <ncardwell@google.com>\nReviewed-by: Cong Wang <xiyou.wangcong@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 208465}
{"func": "static inline void tcp_check_send_head(struct sock *sk, struct sk_buff *skb_unlinked)\n{\n\tif (sk->sk_send_head == skb_unlinked)\n\t\tsk->sk_send_head = NULL;\n\tif (tcp_sk(sk)->highest_sack == skb_unlinked)\n\t\ttcp_sk(sk)->highest_sack = NULL;\n}", "project": "linux", "hash": 5947355706701478126786824025705054068, "size": 7, "commit_id": "bb1fceca22492109be12640d49f5ea5a544c6bb4", "message": "tcp: fix use after free in tcp_xmit_retransmit_queue()\n\nWhen tcp_sendmsg() allocates a fresh and empty skb, it puts it at the\ntail of the write queue using tcp_add_write_queue_tail()\n\nThen it attempts to copy user data into this fresh skb.\n\nIf the copy fails, we undo the work and remove the fresh skb.\n\nUnfortunately, this undo lacks the change done to tp->highest_sack and\nwe can leave a dangling pointer (to a freed skb)\n\nLater, tcp_xmit_retransmit_queue() can dereference this pointer and\naccess freed memory. For regular kernels where memory is not unmapped,\nthis might cause SACK bugs because tcp_highest_sack_seq() is buggy,\nreturning garbage instead of tp->snd_nxt, but with various debug\nfeatures like CONFIG_DEBUG_PAGEALLOC, this can crash the kernel.\n\nThis bug was found by Marco Grassi thanks to syzkaller.\n\nFixes: 6859d49475d4 (\"[TCP]: Abstract tp->highest_sack accessing & point to next skb\")\nReported-by: Marco Grassi <marco.gra@gmail.com>\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: Ilpo J\u00e4rvinen <ilpo.jarvinen@helsinki.fi>\nCc: Yuchung Cheng <ycheng@google.com>\nCc: Neal Cardwell <ncardwell@google.com>\nAcked-by: Neal Cardwell <ncardwell@google.com>\nReviewed-by: Cong Wang <xiyou.wangcong@gmail.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 410730}
{"func": "static INLINE BOOL ensure_capacity(const BYTE* start, const BYTE* end, size_t size, size_t base)\n{\n\tconst size_t available = (uintptr_t)end - (uintptr_t)start;\n\tconst BOOL rc = available >= size * base;\n\treturn rc;\n}", "project": "FreeRDP", "hash": 203800133322701028714179275363172565549, "size": 6, "commit_id": "7b1d4b49391b4512402840431757703a96946820", "message": "Fix CVE-2020-11524: out of bounds access in interleaved\n\nThanks to Sunglin and HuanGMz from Knownsec 404", "target": 1, "dataset": "other", "idx": 208486}
{"func": "static INLINE BOOL ensure_capacity(const BYTE* start, const BYTE* end, size_t size, size_t base)\n{\n\tconst size_t available = (uintptr_t)end - (uintptr_t)start;\n\tconst BOOL rc = available >= size * base;\n\treturn rc && (start <= end);\n}", "project": "FreeRDP", "hash": 224150333890835595116889465118463357527, "size": 6, "commit_id": "7b1d4b49391b4512402840431757703a96946820", "message": "Fix CVE-2020-11524: out of bounds access in interleaved\n\nThanks to Sunglin and HuanGMz from Knownsec 404", "target": 0, "dataset": "other", "idx": 411767}
{"func": "\tr = mkrule(scanname(s));\n\tscannewline(s);\n\twhile (scanindent(s)) {\n\t\tvar = scanname(s);\n\t\tparselet(s, &val);\n\t\truleaddvar(r, var, val);\n\t\tif (strcmp(var, \"command\") == 0)\n\t\t\thascommand = true;\n\t\telse if (strcmp(var, \"rspfile\") == 0)\n\t\t\thasrspfile = true;\n\t\telse if (strcmp(var, \"rspfile_content\") == 0)", "project": "samurai", "hash": 335382966921873606289133752083182751104, "size": 26, "commit_id": "d2af3bc375e2a77139c3a28d6128c60cd8d08655", "message": "parse: Check for non-empty command/rspfile/rspfile_content\n\nThis matches ninja behavior and prevents the possibility of a rule\nwith an empty (NULL) command string.\n\nFixes #68.", "target": 1, "dataset": "other", "idx": 208509}
{"func": "\tscannewline(s);\n\twhile (scanindent(s)) {\n\t\tvar = scanname(s);\n\t\tparselet(s, &val);\n\t\truleaddvar(r, var, val);\n\t\tif (!val)\n\t\t\tcontinue;\n\t\tif (strcmp(var, \"command\") == 0)\n\t\t\thascommand = true;\n\t\telse if (strcmp(var, \"rspfile\") == 0)\n\t\t\thasrspfile = true;\n\t\telse if (strcmp(var, \"rspfile_content\") == 0)", "project": "samurai", "hash": 25964334636848040242473999480648310370, "size": 28, "commit_id": "d2af3bc375e2a77139c3a28d6128c60cd8d08655", "message": "parse: Check for non-empty command/rspfile/rspfile_content\n\nThis matches ninja behavior and prevents the possibility of a rule\nwith an empty (NULL) command string.\n\nFixes #68.", "target": 0, "dataset": "other", "idx": 411968}
{"func": "        case 's': // POSIX shared memory\n            if (g->payload_sz > 2048) ABRT(EINVAL, \"Filename too long\");\n            snprintf(fname, sizeof(fname)/sizeof(fname[0]), \"%.*s\", (int)g->payload_sz, payload);\n            if (tt == 's') fd = shm_open(fname, O_RDONLY, 0);\n            else fd = open(fname, O_CLOEXEC | O_RDONLY);\n            if (fd == -1) ABRT(EBADF, \"Failed to open file %s for graphics transmission with error: [%d] %s\", fname, errno, strerror(errno));\n            img->data_loaded = mmap_img_file(self, img, fd, g->data_sz, g->data_offset);\n            safe_close(fd, __FILE__, __LINE__);\n            if (tt == 't') {\n                if (global_state.boss) { call_boss(safe_delete_temp_file, \"s\", fname); }\n                else unlink(fname);", "project": "kitty", "hash": 193277586803458068601987516058015840256, "size": 163, "commit_id": "82c137878c2b99100a3cdc1c0f0efea069313901", "message": "Graphics protocol: Dont return filename in the error message when opening file fails, since filenames can contain control characters\n\nFixes #3128", "target": 1, "dataset": "other", "idx": 208514}
{"func": "        case 's': // POSIX shared memory\n            if (g->payload_sz > 2048) ABRT(EINVAL, \"Filename too long\");\n            snprintf(fname, sizeof(fname)/sizeof(fname[0]), \"%.*s\", (int)g->payload_sz, payload);\n            if (tt == 's') fd = shm_open(fname, O_RDONLY, 0);\n            else fd = open(fname, O_CLOEXEC | O_RDONLY);\n            if (fd == -1) ABRT(EBADF, \"Failed to open file for graphics transmission with error: [%d] %s\", errno, strerror(errno));\n            img->data_loaded = mmap_img_file(self, img, fd, g->data_sz, g->data_offset);\n            safe_close(fd, __FILE__, __LINE__);\n            if (tt == 't') {\n                if (global_state.boss) { call_boss(safe_delete_temp_file, \"s\", fname); }\n                else unlink(fname);", "project": "kitty", "hash": 62380342440080942581251333456874353665, "size": 163, "commit_id": "82c137878c2b99100a3cdc1c0f0efea069313901", "message": "Graphics protocol: Dont return filename in the error message when opening file fails, since filenames can contain control characters\n\nFixes #3128", "target": 0, "dataset": "other", "idx": 412024}
{"func": "\t\t\t*d++ = *str++;\n\t\t\tlength--;\n\t\t\tlp = 0;\n\t\t} else {\n\t\t\tif (iscntrl (c) || (c == 0x7f) || (c & 0x80) || (c == '=') || ((c == ' ') && (*str == '\\015'))) {\n\t\t\t\tif ((lp += 3) > PHP_QPRINT_MAXL) {\n\t\t\t\t\t*d++ = '=';\n\t\t\t\t\t*d++ = '\\015';\n\t\t\t\t\t*d++ = '\\012';\n\t\t\t\t\tlp = 3;\n\t\t\t\t}", "project": "php-src", "hash": 201039805180297414263699867243035103127, "size": 43, "commit_id": "18bb426587d62f93c54c40bf8535eb8416603629", "message": "Bug 62462: Prevent multibyte characters from being split between the lines\n\nMerged from https://github.com/php/php-src/pull/120", "target": 1, "dataset": "other", "idx": 208542}
{"func": "\t\t\t*d++ = *str++;\n\t\t\tlength--;\n\t\t\tlp = 0;\n\t\t} else {\n\t\t\tif (iscntrl (c) || (c == 0x7f) || (c & 0x80) || (c == '=') || ((c == ' ') && (*str == '\\015'))) {\n\t\t\t\tif ((((lp+= 3) > PHP_QPRINT_MAXL) && (c <= 0x7f)) \n            || ((c > 0x7f) && (c <= 0xdf) && ((lp + 3) > PHP_QPRINT_MAXL)) \n            || ((c > 0xdf) && (c <= 0xef) && ((lp + 6) > PHP_QPRINT_MAXL)) \n            || ((c > 0xef) && (c <= 0xf4) && ((lp + 9) > PHP_QPRINT_MAXL))) {\n\t\t\t\t\t*d++ = '=';\n\t\t\t\t\t*d++ = '\\015';\n\t\t\t\t\t*d++ = '\\012';\n\t\t\t\t\tlp = 3;\n\t\t\t\t}", "project": "php-src", "hash": 194162703673769915280815161904119135108, "size": 46, "commit_id": "18bb426587d62f93c54c40bf8535eb8416603629", "message": "Bug 62462: Prevent multibyte characters from being split between the lines\n\nMerged from https://github.com/php/php-src/pull/120", "target": 0, "dataset": "other", "idx": 412529}
{"func": "HRESULT Http::HrReadHeaders()\n{\n\tHRESULT hr;\n\tstd::string strBuffer;\n\tULONG n = 0;\n\tstd::map<std::string, std::string>::iterator iHeader = mapHeaders.end();\n\n\tec_log_debug(\"Receiving headers:\");\n\tdo\n\t{\n\t\thr = m_lpChannel->HrReadLine(strBuffer);\n\t\tif (hr != hrSuccess)\n\t\t\treturn hr;\n\t\tif (strBuffer.empty())\n\t\t\tbreak;\n\n\t\tif (n == 0) {\n\t\t\tm_strAction = strBuffer;\n\t\t} else {\n\t\t\tauto pos = strBuffer.find(':');", "project": "kopano-core", "hash": 252899856337214612444746933964237135309, "size": 48, "commit_id": "512457466b87039c6a8d25887fdaca6173619546", "message": "Set limit on header size to prevent bad alloc\n\nThis sets a hard limit of 64 KiB to the header to prevent a memory\nallocation exception from being thrown during the parsing of the request\nheaders.", "target": 1, "dataset": "other", "idx": 208547}
{"func": "{\n\tHRESULT hr;\n\tstd::string strBuffer;\n\tULONG n = 0;\n\tstd::map<std::string, std::string>::iterator iHeader = mapHeaders.end();\n\tstatic constexpr std::size_t MAX_HEADER_LENGTH = 65536;\n\tstd::size_t numOfBytesRead = 0;\n\n\tec_log_debug(\"Receiving headers:\");\n\tdo\n\t{\n\t\thr = m_lpChannel->HrReadLine(strBuffer);\n\t\tif (hr != hrSuccess)\n\t\t\treturn hr;\n\t\tif (strBuffer.empty())\n\t\t\tbreak;\n\n\t\tnumOfBytesRead += strBuffer.size();\n\t\tif(numOfBytesRead > MAX_HEADER_LENGTH) {\n\t\t\treturn MAPI_E_TOO_BIG;\n\t\t}\n\n\t\tif (n == 0) {\n\t\t\tm_strAction = strBuffer;\n\t\t} else {\n\t\t\tauto pos = strBuffer.find(':');", "project": "kopano-core", "hash": 80967487134144399005768631526292358142, "size": 55, "commit_id": "512457466b87039c6a8d25887fdaca6173619546", "message": "Set limit on header size to prevent bad alloc\n\nThis sets a hard limit of 64 KiB to the header to prevent a memory\nallocation exception from being thrown during the parsing of the request\nheaders.", "target": 0, "dataset": "other", "idx": 412620}
{"func": "        state->buffer = new_data;\n\n        TRACE((\"TIFFTileSize: %d\\n\", state->bytes));\n\n        for (y = state->yoff; y < state->ysize; y += tile_length) {\n            for (x = state->xoff; x < state->xsize; x += tile_width) {\n                if (isYCbCr) {\n                    /* To avoid dealing with YCbCr subsampling, let libtiff handle it */\n                    if (!TIFFReadRGBATile(tiff, x, y, (UINT32 *)state->buffer)) {\n                        TRACE((\"Decode Error, Tile at %dx%d\\n\", x, y));\n                        state->errcode = IMAGING_CODEC_BROKEN;", "project": "Pillow", "hash": 337949595576035235304375695306533039366, "size": 227, "commit_id": "cbdce6c5d054fccaf4af34b47f212355c64ace7a", "message": "Fix for CVE-2021-25291\n\n* Invalid tile boundaries lead to OOB Read in TiffDecode.c, in TiffReadRGBATile\n* Check the tile validity before attempting to read.", "target": 1, "dataset": "other", "idx": 208556}
{"func": "\n        TRACE((\"TIFFTileSize: %d\\n\", state->bytes));\n\n        for (y = state->yoff; y < state->ysize; y += tile_length) {\n            for (x = state->xoff; x < state->xsize; x += tile_width) {\n                /* Sanity Check. Apparently in some cases, the TiffReadRGBA* functions\n                   have a different view of the size of the tiff than we're getting from\n                   other functions. So, we need to check here. \n                */\n                if (!TIFFCheckTile(tiff, x, y, 0, 0)) {\n                    TRACE((\"Check Tile Error, Tile at %dx%d\\n\", x, y));\n                    state->errcode = IMAGING_CODEC_BROKEN;\n                    goto decode_err;\n                }\n                if (isYCbCr) {\n                    /* To avoid dealing with YCbCr subsampling, let libtiff handle it */\n                    if (!TIFFReadRGBATile(tiff, x, y, (UINT32 *)state->buffer)) {\n                        TRACE((\"Decode Error, Tile at %dx%d\\n\", x, y));\n                        state->errcode = IMAGING_CODEC_BROKEN;", "project": "Pillow", "hash": 201448095726443829294002981038733584513, "size": 236, "commit_id": "cbdce6c5d054fccaf4af34b47f212355c64ace7a", "message": "Fix for CVE-2021-25291\n\n* Invalid tile boundaries lead to OOB Read in TiffDecode.c, in TiffReadRGBATile\n* Check the tile validity before attempting to read.", "target": 0, "dataset": "other", "idx": 412799}
{"func": "mt76_add_fragment(struct mt76_dev *dev, struct mt76_queue *q, void *data,\n\t\t  int len, bool more)\n{\n\tstruct page *page = virt_to_head_page(data);\n\tint offset = data - page_address(page);\n\tstruct sk_buff *skb = q->rx_head;\n\n\toffset += q->buf_offset;\n\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page, offset, len,\n\t\t\tq->buf_size);\n\n\tif (more)\n\t\treturn;\n\n\tq->rx_head = NULL;", "project": "linux", "hash": 146050973517401472759282716419538748236, "size": 17, "commit_id": "b102f0c522cf668c8382c56a4f771b37d011cda2", "message": "mt76: fix array overflow on receiving too many fragments for a packet\n\nIf the hardware receives an oversized packet with too many rx fragments,\nskb_shinfo(skb)->frags can overflow and corrupt memory of adjacent pages.\nThis becomes especially visible if it corrupts the freelist pointer of\na slab page.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Felix Fietkau <nbd@nbd.name>\nSigned-off-by: Kalle Valo <kvalo@codeaurora.org>", "target": 1, "dataset": "other", "idx": 208640}
{"func": "\t\t  int len, bool more)\n{\n\tstruct page *page = virt_to_head_page(data);\n\tint offset = data - page_address(page);\n\tstruct sk_buff *skb = q->rx_head;\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\n\tif (shinfo->nr_frags < ARRAY_SIZE(shinfo->frags)) {\n\t\toffset += q->buf_offset;\n\t\tskb_add_rx_frag(skb, shinfo->nr_frags, page, offset, len,\n\t\t\t\tq->buf_size);\n\t}\n\n\tif (more)\n\t\treturn;\n\n\tq->rx_head = NULL;", "project": "linux", "hash": 257436133503741741969364886008365609105, "size": 20, "commit_id": "b102f0c522cf668c8382c56a4f771b37d011cda2", "message": "mt76: fix array overflow on receiving too many fragments for a packet\n\nIf the hardware receives an oversized packet with too many rx fragments,\nskb_shinfo(skb)->frags can overflow and corrupt memory of adjacent pages.\nThis becomes especially visible if it corrupts the freelist pointer of\na slab page.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Felix Fietkau <nbd@nbd.name>\nSigned-off-by: Kalle Valo <kvalo@codeaurora.org>", "target": 0, "dataset": "other", "idx": 413227}
{"func": "\n    assert(name);\n    assert(module);\n\n    for (u = 0; u < ext_plugins_count; u++) {\n        if (!strcmp(name, ext_plugins[u].name) &&\n                !strcmp(module, ext_plugins[u].module) &&\n                (!ext_plugins[u].revision || !strcmp(revision, ext_plugins[u].revision))) {\n            /* we have the match */\n            return ext_plugins[u].plugin;\n        }\n    }\n", "project": "libyang", "hash": 218030599751924346754684311149140190519, "size": 19, "commit_id": "59a0bff1a5a2f0a0eac07e4bf94d4aea9dd3708d", "message": "plugins BUGFIX handle empty revision correctly\n\nFixes #1451", "target": 1, "dataset": "other", "idx": 208672}
{"func": "\n    assert(name);\n    assert(module);\n\n    for (u = 0; u < ext_plugins_count; u++) {\n        if (!strcmp(name, ext_plugins[u].name) && !strcmp(module, ext_plugins[u].module) &&\n                ((!revision && !ext_plugins[u].revision) || (revision && !strcmp(revision, ext_plugins[u].revision)))) {\n            /* we have the match */\n            return ext_plugins[u].plugin;\n        }\n    }\n", "project": "libyang", "hash": 206226070812095655612586328288084046946, "size": 18, "commit_id": "59a0bff1a5a2f0a0eac07e4bf94d4aea9dd3708d", "message": "plugins BUGFIX handle empty revision correctly\n\nFixes #1451", "target": 0, "dataset": "other", "idx": 413412}
{"func": "        double* countTmp = new double[_maxCodeLength+1];\n\n        for (int l = _minCodeLength; l <= _maxCodeLength; ++l)\n        {\n            countTmp[l] = (double)codeCount[l] * \n                          (double)(2 << (_maxCodeLength-l));\n        }\n    \n        for (int l = _minCodeLength; l <= _maxCodeLength; ++l)\n        {\n            double tmp = 0;\n\n            for (int k =l + 1; k <= _maxCodeLength; ++k)\n                tmp += countTmp[k];\n            \n            tmp /= (double)(2 << (_maxCodeLength - l));\n\n            base[l] = (Int64)ceil (tmp);\n        }\n\n        delete [] countTmp;", "project": "openexr", "hash": 295327035492456381923444384031538504049, "size": 220, "commit_id": "c3ed4a1db1f39bf4524a644cb2af81dc8cfab33f", "message": "compute Huf codelengths using 64 bit to prevent shift overflow\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>", "target": 1, "dataset": "other", "idx": 208675}
{"func": "        double* countTmp = new double[_maxCodeLength+1];\n\n        for (int l = _minCodeLength; l <= _maxCodeLength; ++l)\n        {\n            countTmp[l] = (double)codeCount[l] * \n                          (double)(2ll << (_maxCodeLength-l));\n        }\n    \n        for (int l = _minCodeLength; l <= _maxCodeLength; ++l)\n        {\n            double tmp = 0;\n\n            for (int k =l + 1; k <= _maxCodeLength; ++k)\n                tmp += countTmp[k];\n            \n            tmp /= (double)(2ll << (_maxCodeLength - l));\n\n            base[l] = (Int64)ceil (tmp);\n        }\n\n        delete [] countTmp;", "project": "openexr", "hash": 153643357904600972132151102261211578620, "size": 220, "commit_id": "c3ed4a1db1f39bf4524a644cb2af81dc8cfab33f", "message": "compute Huf codelengths using 64 bit to prevent shift overflow\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>", "target": 0, "dataset": "other", "idx": 413449}
{"func": "\t\tefree(oldpath);\n\t\tzend_throw_exception_ex(spl_ce_BadMethodCallException, 0 TSRMLS_CC, \"Unable to add newly converted phar \\\"%s\\\" to the list of phars, a phar with that name already exists\", phar->fname);\n\t\treturn NULL;\n\t}\nits_ok:\n\tif (SUCCESS == php_stream_stat_path(newpath, &ssb)) {\n\t\tefree(oldpath);\n\t\tzend_throw_exception_ex(spl_ce_BadMethodCallException, 0 TSRMLS_CC, \"phar \\\"%s\\\" exists and must be unlinked prior to conversion\", newpath);\n\t\treturn NULL;\n\t}\n\tif (!phar->is_data) {\n\t\tif (SUCCESS != phar_detect_phar_fname_ext(newpath, phar->fname_len, (const char **) &(phar->ext), &(phar->ext_len), 1, 1, 1 TSRMLS_CC)) {\n\t\t\tefree(oldpath);", "project": "php-src", "hash": 55514648493563640623085659819286233714, "size": 194, "commit_id": "b2cf3f064b8f5efef89bb084521b61318c71781b", "message": "Fixed bug #68901 (use after free)", "target": 1, "dataset": "other", "idx": 208677}
{"func": "\t\tzend_throw_exception_ex(spl_ce_BadMethodCallException, 0 TSRMLS_CC, \"Unable to add newly converted phar \\\"%s\\\" to the list of phars, a phar with that name already exists\", phar->fname);\n\t\treturn NULL;\n\t}\nits_ok:\n\tif (SUCCESS == php_stream_stat_path(newpath, &ssb)) {\n\t\tzend_throw_exception_ex(spl_ce_BadMethodCallException, 0 TSRMLS_CC, \"phar \\\"%s\\\" exists and must be unlinked prior to conversion\", newpath);\n\t\tefree(oldpath);\n\t\treturn NULL;\n\t}\n\tif (!phar->is_data) {\n\t\tif (SUCCESS != phar_detect_phar_fname_ext(newpath, phar->fname_len, (const char **) &(phar->ext), &(phar->ext_len), 1, 1, 1 TSRMLS_CC)) {\n\t\t\tefree(oldpath);", "project": "php-src", "hash": 85820391847910228353825170949751953610, "size": 194, "commit_id": "b2cf3f064b8f5efef89bb084521b61318c71781b", "message": "Fixed bug #68901 (use after free)", "target": 0, "dataset": "other", "idx": 413459}
{"func": "rndr_quote(struct buf *ob, const struct buf *text, void *opaque)\n{\n\tif (!text || !text->size)\n\t\treturn 0;\n\n\tBUFPUTSL(ob, \"<q>\");\n\tbufput(ob, text->data, text->size);\n\tBUFPUTSL(ob, \"</q>\");\n\n\treturn 1;\n}", "project": "redcarpet", "hash": 339130827029288585257779128996016727908, "size": 11, "commit_id": "a699c82292b17c8e6a62e1914d5eccc252272793", "message": "Fix a security issue using `:quote` with `:escape_html`\n\nReported by @johan-smits.", "target": 1, "dataset": "other", "idx": 208718}
{"func": "rndr_underline(struct buf *ob, const struct buf *text, void *opaque)\n{\n\tif (!text || !text->size)\n\t\treturn 0;\n\n\tBUFPUTSL(ob, \"<u>\");\n\tbufput(ob, text->data, text->size);\n\tBUFPUTSL(ob, \"</u>\");\n\n\treturn 1;\n}", "project": "redcarpet", "hash": 84760104797261814325900181895738182717, "size": 11, "commit_id": "a699c82292b17c8e6a62e1914d5eccc252272793", "message": "Fix a security issue using `:quote` with `:escape_html`\n\nReported by @johan-smits.", "target": 0, "dataset": "other", "idx": 414517}
{"func": "      {\n        fgets (header,MAX_CHARS_IN_ROW,fp);\n      }\n\n    /* Get Width and Height */\n    img->width  = strtol (header,&ptr,0);\n    img->height = atoi (ptr);\n\n    fgets (header,MAX_CHARS_IN_ROW,fp);\n    maxval = strtol (header,&ptr,0);\n\n    if ((maxval != 255) && (maxval != 65535))\n      {\n        g_warning (\"Image is not an 8-bit or 16-bit portable pixmap\");\n        return FALSE;", "project": "gegl", "hash": 156544789859611251475411793449672138344, "size": 68, "commit_id": "4757cdf73d3675478d645a3ec8250ba02168a230", "message": "ppm-load: CVE-2012-4433: add plausibility checks for header fields\n\nRefuse values that are non-decimal, negative or overflow the target\ntype.", "target": 1, "dataset": "other", "idx": 208743}
{"func": "      {\n        fgets (header,MAX_CHARS_IN_ROW,fp);\n      }\n\n    /* Get Width and Height */\n    errno = 0;\n    img->width  = strtol (header,&ptr,10);\n    if (errno)\n      {\n        g_warning (\"Error reading width: %s\", strerror(errno));\n        return FALSE;\n      }\n    else if (img->width < 0)\n      {\n        g_warning (\"Error: width is negative\");\n        return FALSE;\n      }\n\n    img->height = strtol (ptr,&ptr,10);\n    if (errno)\n      {\n        g_warning (\"Error reading height: %s\", strerror(errno));\n        return FALSE;\n      }\n    else if (img->width < 0)\n      {\n        g_warning (\"Error: height is negative\");\n        return FALSE;\n      }\n\n    fgets (header,MAX_CHARS_IN_ROW,fp);\n    maxval = strtol (header,&ptr,10);\n\n    if ((maxval != 255) && (maxval != 65535))\n      {\n        g_warning (\"Image is not an 8-bit or 16-bit portable pixmap\");\n        return FALSE;", "project": "gegl", "hash": 2766754304084121512241040743874346746, "size": 90, "commit_id": "4757cdf73d3675478d645a3ec8250ba02168a230", "message": "ppm-load: CVE-2012-4433: add plausibility checks for header fields\n\nRefuse values that are non-decimal, negative or overflow the target\ntype.", "target": 0, "dataset": "other", "idx": 414969}
{"func": "\tunsigned int i;\n\tzend_bool last_field_was_string = FALSE;\n\tzval **current_field, **end_field, **start_field;\n\tzend_uchar * p = row_buffer->ptr;\n\tsize_t data_size = row_buffer->app;\n\tzend_uchar * bit_area = (zend_uchar*) row_buffer->ptr + data_size + 1; /* we allocate from here */\n\n\tDBG_ENTER(\"php_mysqlnd_rowp_read_text_protocol_aux\");\n\n\tif (!fields) {\n\t\tDBG_RETURN(FAIL);\n\n\tfor (i = 0, current_field = start_field; current_field < end_field; current_field++, i++) {\n\t\t/* Don't reverse the order. It is significant!*/\n\t\tzend_uchar *this_field_len_pos = p;\n\t\t/* php_mysqlnd_net_field_length() call should be after *this_field_len_pos = p; */\n\t\tunsigned long len = php_mysqlnd_net_field_length(&p);\n\n\t\tif (copy_data == FALSE && current_field > start_field && last_field_was_string) {\n\t\t\t/*\n\t\t\t  Normal queries:\n\t\t\t  We have to put \\0 now to the end of the previous field, if it was\n\t\t\t  a string. IS_NULL doesn't matter. Because we have already read our", "project": "php-src", "hash": 185803420179456454427036827670396437100, "size": 182, "commit_id": "28f80baf3c53e267c9ce46a2a0fadbb981585132", "message": "Fix bug #72293 - Heap overflow in mysqlnd related to BIT fields", "target": 1, "dataset": "other", "idx": 208933}
{"func": "\tzend_bool last_field_was_string = FALSE;\n\tzval **current_field, **end_field, **start_field;\n\tzend_uchar * p = row_buffer->ptr;\n\tsize_t data_size = row_buffer->app;\n\tzend_uchar * bit_area = (zend_uchar*) row_buffer->ptr + data_size + 1; /* we allocate from here */\n\tconst zend_uchar * const packet_end = (zend_uchar*) row_buffer->ptr + data_size;\n\n\tDBG_ENTER(\"php_mysqlnd_rowp_read_text_protocol_aux\");\n\n\tif (!fields) {\n\t\tDBG_RETURN(FAIL);\n\n\tfor (i = 0, current_field = start_field; current_field < end_field; current_field++, i++) {\n\t\t/* Don't reverse the order. It is significant!*/\n\t\tzend_uchar *this_field_len_pos = p;\n\t\t/* php_mysqlnd_net_field_length() call should be after *this_field_len_pos = p; */\n\t\tconst unsigned long len = php_mysqlnd_net_field_length(&p);\n\n\t\tif (len != MYSQLND_NULL_LENGTH && ((p + len) > packet_end)) {\n\t\t\tphp_error_docref(NULL, E_WARNING, \"Malformed server packet. Field length pointing \"MYSQLND_SZ_T_SPEC\n\t\t\t\t\t\t\t\t\t\t\t  \" bytes after end of packet\", (p + len) - packet_end - 1);\n\t\t\tDBG_RETURN(FAIL);\n\t\t}\n\t\tif (copy_data == FALSE && current_field > start_field && last_field_was_string) {\n\t\t\t/*\n\t\t\t  Normal queries:\n\t\t\t  We have to put \\0 now to the end of the previous field, if it was\n\t\t\t  a string. IS_NULL doesn't matter. Because we have already read our", "project": "php-src", "hash": 325703409906620959975800534935313037728, "size": 188, "commit_id": "28f80baf3c53e267c9ce46a2a0fadbb981585132", "message": "Fix bug #72293 - Heap overflow in mysqlnd related to BIT fields", "target": 0, "dataset": "other", "idx": 416671}
{"func": "static void cil_reset_classperms_set(struct cil_classperms_set *cp_set)\n{\n\tcil_reset_classpermission(cp_set->set);\n}", "project": "selinux", "hash": 228898186887741153148874390102479645011, "size": 4, "commit_id": "c49a8ea09501ad66e799ea41b8154b6770fec2c8", "message": "libsepol/cil: cil_reset_classperms_set() should not reset classpermission\n\nIn struct cil_classperms_set, the set field is a pointer to a\nstruct cil_classpermission which is looked up in the symbol table.\nSince the cil_classperms_set does not create the cil_classpermission,\nit should not reset it.\n\nSet the set field to NULL instead of resetting the classpermission\nthat it points to.\n\nSigned-off-by: James Carter <jwcart2@gmail.com>", "target": 1, "dataset": "other", "idx": 208940}
{"func": "static void cil_reset_classpermissionset(struct cil_classpermissionset *cps)\n{\n\tcil_reset_classperms_list(cps->classperms);\n}", "project": "selinux", "hash": 235137699480676414178363936394519566185, "size": 4, "commit_id": "c49a8ea09501ad66e799ea41b8154b6770fec2c8", "message": "libsepol/cil: cil_reset_classperms_set() should not reset classpermission\n\nIn struct cil_classperms_set, the set field is a pointer to a\nstruct cil_classpermission which is looked up in the symbol table.\nSince the cil_classperms_set does not create the cil_classpermission,\nit should not reset it.\n\nSet the set field to NULL instead of resetting the classpermission\nthat it points to.\n\nSigned-off-by: James Carter <jwcart2@gmail.com>", "target": 0, "dataset": "other", "idx": 416786}
{"func": "void APar_ExtractDetails(FILE *isofile, uint8_t optional_output) {\n  char uint32_buffer[5];\n  Trackage track = {0};\n\n  AtomicInfo *mvhdAtom = APar_FindAtom(\"moov.mvhd\", false, VERSIONED_ATOM, 0);\n  if (mvhdAtom != NULL) {\n    APar_ExtractMovieDetails(uint32_buffer, isofile, mvhdAtom);", "project": "atomicparsley", "hash": 235760196453077041717585104526456578938, "size": 102, "commit_id": "d72ccf06c98259d7261e0f3ac4fd8717778782c1", "message": "Avoid stack overflow\n\nrefs: https://github.com/wez/atomicparsley/issues/32", "target": 1, "dataset": "other", "idx": 208981}
{"func": "void APar_ExtractDetails(FILE *isofile, uint8_t optional_output) {\n  char uint32_buffer[8];\n  Trackage track = {0};\n\n  AtomicInfo *mvhdAtom = APar_FindAtom(\"moov.mvhd\", false, VERSIONED_ATOM, 0);\n  if (mvhdAtom != NULL) {\n    APar_ExtractMovieDetails(uint32_buffer, isofile, mvhdAtom);", "project": "atomicparsley", "hash": 27305014511647361793546293488526992046, "size": 102, "commit_id": "d72ccf06c98259d7261e0f3ac4fd8717778782c1", "message": "Avoid stack overflow\n\nrefs: https://github.com/wez/atomicparsley/issues/32", "target": 0, "dataset": "other", "idx": 417017}
{"func": "  if (ret < 0\n      && session->security_parameters.read_mac_algorithm != GNUTLS_MAC_NULL)\n    {\n      gnutls_assert ();\n      return GNUTLS_E_INTERNAL_ERROR;\n    }\n\n  if (ciphertext.size < (unsigned) blocksize + hash_size)\n    {\n      _gnutls_record_log\n\t(\"REC[%x]: Short record length %d < %d + %d (under attack?)\\n\",\n\t session, ciphertext.size, blocksize, hash_size);\n      gnutls_assert ();\n      return GNUTLS_E_DECRYPTION_FAILED;\n    }\n\n  /* actual decryption (inplace)\n   */\n  switch (_gnutls_cipher_is_block\n\n      pad = ciphertext.data[ciphertext.size - 1] + 1;\t/* pad */\n\n      if ((int)pad > (int)ciphertext.size - hash_size)\n\t{\n\t  gnutls_assert ();\n\t  /* We do not fail here. We check below for the\n\t   * the pad_failed. If zero means success.\n\t   */\n\t  pad_failed = GNUTLS_E_DECRYPTION_FAILED;\n\t}", "project": "gnutls", "hash": 133466176970244054302162994213823627570, "size": 175, "commit_id": "d223040e498bd50a4b9e0aa493e78587ae1ed653", "message": "Fix broken debug check for GNUTLS-SA-2008-1.", "target": 1, "dataset": "other", "idx": 209003}
{"func": "\n  if (ret < 0\n      && session->security_parameters.read_mac_algorithm != GNUTLS_MAC_NULL)\n    {\n      gnutls_assert ();\n      return GNUTLS_E_INTERNAL_ERROR;\n    }\n\n  /* actual decryption (inplace)\n   */\n  switch (_gnutls_cipher_is_block\n      pad = ciphertext.data[ciphertext.size - 1] + 1;\t/* pad */\n\n      if ((int)pad > (int)ciphertext.size - hash_size)\n\t{\n\t  gnutls_assert ();\n\t  _gnutls_record_log\n\t    (\"REC[%x]: Short record length %d > %d - %d (under attack?)\\n\",\n\t     session, pad, ciphertext.size, hash_size);\n\t  /* We do not fail here. We check below for the\n\t   * the pad_failed. If zero means success.\n\t   */\n\t  pad_failed = GNUTLS_E_DECRYPTION_FAILED;\n\t}", "project": "gnutls", "hash": 71075734724174952553502640199265785099, "size": 169, "commit_id": "d223040e498bd50a4b9e0aa493e78587ae1ed653", "message": "Fix broken debug check for GNUTLS-SA-2008-1.", "target": 0, "dataset": "other", "idx": 417234}
{"func": "\tgethex(sfd,(uint32 *)&sf->sfntRevision);\n    }\n    else if ( strmatch(tok,\"LayerCount:\")==0 )\n    {\n\td->had_layer_cnt = true;\n\tgetint(sfd,&sf->layer_cnt);\n\tif ( sf->layer_cnt>2 ) {\n\t    sf->layers = realloc(sf->layers,sf->layer_cnt*sizeof(LayerInfo));\n\t    memset(sf->layers+2,0,(sf->layer_cnt-2)*sizeof(LayerInfo));\n\t}\n    }\n    else if ( strmatch(tok,\"Layer:\")==0 )\n    {\n        // TODO: Read the U. F. O. path.", "project": "fontforge", "hash": 174470206429737553728553607144275060858, "size": 826, "commit_id": "048a91e2682c1a8936ae34dbc7bd70291ec05410", "message": "Fix for #4084 Use-after-free (heap) in the SFD_GetFontMetaData() function\nFix for #4086 NULL pointer dereference in the SFDGetSpiros() function\nFix for #4088 NULL pointer dereference in the SFD_AssignLookups() function\nAdd empty sf->fontname string if it isn't set, fixing #4089 #4090 and many\n  other potential issues (many downstream calls to strlen() on the value).", "target": 1, "dataset": "other", "idx": 209042}
{"func": "\tgethex(sfd,(uint32 *)&sf->sfntRevision);\n    }\n    else if ( strmatch(tok,\"LayerCount:\")==0 )\n    {\n\td->had_layer_cnt = true;\n\tint layer_cnt_tmp;\n\tgetint(sfd,&layer_cnt_tmp);\n\tif ( layer_cnt_tmp>2 ) {\n\t    sf->layers = realloc(sf->layers,sf->layer_cnt*sizeof(LayerInfo));\n\t    memset(sf->layers+2,0,(sf->layer_cnt-2)*sizeof(LayerInfo));\n\t    sf->layer_cnt = layer_cnt_tmp;\n\t}\n    }\n    else if ( strmatch(tok,\"Layer:\")==0 )\n    {\n        // TODO: Read the U. F. O. path.", "project": "fontforge", "hash": 325003590001280455271075128217041102898, "size": 828, "commit_id": "048a91e2682c1a8936ae34dbc7bd70291ec05410", "message": "Fix for #4084 Use-after-free (heap) in the SFD_GetFontMetaData() function\nFix for #4086 NULL pointer dereference in the SFDGetSpiros() function\nFix for #4088 NULL pointer dereference in the SFD_AssignLookups() function\nAdd empty sf->fontname string if it isn't set, fixing #4089 #4090 and many\n  other potential issues (many downstream calls to strlen() on the value).", "target": 0, "dataset": "other", "idx": 417765}
{"func": "    size_t out_size;\n    void *out_buf;\n    z_stream strm;\n    mz_ulong crc;\n\n    out_size = in_len + 32;\n    out_buf = flb_malloc(out_size);\n    if (!out_buf) {\n        flb_errno();\n        flb_error(\"[gzip] could not allocate outgoing buffer\");\n        return -1;\n    }", "project": "fluent-bit", "hash": 298785041176571465256262117903103760897, "size": 90, "commit_id": "cadff53c093210404aed01c4cf586adb8caa07af", "message": "gzip: fix compression size calculation (oss-fuzz 27261)\n\nSigned-off-by: davkor <david@adalogics.com>", "target": 1, "dataset": "other", "idx": 209045}
{"func": "    size_t out_size;\n    void *out_buf;\n    z_stream strm;\n    mz_ulong crc;\n\n\n    /*\n     * GZIP relies on an algorithm with worst-case expansion\n     * of 5 bytes per 32KB data. This means we need to create a variable\n     * length output, that depends on the input length.\n     * See RFC 1951 for details.\n     */\n    int max_input_expansion = ((int)(in_len / 32000) + 1) * 5;\n\n    /*\n     * Max compressed size is equal to sum of:\n     *   10 byte header\n     *   8 byte foot\n     *   max input expansion\n     *   size of input\n     */\n    out_size = 10 + 8 + max_input_expansion + in_len;\n    out_buf = flb_malloc(out_size);\n\n    if (!out_buf) {\n        flb_errno();\n        flb_error(\"[gzip] could not allocate outgoing buffer\");\n        return -1;\n    }", "project": "fluent-bit", "hash": 330549964469517458967295596525565164446, "size": 107, "commit_id": "cadff53c093210404aed01c4cf586adb8caa07af", "message": "gzip: fix compression size calculation (oss-fuzz 27261)\n\nSigned-off-by: davkor <david@adalogics.com>", "target": 0, "dataset": "other", "idx": 417948}
{"func": "\t\t\treturn;\n\n\t\tfilter_data(fs->id, line);\n\n\t\tgoto nextline;\n\n\tcase IO_DISCONNECTED:\n\t\tio_free(fs->io);\n\t\tfs->io = NULL;\n\t\tbreak;\n\t}\n}", "project": "src", "hash": 83839070917012004449638635653379607384, "size": 27, "commit_id": "6c3220444ed06b5796dedfd53a0f4becd903c0d1", "message": "smtpd's filter state machine can prematurely release resources\nleading to a crash.  From gilles@", "target": 1, "dataset": "other", "idx": 209807}
{"func": "\t\tif (line == NULL)\n\t\t\treturn;\n\n\t\tfilter_data(fs->id, line);\n\n\t\tgoto nextline;\n\t}\n}", "project": "src", "hash": 4978771540234783063665004784258488512, "size": 22, "commit_id": "6c3220444ed06b5796dedfd53a0f4becd903c0d1", "message": "smtpd's filter state machine can prematurely release resources\nleading to a crash.  From gilles@", "target": 0, "dataset": "other", "idx": 421514}
{"func": "\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_ADDR, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, p->data_len);\n\t\t/* set new ipaddr */\n\t\tmemcpy(data+3, temp, 4);\n\t\tprintf(\"Setting LAN Alert %d IP Address to %d.%d.%d.%d\\n\", alert,\n\t\t       data[3], data[4], data[5], data[6]);\n\t\trc = set_lan_param_nowait(intf, chan, IPMI_LANP_DEST_ADDR, data, p->data_len);\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_ADDR, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, p->data_len);\n\t\t/* set new macaddr */\n\t\tmemcpy(data+7, temp, 6);\n\t\tprintf(\"Setting LAN Alert %d MAC Address to \"\n\t\t       \"%s\\n\", alert, mac2str(&data[7]));\n\t\trc = set_lan_param_nowait(intf, chan, IPMI_LANP_DEST_ADDR, data, p->data_len);\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_ADDR, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, p->data_len);\n\n\t\tif (strncasecmp(argv[1], \"def\", 3) == 0 ||\n\t\t    strncasecmp(argv[1], \"default\", 7) == 0) {\n\t\t\tprintf(\"Setting LAN Alert %d to use Default Gateway\\n\", alert);\n\t\t\tdata[2] = 0;\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, p->data_len);\n\n\t\tif (strncasecmp(argv[1], \"on\", 2) == 0 ||\n\t\t    strncasecmp(argv[1], \"yes\", 3) == 0) {\n\t\t\tprintf(\"Setting LAN Alert %d to Acknowledged\\n\", alert);\n\t\t\tdata[1] |= 0x80;\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, p->data_len);\n\n\t\tif (strncasecmp(argv[1], \"pet\", 3) == 0) {\n\t\t\tprintf(\"Setting LAN Alert %d destination to PET Trap\\n\", alert);\n\t\t\tdata[1] &= ~0x07;\n\t\t}\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, p->data_len);\n\n\t\tif (str2uchar(argv[1], &data[2]) != 0) {\n\t\t\tlprintf(LOG_ERR, \"Invalid time: %s\", argv[1]);\n\t\t\treturn (-1);\n\t\t}\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, p->data_len);\n\n\t\tif (str2uchar(argv[1], &data[3]) != 0) {\n\t\t\tlprintf(LOG_ERR, \"Invalid retry: %s\", argv[1]);\n\t\t\treturn (-1);\n\t\t}", "project": "ipmitool", "hash": 84380158635072339554778817955037156665, "size": 170, "commit_id": "d45572d71e70840e0d4c50bf48218492b79c1a10", "message": "lanp: Fix buffer overflows in get_lan_param_select\n\nPartial fix for CVE-2020-5208, see\nhttps://github.com/ipmitool/ipmitool/security/advisories/GHSA-g659-9qxw-p7cp\n\nThe `get_lan_param_select` function is missing a validation check on the\nresponse\u2019s `data_len`, which it then returns to caller functions, where\nstack buffer overflow can occur.", "target": 1, "dataset": "other", "idx": 209813}
{"func": "\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_ADDR, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, __min(p->data_len, sizeof(data)));\n\t\t/* set new ipaddr */\n\t\tmemcpy(data+3, temp, 4);\n\t\tprintf(\"Setting LAN Alert %d IP Address to %d.%d.%d.%d\\n\", alert,\n\t\t       data[3], data[4], data[5], data[6]);\n\t\trc = set_lan_param_nowait(intf, chan, IPMI_LANP_DEST_ADDR, data, p->data_len);\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_ADDR, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, __min(p->data_len, sizeof(data)));\n\t\t/* set new macaddr */\n\t\tmemcpy(data+7, temp, 6);\n\t\tprintf(\"Setting LAN Alert %d MAC Address to \"\n\t\t       \"%s\\n\", alert, mac2str(&data[7]));\n\t\trc = set_lan_param_nowait(intf, chan, IPMI_LANP_DEST_ADDR, data, p->data_len);\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_ADDR, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, __min(p->data_len, sizeof(data)));\n\n\t\tif (strncasecmp(argv[1], \"def\", 3) == 0 ||\n\t\t    strncasecmp(argv[1], \"default\", 7) == 0) {\n\t\t\tprintf(\"Setting LAN Alert %d to use Default Gateway\\n\", alert);\n\t\t\tdata[2] = 0;\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, __min(p->data_len, sizeof(data)));\n\n\t\tif (strncasecmp(argv[1], \"on\", 2) == 0 ||\n\t\t    strncasecmp(argv[1], \"yes\", 3) == 0) {\n\t\t\tprintf(\"Setting LAN Alert %d to Acknowledged\\n\", alert);\n\t\t\tdata[1] |= 0x80;\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, __min(p->data_len, sizeof(data)));\n\n\t\tif (strncasecmp(argv[1], \"pet\", 3) == 0) {\n\t\t\tprintf(\"Setting LAN Alert %d destination to PET Trap\\n\", alert);\n\t\t\tdata[1] &= ~0x07;\n\t\t}\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, __min(p->data_len, sizeof(data)));\n\n\t\tif (str2uchar(argv[1], &data[2]) != 0) {\n\t\t\tlprintf(LOG_ERR, \"Invalid time: %s\", argv[1]);\n\t\t\treturn (-1);\n\t\t}\n\t\t/* get current parameter */\n\t\tp = get_lan_param_select(intf, chan, IPMI_LANP_DEST_TYPE, alert);\n\t\tif (!p) {\n\t\t\treturn (-1);\n\t\t}\n\t\tmemcpy(data, p->data, __min(p->data_len, sizeof(data)));\n\n\t\tif (str2uchar(argv[1], &data[3]) != 0) {\n\t\t\tlprintf(LOG_ERR, \"Invalid retry: %s\", argv[1]);\n\t\t\treturn (-1);\n\t\t}", "project": "ipmitool", "hash": 128890414513340911964604109333752466918, "size": 170, "commit_id": "d45572d71e70840e0d4c50bf48218492b79c1a10", "message": "lanp: Fix buffer overflows in get_lan_param_select\n\nPartial fix for CVE-2020-5208, see\nhttps://github.com/ipmitool/ipmitool/security/advisories/GHSA-g659-9qxw-p7cp\n\nThe `get_lan_param_select` function is missing a validation check on the\nresponse\u2019s `data_len`, which it then returns to caller functions, where\nstack buffer overflow can occur.", "target": 0, "dataset": "other", "idx": 421797}
{"func": "      goto err2;\n    }\n    \n    refObj.initRef(embRef.num, embRef.gen);\n    refObj.fetch(xref, &strObj);\n    refObj.free();\n    strObj.streamReset();\n    while ((c = strObj.streamGetChar()) != EOF) {\n      fputc(c, tmpFile);\n    }\n    strObj.streamClose();", "project": "poppler", "hash": 294666036737394333902439398055542844956, "size": 195, "commit_id": "1a531dcfee1c6fc79a414c38cbe7327fbf9a59d8", "message": "Fix a crash with invalid embedded fonts", "target": 1, "dataset": "other", "idx": 209818}
{"func": "    }\n    \n    refObj.initRef(embRef.num, embRef.gen);\n    refObj.fetch(xref, &strObj);\n    refObj.free();\n    if (!strObj.isStream()) {\n      error(-1, \"Embedded font object is wrong type\");\n      strObj.free();\n      fclose(tmpFile);\n      goto err2;\n    }\n    strObj.streamReset();\n    while ((c = strObj.streamGetChar()) != EOF) {\n      fputc(c, tmpFile);\n    }\n    strObj.streamClose();", "project": "poppler", "hash": 117103155606137295047808127184080162393, "size": 201, "commit_id": "1a531dcfee1c6fc79a414c38cbe7327fbf9a59d8", "message": "Fix a crash with invalid embedded fonts", "target": 0, "dataset": "other", "idx": 421892}
{"func": "            \"Regular expression cannot contain an embedded null byte\",\n            _regex.find('\\0') == std::string::npos);\n\n    uassert(ErrorCodes::BadValue,\n            \"Regular expression options string cannot contain an embedded null byte\",\n            _flags.find('\\0') == std::string::npos);\n}", "project": "mongo", "hash": 196906255193958728876698958313286561082, "size": 12, "commit_id": "64095239f41e9f3841d8be9088347db56d35c891", "message": "SERVER-51083 Reject invalid UTF-8 from $regex match expressions", "target": 1, "dataset": "other", "idx": 209844}
{"func": "            _regex.find('\\0') == std::string::npos);\n\n    uassert(ErrorCodes::BadValue,\n            \"Regular expression options string cannot contain an embedded null byte\",\n            _flags.find('\\0') == std::string::npos);\n\n    // isValidUTF8() checks for UTF-8 which does not map to a series of codepoints but does not\n    // check the validity of the code points themselves. These situations do not cause problems\n    // downstream so we do not do additional work to enforce that the code points are valid.\n    uassert(\n        5108300, \"Regular expression is invalid UTF-8\", isValidUTF8(_regex) && isValidUTF8(_flags));\n}", "project": "mongo", "hash": 296417704049406944583817007241379160223, "size": 18, "commit_id": "64095239f41e9f3841d8be9088347db56d35c891", "message": "SERVER-51083 Reject invalid UTF-8 from $regex match expressions", "target": 0, "dataset": "other", "idx": 422547}
{"func": "\tObjectAddress refAddr;\n\tRelation\trel;\n\n\taddress =\n\t\tget_object_address_rv(stmt->objectType, stmt->relation, (List *) stmt->object,\n\t\t\t\t\t\t\t  &rel, AccessExclusiveLock, false);\n\n\t/*\n\t * If a relation was involved, it would have been opened and locked. We\n\t * don't need the relation here, but we'll retain the lock until commit.\n\t */", "project": "postgres", "hash": 24619234578332954271203137488354771057, "size": 27, "commit_id": "b048f558dd7c26a0c630a2cff29d3d8981eaf6b9", "message": "Fix priv checks for ALTER <object> DEPENDS ON EXTENSION\n\nMarking an object as dependant on an extension did not have any\nprivilege check whatsoever; this allowed any user to mark objects as\ndroppable by anyone able to DROP EXTENSION, which could be used to cause\nsystem-wide havoc.  Disallow by checking that the calling user owns the\nmentioned object.\n\n(No constraints are placed on the extension.)\n\nSecurity: CVE-2020-1720\nReported-by: Tom Lane\nDiscussion: 31605.1566429043@sss.pgh.pa.us", "target": 1, "dataset": "other", "idx": 209909}
{"func": "\tRelation\trel;\n\n\taddress =\n\t\tget_object_address_rv(stmt->objectType, stmt->relation, (List *) stmt->object,\n\t\t\t\t\t\t\t  &rel, AccessExclusiveLock, false);\n\n\t/*\n\t * Verify that the user is entitled to run the command.\n\t *\n\t * We don't check any privileges on the extension, because that's not\n\t * needed.  The object owner is stipulating, by running this command, that\n\t * the extension owner can drop the object whenever they feel like it,\n\t * which is not considered a problem.\n\t */\n\tcheck_object_ownership(GetUserId(),\n\t\t\t\t\t\t   stmt->objectType, address, stmt->object, rel);\n\n\t/*\n\t * If a relation was involved, it would have been opened and locked. We\n\t * don't need the relation here, but we'll retain the lock until commit.\n\t */", "project": "postgres", "hash": 169828503212497787193156423479860345699, "size": 38, "commit_id": "b048f558dd7c26a0c630a2cff29d3d8981eaf6b9", "message": "Fix priv checks for ALTER <object> DEPENDS ON EXTENSION\n\nMarking an object as dependant on an extension did not have any\nprivilege check whatsoever; this allowed any user to mark objects as\ndroppable by anyone able to DROP EXTENSION, which could be used to cause\nsystem-wide havoc.  Disallow by checking that the calling user owns the\nmentioned object.\n\n(No constraints are placed on the extension.)\n\nSecurity: CVE-2020-1720\nReported-by: Tom Lane\nDiscussion: 31605.1566429043@sss.pgh.pa.us", "target": 0, "dataset": "other", "idx": 423628}
{"func": "    if (CopyXPMColor(key,p,length) != (ssize_t) length)\n      break;\n    status=AddValueToSplayTree(xpm_colors,ConstantString(key),(void *) j);\n    /*\n      Parse color.\n    */\n    (void) CopyMagickString(target,\"gray\",MaxTextExtent);\n    q=(char *) NULL;\n    if (strlen(p) > width)\n      q=ParseXPMColor(p+width,MagickTrue);\n    *symbolic='\\0';\n    if (q != (char *) NULL)\n      {\n        while ((isspace((int) ((unsigned char) *q)) == 0) && (*q != '\\0'))\n          q++;", "project": "ImageMagick6", "hash": 259050726019501944998374563698666005017, "size": 275, "commit_id": "26538669546730c5b2dc36e7d48850f1f6928f94", "message": "https://github.com/ImageMagick/ImageMagick/issues/1895", "target": 1, "dataset": "other", "idx": 209922}
{"func": "      break;\n    status=AddValueToSplayTree(xpm_colors,ConstantString(key),(void *) j);\n    /*\n      Parse color.\n    */\n    (void) memset(target,0,sizeof(target));\n    (void) CopyMagickString(target,\"gray\",MaxTextExtent);\n    q=(char *) NULL;\n    if (strlen(p) > width)\n      q=ParseXPMColor(p+width,MagickTrue);\n    (void) memset(symbolic,0,sizeof(symbolic));\n    *symbolic='\\0';\n    if (q != (char *) NULL)\n      {\n        while ((isspace((int) ((unsigned char) *q)) == 0) && (*q != '\\0'))\n          q++;", "project": "ImageMagick6", "hash": 89856644996065360037481487117834578276, "size": 277, "commit_id": "26538669546730c5b2dc36e7d48850f1f6928f94", "message": "https://github.com/ImageMagick/ImageMagick/issues/1895", "target": 0, "dataset": "other", "idx": 424306}
{"func": "rfbSetClientColourMapBGR233(rfbClientPtr cl)\n{\n    char buf[sz_rfbSetColourMapEntriesMsg + 256 * 3 * 2];\n    rfbSetColourMapEntriesMsg *scme = (rfbSetColourMapEntriesMsg *)buf;\n    uint16_t *rgb = (uint16_t *)(&buf[sz_rfbSetColourMapEntriesMsg]);\n    int i, len;\n    int r, g, b;\n\n    if (cl->format.bitsPerPixel != 8 ) {\n        rfbErr(\"%s: client not 8 bits per pixel\\n\",\n        }\n    }\n\n    len += 256 * 3 * 2;\n\n    if (rfbWriteExact(cl, buf, len) < 0) {\n        rfbLogPerror(\"rfbSetClientColourMapBGR233: write\");\n        rfbCloseClient(cl);\n        return FALSE;\n    }\n    return TRUE;", "project": "libvncserver", "hash": 78377316020899026641978446341637956952, "size": 43, "commit_id": "53073c8d7e232151ea2ecd8a1243124121e10e2d", "message": "libvncserver: fix pointer aliasing/alignment issue\n\nAccessing byte-aligned data through uint16_t pointers can cause crashes\non some platforms or reduce the performance. Therefore ensure a proper\nstack alignment.", "target": 1, "dataset": "other", "idx": 209932}
{"func": "rfbSetClientColourMapBGR233(rfbClientPtr cl)\n{\n    union {\n        char bytes[sz_rfbSetColourMapEntriesMsg + 256 * 3 * 2];\n        rfbSetColourMapEntriesMsg msg;\n    } buf;\n    rfbSetColourMapEntriesMsg *scme = &buf.msg;\n    uint16_t *rgb = (uint16_t *)(&buf.bytes[sz_rfbSetColourMapEntriesMsg]);\n    int i, len;\n    int r, g, b;\n\n    if (cl->format.bitsPerPixel != 8 ) {\n        rfbErr(\"%s: client not 8 bits per pixel\\n\",\n        }\n    }\n\n    len += 256 * 3 * 2;\n\n    if (rfbWriteExact(cl, buf.bytes, len) < 0) {\n        rfbLogPerror(\"rfbSetClientColourMapBGR233: write\");\n        rfbCloseClient(cl);\n        return FALSE;\n    }\n    return TRUE;", "project": "libvncserver", "hash": 332019210180149550878360388455728173657, "size": 46, "commit_id": "53073c8d7e232151ea2ecd8a1243124121e10e2d", "message": "libvncserver: fix pointer aliasing/alignment issue\n\nAccessing byte-aligned data through uint16_t pointers can cause crashes\non some platforms or reduce the performance. Therefore ensure a proper\nstack alignment.", "target": 0, "dataset": "other", "idx": 424538}
{"func": "static int ntlm_read_ntlm_v2_client_challenge(wStream* s, NTLMv2_CLIENT_CHALLENGE* challenge)\n{\n\tsize_t size;\n\tStream_Read_UINT8(s, challenge->RespType);\n\tStream_Read_UINT8(s, challenge->HiRespType);\n\tStream_Read_UINT16(s, challenge->Reserved1);\n\tStream_Read_UINT32(s, challenge->Reserved2);\n\tStream_Read(s, challenge->Timestamp, 8);", "project": "FreeRDP", "hash": 112521265647314697012790560885353393661, "size": 24, "commit_id": "c098f21fdaadca57ff649eee1674f6cc321a2ec4", "message": "Fixed oob read in ntlm_read_ntlm_v2_response", "target": 1, "dataset": "other", "idx": 209951}
{"func": "static int ntlm_read_ntlm_v2_client_challenge(wStream* s, NTLMv2_CLIENT_CHALLENGE* challenge)\n{\n\tsize_t size;\n\tif (Stream_GetRemainingLength(s) < 28)\n\t\treturn -1;\n\n\tStream_Read_UINT8(s, challenge->RespType);\n\tStream_Read_UINT8(s, challenge->HiRespType);\n\tStream_Read_UINT16(s, challenge->Reserved1);\n\tStream_Read_UINT32(s, challenge->Reserved2);\n\tStream_Read(s, challenge->Timestamp, 8);", "project": "FreeRDP", "hash": 325236519997364558892068820772982758036, "size": 27, "commit_id": "c098f21fdaadca57ff649eee1674f6cc321a2ec4", "message": "Fixed oob read in ntlm_read_ntlm_v2_response", "target": 0, "dataset": "other", "idx": 424858}
{"func": "      ssh_set_error_oom(session);\n      sftp_client_message_free(msg);\n      return NULL;\n  }\n\n  ssh_buffer_add_data(msg->complete_message,\n                      ssh_buffer_get(payload),\n                      ssh_buffer_get_len(payload));\n\n  ssh_buffer_get_u32(payload, &msg->id);\n\n  switch(msg->type) {\n    case SSH_FXP_CLOSE:", "project": "libssh-mirror", "hash": 104812262391619336608412301470901113139, "size": 203, "commit_id": "2782cb0495b7450bd8fe43ce4af886b66fea6c40", "message": "sftpserver: Add missing return check for ssh_buffer_add_data()\n\nSigned-off-by: Andreas Schneider <asn@cryptomilk.org>\nReviewed-by: Anderson Toshiyuki Sasaki <ansasaki@redhat.com>\nReviewed-by: Jakub Jelen <jjelen@redhat.com>", "target": 1, "dataset": "other", "idx": 209954}
{"func": "      ssh_set_error_oom(session);\n      sftp_client_message_free(msg);\n      return NULL;\n  }\n\n  rc = ssh_buffer_add_data(msg->complete_message,\n                           ssh_buffer_get(payload),\n                           ssh_buffer_get_len(payload));\n  if (rc < 0) {\n      ssh_set_error_oom(session);\n      sftp_client_message_free(msg);\n      return NULL;\n  }\n\n  ssh_buffer_get_u32(payload, &msg->id);\n\n  switch(msg->type) {\n    case SSH_FXP_CLOSE:", "project": "libssh-mirror", "hash": 111915200315433769761170865971793335091, "size": 208, "commit_id": "2782cb0495b7450bd8fe43ce4af886b66fea6c40", "message": "sftpserver: Add missing return check for ssh_buffer_add_data()\n\nSigned-off-by: Andreas Schneider <asn@cryptomilk.org>\nReviewed-by: Anderson Toshiyuki Sasaki <ansasaki@redhat.com>\nReviewed-by: Jakub Jelen <jjelen@redhat.com>", "target": 0, "dataset": "other", "idx": 424886}
{"func": "static int changedline (const Proto *p, int oldpc, int newpc) {\n  while (oldpc++ < newpc) {\n    if (p->lineinfo[oldpc] != 0)\n      return (luaG_getfuncline(p, oldpc - 1) != luaG_getfuncline(p, newpc));\n  }\n  return 0;  /* no line changes in the way */\n}", "project": "lua", "hash": 103380299185166225191075529446543003471, "size": 7, "commit_id": "ae5b5ba529753c7a653901ffc29b5ea24c3fdf3a", "message": "Fixed bug: line hooks in stripped functions\n\nLine-hook handling was accessing debug info. without checking whether\nit was present.", "target": 1, "dataset": "other", "idx": 210098}
{"func": "static int changedline (const Proto *p, int oldpc, int newpc) {\n  if (p->lineinfo == NULL)  /* no debug information? */\n    return 0;\n  while (oldpc++ < newpc) {\n    if (p->lineinfo[oldpc] != 0)\n      return (luaG_getfuncline(p, oldpc - 1) != luaG_getfuncline(p, newpc));\n  }\n  return 0;  /* no line changes between positions */\n}", "project": "lua", "hash": 219668759070622296330128934071627053940, "size": 9, "commit_id": "ae5b5ba529753c7a653901ffc29b5ea24c3fdf3a", "message": "Fixed bug: line hooks in stripped functions\n\nLine-hook handling was accessing debug info. without checking whether\nit was present.", "target": 0, "dataset": "other", "idx": 427930}
{"func": "            if ((head = tail = malloc(sizeof *head)) == NULL ||\n                (tail->alias = strdup(alias)) == NULL ||\n                (tail->dir = strdup(dir)) == NULL) {\n                die_mem();\n            }\n            tail->next = NULL;\n        } else {\n            DirAlias *curr;\n\n            if ((curr = malloc(sizeof *curr)) == NULL ||\n                (curr->alias = strdup(alias)) == NULL ||\n                (curr->dir = strdup(dir)) == NULL) {\n                die_mem();\n            }\n            tail->next = curr;\n            tail = curr;\n        }\n    }\n    fclose(fp);\n    aliases_up++;\n\n    return 0;", "project": "pure-ftpd", "hash": 117767822613476471687393411275640975670, "size": 63, "commit_id": "8d0d42542e2cb7a56d645fbe4d0ef436e38bcefa", "message": "diraliases: always set the tail of the list to NULL\n\nSpotted and reported by Antonio Norales from GitHub Security Labs.\nThanks!", "target": 1, "dataset": "other", "idx": 210109}
{"func": "        if (head == NULL) {\n            if ((head = tail = malloc(sizeof *head)) == NULL ||\n                (tail->alias = strdup(alias)) == NULL ||\n                (tail->dir = strdup(dir)) == NULL) {\n                die_mem();\n            }\n        } else {\n            DirAlias *curr;\n\n            if ((curr = malloc(sizeof *curr)) == NULL ||\n                (curr->alias = strdup(alias)) == NULL ||\n                die_mem();\n            }\n            tail->next = curr;\n            tail = curr;\n        }\n        tail->next = NULL;\n    }\n    fclose(fp);\n    aliases_up++;\n\n    return 0;", "project": "pure-ftpd", "hash": 109088355298658185757450174813098275639, "size": 63, "commit_id": "8d0d42542e2cb7a56d645fbe4d0ef436e38bcefa", "message": "diraliases: always set the tail of the list to NULL\n\nSpotted and reported by Antonio Norales from GitHub Security Labs.\nThanks!", "target": 0, "dataset": "other", "idx": 428216}
{"func": "read_and_discard_scanlines(j_decompress_ptr cinfo, JDIMENSION num_lines)\n{\n  JDIMENSION n;\n  my_master_ptr master = (my_master_ptr)cinfo->master;\n  JSAMPARRAY scanlines = NULL;\n  void (*color_convert) (j_decompress_ptr cinfo, JSAMPIMAGE input_buf,\n                         JDIMENSION input_row, JSAMPARRAY output_buf,\n                         int num_rows) = NULL;\n  void (*color_quantize) (j_decompress_ptr cinfo, JSAMPARRAY input_buf,\n                          JSAMPARRAY output_buf, int num_rows) = NULL;\n\n  if (cinfo->cconvert && cinfo->cconvert->color_convert) {\n    color_convert = cinfo->cconvert->color_convert;\n    cinfo->cconvert->color_convert = noop_convert;\n  }\n\n  if (cinfo->cquantize && cinfo->cquantize->color_quantize) {\n    color_quantize = cinfo->cquantize->color_quantize;\n    cinfo->cquantize->color_quantize = noop_quantize;", "project": "libjpeg-turbo", "hash": 164487153551446965761993154553490265876, "size": 35, "commit_id": "6d2e8837b440ce4d8befd805a5abc0d351028d70", "message": "jpeg_skip_scanlines(): Avoid NULL + 0 UBSan error\n\nThis error occurs at the call to (*cinfo->cconvert->color_convert)() in\nsep_upsample() whenever cinfo->upsample->need_context_rows == TRUE\n(i.e. whenever h2v2 or h1v2 fancy upsampling is used.)  The error is\ninnocuous, since (*cinfo->cconvert->color_convert)() points to a dummy\nfunction (noop_convert()) in that case.\n\nFixes #470", "target": 1, "dataset": "other", "idx": 210147}
{"func": "read_and_discard_scanlines(j_decompress_ptr cinfo, JDIMENSION num_lines)\n{\n  JDIMENSION n;\n  my_master_ptr master = (my_master_ptr)cinfo->master;\n  JSAMPLE dummy_sample[1] = { 0 };\n  JSAMPROW dummy_row = dummy_sample;\n  JSAMPARRAY scanlines = NULL;\n  void (*color_convert) (j_decompress_ptr cinfo, JSAMPIMAGE input_buf,\n                         JDIMENSION input_row, JSAMPARRAY output_buf,\n                         int num_rows) = NULL;\n  void (*color_quantize) (j_decompress_ptr cinfo, JSAMPARRAY input_buf,\n                          JSAMPARRAY output_buf, int num_rows) = NULL;\n\n  if (cinfo->cconvert && cinfo->cconvert->color_convert) {\n    color_convert = cinfo->cconvert->color_convert;\n    cinfo->cconvert->color_convert = noop_convert;\n    /* This just prevents UBSan from complaining about adding 0 to a NULL\n     * pointer.  The pointer isn't actually used.\n     */\n    scanlines = &dummy_row;\n  }\n\n  if (cinfo->cquantize && cinfo->cquantize->color_quantize) {\n    color_quantize = cinfo->cquantize->color_quantize;\n    cinfo->cquantize->color_quantize = noop_quantize;", "project": "libjpeg-turbo", "hash": 83983514538588629499615690431545580587, "size": 41, "commit_id": "6d2e8837b440ce4d8befd805a5abc0d351028d70", "message": "jpeg_skip_scanlines(): Avoid NULL + 0 UBSan error\n\nThis error occurs at the call to (*cinfo->cconvert->color_convert)() in\nsep_upsample() whenever cinfo->upsample->need_context_rows == TRUE\n(i.e. whenever h2v2 or h1v2 fancy upsampling is used.)  The error is\ninnocuous, since (*cinfo->cconvert->color_convert)() points to a dummy\nfunction (noop_convert()) in that case.\n\nFixes #470", "target": 0, "dataset": "other", "idx": 428737}
{"func": "\t\tphp_error_docref(NULL TSRMLS_CC, E_WARNING, \"Tag cannot be empty\");\n\t\tRETURN_FALSE;\n\t}\n\n\td = enchant_broker_request_dict(pbroker->pbroker, (const char *)tag);\n\tif (d) {\n\t\tif (pbroker->dictcnt) {\n\t\t\tpbroker->dict = (enchant_dict **)erealloc(pbroker->dict, sizeof(enchant_dict *) * pbroker->dictcnt);\n\t\t\tpos = pbroker->dictcnt++;\n\t\t} else {\n\t\t\tpbroker->dict = (enchant_dict **)emalloc(sizeof(enchant_dict *));\n\t\t\tpos = 0;\n\t\t\tpbroker->dictcnt++;\n\t\t}\n\n\t\tdict = pbroker->dict[pos] = (enchant_dict *)emalloc(sizeof(enchant_dict));\n\t\tdict->id = pos;\n\t\tdict->pbroker = pbroker;", "project": "php-src", "hash": 13727473563489801685086003443308994419, "size": 50, "commit_id": "bdfe457a2c1b47209e32783b3a6447e81baf179a", "message": "Port for for bug #68552", "target": 1, "dataset": "other", "idx": 210165}
{"func": "\t\tRETURN_FALSE;\n\t}\n\n\td = enchant_broker_request_dict(pbroker->pbroker, (const char *)tag);\n\tif (d) {\n\t\tpos = pbroker->dictcnt++;\n\t\tif (pbroker->dictcnt) {\n\t\t\tpbroker->dict = (enchant_dict **)erealloc(pbroker->dict, sizeof(enchant_dict *) * pbroker->dictcnt);\n\t\t} else {\n\t\t\tpbroker->dict = (enchant_dict **)emalloc(sizeof(enchant_dict *));\n\t\t\tpos = 0;\n\t\t}\n\n\t\tdict = pbroker->dict[pos] = (enchant_dict *)emalloc(sizeof(enchant_dict));\n\t\tdict->id = pos;\n\t\tdict->pbroker = pbroker;", "project": "php-src", "hash": 290261600071308625104841727243059581538, "size": 49, "commit_id": "bdfe457a2c1b47209e32783b3a6447e81baf179a", "message": "Port for for bug #68552", "target": 0, "dataset": "other", "idx": 429189}
{"func": "\t\t\treturn NULL;\n\t\t}\n\t\ttty = strrchr(tty, '/') + 1;\n\t}\n\t/* Make sure the tty wasn't actually a directory (no basename). */\n\tif (strlen(tty) == 0) {\n\t\treturn NULL;\n\t}\n\treturn tty;\n}", "project": "linux-pam", "hash": 336131573636879399421041172856936495490, "size": 21, "commit_id": "9dcead87e6d7f66d34e7a56d11a30daca367dffb", "message": "pam_timestamp: fix potential directory traversal issue (ticket #27)\n\npam_timestamp uses values of PAM_RUSER and PAM_TTY as components of\nthe timestamp pathname it creates, so extra care should be taken to\navoid potential directory traversal issues.\n\n* modules/pam_timestamp/pam_timestamp.c (check_tty): Treat\n\".\" and \"..\" tty values as invalid.\n(get_ruser): Treat \".\" and \"..\" ruser values, as well as any ruser\nvalue containing '/', as invalid.\n\nFixes CVE-2014-2583.\n\nReported-by: Sebastian Krahmer <krahmer@suse.de>", "target": 1, "dataset": "other", "idx": 210171}
{"func": "\t\t\treturn NULL;\n\t\t}\n\t\ttty = strrchr(tty, '/') + 1;\n\t}\n\t/* Make sure the tty wasn't actually a directory (no basename). */\n\tif (!strlen(tty) || !strcmp(tty, \".\") || !strcmp(tty, \"..\")) {\n\t\treturn NULL;\n\t}\n\treturn tty;\n}", "project": "linux-pam", "hash": 1321413214506957733795541791949231362, "size": 21, "commit_id": "9dcead87e6d7f66d34e7a56d11a30daca367dffb", "message": "pam_timestamp: fix potential directory traversal issue (ticket #27)\n\npam_timestamp uses values of PAM_RUSER and PAM_TTY as components of\nthe timestamp pathname it creates, so extra care should be taken to\navoid potential directory traversal issues.\n\n* modules/pam_timestamp/pam_timestamp.c (check_tty): Treat\n\".\" and \"..\" tty values as invalid.\n(get_ruser): Treat \".\" and \"..\" ruser values, as well as any ruser\nvalue containing '/', as invalid.\n\nFixes CVE-2014-2583.\n\nReported-by: Sebastian Krahmer <krahmer@suse.de>", "target": 0, "dataset": "other", "idx": 429355}
{"func": "\tif ((ruser == NULL) || (strlen(ruser) == 0)) {\n\t\t/* Barring that, use the current RUID. */\n\t\tpwd = pam_modutil_getpwuid(pamh, getuid());\n\t\tif (pwd != NULL) {\n\t\t\truser = pwd->pw_name;\n\t\t}\n\t}\n\tif (ruser == NULL || strlen(ruser) >= ruserbuflen) {\n\t\t*ruserbuf = '\\0';\n\t\treturn -1;\n\t}", "project": "linux-pam", "hash": 227818384334126316111651650155998253999, "size": 25, "commit_id": "9dcead87e6d7f66d34e7a56d11a30daca367dffb", "message": "pam_timestamp: fix potential directory traversal issue (ticket #27)\n\npam_timestamp uses values of PAM_RUSER and PAM_TTY as components of\nthe timestamp pathname it creates, so extra care should be taken to\navoid potential directory traversal issues.\n\n* modules/pam_timestamp/pam_timestamp.c (check_tty): Treat\n\".\" and \"..\" tty values as invalid.\n(get_ruser): Treat \".\" and \"..\" ruser values, as well as any ruser\nvalue containing '/', as invalid.\n\nFixes CVE-2014-2583.\n\nReported-by: Sebastian Krahmer <krahmer@suse.de>", "target": 1, "dataset": "other", "idx": 210172}
{"func": "\t\t/* Barring that, use the current RUID. */\n\t\tpwd = pam_modutil_getpwuid(pamh, getuid());\n\t\tif (pwd != NULL) {\n\t\t\truser = pwd->pw_name;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * This ruser is used by format_timestamp_name as a component\n\t\t * of constructed timestamp pathname, so \".\", \"..\", and '/'\n\t\t * are disallowed to avoid potential path traversal issues.\n\t\t */\n\t\tif (!strcmp(ruser, \".\") ||\n\t\t    !strcmp(ruser, \"..\") ||\n\t\t    strchr(ruser, '/')) {\n\t\t\truser = NULL;\n\t\t}\n\t}\n\tif (ruser == NULL || strlen(ruser) >= ruserbuflen) {\n\t\t*ruserbuf = '\\0';\n\t\treturn -1;\n\t}", "project": "linux-pam", "hash": 113360647922111213131290475273018648503, "size": 36, "commit_id": "9dcead87e6d7f66d34e7a56d11a30daca367dffb", "message": "pam_timestamp: fix potential directory traversal issue (ticket #27)\n\npam_timestamp uses values of PAM_RUSER and PAM_TTY as components of\nthe timestamp pathname it creates, so extra care should be taken to\navoid potential directory traversal issues.\n\n* modules/pam_timestamp/pam_timestamp.c (check_tty): Treat\n\".\" and \"..\" tty values as invalid.\n(get_ruser): Treat \".\" and \"..\" ruser values, as well as any ruser\nvalue containing '/', as invalid.\n\nFixes CVE-2014-2583.\n\nReported-by: Sebastian Krahmer <krahmer@suse.de>", "target": 0, "dataset": "other", "idx": 429356}
{"func": "int ma_read_ok_packet(MYSQL *mysql, uchar *pos, ulong length)\n{\n  size_t item_len;\n  mysql->affected_rows= net_field_length_ll(&pos);\n  mysql->insert_id=\t  net_field_length_ll(&pos);\n  mysql->server_status=uint2korr(pos);\n  pos+=2;\n  mysql->warning_count=uint2korr(pos);\n  pos+=2;\n  if (pos < mysql->net.read_pos+length)\n  {\n    if ((item_len= net_field_length(&pos)))\n      mysql->info=(char*) pos;\n\n    /* check if server supports session tracking */\n    if (mysql->server_capabilities & CLIENT_SESSION_TRACKING)\n    {\n      ma_clear_session_state(mysql);\n      pos+= item_len;\n\n      if (mysql->server_status & SERVER_SESSION_STATE_CHANGED)\n      {\n        int i;\n        if (pos < mysql->net.read_pos + length)\n        {\n          LIST *session_item;\n          MYSQL_LEX_STRING *str= NULL;\n          enum enum_session_state_type si_type;\n          uchar *old_pos= pos;\n          size_t item_len= net_field_length(&pos);  /* length for all items */\n\n          /* length was already set, so make sure that info will be zero terminated */\n          if (mysql->info)\n            *old_pos= 0;\n\n          while (item_len > 0)\n          {\n            size_t plen;\n            char *data;\n            old_pos= pos;\n            si_type= (enum enum_session_state_type)net_field_length(&pos);\n            switch(si_type) {\n            case SESSION_TRACK_SCHEMA:\n            case SESSION_TRACK_STATE_CHANGE:\n            case SESSION_TRACK_TRANSACTION_CHARACTERISTICS:\n            case SESSION_TRACK_SYSTEM_VARIABLES:\n              if (si_type != SESSION_TRACK_STATE_CHANGE)\n                net_field_length(&pos); /* ignore total length, item length will follow next */\n              plen= net_field_length(&pos);\n              if (!(session_item= ma_multi_malloc(0,\n                                  &session_item, sizeof(LIST),\n                                  &str, sizeof(MYSQL_LEX_STRING),\n                                  &data, plen,\n                                  NULL)))\n              {\n                ma_clear_session_state(mysql);\n                SET_CLIENT_ERROR(mysql, CR_OUT_OF_MEMORY, SQLSTATE_UNKNOWN, 0);\n                return -1;\n              }\n              str->length= plen;\n              str->str= data;\n              memcpy(str->str, (char *)pos, plen);\n              pos+= plen;\n              session_item->data= str;\n              {\n                my_bool set_charset= 0;\n                /* make sure that we update charset in case it has changed */\n                if (!strncmp(str->str, \"character_set_client\", str->length))\n                  set_charset= 1;\n                plen= net_field_length(&pos);\n                if (!(session_item= ma_multi_malloc(0,\n                                    &session_item, sizeof(LIST),\n                                    &str, sizeof(MYSQL_LEX_STRING),\n                                    &data, plen,\n                                    NULL)))\n                {\n                  ma_clear_session_state(mysql);\n                  SET_CLIENT_ERROR(mysql, CR_OUT_OF_MEMORY, SQLSTATE_UNKNOWN, 0);\n                  return -1;\n                }\n                str->length= plen;\n                str->str= data;\n                memcpy(str->str, (char *)pos, plen);\n                pos+= plen;\n                session_item->data= str;\n                mysql->extension->session_state[si_type].list= list_add(mysql->extension->session_state[si_type].list, session_item);\n                if (set_charset &&\n                    strncmp(mysql->charset->csname, str->str, str->length) != 0)\n                {\n                  char cs_name[64];\n                  MARIADB_CHARSET_INFO *cs_info;\n                  memcpy(cs_name, str->str, str->length);\n                  cs_name[str->length]= 0;\n                  if ((cs_info = (MARIADB_CHARSET_INFO *)mysql_find_charset_name(cs_name)))\n                    mysql->charset= cs_info;\n                }\n              }\n              break;\n            default:\n              /* not supported yet */\n              plen= net_field_length(&pos);\n              pos+= plen;\n              break;\n            }\n            item_len-= (pos - old_pos);\n          }\n        }\n        for (i= SESSION_TRACK_BEGIN; i <= SESSION_TRACK_END; i++)\n        {\n          mysql->extension->session_state[i].list= list_reverse(mysql->extension->session_state[i].list);\n    }\n  }\n  /* CONC-351: clear session state information */\n  else if (mysql->server_capabilities & CLIENT_SESSION_TRACKING)\n    ma_clear_session_state(mysql);\n  return(0);\n}", "project": "mariadb-connector-c", "hash": 76318025094495661937304907006689356074, "size": 131, "commit_id": "2759b87d72926b7c9b5426437a7c8dd15ff57945", "message": "sanity checks for client-supplied OK packet content\n\nreported by Matthias Kaiser, Apple Information Security", "target": 1, "dataset": "other", "idx": 210193}
{"func": "int ma_read_ok_packet(MYSQL *mysql, uchar *pos, ulong length)\n{\n  uchar *end= mysql->net.read_pos+length;\n  size_t item_len;\n  mysql->affected_rows= net_field_length_ll(&pos);\n  mysql->insert_id=\t  net_field_length_ll(&pos);\n  mysql->server_status=uint2korr(pos);\n  pos+=2;\n  mysql->warning_count=uint2korr(pos);\n  pos+=2;\n  if (pos > end)\n    goto corrupted;\n  if (pos < end)\n  {\n    if ((item_len= net_field_length(&pos)))\n      mysql->info=(char*) pos;\n    if (pos + item_len > end)\n      goto corrupted;\n\n    /* check if server supports session tracking */\n    if (mysql->server_capabilities & CLIENT_SESSION_TRACKING)\n    {\n      ma_clear_session_state(mysql);\n      pos+= item_len;\n\n      if (mysql->server_status & SERVER_SESSION_STATE_CHANGED)\n      {\n        int i;\n        if (pos < end)\n        {\n          LIST *session_item;\n          MYSQL_LEX_STRING *str= NULL;\n          enum enum_session_state_type si_type;\n          uchar *old_pos= pos;\n\n          item_len= net_field_length(&pos);  /* length for all items */\n          if (pos + item_len > end)\n            goto corrupted;\n          end= pos + item_len;\n\n          /* length was already set, so make sure that info will be zero terminated */\n          if (mysql->info)\n            *old_pos= 0;\n\n          while (pos < end)\n          {\n            size_t plen;\n            char *data;\n            si_type= (enum enum_session_state_type)net_field_length(&pos);\n            switch(si_type) {\n            case SESSION_TRACK_SCHEMA:\n            case SESSION_TRACK_STATE_CHANGE:\n            case SESSION_TRACK_TRANSACTION_CHARACTERISTICS:\n            case SESSION_TRACK_SYSTEM_VARIABLES:\n              if (si_type != SESSION_TRACK_STATE_CHANGE)\n                net_field_length(&pos); /* ignore total length, item length will follow next */\n              plen= net_field_length(&pos);\n              if (pos + plen > end)\n                goto corrupted;\n              if (!(session_item= ma_multi_malloc(0,\n                                  &session_item, sizeof(LIST),\n                                  &str, sizeof(MYSQL_LEX_STRING),\n                                  &data, plen,\n                                  NULL)))\n                  goto oom;\n              str->length= plen;\n              str->str= data;\n              memcpy(str->str, (char *)pos, plen);\n              pos+= plen;\n              session_item->data= str;\n                my_bool set_charset= 0;\n                /* make sure that we update charset in case it has changed */\n                if (!strncmp(str->str, \"character_set_client\", str->length))\n                  set_charset= 1;\n                plen= net_field_length(&pos);\n                if (pos + plen > end)\n                  goto corrupted;\n                if (!(session_item= ma_multi_malloc(0,\n                                    &session_item, sizeof(LIST),\n                                    &str, sizeof(MYSQL_LEX_STRING),\n                                    &data, plen,\n                                    NULL)))\n                  goto oom;\n                str->length= plen;\n                str->str= data;\n                memcpy(str->str, (char *)pos, plen);\n                pos+= plen;\n                session_item->data= str;\n                mysql->extension->session_state[si_type].list= list_add(mysql->extension->session_state[si_type].list, session_item);\n                if (set_charset && str->length < CHARSET_NAME_LEN &&\n                    strncmp(mysql->charset->csname, str->str, str->length) != 0)\n                {\n                  char cs_name[CHARSET_NAME_LEN];\n                  const MARIADB_CHARSET_INFO *cs_info;\n                  memcpy(cs_name, str->str, str->length);\n                  cs_name[str->length]= 0;\n                  if ((cs_info = mysql_find_charset_name(cs_name)))\n                    mysql->charset= cs_info;\n                }\n              }\n              break;\n            default:\n              /* not supported yet */\n              plen= net_field_length(&pos);\n              if (pos + plen > end)\n                goto corrupted;\n              pos+= plen;\n              break;\n            }\n          }\n        }\n        for (i= SESSION_TRACK_BEGIN; i <= SESSION_TRACK_END; i++)\n        {\n          mysql->extension->session_state[i].list= list_reverse(mysql->extension->session_state[i].list);\n  }\n  /* CONC-351: clear session state information */\n  else if (mysql->server_capabilities & CLIENT_SESSION_TRACKING)\n    ma_clear_session_state(mysql);\n  return(0);\n\noom:\n  ma_clear_session_state(mysql);\n  SET_CLIENT_ERROR(mysql, CR_OUT_OF_MEMORY, SQLSTATE_UNKNOWN, 0);\n  return -1;\n\ncorrupted:\n  ma_clear_session_state(mysql);\n  SET_CLIENT_ERROR(mysql, CR_MALFORMED_PACKET, SQLSTATE_UNKNOWN, 0);\n  return -1;\n}", "project": "mariadb-connector-c", "hash": 291866632911754157865017334546963738553, "size": 146, "commit_id": "2759b87d72926b7c9b5426437a7c8dd15ff57945", "message": "sanity checks for client-supplied OK packet content\n\nreported by Matthias Kaiser, Apple Information Security", "target": 0, "dataset": "other", "idx": 429686}
{"func": "        int c2 = virRandomInt(catRange);\n\n        VIR_DEBUG(\"Try cat %s:c%d,c%d\", sens, c1 + catMin, c2 + catMin);\n\n        if (c1 == c2) {\n            mcs = g_strdup_printf(\"%s:c%d\", sens, catMin + c1);\n        } else {\n            if (c1 > c2) {\n                int t = c1;\n                c1 = c2;\n                c2 = t;", "project": "libvirt", "hash": 268703927834014537463145149594703911908, "size": 47, "commit_id": "15073504dbb624d3f6c911e85557019d3620fdb2", "message": "security: fix SELinux label generation logic\n\nA process can access a file if the set of MCS categories\nfor the file is equal-to *or* a subset-of, the set of\nMCS categories for the process.\n\nIf there are two VMs:\n\n  a) svirt_t:s0:c117\n  b) svirt_t:s0:c117,c720\n\nThen VM (b) is able to access files labelled for VM (a).\n\nIOW, we must discard case where the categories are equal\nbecause that is a subset of many other valid category pairs.\n\nFixes: https://gitlab.com/libvirt/libvirt/-/issues/153\nCVE-2021-3631\nReviewed-by: Peter Krempa <pkrempa@redhat.com>\nSigned-off-by: Daniel P. Berrang\u00e9 <berrange@redhat.com>", "target": 1, "dataset": "other", "idx": 210201}
{"func": "        int c2 = virRandomInt(catRange);\n\n        VIR_DEBUG(\"Try cat %s:c%d,c%d\", sens, c1 + catMin, c2 + catMin);\n\n        if (c1 == c2) {\n            /*\n             * A process can access a file if the set of MCS categories\n             * for the file is equal-to *or* a subset-of, the set of\n             * MCS categories for the process.\n             *\n             * IOW, we must discard case where the categories are equal\n             * because that is a subset of other category pairs.\n             */\n            continue;\n        } else {\n            if (c1 > c2) {\n                int t = c1;\n                c1 = c2;\n                c2 = t;", "project": "libvirt", "hash": 143827953277160340656527682050918218776, "size": 55, "commit_id": "15073504dbb624d3f6c911e85557019d3620fdb2", "message": "security: fix SELinux label generation logic\n\nA process can access a file if the set of MCS categories\nfor the file is equal-to *or* a subset-of, the set of\nMCS categories for the process.\n\nIf there are two VMs:\n\n  a) svirt_t:s0:c117\n  b) svirt_t:s0:c117,c720\n\nThen VM (b) is able to access files labelled for VM (a).\n\nIOW, we must discard case where the categories are equal\nbecause that is a subset of many other valid category pairs.\n\nFixes: https://gitlab.com/libvirt/libvirt/-/issues/153\nCVE-2021-3631\nReviewed-by: Peter Krempa <pkrempa@redhat.com>\nSigned-off-by: Daniel P. Berrang\u00e9 <berrange@redhat.com>", "target": 0, "dataset": "other", "idx": 430309}
{"func": "      return *this;\n    }\n  }\n\n  int32_t oldLength = length();\n  int32_t newLength = oldLength + srcLength;\n\n  // Check for append onto ourself\n  const UChar* oldArray = getArrayStart();\n  if (isBufferWritable() &&\n      oldArray < srcChars + srcLength &&", "project": "icu", "hash": 78872655036842327692103499513475894622, "size": 51, "commit_id": "b7d08bc04a4296982fcef8b6b8a354a9e4e7afca", "message": "ICU-20958 Prevent SEGV_MAPERR in append\n\nSee #971", "target": 1, "dataset": "other", "idx": 210237}
{"func": "      return *this;\n    }\n  }\n\n  int32_t oldLength = length();\n  int32_t newLength;\n  if (uprv_add32_overflow(oldLength, srcLength, &newLength)) {\n    setToBogus();\n    return *this;\n  }\n\n  // Check for append onto ourself\n  const UChar* oldArray = getArrayStart();\n  if (isBufferWritable() &&\n      oldArray < srcChars + srcLength &&", "project": "icu", "hash": 202583290338277335296873764122279909646, "size": 55, "commit_id": "b7d08bc04a4296982fcef8b6b8a354a9e4e7afca", "message": "ICU-20958 Prevent SEGV_MAPERR in append\n\nSee #971", "target": 0, "dataset": "other", "idx": 430797}
{"func": "HeaderLookupTable_t::lookup (const char *buf, const std::size_t len) const {\n    const HeaderTableRecord *r = HttpHeaderHashTable::lookup(buf, len);\n    if (!r)\n        return BadHdr;\n    return *r;\n}", "project": "squid", "hash": 2837537094058907697257649187599047333, "size": 6, "commit_id": "d09b34de2575af3bab4b34c775f93acb7270b4c3", "message": "Limit HeaderLookupTable_t::lookup() to BadHdr and specific IDs", "target": 1, "dataset": "other", "idx": 210242}
{"func": "HeaderLookupTable_t::lookup (const char *buf, const std::size_t len) const {\n    const HeaderTableRecord *r = HttpHeaderHashTable::lookup(buf, len);\n    if (!r || r->id == Http::HdrType::OTHER)\n        return BadHdr;\n    return *r;\n}", "project": "squid", "hash": 43860048861657268208848208606759751881, "size": 6, "commit_id": "d09b34de2575af3bab4b34c775f93acb7270b4c3", "message": "Limit HeaderLookupTable_t::lookup() to BadHdr and specific IDs", "target": 0, "dataset": "other", "idx": 430901}
{"func": "\t\tp = xdr_inline_decode(xdr, len);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tif (len < NFS4_MAXLABELLEN) {\n\t\t\tif (label) {\n\t\t\t\tmemcpy(label->label, p, len);\n\t\t\t\tlabel->len = len;\n\t\t\t\tlabel->pi = pi;\n\t\t\t\tlabel->lfs = lfs;\n\t\t\t\tstatus = NFS_ATTR_FATTR_V4_SECURITY_LABEL;\n\t\t\t}", "project": "linux", "hash": 293774740721234251683748491997728460674, "size": 45, "commit_id": "b4487b93545214a9db8cbf32e86411677b0cca21", "message": "nfs: Fix getxattr kernel panic and memory overflow\n\nMove the buffer size check to decode_attr_security_label() before memcpy()\nOnly call memcpy() if the buffer is large enough\n\nFixes: aa9c2669626c (\"NFS: Client implementation of Labeled-NFS\")\nSigned-off-by: Jeffrey Mitchell <jeffrey.mitchell@starlab.io>\n[Trond: clean up duplicate test of label->len != 0]\nSigned-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>", "target": 1, "dataset": "other", "idx": 210250}
{"func": "\t\tp = xdr_inline_decode(xdr, len);\n\t\tif (unlikely(!p))\n\t\t\treturn -EIO;\n\t\tif (len < NFS4_MAXLABELLEN) {\n\t\t\tif (label) {\n\t\t\t\tif (label->len) {\n\t\t\t\t\tif (label->len < len)\n\t\t\t\t\t\treturn -ERANGE;\n\t\t\t\t\tmemcpy(label->label, p, len);\n\t\t\t\t}\n\t\t\t\tlabel->len = len;\n\t\t\t\tlabel->pi = pi;\n\t\t\t\tlabel->lfs = lfs;\n\t\t\t\tstatus = NFS_ATTR_FATTR_V4_SECURITY_LABEL;\n\t\t\t}", "project": "linux", "hash": 57059966302914892646918225923262002221, "size": 49, "commit_id": "b4487b93545214a9db8cbf32e86411677b0cca21", "message": "nfs: Fix getxattr kernel panic and memory overflow\n\nMove the buffer size check to decode_attr_security_label() before memcpy()\nOnly call memcpy() if the buffer is large enough\n\nFixes: aa9c2669626c (\"NFS: Client implementation of Labeled-NFS\")\nSigned-off-by: Jeffrey Mitchell <jeffrey.mitchell@starlab.io>\n[Trond: clean up duplicate test of label->len != 0]\nSigned-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>", "target": 0, "dataset": "other", "idx": 431495}
{"func": "\tint num_rsp = *((__u8 *) skb->data);\n\tsize_t eir_len;\n\n\tBT_DBG(\"%s num_rsp %d\", hdev->name, num_rsp);\n\n\tif (!num_rsp)\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_PERIODIC_INQ))\n\t\treturn;\n", "project": "linux", "hash": 246597494627569744149252198466981823503, "size": 49, "commit_id": "51c19bf3d5cfaa66571e4b88ba2a6f6295311101", "message": "Bluetooth: Fix slab-out-of-bounds read in hci_extended_inquiry_result_evt()\n\nCheck upon `num_rsp` is insufficient. A malformed event packet with a\nlarge `num_rsp` number makes hci_extended_inquiry_result_evt() go out\nof bounds. Fix it.\n\nThis patch fixes the following syzbot bug:\n\n    https://syzkaller.appspot.com/bug?id=4bf11aa05c4ca51ce0df86e500fce486552dc8d2\n\nReported-by: syzbot+d8489a79b781849b9c46@syzkaller.appspotmail.com\nCc: stable@vger.kernel.org\nSigned-off-by: Peilin Ye <yepeilin.cs@gmail.com>\nAcked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\nSigned-off-by: Marcel Holtmann <marcel@holtmann.org>", "target": 1, "dataset": "other", "idx": 210268}
{"func": "\tint num_rsp = *((__u8 *) skb->data);\n\tsize_t eir_len;\n\n\tBT_DBG(\"%s num_rsp %d\", hdev->name, num_rsp);\n\n\tif (!num_rsp || skb->len < num_rsp * sizeof(*info) + 1)\n\t\treturn;\n\n\tif (hci_dev_test_flag(hdev, HCI_PERIODIC_INQ))\n\t\treturn;\n", "project": "linux", "hash": 330631060070038770247000748836217689829, "size": 49, "commit_id": "51c19bf3d5cfaa66571e4b88ba2a6f6295311101", "message": "Bluetooth: Fix slab-out-of-bounds read in hci_extended_inquiry_result_evt()\n\nCheck upon `num_rsp` is insufficient. A malformed event packet with a\nlarge `num_rsp` number makes hci_extended_inquiry_result_evt() go out\nof bounds. Fix it.\n\nThis patch fixes the following syzbot bug:\n\n    https://syzkaller.appspot.com/bug?id=4bf11aa05c4ca51ce0df86e500fce486552dc8d2\n\nReported-by: syzbot+d8489a79b781849b9c46@syzkaller.appspotmail.com\nCc: stable@vger.kernel.org\nSigned-off-by: Peilin Ye <yepeilin.cs@gmail.com>\nAcked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>\nSigned-off-by: Marcel Holtmann <marcel@holtmann.org>", "target": 0, "dataset": "other", "idx": 431954}
{"func": "static int svm_cpu_init(int cpu)\n{\n\tstruct svm_cpu_data *sd;\n\tint r;\n\n\tsd = kzalloc(sizeof(struct svm_cpu_data), GFP_KERNEL);\n\tif (!sd)\n\t\treturn -ENOMEM;\n\tsd->cpu = cpu;\n\tr = -ENOMEM;\n\tsd->save_area = alloc_page(GFP_KERNEL);\n\tif (!sd->save_area)\n\t\tgoto err_1;\n\n\tif (svm_sev_enabled()) {\n\t\tr = -ENOMEM;\n\t\tsd->sev_vmcbs = kmalloc_array(max_sev_asid + 1,\n\t\t\t\t\t      sizeof(void *),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!sd->sev_vmcbs)\n\t\t\tgoto err_1;\n\t}\n\n\tper_cpu(svm_data, cpu) = sd;\n\n\treturn 0;\n\nerr_1:\n\tkfree(sd);\n\treturn r;\n\n}", "project": "linux", "hash": 19969613992103226668284025200331411815, "size": 32, "commit_id": "d80b64ff297e40c2b6f7d7abc1b3eba70d22a068", "message": "KVM: SVM: Fix potential memory leak in svm_cpu_init()\n\nWhen kmalloc memory for sd->sev_vmcbs failed, we forget to free the page\nheld by sd->save_area. Also get rid of the var r as '-ENOMEM' is actually\nthe only possible outcome here.\n\nReviewed-by: Liran Alon <liran.alon@oracle.com>\nReviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>\nSigned-off-by: Miaohe Lin <linmiaohe@huawei.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 210296}
{"func": "static int svm_cpu_init(int cpu)\n{\n\tstruct svm_cpu_data *sd;\n\n\tsd = kzalloc(sizeof(struct svm_cpu_data), GFP_KERNEL);\n\tif (!sd)\n\t\treturn -ENOMEM;\n\tsd->cpu = cpu;\n\tsd->save_area = alloc_page(GFP_KERNEL);\n\tif (!sd->save_area)\n\t\tgoto free_cpu_data;\n\n\tif (svm_sev_enabled()) {\n\t\tsd->sev_vmcbs = kmalloc_array(max_sev_asid + 1,\n\t\t\t\t\t      sizeof(void *),\n\t\t\t\t\t      GFP_KERNEL);\n\t\tif (!sd->sev_vmcbs)\n\t\t\tgoto free_save_area;\n\t}\n\n\tper_cpu(svm_data, cpu) = sd;\n\n\treturn 0;\n\nfree_save_area:\n\t__free_page(sd->save_area);\nfree_cpu_data:\n\tkfree(sd);\n\treturn -ENOMEM;\n\n}", "project": "linux", "hash": 183067082748724995934254062159834476787, "size": 31, "commit_id": "d80b64ff297e40c2b6f7d7abc1b3eba70d22a068", "message": "KVM: SVM: Fix potential memory leak in svm_cpu_init()\n\nWhen kmalloc memory for sd->sev_vmcbs failed, we forget to free the page\nheld by sd->save_area. Also get rid of the var r as '-ENOMEM' is actually\nthe only possible outcome here.\n\nReviewed-by: Liran Alon <liran.alon@oracle.com>\nReviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>\nSigned-off-by: Miaohe Lin <linmiaohe@huawei.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 432423}
{"func": "\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index > glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n", "project": "FreeRDP", "hash": 73282285903996096925940375434935798209, "size": 26, "commit_id": "c0fd449ec0870b050d350d6d844b1ea6dad4bc7d", "message": "Fixed Out-of-bound read in glyph_cache_put\n\nCVE-2020-11098 thanks to @antonio-morales for finding this.", "target": 1, "dataset": "other", "idx": 210325}
{"func": "\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache id: %\" PRIu32 \"\", id);\n\t\treturn FALSE;\n\t}\n\n\tif (index >= glyphCache->glyphCache[id].number)\n\t{\n\t\tWLog_ERR(TAG, \"invalid glyph cache index: %\" PRIu32 \" in cache id: %\" PRIu32 \"\", index, id);\n\t\treturn FALSE;\n\t}\n", "project": "FreeRDP", "hash": 206456276622603553888667254120967264481, "size": 26, "commit_id": "c0fd449ec0870b050d350d6d844b1ea6dad4bc7d", "message": "Fixed Out-of-bound read in glyph_cache_put\n\nCVE-2020-11098 thanks to @antonio-morales for finding this.", "target": 0, "dataset": "other", "idx": 432859}
{"func": "\tBOOL success = TRUE;\n\n\tif (autodetectRspPdu->headerLength != 0x0E)\n\t\treturn FALSE;\n\n\tWLog_VRB(AUTODETECT_TAG, \"received Bandwidth Measure Results PDU\");\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureTimeDelta); /* timeDelta (4 bytes) */\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureByteCount); /* byteCount (4 bytes) */\n\n\tif (rdp->autodetect->bandwidthMeasureTimeDelta > 0)\n\t\trdp->autodetect->netCharBandwidth = rdp->autodetect->bandwidthMeasureByteCount * 8 /", "project": "FreeRDP", "hash": 216691492795313936043673492828130509835, "size": 22, "commit_id": "f5e73cc7c9cd973b516a618da877c87b80950b65", "message": "Fixed #6009: Bounds checks in autodetect_recv_bandwidth_measure_results", "target": 1, "dataset": "other", "idx": 210399}
{"func": "\n\tif (autodetectRspPdu->headerLength != 0x0E)\n\t\treturn FALSE;\n\n\tWLog_VRB(AUTODETECT_TAG, \"received Bandwidth Measure Results PDU\");\n\tif (Stream_GetRemainingLength(s) < 8)\n\t\treturn -1;\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureTimeDelta); /* timeDelta (4 bytes) */\n\tStream_Read_UINT32(s, rdp->autodetect->bandwidthMeasureByteCount); /* byteCount (4 bytes) */\n\n\tif (rdp->autodetect->bandwidthMeasureTimeDelta > 0)\n\t\trdp->autodetect->netCharBandwidth = rdp->autodetect->bandwidthMeasureByteCount * 8 /", "project": "FreeRDP", "hash": 251258706899851186161828557200980478981, "size": 24, "commit_id": "f5e73cc7c9cd973b516a618da877c87b80950b65", "message": "Fixed #6009: Bounds checks in autodetect_recv_bandwidth_measure_results", "target": 0, "dataset": "other", "idx": 434145}
{"func": "PCRE2_SPTR end_subject = args->end;\nint lgb, rgb, ricount;\nPCRE2_SPTR bptr;\nuint32_t c;\n\nGETCHARINC(c, cc);\nlgb = UCD_GRAPHBREAK(c);\n\nwhile (cc < end_subject)\n  {\n  c = *cc;", "project": "php-src", "hash": 23685102313507176772328800273051474560, "size": 52, "commit_id": "8947fd9e9fdce87cd6c59817b1db58e789538fe9", "message": "Fix #78338: Array cross-border reading in PCRE\n\nWe backport r1092 from pcre2.", "target": 1, "dataset": "other", "idx": 210402}
{"func": "PCRE2_SPTR end_subject = args->end;\nint lgb, rgb, ricount;\nPCRE2_SPTR bptr;\nuint32_t c;\n\nc = *cc++;\nlgb = UCD_GRAPHBREAK(c);\n\nwhile (cc < end_subject)\n  {\n  c = *cc;", "project": "php-src", "hash": 53992940305778057865824736494755359235, "size": 52, "commit_id": "8947fd9e9fdce87cd6c59817b1db58e789538fe9", "message": "Fix #78338: Array cross-border reading in PCRE\n\nWe backport r1092 from pcre2.", "target": 0, "dataset": "other", "idx": 434186}
{"func": "    viff_info.maps_per_cycle=ReadBlobLong(image);\n    viff_info.color_space_model=ReadBlobLong(image);\n    for (i=0; i < 420; i++)\n      (void) ReadBlobByte(image);\n    if (EOFBlob(image) != MagickFalse)\n      ThrowReaderException(CorruptImageError,\"UnexpectedEndOfFile\");\n    image->columns=viff_info.rows;\n    image->rows=viff_info.columns;\n    image->depth=viff_info.x_bits_per_pixel <= 8 ? 8UL :\n      MAGICKCORE_QUANTUM_DEPTH;\n    image->matte=viff_info.number_data_bands == 4 ? MagickTrue : MagickFalse;\n      }\n    (void) SetImageBackgroundColor(image);\n    /*\n      Verify that we can read this VIFF image.\n    */\n    number_pixels=(MagickSizeType) viff_info.columns*viff_info.rows;\n    if (number_pixels != (size_t) number_pixels)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if (number_pixels == 0)\n      ThrowReaderException(CoderError,\"ImageColumnOrRowSizeIsNotSupported\");\n    if ((viff_info.number_data_bands < 1) || (viff_info.number_data_bands > 4))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    if ((viff_info.data_storage_type != VFF_TYP_BIT) &&\n        (viff_info.data_storage_type != VFF_TYP_1_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_2_BYTE) &&", "project": "ImageMagick6", "hash": 69229050309440507101939339857355577375, "size": 649, "commit_id": "d5e7c2b5ba384e7d0d8ddac6c9ae2319cb74b9c5", "message": "https://github.com/ImageMagick/ImageMagick/issues/1286", "target": 1, "dataset": "other", "idx": 210554}
{"func": "    viff_info.color_space_model=ReadBlobLong(image);\n    for (i=0; i < 420; i++)\n      (void) ReadBlobByte(image);\n    if (EOFBlob(image) != MagickFalse)\n      ThrowReaderException(CorruptImageError,\"UnexpectedEndOfFile\");\n    number_pixels=(MagickSizeType) viff_info.columns*viff_info.rows;\n    if (number_pixels != (size_t) number_pixels)\n      ThrowReaderException(ResourceLimitError,\"MemoryAllocationFailed\");\n    if (number_pixels > GetBlobSize(image))\n      ThrowReaderException(CorruptImageError,\"InsufficientImageDataInFile\");\n    if (number_pixels == 0)\n      ThrowReaderException(CoderError,\"ImageColumnOrRowSizeIsNotSupported\");\n    image->columns=viff_info.rows;\n    image->rows=viff_info.columns;\n    image->depth=viff_info.x_bits_per_pixel <= 8 ? 8UL :\n      MAGICKCORE_QUANTUM_DEPTH;\n    image->matte=viff_info.number_data_bands == 4 ? MagickTrue : MagickFalse;\n        return(DestroyImageList(image));\n      }\n    (void) SetImageBackgroundColor(image);\n    /*\n      Verify that we can read this VIFF image.\n    */\n    if ((viff_info.number_data_bands < 1) || (viff_info.number_data_bands > 4))\n      ThrowReaderException(CorruptImageError,\"ImproperImageHeader\");\n    if ((viff_info.data_storage_type != VFF_TYP_BIT) &&\n        (viff_info.data_storage_type != VFF_TYP_1_BYTE) &&\n        (viff_info.data_storage_type != VFF_TYP_2_BYTE) &&", "project": "ImageMagick6", "hash": 228274360687575937756915751337592850337, "size": 651, "commit_id": "d5e7c2b5ba384e7d0d8ddac6c9ae2319cb74b9c5", "message": "https://github.com/ImageMagick/ImageMagick/issues/1286", "target": 0, "dataset": "other", "idx": 437401}
{"func": "    {\n      GeometryInfo\n        geometry_info;\n\n      flags=ParseGeometry(image_info->density,&geometry_info);\n      image->x_resolution=geometry_info.rho;\n      image->y_resolution=geometry_info.sigma;\n      if ((flags & SigmaValue) == 0)\n        image->y_resolution=image->x_resolution;\n    }\n  if (image_info->page != (char *) NULL)\n    {\n      char\n        *geometry;", "project": "ImageMagick6", "hash": 172244786076976468574270858869555969333, "size": 160, "commit_id": "27b1c74979ac473a430e266ff6c4b645664bc805", "message": "https://github.com/ImageMagick/ImageMagick/issues/1522", "target": 1, "dataset": "other", "idx": 210612}
{"func": "    {\n      GeometryInfo\n        geometry_info;\n\n      flags=ParseGeometry(image_info->density,&geometry_info);\n      if ((flags & RhoValue) != 0)\n        image->x_resolution=geometry_info.rho;\n      image->y_resolution=image->x_resolution;\n      if ((flags & SigmaValue) != 0)\n        image->y_resolution=geometry_info.sigma;\n    }\n  if (image_info->page != (char *) NULL)\n    {\n      char\n        *geometry;", "project": "ImageMagick6", "hash": 10506252874396520598024428032753043560, "size": 161, "commit_id": "27b1c74979ac473a430e266ff6c4b645664bc805", "message": "https://github.com/ImageMagick/ImageMagick/issues/1522", "target": 0, "dataset": "other", "idx": 438548}
{"func": "\tspin_lock_irqsave(&bfad->bfad_lock, flags);\n\tmemset(hstats, 0, sizeof(struct fc_host_statistics));\n\trc = bfa_port_get_stats(BFA_FCPORT(&bfad->bfa),\n\t\t\t\tfcstats, bfad_hcb_comp, &fcomp);\n\tspin_unlock_irqrestore(&bfad->bfad_lock, flags);\n\tif (rc != BFA_STATUS_OK)\n\t\treturn NULL;\n\n\twait_for_completion(&fcomp.comp);\n\n\t/* Fill the fc_host_statistics structure */\n\thstats->seconds_since_last_reset = fcstats->fc.secs_reset;", "project": "linux", "hash": 276831105069953521205119097095926483692, "size": 46, "commit_id": "0e62395da2bd5166d7c9e14cbc7503b256a34cb0", "message": "scsi: bfa: release allocated memory in case of error\n\nIn bfad_im_get_stats if bfa_port_get_stats fails, allocated memory needs to\nbe released.\n\nLink: https://lore.kernel.org/r/20190910234417.22151-1-navid.emamdoost@gmail.com\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>", "target": 1, "dataset": "other", "idx": 210708}
{"func": "\tspin_lock_irqsave(&bfad->bfad_lock, flags);\n\tmemset(hstats, 0, sizeof(struct fc_host_statistics));\n\trc = bfa_port_get_stats(BFA_FCPORT(&bfad->bfa),\n\t\t\t\tfcstats, bfad_hcb_comp, &fcomp);\n\tspin_unlock_irqrestore(&bfad->bfad_lock, flags);\n\tif (rc != BFA_STATUS_OK) {\n\t\tkfree(fcstats);\n\t\treturn NULL;\n\t}\n\n\twait_for_completion(&fcomp.comp);\n\n\t/* Fill the fc_host_statistics structure */\n\thstats->seconds_since_last_reset = fcstats->fc.secs_reset;", "project": "linux", "hash": 16138988951117577206220013462963868888, "size": 48, "commit_id": "0e62395da2bd5166d7c9e14cbc7503b256a34cb0", "message": "scsi: bfa: release allocated memory in case of error\n\nIn bfad_im_get_stats if bfa_port_get_stats fails, allocated memory needs to\nbe released.\n\nLink: https://lore.kernel.org/r/20190910234417.22151-1-navid.emamdoost@gmail.com\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Martin K. Petersen <martin.petersen@oracle.com>", "target": 0, "dataset": "other", "idx": 439536}
{"func": "\t\t\t\tldns_rr_descriptor_field_type(\n\t\t\t\tdesc, r_cnt)) &&\n\t\t\t\tldns_buffer_remaining(rd_buf) > 0){\n\n\t\t\t/* skip spaces */\n\t\t\twhile (*(ldns_buffer_current(rd_buf)) == ' ') {\n\t\t\t\tldns_buffer_skip(rd_buf, 1);\n\t\t\t}\n\n\t\t\tif (*(ldns_buffer_current(rd_buf)) == '\\\"') {\n\t\t\t\tdelimiters = \"\\\"\\0\";\n\t\t\t\tldns_buffer_skip(rd_buf, 1);\n\t\t\t\tquoted = true;\n\t\t\t} else if (ldns_rr_descriptor_field_type(desc, r_cnt)\n\t\t\t\t\t== LDNS_RDF_TYPE_LONG_STR) {\n\n\t\t\t\tstatus = LDNS_STATUS_SYNTAX_RDATA_ERR;\n\t\t\t\tgoto error;\n\t\t\t}", "project": "ldns", "hash": 275591331531328108995815488932308997000, "size": 550, "commit_id": "15d96206996bea969fbc918eb0a4a346f514b9f3", "message": "* bugfix #70: heap Out-of-bound Read vulnerability in\n  rr_frm_str_internal reported by pokerfacett.", "target": 1, "dataset": "other", "idx": 210738}
{"func": "\t\t\t\tldns_rr_descriptor_field_type(\n\t\t\t\tdesc, r_cnt)) &&\n\t\t\t\tldns_buffer_remaining(rd_buf) > 0){\n\n\t\t\t/* skip spaces */\n\t\t\twhile (sldns_buffer_remaining(strbuf) > 0 &&\n\t\t\t\t*(ldns_buffer_current(rd_buf)) == ' ') {\n\t\t\t\tldns_buffer_skip(rd_buf, 1);\n\t\t\t}\n\n\t\t\tif (sldns_buffer_remaining(strbuf) > 0 &&\n\t\t\t\t*(ldns_buffer_current(rd_buf)) == '\\\"') {\n\t\t\t\tdelimiters = \"\\\"\\0\";\n\t\t\t\tldns_buffer_skip(rd_buf, 1);\n\t\t\t\tquoted = true;\n\t\t\t}\n\t\t\tif (!quoted && ldns_rr_descriptor_field_type(desc, r_cnt)\n\t\t\t\t\t== LDNS_RDF_TYPE_LONG_STR) {\n\n\t\t\t\tstatus = LDNS_STATUS_SYNTAX_RDATA_ERR;\n\t\t\t\tgoto error;\n\t\t\t}", "project": "ldns", "hash": 35425557283156952799065220793970367868, "size": 553, "commit_id": "15d96206996bea969fbc918eb0a4a346f514b9f3", "message": "* bugfix #70: heap Out-of-bound Read vulnerability in\n  rr_frm_str_internal reported by pokerfacett.", "target": 0, "dataset": "other", "idx": 439914}
{"func": "        if (size <= 0) \n            throw IEX_NAMESPACE::InputExc(\"Error uncompressing DWA data\"\n                                \" (truncated rule).\");\n            \n        {\n            char suffix[Name::SIZE];\n            memset (suffix, 0, Name::SIZE);\n            Xdr::read<CharPtrIO> (ptr, std::min(size, Name::SIZE-1), suffix);\n            _suffix = std::string(suffix);\n        }\n\n        if (static_cast<size_t>(size) < _suffix.length() + 1 + 2*Xdr::size<char>()) ", "project": "openexr", "hash": 286477158467248343403232113020885568232, "size": 38, "commit_id": "3eda5d70aba127bae9bd6bae9956fcf024b64031", "message": "fixes for DWA uncompress: sanity check unknown data reading, off-by-one error on max suffix string length\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>", "target": 1, "dataset": "other", "idx": 210760}
{"func": "        if (size <= 0) \n            throw IEX_NAMESPACE::InputExc(\"Error uncompressing DWA data\"\n                                \" (truncated rule).\");\n            \n        {\n            // maximum length of string plus one byte for terminating NULL\n            char suffix[Name::SIZE+1];\n            memset (suffix, 0, Name::SIZE+1);\n            Xdr::read<CharPtrIO> (ptr, std::min(size, Name::SIZE-1), suffix);\n            _suffix = std::string(suffix);\n        }\n\n        if (static_cast<size_t>(size) < _suffix.length() + 1 + 2*Xdr::size<char>()) ", "project": "openexr", "hash": 259403700882642134149486624904642429148, "size": 39, "commit_id": "3eda5d70aba127bae9bd6bae9956fcf024b64031", "message": "fixes for DWA uncompress: sanity check unknown data reading, off-by-one error on max suffix string length\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>", "target": 0, "dataset": "other", "idx": 440093}
{"func": "  #ifndef SQLITE_OMIT_EXPLAIN\n    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **", "project": "sqlite", "hash": 129135244890151070904216177945464072400, "size": 342, "commit_id": "8428b3b437569338a9d1e10c4cd8154acbe33089", "message": "Continuation of [e2bddcd4c55ba3cb]: Add another spot where it is necessary\nto abort early due to prior errors in sqlite3WindowRewrite().\n\nFossilOrigin-Name: cba2a2a44cdf138a629109bb0ad088ed4ef67fc66bed3e0373554681a39615d2", "target": 1, "dataset": "other", "idx": 210816}
{"func": "    if( p->pNext==0 ){\n      ExplainQueryPlanPop(pParse);\n    }\n  #endif\n  }\n  if( pParse->nErr ) goto multi_select_end;\n  \n  /* Compute collating sequences used by \n  ** temporary tables needed to implement the compound select.\n  ** Attach the KeyInfo structure to all temporary tables.\n  **", "project": "sqlite", "hash": 142655477716831857820189871529105126571, "size": 343, "commit_id": "8428b3b437569338a9d1e10c4cd8154acbe33089", "message": "Continuation of [e2bddcd4c55ba3cb]: Add another spot where it is necessary\nto abort early due to prior errors in sqlite3WindowRewrite().\n\nFossilOrigin-Name: cba2a2a44cdf138a629109bb0ad088ed4ef67fc66bed3e0373554681a39615d2", "target": 0, "dataset": "other", "idx": 440749}
{"func": "\t\t\t    const char *buf, size_t count)\n{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\t\tif (socket->type != SOCK_STREAM) {\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\tsdev->ud.tcp_rx = kthread_get_run(stub_rx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_rx\");\n\t\tsdev->ud.tcp_tx = kthread_get_run(stub_tx_loop, &sdev->ud,\n\t\t\t\t\t\t  \"stub_tx\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);", "project": "linux", "hash": 78418534195062063945792345058960368186, "size": 75, "commit_id": "9380afd6df70e24eacbdbde33afc6a3950965d22", "message": "usbip: fix stub_dev usbip_sockfd_store() races leading to gpf\n\nusbip_sockfd_store() is invoked when user requests attach (import)\ndetach (unimport) usb device from usbip host. vhci_hcd sends import\nrequest and usbip_sockfd_store() exports the device if it is free\nfor export.\n\nExport and unexport are governed by local state and shared state\n- Shared state (usbip device status, sockfd) - sockfd and Device\n  status are used to determine if stub should be brought up or shut\n  down.\n- Local state (tcp_socket, rx and tx thread task_struct ptrs)\n  A valid tcp_socket controls rx and tx thread operations while the\n  device is in exported state.\n- While the device is exported, device status is marked used and socket,\n  sockfd, and thread pointers are valid.\n\nExport sequence (stub-up) includes validating the socket and creating\nreceive (rx) and transmit (tx) threads to talk to the client to provide\naccess to the exported device. rx and tx threads depends on local and\nshared state to be correct and in sync.\n\nUnexport (stub-down) sequence shuts the socket down and stops the rx and\ntx threads. Stub-down sequence relies on local and shared states to be\nin sync.\n\nThere are races in updating the local and shared status in the current\nstub-up sequence resulting in crashes. These stem from starting rx and\ntx threads before local and global state is updated correctly to be in\nsync.\n\n1. Doesn't handle kthread_create() error and saves invalid ptr in local\n   state that drives rx and tx threads.\n2. Updates tcp_socket and sockfd,  starts stub_rx and stub_tx threads\n   before updating usbip_device status to SDEV_ST_USED. This opens up a\n   race condition between the threads and usbip_sockfd_store() stub up\n   and down handling.\n\nFix the above problems:\n- Stop using kthread_get_run() macro to create/start threads.\n- Create threads and get task struct reference.\n- Add kthread_create() failure handling and bail out.\n- Hold usbip_device lock to update local and shared states after\n  creating rx and tx threads.\n- Update usbip_device status to SDEV_ST_USED.\n- Update usbip_device tcp_socket, sockfd, tcp_rx, and tcp_tx\n- Start threads after usbip_device (tcp_socket, sockfd, tcp_rx, tcp_tx,\n  and status) is complete.\n\nCredit goes to syzbot and Tetsuo Handa for finding and root-causing the\nkthread_get_run() improper error handling problem and others. This is a\nhard problem to find and debug since the races aren't seen in a normal\ncase. Fuzzing forces the race window to be small enough for the\nkthread_get_run() error path bug and starting threads before updating the\nlocal and shared state bug in the stub-up sequence.\n\nTested with syzbot reproducer:\n- https://syzkaller.appspot.com/text?tag=ReproC&x=14801034d00000\n\nFixes: 9720b4bc76a83807 (\"staging/usbip: convert to kthread\")\nCc: stable@vger.kernel.org\nReported-by: syzbot <syzbot+a93fba6d384346a761e3@syzkaller.appspotmail.com>\nReported-by: syzbot <syzbot+bf1a360e305ee719e364@syzkaller.appspotmail.com>\nReported-by: syzbot <syzbot+95ce4b142579611ef0a9@syzkaller.appspotmail.com>\nReported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>\nSigned-off-by: Shuah Khan <skhan@linuxfoundation.org>\nLink: https://lore.kernel.org/r/268a0668144d5ff36ec7d87fdfa90faf583b7ccc.1615171203.git.skhan@linuxfoundation.org\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 1, "dataset": "other", "idx": 210838}
{"func": "{\n\tstruct stub_device *sdev = dev_get_drvdata(dev);\n\tint sockfd = 0;\n\tstruct socket *socket;\n\tint rv;\n\tstruct task_struct *tcp_rx = NULL;\n\tstruct task_struct *tcp_tx = NULL;\n\n\tif (!sdev) {\n\t\tdev_err(dev, \"sdev is null\\n\");\n\t\treturn -ENODEV;\n\t}\n\t\t\tdev_err(dev, \"Expecting SOCK_STREAM - found %d\",\n\t\t\t\tsocket->type);\n\t\t\tgoto sock_err;\n\t\t}\n\n\t\t/* unlock and create threads and get tasks */\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\t\ttcp_rx = kthread_create(stub_rx_loop, &sdev->ud, \"stub_rx\");\n\t\tif (IS_ERR(tcp_rx)) {\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\ttcp_tx = kthread_create(stub_tx_loop, &sdev->ud, \"stub_tx\");\n\t\tif (IS_ERR(tcp_tx)) {\n\t\t\tkthread_stop(tcp_rx);\n\t\t\tsockfd_put(socket);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t/* get task structs now */\n\t\tget_task_struct(tcp_rx);\n\t\tget_task_struct(tcp_tx);\n\n\t\t/* lock and update sdev->ud state */\n\t\tspin_lock_irq(&sdev->ud.lock);\n\t\tsdev->ud.tcp_socket = socket;\n\t\tsdev->ud.sockfd = sockfd;\n\t\tsdev->ud.tcp_rx = tcp_rx;\n\t\tsdev->ud.tcp_tx = tcp_tx;\n\t\tsdev->ud.status = SDEV_ST_USED;\n\t\tspin_unlock_irq(&sdev->ud.lock);\n\n\t\twake_up_process(sdev->ud.tcp_rx);\n\t\twake_up_process(sdev->ud.tcp_tx);\n\n\t} else {\n\t\tdev_info(dev, \"stub down\\n\");\n\n\t\tspin_lock_irq(&sdev->ud.lock);", "project": "linux", "hash": 151624572692064030266631166703937205385, "size": 93, "commit_id": "9380afd6df70e24eacbdbde33afc6a3950965d22", "message": "usbip: fix stub_dev usbip_sockfd_store() races leading to gpf\n\nusbip_sockfd_store() is invoked when user requests attach (import)\ndetach (unimport) usb device from usbip host. vhci_hcd sends import\nrequest and usbip_sockfd_store() exports the device if it is free\nfor export.\n\nExport and unexport are governed by local state and shared state\n- Shared state (usbip device status, sockfd) - sockfd and Device\n  status are used to determine if stub should be brought up or shut\n  down.\n- Local state (tcp_socket, rx and tx thread task_struct ptrs)\n  A valid tcp_socket controls rx and tx thread operations while the\n  device is in exported state.\n- While the device is exported, device status is marked used and socket,\n  sockfd, and thread pointers are valid.\n\nExport sequence (stub-up) includes validating the socket and creating\nreceive (rx) and transmit (tx) threads to talk to the client to provide\naccess to the exported device. rx and tx threads depends on local and\nshared state to be correct and in sync.\n\nUnexport (stub-down) sequence shuts the socket down and stops the rx and\ntx threads. Stub-down sequence relies on local and shared states to be\nin sync.\n\nThere are races in updating the local and shared status in the current\nstub-up sequence resulting in crashes. These stem from starting rx and\ntx threads before local and global state is updated correctly to be in\nsync.\n\n1. Doesn't handle kthread_create() error and saves invalid ptr in local\n   state that drives rx and tx threads.\n2. Updates tcp_socket and sockfd,  starts stub_rx and stub_tx threads\n   before updating usbip_device status to SDEV_ST_USED. This opens up a\n   race condition between the threads and usbip_sockfd_store() stub up\n   and down handling.\n\nFix the above problems:\n- Stop using kthread_get_run() macro to create/start threads.\n- Create threads and get task struct reference.\n- Add kthread_create() failure handling and bail out.\n- Hold usbip_device lock to update local and shared states after\n  creating rx and tx threads.\n- Update usbip_device status to SDEV_ST_USED.\n- Update usbip_device tcp_socket, sockfd, tcp_rx, and tcp_tx\n- Start threads after usbip_device (tcp_socket, sockfd, tcp_rx, tcp_tx,\n  and status) is complete.\n\nCredit goes to syzbot and Tetsuo Handa for finding and root-causing the\nkthread_get_run() improper error handling problem and others. This is a\nhard problem to find and debug since the races aren't seen in a normal\ncase. Fuzzing forces the race window to be small enough for the\nkthread_get_run() error path bug and starting threads before updating the\nlocal and shared state bug in the stub-up sequence.\n\nTested with syzbot reproducer:\n- https://syzkaller.appspot.com/text?tag=ReproC&x=14801034d00000\n\nFixes: 9720b4bc76a83807 (\"staging/usbip: convert to kthread\")\nCc: stable@vger.kernel.org\nReported-by: syzbot <syzbot+a93fba6d384346a761e3@syzkaller.appspotmail.com>\nReported-by: syzbot <syzbot+bf1a360e305ee719e364@syzkaller.appspotmail.com>\nReported-by: syzbot <syzbot+95ce4b142579611ef0a9@syzkaller.appspotmail.com>\nReported-by: Tetsuo Handa <penguin-kernel@I-love.SAKURA.ne.jp>\nSigned-off-by: Shuah Khan <skhan@linuxfoundation.org>\nLink: https://lore.kernel.org/r/268a0668144d5ff36ec7d87fdfa90faf583b7ccc.1615171203.git.skhan@linuxfoundation.org\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 0, "dataset": "other", "idx": 440929}
{"func": "static struct socket *get_raw_socket(int fd)\n{\n\tstruct {\n\t\tstruct sockaddr_ll sa;\n\t\tchar  buf[MAX_ADDR_LEN];\n\t} uaddr;\n\tint r;\n\tstruct socket *sock = sockfd_lookup(fd, &r);\n\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\tif (sock->sk->sk_type != SOCK_RAW) {\n\t\tr = -ESOCKTNOSUPPORT;\n\t\tgoto err;\n\t}\n\n\tr = sock->ops->getname(sock, (struct sockaddr *)&uaddr.sa, 0);\n\tif (r < 0)\n\t\tgoto err;\n\n\tif (uaddr.sa.sll_family != AF_PACKET) {\n\t\tr = -EPFNOSUPPORT;\n\t\tgoto err;\n\t}\n\treturn sock;\nerr:", "project": "linux", "hash": 238781874818479014817245249246228980971, "size": 31, "commit_id": "42d84c8490f9f0931786f1623191fcab397c3d64", "message": "vhost: Check docket sk_family instead of call getname\n\nDoing so, we save one call to get data we already have in the struct.\n\nAlso, since there is no guarantee that getname use sockaddr_ll\nparameter beyond its size, we add a little bit of security here.\nIt should do not do beyond MAX_ADDR_LEN, but syzbot found that\nax25_getname writes more (72 bytes, the size of full_sockaddr_ax25,\nversus 20 + 32 bytes of sockaddr_ll + MAX_ADDR_LEN in syzbot repro).\n\nFixes: 3a4d5c94e9593 (\"vhost_net: a kernel-level virtio server\")\nReported-by: syzbot+f2a62d07a5198c819c7b@syzkaller.appspotmail.com\nSigned-off-by: Eugenio P\u00e9rez <eperezma@redhat.com>\nAcked-by: Michael S. Tsirkin <mst@redhat.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 210873}
{"func": "static struct socket *get_raw_socket(int fd)\n{\n\tint r;\n\tstruct socket *sock = sockfd_lookup(fd, &r);\n\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\tif (sock->sk->sk_type != SOCK_RAW) {\n\t\tr = -ESOCKTNOSUPPORT;\n\t\tgoto err;\n\t}\n\n\tif (sock->sk->sk_family != AF_PACKET) {\n\t\tr = -EPFNOSUPPORT;\n\t\tgoto err;\n\t}\n\treturn sock;\nerr:", "project": "linux", "hash": 206714559947468791292926111562093878142, "size": 23, "commit_id": "42d84c8490f9f0931786f1623191fcab397c3d64", "message": "vhost: Check docket sk_family instead of call getname\n\nDoing so, we save one call to get data we already have in the struct.\n\nAlso, since there is no guarantee that getname use sockaddr_ll\nparameter beyond its size, we add a little bit of security here.\nIt should do not do beyond MAX_ADDR_LEN, but syzbot found that\nax25_getname writes more (72 bytes, the size of full_sockaddr_ax25,\nversus 20 + 32 bytes of sockaddr_ll + MAX_ADDR_LEN in syzbot repro).\n\nFixes: 3a4d5c94e9593 (\"vhost_net: a kernel-level virtio server\")\nReported-by: syzbot+f2a62d07a5198c819c7b@syzkaller.appspotmail.com\nSigned-off-by: Eugenio P\u00e9rez <eperezma@redhat.com>\nAcked-by: Michael S. Tsirkin <mst@redhat.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 441966}
{"func": "\t * Firmware will fill up beacon period, Basic rates\n\t * and operational rates.\n\t */\n\n\tmemset(adhoc_start->ssid, 0, IEEE80211_MAX_SSID_LEN);\n\n\tmemcpy(adhoc_start->ssid, req_ssid->ssid, req_ssid->ssid_len);\n\n\tmwifiex_dbg(adapter, INFO, \"info: ADHOC_S_CMD: SSID = %s\\n\",\n\t\t    adhoc_start->ssid);\n", "project": "linux", "hash": 70830342815127398723710324714713389506, "size": 260, "commit_id": "5c455c5ab332773464d02ba17015acdca198f03d", "message": "mwifiex: Fix possible buffer overflows in mwifiex_cmd_802_11_ad_hoc_start\n\nmwifiex_cmd_802_11_ad_hoc_start() calls memcpy() without checking\nthe destination size may trigger a buffer overflower,\nwhich a local user could use to cause denial of service\nor the execution of arbitrary code.\nFix it by putting the length check before calling memcpy().\n\nSigned-off-by: Zhang Xiaohui <ruc_zhangxiaohui@163.com>\nSigned-off-by: Kalle Valo <kvalo@codeaurora.org>\nLink: https://lore.kernel.org/r/20201206084801.26479-1-ruc_zhangxiaohui@163.com", "target": 1, "dataset": "other", "idx": 211030}
{"func": "\t * and operational rates.\n\t */\n\n\tmemset(adhoc_start->ssid, 0, IEEE80211_MAX_SSID_LEN);\n\n\tif (req_ssid->ssid_len > IEEE80211_MAX_SSID_LEN)\n\t\treq_ssid->ssid_len = IEEE80211_MAX_SSID_LEN;\n\tmemcpy(adhoc_start->ssid, req_ssid->ssid, req_ssid->ssid_len);\n\n\tmwifiex_dbg(adapter, INFO, \"info: ADHOC_S_CMD: SSID = %s\\n\",\n\t\t    adhoc_start->ssid);\n", "project": "linux", "hash": 14309788059901564103388827559722802715, "size": 262, "commit_id": "5c455c5ab332773464d02ba17015acdca198f03d", "message": "mwifiex: Fix possible buffer overflows in mwifiex_cmd_802_11_ad_hoc_start\n\nmwifiex_cmd_802_11_ad_hoc_start() calls memcpy() without checking\nthe destination size may trigger a buffer overflower,\nwhich a local user could use to cause denial of service\nor the execution of arbitrary code.\nFix it by putting the length check before calling memcpy().\n\nSigned-off-by: Zhang Xiaohui <ruc_zhangxiaohui@163.com>\nSigned-off-by: Kalle Valo <kvalo@codeaurora.org>\nLink: https://lore.kernel.org/r/20201206084801.26479-1-ruc_zhangxiaohui@163.com", "target": 0, "dataset": "other", "idx": 444812}
{"func": "  wvh.block_index = audiocontext->wvpk_block_index;\n\n  if (audiocontext->channels <= 2) {\n    guint32 block_samples, tmp;\n    gsize size = gst_buffer_get_size (*buf);\n\n    gst_buffer_extract (*buf, 0, &tmp, sizeof (guint32));\n    block_samples = GUINT32_FROM_LE (tmp);\n    /* we need to reconstruct the header of the wavpack block */\n\n    /* -20 because ck_size is the size of the wavpack block -8\n     * and lace_size is the size of the wavpack block + 12\n     * (the three guint32 of the header that already are in the buffer) */\n    wvh.ck_size = size + sizeof (Wavpack4Header) - 20;\n\n    /* block_samples, flags and crc are already in the buffer */\n    newbuf = gst_buffer_new_allocate (NULL, sizeof (Wavpack4Header) - 12, NULL);\n\n    gst_buffer_map (newbuf, &outmap, GST_MAP_WRITE);\n    data = outmap.data;\n    data[0] = 'w';\n    data[1] = 'v';\n    gst_buffer_unref (*buf);\n    *buf = newbuf;\n    audiocontext->wvpk_block_index += block_samples;\n  } else {\n    guint8 *outdata = NULL;\n    guint outpos = 0;\n    gsize buf_size, size, out_size = 0;\n    guint32 block_samples, flags, crc, blocksize;\n\n    gst_buffer_map (*buf, &map, GST_MAP_READ);\n    buf_data = map.data;\n    buf_size = map.size;\n\n    if (buf_size < 4) {\n      GST_ERROR_OBJECT (element, \"Too small wavpack buffer\");\n      gst_buffer_unmap (*buf, &map);\n      return GST_FLOW_ERROR;\n    }\n\n    data = buf_data;\n    size = buf_size;\n      size -= 4;\n      blocksize = GST_READ_UINT32_LE (data);\n      data += 4;\n      size -= 4;\n\n      if (blocksize == 0 || size < blocksize)\n        break;\n\n      g_assert ((newbuf == NULL) == (outdata == NULL));\n\n      if (newbuf == NULL) {\n        out_size = sizeof (Wavpack4Header) + blocksize;\n        newbuf = gst_buffer_new_allocate (NULL, out_size, NULL);\n\n        gst_buffer_copy_into (newbuf, *buf,\n            GST_BUFFER_COPY_TIMESTAMPS | GST_BUFFER_COPY_FLAGS, 0, -1);\n\n        outpos = 0;\n        gst_buffer_map (newbuf, &outmap, GST_MAP_WRITE);\n        outdata = outmap.data;\n      } else {\n        gst_buffer_unmap (newbuf, &outmap);\n        out_size += sizeof (Wavpack4Header) + blocksize;\n        gst_buffer_set_size (newbuf, out_size);\n        gst_buffer_map (newbuf, &outmap, GST_MAP_WRITE);\n        outdata = outmap.data;\n      }\n\n      outdata[outpos] = 'w';\n      outdata[outpos + 1] = 'v';\n      outdata[outpos + 2] = 'p';\n      outdata[outpos + 3] = 'k';\n      outpos += 4;\n\n      GST_WRITE_UINT32_LE (outdata + outpos,\n          blocksize + sizeof (Wavpack4Header) - 8);\n      GST_WRITE_UINT16_LE (outdata + outpos + 4, wvh.version);\n      GST_WRITE_UINT8 (outdata + outpos + 6, wvh.track_no);\n      GST_WRITE_UINT8 (outdata + outpos + 7, wvh.index_no);\n      GST_WRITE_UINT32_LE (outdata + outpos + 8, wvh.total_samples);\n      GST_WRITE_UINT32_LE (outdata + outpos + 12, wvh.block_index);\n      GST_WRITE_UINT32_LE (outdata + outpos + 16, block_samples);\n      GST_WRITE_UINT32_LE (outdata + outpos + 20, flags);\n      GST_WRITE_UINT32_LE (outdata + outpos + 24, crc);\n      outpos += 28;\n\n      memmove (outdata + outpos, data, blocksize);\n      outpos += blocksize;\n      data += blocksize;\n      size -= blocksize;\n    }\n    gst_buffer_unmap (*buf, &map);\n    gst_buffer_unref (*buf);\n\n    if (newbuf)\n      gst_buffer_unmap (newbuf, &outmap);\n\n    *buf = newbuf;\n    audiocontext->wvpk_block_index += block_samples;\n  }\n\n  return GST_FLOW_OK;\n}", "project": "gst-plugins-good", "hash": 268357661659485737291790303030263514365, "size": 150, "commit_id": "9181191511f9c0be6a89c98b311f49d66bd46dc3", "message": "matroskademux: Fix extraction of multichannel WavPack\n\nThe old code had a couple of issues that all lead to potential memory\nsafety bugs.\n\n  - Use a constant for the Wavpack4Header size instead of using sizeof.\n    It's written out into the data and not from the struct and who knows\n    what special alignment/padding requirements some C compilers have.\n  - gst_buffer_set_size() does not realloc the buffer when setting a\n    bigger size than allocated, it only allows growing up to the maximum\n    allocated size. Instead use a GstAdapter to collect all the blocks\n    and take out everything at once in the end.\n  - Check that enough data is actually available in the input and\n    otherwise handle it an error in all cases instead of silently\n    ignoring it.\n\nAmong other things this fixes out of bounds writes because the code\nassumed gst_buffer_set_size() can grow the buffer and simply wrote after\nthe end of the buffer.\n\nThanks to Natalie Silvanovich for reporting.\n\nFixes https://gitlab.freedesktop.org/gstreamer/gst-plugins-good/-/issues/859\n\nPart-of: <https://gitlab.freedesktop.org/gstreamer/gst-plugins-good/-/merge_requests/903>", "target": 1, "dataset": "other", "idx": 211032}
{"func": "\n  if (audiocontext->channels <= 2) {\n    guint32 block_samples, tmp;\n    gsize size = gst_buffer_get_size (*buf);\n\n    if (size < 4) {\n      GST_ERROR_OBJECT (element, \"Too small wavpack buffer\");\n      gst_buffer_unmap (*buf, &map);\n      return GST_FLOW_ERROR;\n    }\n\n    gst_buffer_extract (*buf, 0, &tmp, sizeof (guint32));\n    block_samples = GUINT32_FROM_LE (tmp);\n    /* we need to reconstruct the header of the wavpack block */\n\n    /* -20 because ck_size is the size of the wavpack block -8\n     * and lace_size is the size of the wavpack block + 12\n     * (the three guint32 of the header that already are in the buffer) */\n    wvh.ck_size = size + WAVPACK4_HEADER_SIZE - 20;\n\n    /* block_samples, flags and crc are already in the buffer */\n    newbuf = gst_buffer_new_allocate (NULL, WAVPACK4_HEADER_SIZE - 12, NULL);\n\n    gst_buffer_map (newbuf, &outmap, GST_MAP_WRITE);\n    data = outmap.data;\n    data[0] = 'w';\n    data[1] = 'v';\n    gst_buffer_unref (*buf);\n    *buf = newbuf;\n    audiocontext->wvpk_block_index += block_samples;\n  } else {\n    guint8 *outdata = NULL;\n    gsize buf_size, size;\n    guint32 block_samples, flags, crc, blocksize;\n    GstAdapter *adapter;\n\n    adapter = gst_adapter_new ();\n\n    gst_buffer_map (*buf, &map, GST_MAP_READ);\n    buf_data = map.data;\n    buf_size = map.size;\n\n    if (buf_size < 4) {\n      GST_ERROR_OBJECT (element, \"Too small wavpack buffer\");\n      gst_buffer_unmap (*buf, &map);\n      g_object_unref (adapter);\n      return GST_FLOW_ERROR;\n    }\n\n    data = buf_data;\n    size = buf_size;\n      size -= 4;\n      blocksize = GST_READ_UINT32_LE (data);\n      data += 4;\n      size -= 4;\n\n      if (blocksize == 0 || size < blocksize) {\n        GST_ERROR_OBJECT (element, \"Too small wavpack buffer\");\n        gst_buffer_unmap (*buf, &map);\n        g_object_unref (adapter);\n        return GST_FLOW_ERROR;\n      }\n\n      g_assert (newbuf == NULL);\n\n      newbuf =\n          gst_buffer_new_allocate (NULL, WAVPACK4_HEADER_SIZE + blocksize,\n          NULL);\n      gst_buffer_map (newbuf, &outmap, GST_MAP_WRITE);\n      outdata = outmap.data;\n\n      outdata[0] = 'w';\n      outdata[1] = 'v';\n      outdata[2] = 'p';\n      outdata[3] = 'k';\n      outdata += 4;\n\n      GST_WRITE_UINT32_LE (outdata, blocksize + WAVPACK4_HEADER_SIZE - 8);\n      GST_WRITE_UINT16_LE (outdata + 4, wvh.version);\n      GST_WRITE_UINT8 (outdata + 6, wvh.track_no);\n      GST_WRITE_UINT8 (outdata + 7, wvh.index_no);\n      GST_WRITE_UINT32_LE (outdata + 8, wvh.total_samples);\n      GST_WRITE_UINT32_LE (outdata + 12, wvh.block_index);\n      GST_WRITE_UINT32_LE (outdata + 16, block_samples);\n      GST_WRITE_UINT32_LE (outdata + 20, flags);\n      GST_WRITE_UINT32_LE (outdata + 24, crc);\n      outdata += 28;\n\n      memcpy (outdata, data, blocksize);\n\n      gst_buffer_unmap (newbuf, &outmap);\n      gst_adapter_push (adapter, newbuf);\n      newbuf = NULL;\n\n      data += blocksize;\n      size -= blocksize;\n    }\n    gst_buffer_unmap (*buf, &map);\n\n    newbuf = gst_adapter_take_buffer (adapter, gst_adapter_available (adapter));\n    g_object_unref (adapter);\n\n    gst_buffer_copy_into (newbuf, *buf,\n        GST_BUFFER_COPY_TIMESTAMPS | GST_BUFFER_COPY_FLAGS, 0, -1);\n    gst_buffer_unref (*buf);\n    *buf = newbuf;\n\n    audiocontext->wvpk_block_index += block_samples;\n  }\n\n  return GST_FLOW_OK;\n}", "project": "gst-plugins-good", "hash": 171585410452689613966315464094758896783, "size": 157, "commit_id": "9181191511f9c0be6a89c98b311f49d66bd46dc3", "message": "matroskademux: Fix extraction of multichannel WavPack\n\nThe old code had a couple of issues that all lead to potential memory\nsafety bugs.\n\n  - Use a constant for the Wavpack4Header size instead of using sizeof.\n    It's written out into the data and not from the struct and who knows\n    what special alignment/padding requirements some C compilers have.\n  - gst_buffer_set_size() does not realloc the buffer when setting a\n    bigger size than allocated, it only allows growing up to the maximum\n    allocated size. Instead use a GstAdapter to collect all the blocks\n    and take out everything at once in the end.\n  - Check that enough data is actually available in the input and\n    otherwise handle it an error in all cases instead of silently\n    ignoring it.\n\nAmong other things this fixes out of bounds writes because the code\nassumed gst_buffer_set_size() can grow the buffer and simply wrote after\nthe end of the buffer.\n\nThanks to Natalie Silvanovich for reporting.\n\nFixes https://gitlab.freedesktop.org/gstreamer/gst-plugins-good/-/issues/859\n\nPart-of: <https://gitlab.freedesktop.org/gstreamer/gst-plugins-good/-/merge_requests/903>", "target": 0, "dataset": "other", "idx": 444835}
{"func": "\tif (!buf->buffer)\n\t\treturn -ENOMEM;\n\n\tbuf->data = alloc_percpu(struct trace_array_cpu);\n\tif (!buf->data) {\n\t\tring_buffer_free(buf->buffer);\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Allocate the first page for all buffers */\n\tset_buffer_entries(&tr->trace_buffer,", "project": "linux", "hash": 326507675290447432448249658202476844227, "size": 24, "commit_id": "4397f04575c44e1440ec2e49b6302785c95fd2f8", "message": "tracing: Fix possible double free on failure of allocating trace buffer\n\nJing Xia and Chunyan Zhang reported that on failing to allocate part of the\ntracing buffer, memory is freed, but the pointers that point to them are not\ninitialized back to NULL, and later paths may try to free the freed memory\nagain. Jing and Chunyan fixed one of the locations that does this, but\nmissed a spot.\n\nLink: http://lkml.kernel.org/r/20171226071253.8968-1-chunyan.zhang@spreadtrum.com\n\nCc: stable@vger.kernel.org\nFixes: 737223fbca3b1 (\"tracing: Consolidate buffer allocation code\")\nReported-by: Jing Xia <jing.xia@spreadtrum.com>\nReported-by: Chunyan Zhang <chunyan.zhang@spreadtrum.com>\nSigned-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>", "target": 1, "dataset": "other", "idx": 211099}
{"func": "\t\treturn -ENOMEM;\n\n\tbuf->data = alloc_percpu(struct trace_array_cpu);\n\tif (!buf->data) {\n\t\tring_buffer_free(buf->buffer);\n\t\tbuf->buffer = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\t/* Allocate the first page for all buffers */\n\tset_buffer_entries(&tr->trace_buffer,", "project": "linux", "hash": 186387913486476739467017691738960848735, "size": 25, "commit_id": "4397f04575c44e1440ec2e49b6302785c95fd2f8", "message": "tracing: Fix possible double free on failure of allocating trace buffer\n\nJing Xia and Chunyan Zhang reported that on failing to allocate part of the\ntracing buffer, memory is freed, but the pointers that point to them are not\ninitialized back to NULL, and later paths may try to free the freed memory\nagain. Jing and Chunyan fixed one of the locations that does this, but\nmissed a spot.\n\nLink: http://lkml.kernel.org/r/20171226071253.8968-1-chunyan.zhang@spreadtrum.com\n\nCc: stable@vger.kernel.org\nFixes: 737223fbca3b1 (\"tracing: Consolidate buffer allocation code\")\nReported-by: Jing Xia <jing.xia@spreadtrum.com>\nReported-by: Chunyan Zhang <chunyan.zhang@spreadtrum.com>\nSigned-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>", "target": 0, "dataset": "other", "idx": 445672}
{"func": "\tint len;\n\tint hit = 0;\n\tstruct name_entry entry;\n\tchar *down;\n\tint tn_len = strlen(tree_name);\n\tchar *path_buf = xmalloc(PATH_MAX + tn_len + 100);\n\n\tif (tn_len) {\n\t\ttn_len = sprintf(path_buf, \"%s:\", tree_name);\n\t\tdown = path_buf + tn_len;\n\t\tstrcat(down, base);\n\t}\n\telse {\n\t\tdown = path_buf;\n\t\tstrcpy(down, base);\n\t}\n\tlen = strlen(path_buf);\n\n\twhile (tree_entry(tree, &entry)) {\n\t\tstrcpy(path_buf + len, entry.path);\n\n\t\tif (S_ISDIR(entry.mode))\n\t\t\t/* Match \"abc/\" against pathspec to\n\t\t\t * decide if we want to descend into \"abc\"\n\t\t\t * directory.\n\t\t\t */\n\t\t\tstrcpy(path_buf + len + tree_entry_len(entry.path, entry.sha1), \"/\");\n\n\t\tif (!pathspec_matches(paths, down))\n\t\t\t;\n\t\telse if (S_ISREG(entry.mode))\n\t\t\thit |= grep_sha1(opt, entry.sha1, path_buf, tn_len);\n\t\telse if (S_ISDIR(entry.mode)) {\n\t\t\tenum object_type type;\n\t\t\tstruct tree_desc sub;\n\t\t\tvoid *data;\n\t\t\tunsigned long size;\n\t\t\t\t    sha1_to_hex(entry.sha1));\n\t\t\tinit_tree_desc(&sub, data, size);\n\t\t\thit |= grep_tree(opt, paths, &sub, tree_name, down);\n\t\t\tfree(data);\n\t\t}\n\t}\n\treturn hit;\n}", "project": "git", "hash": 77154386793728243633685029216704517884, "size": 53, "commit_id": "620e2bb93785ed8eb60846d94fd4753d4817c8ec", "message": "Fix buffer overflow in git-grep\n\nIf PATH_MAX on your system is smaller than any path stored in the git\nrepository, that can cause memory corruption inside of the grep_tree\nfunction used by git-grep.\n\nSigned-off-by: Dmitry Potapov <dpotapov@gmail.com>\nSigned-off-by: Junio C Hamano <gitster@pobox.com>", "target": 1, "dataset": "other", "idx": 211109}
{"func": "\tint len;\n\tint hit = 0;\n\tstruct name_entry entry;\n\tchar *down;\n\tint tn_len = strlen(tree_name);\n\tstruct strbuf pathbuf;\n\n\tstrbuf_init(&pathbuf, PATH_MAX + tn_len);\n\n\tif (tn_len) {\n\t\tstrbuf_add(&pathbuf, tree_name, tn_len);\n\t\tstrbuf_addch(&pathbuf, ':');\n\t\ttn_len = pathbuf.len;\n\t}\n\tstrbuf_addstr(&pathbuf, base);\n\tlen = pathbuf.len;\n\n\twhile (tree_entry(tree, &entry)) {\n\t\tint te_len = tree_entry_len(entry.path, entry.sha1);\n\t\tpathbuf.len = len;\n\t\tstrbuf_add(&pathbuf, entry.path, te_len);\n\n\t\tif (S_ISDIR(entry.mode))\n\t\t\t/* Match \"abc/\" against pathspec to\n\t\t\t * decide if we want to descend into \"abc\"\n\t\t\t * directory.\n\t\t\t */\n\t\t\tstrbuf_addch(&pathbuf, '/');\n\n\t\tdown = pathbuf.buf + tn_len;\n\t\tif (!pathspec_matches(paths, down))\n\t\t\t;\n\t\telse if (S_ISREG(entry.mode))\n\t\t\thit |= grep_sha1(opt, entry.sha1, pathbuf.buf, tn_len);\n\t\telse if (S_ISDIR(entry.mode)) {\n\t\t\tenum object_type type;\n\t\t\tstruct tree_desc sub;\n\t\t\tvoid *data;\n\t\t\tunsigned long size;\n\t\t\tinit_tree_desc(&sub, data, size);\n\t\t\thit |= grep_tree(opt, paths, &sub, tree_name, down);\n\t\t\tfree(data);\n\t\t}\n\t}\n\tstrbuf_release(&pathbuf);\n\treturn hit;\n}", "project": "git", "hash": 315062688524784334062144621049330398394, "size": 56, "commit_id": "620e2bb93785ed8eb60846d94fd4753d4817c8ec", "message": "Fix buffer overflow in git-grep\n\nIf PATH_MAX on your system is smaller than any path stored in the git\nrepository, that can cause memory corruption inside of the grep_tree\nfunction used by git-grep.\n\nSigned-off-by: Dmitry Potapov <dpotapov@gmail.com>\nSigned-off-by: Junio C Hamano <gitster@pobox.com>", "target": 0, "dataset": "other", "idx": 446048}
{"func": "\n        int lineOffsetSize = (dataWindow.max.y - dataWindow.min.y +\n                              _data->linesInBuffer) / _data->linesInBuffer;\n\n        //\n        // avoid allocating excessive memory due to large lineOffsets table size.\n        // If the chunktablesize claims to be large,\n        // check the file is big enough to contain the table before allocating memory\n        // in the bytesPerLineTable and the lineOffsets table.\n        // Attempt to read the last entry in the table. Either the seekg() or the read()\n        // call will throw an exception if the file is too small to contain the table\n        //\n        if (lineOffsetSize > gLargeChunkTableSize)\n        {\n            Int64 pos = _streamData->is->tellg();\n            _streamData->is->seekg(pos + (lineOffsetSize-1)*sizeof(Int64));\n            Int64 temp;\n            OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (*_streamData->is, temp);", "project": "openexr", "hash": 22351819646121800230325071822285424033, "size": 83, "commit_id": "bc88cdb6c97fbf5bc5d11ad8ca55306da931283a", "message": "sanity check ScanlineInput bytesPerLine instead of lineOffset size (#863)\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>\r\n\r\nCo-authored-by: Cary Phillips <cary@ilm.com>", "target": 1, "dataset": "other", "idx": 211127}
{"func": "\n        int lineOffsetSize = (dataWindow.max.y - dataWindow.min.y +\n                              _data->linesInBuffer) / _data->linesInBuffer;\n\n        //\n        // avoid allocating excessive memory due to large lineOffsets and bytesPerLine table sizes.\n        // If the chunktablesize claims to be large,\n        // check the file is big enough to contain the lineOffsets table before allocating memory\n        // in the bytesPerLineTable and the lineOffsets table.\n        // Attempt to read the last entry in the table. Either the seekg() or the read()\n        // call will throw an exception if the file is too small to contain the table\n        //\n        if (lineOffsetSize * _data->linesInBuffer > gLargeChunkTableSize)\n        {\n            Int64 pos = _streamData->is->tellg();\n            _streamData->is->seekg(pos + (lineOffsetSize-1)*sizeof(Int64));\n            Int64 temp;\n            OPENEXR_IMF_INTERNAL_NAMESPACE::Xdr::read <OPENEXR_IMF_INTERNAL_NAMESPACE::StreamIO> (*_streamData->is, temp);", "project": "openexr", "hash": 103823671992022679830878815429871629942, "size": 83, "commit_id": "bc88cdb6c97fbf5bc5d11ad8ca55306da931283a", "message": "sanity check ScanlineInput bytesPerLine instead of lineOffset size (#863)\n\nSigned-off-by: Peter Hillman <peterh@wetafx.co.nz>\r\n\r\nCo-authored-by: Cary Phillips <cary@ilm.com>", "target": 0, "dataset": "other", "idx": 446283}
{"func": "\tstruct module *owner = p->owner;\n\tstruct kobject *kobj;\n\n\tif (owner && !try_module_get(owner))\n\t\treturn NULL;\n\tkobj = kobject_get(&p->kobj);\n\tif (!kobj)\n\t\tmodule_put(owner);\n\treturn kobj;\n}", "project": "linux", "hash": 81735454176414824678551654951534930012, "size": 12, "commit_id": "68faa679b8be1a74e6663c21c3a9d25d32f1c079", "message": "chardev: Avoid potential use-after-free in 'chrdev_open()'\n\n'chrdev_open()' calls 'cdev_get()' to obtain a reference to the\n'struct cdev *' stashed in the 'i_cdev' field of the target inode\nstructure. If the pointer is NULL, then it is initialised lazily by\nlooking up the kobject in the 'cdev_map' and so the whole procedure is\nprotected by the 'cdev_lock' spinlock to serialise initialisation of\nthe shared pointer.\n\nUnfortunately, it is possible for the initialising thread to fail *after*\ninstalling the new pointer, for example if the subsequent '->open()' call\non the file fails. In this case, 'cdev_put()' is called, the reference\ncount on the kobject is dropped and, if nobody else has taken a reference,\nthe release function is called which finally clears 'inode->i_cdev' from\n'cdev_purge()' before potentially freeing the object. The problem here\nis that a racing thread can happily take the 'cdev_lock' and see the\nnon-NULL pointer in the inode, which can result in a refcount increment\nfrom zero and a warning:\n\n  |  ------------[ cut here ]------------\n  |  refcount_t: addition on 0; use-after-free.\n  |  WARNING: CPU: 2 PID: 6385 at lib/refcount.c:25 refcount_warn_saturate+0x6d/0xf0\n  |  Modules linked in:\n  |  CPU: 2 PID: 6385 Comm: repro Not tainted 5.5.0-rc2+ #22\n  |  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014\n  |  RIP: 0010:refcount_warn_saturate+0x6d/0xf0\n  |  Code: 05 55 9a 15 01 01 e8 9d aa c8 ff 0f 0b c3 80 3d 45 9a 15 01 00 75 ce 48 c7 c7 00 9c 62 b3 c6 08\n  |  RSP: 0018:ffffb524c1b9bc70 EFLAGS: 00010282\n  |  RAX: 0000000000000000 RBX: ffff9e9da1f71390 RCX: 0000000000000000\n  |  RDX: ffff9e9dbbd27618 RSI: ffff9e9dbbd18798 RDI: ffff9e9dbbd18798\n  |  RBP: 0000000000000000 R08: 000000000000095f R09: 0000000000000039\n  |  R10: 0000000000000000 R11: ffffb524c1b9bb20 R12: ffff9e9da1e8c700\n  |  R13: ffffffffb25ee8b0 R14: 0000000000000000 R15: ffff9e9da1e8c700\n  |  FS:  00007f3b87d26700(0000) GS:ffff9e9dbbd00000(0000) knlGS:0000000000000000\n  |  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  |  CR2: 00007fc16909c000 CR3: 000000012df9c000 CR4: 00000000000006e0\n  |  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n  |  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n  |  Call Trace:\n  |   kobject_get+0x5c/0x60\n  |   cdev_get+0x2b/0x60\n  |   chrdev_open+0x55/0x220\n  |   ? cdev_put.part.3+0x20/0x20\n  |   do_dentry_open+0x13a/0x390\n  |   path_openat+0x2c8/0x1470\n  |   do_filp_open+0x93/0x100\n  |   ? selinux_file_ioctl+0x17f/0x220\n  |   do_sys_open+0x186/0x220\n  |   do_syscall_64+0x48/0x150\n  |   entry_SYSCALL_64_after_hwframe+0x44/0xa9\n  |  RIP: 0033:0x7f3b87efcd0e\n  |  Code: 89 54 24 08 e8 a3 f4 ff ff 8b 74 24 0c 48 8b 3c 24 41 89 c0 44 8b 54 24 08 b8 01 01 00 00 89 f4\n  |  RSP: 002b:00007f3b87d259f0 EFLAGS: 00000293 ORIG_RAX: 0000000000000101\n  |  RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f3b87efcd0e\n  |  RDX: 0000000000000000 RSI: 00007f3b87d25a80 RDI: 00000000ffffff9c\n  |  RBP: 00007f3b87d25e90 R08: 0000000000000000 R09: 0000000000000000\n  |  R10: 0000000000000000 R11: 0000000000000293 R12: 00007ffe188f504e\n  |  R13: 00007ffe188f504f R14: 00007f3b87d26700 R15: 0000000000000000\n  |  ---[ end trace 24f53ca58db8180a ]---\n\nSince 'cdev_get()' can already fail to obtain a reference, simply move\nit over to use 'kobject_get_unless_zero()' instead of 'kobject_get()',\nwhich will cause the racing thread to return -ENXIO if the initialising\nthread fails unexpectedly.\n\nCc: Hillf Danton <hdanton@sina.com>\nCc: Andrew Morton <akpm@linux-foundation.org>\nCc: Al Viro <viro@zeniv.linux.org.uk>\nReported-by: syzbot+82defefbbd8527e1c2cb@syzkaller.appspotmail.com\nSigned-off-by: Will Deacon <will@kernel.org>\nCc: stable <stable@vger.kernel.org>\nLink: https://lore.kernel.org/r/20191219120203.32691-1-will@kernel.org\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 1, "dataset": "other", "idx": 211135}
{"func": "\tstruct module *owner = p->owner;\n\tstruct kobject *kobj;\n\n\tif (owner && !try_module_get(owner))\n\t\treturn NULL;\n\tkobj = kobject_get_unless_zero(&p->kobj);\n\tif (!kobj)\n\t\tmodule_put(owner);\n\treturn kobj;\n}", "project": "linux", "hash": 94936488811761889392788452521795882904, "size": 12, "commit_id": "68faa679b8be1a74e6663c21c3a9d25d32f1c079", "message": "chardev: Avoid potential use-after-free in 'chrdev_open()'\n\n'chrdev_open()' calls 'cdev_get()' to obtain a reference to the\n'struct cdev *' stashed in the 'i_cdev' field of the target inode\nstructure. If the pointer is NULL, then it is initialised lazily by\nlooking up the kobject in the 'cdev_map' and so the whole procedure is\nprotected by the 'cdev_lock' spinlock to serialise initialisation of\nthe shared pointer.\n\nUnfortunately, it is possible for the initialising thread to fail *after*\ninstalling the new pointer, for example if the subsequent '->open()' call\non the file fails. In this case, 'cdev_put()' is called, the reference\ncount on the kobject is dropped and, if nobody else has taken a reference,\nthe release function is called which finally clears 'inode->i_cdev' from\n'cdev_purge()' before potentially freeing the object. The problem here\nis that a racing thread can happily take the 'cdev_lock' and see the\nnon-NULL pointer in the inode, which can result in a refcount increment\nfrom zero and a warning:\n\n  |  ------------[ cut here ]------------\n  |  refcount_t: addition on 0; use-after-free.\n  |  WARNING: CPU: 2 PID: 6385 at lib/refcount.c:25 refcount_warn_saturate+0x6d/0xf0\n  |  Modules linked in:\n  |  CPU: 2 PID: 6385 Comm: repro Not tainted 5.5.0-rc2+ #22\n  |  Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.12.0-1 04/01/2014\n  |  RIP: 0010:refcount_warn_saturate+0x6d/0xf0\n  |  Code: 05 55 9a 15 01 01 e8 9d aa c8 ff 0f 0b c3 80 3d 45 9a 15 01 00 75 ce 48 c7 c7 00 9c 62 b3 c6 08\n  |  RSP: 0018:ffffb524c1b9bc70 EFLAGS: 00010282\n  |  RAX: 0000000000000000 RBX: ffff9e9da1f71390 RCX: 0000000000000000\n  |  RDX: ffff9e9dbbd27618 RSI: ffff9e9dbbd18798 RDI: ffff9e9dbbd18798\n  |  RBP: 0000000000000000 R08: 000000000000095f R09: 0000000000000039\n  |  R10: 0000000000000000 R11: ffffb524c1b9bb20 R12: ffff9e9da1e8c700\n  |  R13: ffffffffb25ee8b0 R14: 0000000000000000 R15: ffff9e9da1e8c700\n  |  FS:  00007f3b87d26700(0000) GS:ffff9e9dbbd00000(0000) knlGS:0000000000000000\n  |  CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\n  |  CR2: 00007fc16909c000 CR3: 000000012df9c000 CR4: 00000000000006e0\n  |  DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\n  |  DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n  |  Call Trace:\n  |   kobject_get+0x5c/0x60\n  |   cdev_get+0x2b/0x60\n  |   chrdev_open+0x55/0x220\n  |   ? cdev_put.part.3+0x20/0x20\n  |   do_dentry_open+0x13a/0x390\n  |   path_openat+0x2c8/0x1470\n  |   do_filp_open+0x93/0x100\n  |   ? selinux_file_ioctl+0x17f/0x220\n  |   do_sys_open+0x186/0x220\n  |   do_syscall_64+0x48/0x150\n  |   entry_SYSCALL_64_after_hwframe+0x44/0xa9\n  |  RIP: 0033:0x7f3b87efcd0e\n  |  Code: 89 54 24 08 e8 a3 f4 ff ff 8b 74 24 0c 48 8b 3c 24 41 89 c0 44 8b 54 24 08 b8 01 01 00 00 89 f4\n  |  RSP: 002b:00007f3b87d259f0 EFLAGS: 00000293 ORIG_RAX: 0000000000000101\n  |  RAX: ffffffffffffffda RBX: 0000000000000000 RCX: 00007f3b87efcd0e\n  |  RDX: 0000000000000000 RSI: 00007f3b87d25a80 RDI: 00000000ffffff9c\n  |  RBP: 00007f3b87d25e90 R08: 0000000000000000 R09: 0000000000000000\n  |  R10: 0000000000000000 R11: 0000000000000293 R12: 00007ffe188f504e\n  |  R13: 00007ffe188f504f R14: 00007f3b87d26700 R15: 0000000000000000\n  |  ---[ end trace 24f53ca58db8180a ]---\n\nSince 'cdev_get()' can already fail to obtain a reference, simply move\nit over to use 'kobject_get_unless_zero()' instead of 'kobject_get()',\nwhich will cause the racing thread to return -ENXIO if the initialising\nthread fails unexpectedly.\n\nCc: Hillf Danton <hdanton@sina.com>\nCc: Andrew Morton <akpm@linux-foundation.org>\nCc: Al Viro <viro@zeniv.linux.org.uk>\nReported-by: syzbot+82defefbbd8527e1c2cb@syzkaller.appspotmail.com\nSigned-off-by: Will Deacon <will@kernel.org>\nCc: stable <stable@vger.kernel.org>\nLink: https://lore.kernel.org/r/20191219120203.32691-1-will@kernel.org\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 0, "dataset": "other", "idx": 446395}
{"func": "        extent+=image->columns*sizeof(uint64);\n#else\n        extent+=image->columns*sizeof(uint32);\n#endif\n        strip_pixels=(unsigned char *) AcquireQuantumMemory(extent,\n          sizeof(*strip_pixels));\n        if (strip_pixels == (unsigned char *) NULL)\n          ThrowTIFFException(ResourceLimitError,\"MemoryAllocationFailed\");\n        (void) memset(strip_pixels,0,extent*sizeof(*strip_pixels));\n        stride=TIFFVStripSize(tiff,1);\n        strip_id=0;", "project": "ImageMagick", "hash": 319829534730769094685806470185595012447, "size": 989, "commit_id": "6ee5059cd3ac8d82714a1ab1321399b88539abf0", "message": "possible TIFF related-heap buffer overflow (alert & POC by Hardik Shah)", "target": 1, "dataset": "other", "idx": 211237}
{"func": "        extent+=image->columns*sizeof(uint64);\n#else\n        extent+=image->columns*sizeof(uint32);\n#endif\n        strip_pixels=(unsigned char *) AcquireQuantumMemory(extent,\n          2*sizeof(*strip_pixels));\n        if (strip_pixels == (unsigned char *) NULL)\n          ThrowTIFFException(ResourceLimitError,\"MemoryAllocationFailed\");\n        (void) memset(strip_pixels,0,extent*sizeof(*strip_pixels));\n        stride=TIFFVStripSize(tiff,1);\n        strip_id=0;", "project": "ImageMagick", "hash": 83966095014944881926814286235105422470, "size": 989, "commit_id": "6ee5059cd3ac8d82714a1ab1321399b88539abf0", "message": "possible TIFF related-heap buffer overflow (alert & POC by Hardik Shah)", "target": 0, "dataset": "other", "idx": 447633}
{"func": "\t\t\t\tapr_psprintf(r->pool,\n\t\t\t\t\t\t\"logout value \\\"%s\\\" does not match the hostname of the current request \\\"%s\\\"\",\n\t\t\t\t\t\tapr_uri_unparse(r->pool, &uri, 0), c_host);\n\t\toidc_error(r, \"%s: %s\", *err_str, *err_desc);\n\t\treturn FALSE;\n\t} else if (strstr(url, \"/\") != url) {\n\t\t*err_str = apr_pstrdup(r->pool, \"Malformed URL\");\n\t\t*err_desc =\n\t\t\t\tapr_psprintf(r->pool,\n\t\t\t\t\t\t\"No hostname was parsed and it does not seem to be relative, i.e starting with '/': %s\",\n\t\t\t\t\t\turl);", "project": "mod_auth_openidc", "hash": 194756554862366516881552658213654354928, "size": 46, "commit_id": "ce37080c6aea30aabae8b4a9b4eea7808445cc8e", "message": "2.4.0.2 oops\n\nSigned-off-by: Hans Zandbelt <hans.zandbelt@zmartzone.eu>", "target": 1, "dataset": "other", "idx": 211253}
{"func": "\t\t\t\tapr_psprintf(r->pool,\n\t\t\t\t\t\t\"logout value \\\"%s\\\" does not match the hostname of the current request \\\"%s\\\"\",\n\t\t\t\t\t\tapr_uri_unparse(r->pool, &uri, 0), c_host);\n\t\toidc_error(r, \"%s: %s\", *err_str, *err_desc);\n\t\treturn FALSE;\n\t} else if ((uri.hostname == NULL) && (strstr(url, \"/\") != url)) {\n\t\t*err_str = apr_pstrdup(r->pool, \"Malformed URL\");\n\t\t*err_desc =\n\t\t\t\tapr_psprintf(r->pool,\n\t\t\t\t\t\t\"No hostname was parsed and it does not seem to be relative, i.e starting with '/': %s\",\n\t\t\t\t\t\turl);", "project": "mod_auth_openidc", "hash": 168048052487772235272643922731757152176, "size": 46, "commit_id": "ce37080c6aea30aabae8b4a9b4eea7808445cc8e", "message": "2.4.0.2 oops\n\nSigned-off-by: Hans Zandbelt <hans.zandbelt@zmartzone.eu>", "target": 0, "dataset": "other", "idx": 447708}
{"func": "\tbox = 0;\n\n\tneedcdef = 1;\n\tswitch (jas_clrspc_fam(jas_image_clrspc(image))) {\n\tcase JAS_CLRSPC_FAM_RGB:\n\t\tif (jas_image_cmpttype(image, 0) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_RGB_R) &&\n\t\t  jas_image_cmpttype(image, 1) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_RGB_G) &&\n\t\t  jas_image_cmpttype(image, 2) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_RGB_B))\n\t\t\tneedcdef = 0;\n\t\tbreak;\n\tcase JAS_CLRSPC_FAM_YCBCR:\n\t\tif (jas_image_cmpttype(image, 0) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_YCBCR_Y) &&\n\t\t  jas_image_cmpttype(image, 1) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_YCBCR_CB) &&\n\t\t  jas_image_cmpttype(image, 2) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_YCBCR_CR))\n\t\t\tneedcdef = 0;\n\t\tbreak;\n\tcase JAS_CLRSPC_FAM_GRAY:\n\t\tif (jas_image_cmpttype(image, 0) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_IMAGE_CT_GRAY_Y))\n\t\t\tneedcdef = 0;\n\t\tbreak;\n\tdefault:\n\t\tabort();", "project": "jasper", "hash": 19879316639441336270204162551441027935, "size": 330, "commit_id": "03db7c81f6a8a92d896249bc673877749987fd7a", "message": "jp2_enc: check number of components before dereferencing them\n\nFixes CVE-2018-20570\n\nCloses https://github.com/jasper-maint/jasper/issues/11\nCloses https://github.com/mdadams/jasper/issues/191", "target": 1, "dataset": "other", "idx": 211483}
{"func": "\tbox = 0;\n\n\tneedcdef = 1;\n\tswitch (jas_clrspc_fam(jas_image_clrspc(image))) {\n\tcase JAS_CLRSPC_FAM_RGB:\n\t\tif (jas_image_numcmpts(image) >= 3 &&\n\t\t  jas_image_cmpttype(image, 0) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_RGB_R) &&\n\t\t  jas_image_cmpttype(image, 1) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_RGB_G) &&\n\t\t  jas_image_cmpttype(image, 2) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_RGB_B))\n\t\t\tneedcdef = 0;\n\t\tbreak;\n\tcase JAS_CLRSPC_FAM_YCBCR:\n\t\tif (jas_image_numcmpts(image) >= 3 &&\n\t\t  jas_image_cmpttype(image, 0) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_YCBCR_Y) &&\n\t\t  jas_image_cmpttype(image, 1) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_YCBCR_CB) &&\n\t\t  jas_image_cmpttype(image, 2) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_CLRSPC_CHANIND_YCBCR_CR))\n\t\t\tneedcdef = 0;\n\t\tbreak;\n\tcase JAS_CLRSPC_FAM_GRAY:\n\t\tif (jas_image_numcmpts(image) >= 1 &&\n\t\t  jas_image_cmpttype(image, 0) ==\n\t\t  JAS_IMAGE_CT_COLOR(JAS_IMAGE_CT_GRAY_Y))\n\t\t\tneedcdef = 0;\n\t\tbreak;\n\tdefault:\n\t\tabort();", "project": "jasper", "hash": 151144374110773996368945460482067623366, "size": 333, "commit_id": "03db7c81f6a8a92d896249bc673877749987fd7a", "message": "jp2_enc: check number of components before dereferencing them\n\nFixes CVE-2018-20570\n\nCloses https://github.com/jasper-maint/jasper/issues/11\nCloses https://github.com/mdadams/jasper/issues/191", "target": 0, "dataset": "other", "idx": 448645}
{"func": "{\n\tUINT32 x = 0, y = 0;\n\tUINT32 i;\n\tUINT32 pixelCount;\n\tUINT32 bitmapDataOffset;\n\tUINT32 pixelIndex;\n\tUINT32 numBits;\n\tBYTE startIndex;\n\tBYTE stopIndex;\n\tBYTE suiteIndex;\n\tBYTE suiteDepth;\n\t{\n\t\tWLog_ERR(TAG, \"stream short %\" PRIuz \" [%\" PRIu32 \" expected]\",\n\t\t         Stream_GetRemainingLength(s), bitmapDataByteCount);\n\t\treturn FALSE;\n\t}\n\n\tStream_Read_UINT8(s, paletteCount);\n\tbitmapDataOffset = 1 + (paletteCount * 3);\n\n\tif ((paletteCount > 127) || (paletteCount < 1))\n\t{\n\t\tWLog_ERR(TAG, \"paletteCount %\" PRIu8 \"\", paletteCount);\n\t\treturn FALSE;\n\t}\n\n\tfor (i = 0; i < paletteCount; i++)\n\t{\n\t\tBYTE r, g, b;\n\t\tStream_Read_UINT8(s, b);\n\t\tpixelIndex += (suiteDepth + 1);\n\t}\n\n\tif (pixelIndex != pixelCount)\n\t{\n\t\tWLog_ERR(TAG, \"pixelIndex %\" PRIu32 \" != pixelCount %\" PRIu32 \"\", pixelIndex, pixelCount);\n\t\treturn FALSE;\n\t}\n\n\treturn TRUE;\n}", "project": "FreeRDP", "hash": 145478480184200339868644033923186345312, "size": 184, "commit_id": "363d7046dfec4003b91aecf7867e3b05905f3843", "message": "Fixed oob read in clear_decompress_subcode_rlex\n\nFixed length checks before stream read.\nThanks to hac425 CVE-2020-11040", "target": 1, "dataset": "other", "idx": 211489}
{"func": "{\n\tUINT32 x = 0, y = 0;\n\tUINT32 i;\n\tUINT32 pixelCount;\n\tUINT32 bitmapDataOffset;\n\tsize_t pixelIndex;\n\tUINT32 numBits;\n\tBYTE startIndex;\n\tBYTE stopIndex;\n\tBYTE suiteIndex;\n\tBYTE suiteDepth;\n\t\tWLog_ERR(TAG, \"stream short %\" PRIuz \" [%\" PRIu32 \" expected]\",\n\t\t         Stream_GetRemainingLength(s), bitmapDataByteCount);\n\t\treturn FALSE;\n\t}\n\n\tif (Stream_GetRemainingLength(s) < 1)\n\t\treturn FALSE;\n\tStream_Read_UINT8(s, paletteCount);\n\tbitmapDataOffset = 1 + (paletteCount * 3);\n\n\tif ((paletteCount > 127) || (paletteCount < 1))\n\t{\n\t\tWLog_ERR(TAG, \"paletteCount %\" PRIu8 \"\", paletteCount);\n\t\treturn FALSE;\n\t}\n\n\tif (Stream_GetRemainingLength(s) < 3ULL * paletteCount)\n\t\treturn FALSE;\n\n\tfor (i = 0; i < paletteCount; i++)\n\t{\n\t\tBYTE r, g, b;\n\t\tStream_Read_UINT8(s, b);\n\t\tpixelIndex += (suiteDepth + 1);\n\t}\n\n\tif (pixelIndex != pixelCount)\n\t{\n\t\tWLog_ERR(TAG, \"pixelIndex %\" PRIdz \" != pixelCount %\" PRIu32 \"\", pixelIndex, pixelCount);\n\t\treturn FALSE;\n\t}\n\n\treturn TRUE;\n}", "project": "FreeRDP", "hash": 241001505274535680660475700376958696237, "size": 189, "commit_id": "363d7046dfec4003b91aecf7867e3b05905f3843", "message": "Fixed oob read in clear_decompress_subcode_rlex\n\nFixed length checks before stream read.\nThanks to hac425 CVE-2020-11040", "target": 0, "dataset": "other", "idx": 448692}
{"func": "\n    if (bytes < 4)\n\treturn 0;\n\n    /* We don't decode anything unless we have a full chunk in the\n       input buffer (on the other hand, the Python part of the driver\n       makes sure this is always the case) */\n\n    ptr = buf;\n\n    framesize = I32(ptr);\n    if (framesize < I32(ptr))\n\treturn 0;\n\n    /* Make sure this is a frame chunk.  The Python driver takes\n       case of other chunk types. */\n\n    if (I16(ptr+4) != 0xF1FA) {\n\tstate->errcode = IMAGING_CODEC_UNKNOWN;\n\treturn -1;\n    }\n", "project": "Pillow", "hash": 134252300941587921790955744187697074809, "size": 185, "commit_id": "a09acd0decd8a87ccce939d5ff65dab59e7d365b", "message": "Catch FLI buffer overrun", "target": 1, "dataset": "other", "idx": 211493}
{"func": "\n    if (bytes < 4)\n\treturn 0;\n\n    /* We don't decode anything unless we have a full chunk in the\n       input buffer */\n\n    ptr = buf;\n\n    framesize = I32(ptr);\n    if (framesize < I32(ptr))\n\treturn 0;\n\n    /* Make sure this is a frame chunk.  The Python driver takes\n       case of other chunk types. */\n\n    if (bytes < 8) {\n        state->errcode = IMAGING_CODEC_OVERRUN;\n        return -1;\n    }\n    if (I16(ptr+4) != 0xF1FA) {\n\tstate->errcode = IMAGING_CODEC_UNKNOWN;\n\treturn -1;\n    }\n", "project": "Pillow", "hash": 303061285771598319385557632079841780973, "size": 188, "commit_id": "a09acd0decd8a87ccce939d5ff65dab59e7d365b", "message": "Catch FLI buffer overrun", "target": 0, "dataset": "other", "idx": 448723}
{"func": "\n  /* Remove character from the subdir name if it is \"/\". */\n  if (subdir_name.empty()) {\n    return false;\n  } else if (subdir_name.back() == '/') {\n    subdir_name.pop_back();\n  }\n\n  rgw_obj obj(s->bucket, std::move(subdir_name));\n\n  /* First, get attrset of the object we'll try to retrieve. */", "project": "ceph", "hash": 48255843606487150326028348212204674985, "size": 39, "commit_id": "f44a8ae8aa27ecef69528db9aec220f12492810e", "message": "rgw: RGWSwiftWebsiteHandler::is_web_dir checks empty subdir_name\n\nchecking for empty name avoids later assertion in RGWObjectCtx::set_atomic\n\nFixes: CVE-2021-3531\n\nReviewed-by: Casey Bodley <cbodley@redhat.com>\nSigned-off-by: Casey Bodley <cbodley@redhat.com>\n(cherry picked from commit 7196a469b4470f3c8628489df9a41ec8b00a5610)", "target": 1, "dataset": "other", "idx": 211502}
{"func": "  /* Remove character from the subdir name if it is \"/\". */\n  if (subdir_name.empty()) {\n    return false;\n  } else if (subdir_name.back() == '/') {\n    subdir_name.pop_back();\n    if (subdir_name.empty()) {\n      return false;\n    }\n  }\n\n  rgw_obj obj(s->bucket, std::move(subdir_name));\n\n  /* First, get attrset of the object we'll try to retrieve. */", "project": "ceph", "hash": 298807666171918733354542069578347162633, "size": 42, "commit_id": "f44a8ae8aa27ecef69528db9aec220f12492810e", "message": "rgw: RGWSwiftWebsiteHandler::is_web_dir checks empty subdir_name\n\nchecking for empty name avoids later assertion in RGWObjectCtx::set_atomic\n\nFixes: CVE-2021-3531\n\nReviewed-by: Casey Bodley <cbodley@redhat.com>\nSigned-off-by: Casey Bodley <cbodley@redhat.com>\n(cherry picked from commit 7196a469b4470f3c8628489df9a41ec8b00a5610)", "target": 0, "dataset": "other", "idx": 448852}
{"func": "                                    unsigned flags,\n\t\t\t\t    const pjmedia_sdp_session *local)\n{\n    pjmedia_sdp_session *new_offer;\n    pjmedia_sdp_session *old_offer;\n    char media_used[PJMEDIA_MAX_SDP_MEDIA];\n    unsigned oi; /* old offer media index */\n    pj_status_t status;\n\n    /* Check arguments are valid. */\n    PJ_ASSERT_RETURN(pool && neg && local, PJ_EINVAL);\n    if (status != PJ_SUCCESS)\n\treturn status;\n\n    /* Change state to STATE_LOCAL_OFFER */\n    neg->state = PJMEDIA_SDP_NEG_STATE_LOCAL_OFFER;\n\n    /* Init vars */\n    pj_bzero(media_used, sizeof(media_used));\n    old_offer = neg->active_local_sdp;\n    new_offer = pjmedia_sdp_session_clone(pool, local);\n\n    /* RFC 3264 Section 8: When issuing an offer that modifies the session,\n     * the \"o=\" line of the new SDP MUST be identical to that in the", "project": "pjproject", "hash": 197312442692241206063456839234745127746, "size": 121, "commit_id": "97b3d7addbaa720b7ddb0af9bf6f3e443e664365", "message": "Merge pull request from GHSA-hvq6-f89p-frvp", "target": 1, "dataset": "other", "idx": 211591}
{"func": "\t\t\t\t    pjmedia_sdp_neg *neg,\n                                    unsigned flags,\n\t\t\t\t    const pjmedia_sdp_session *local)\n{\n    pjmedia_sdp_session *new_offer;\n    pjmedia_sdp_session *old_offer;\n    unsigned oi; /* old offer media index */\n    pj_status_t status;\n\n    /* Check arguments are valid. */\n    PJ_ASSERT_RETURN(pool && neg && local, PJ_EINVAL);\n\treturn status;\n\n    /* Change state to STATE_LOCAL_OFFER */\n    neg->state = PJMEDIA_SDP_NEG_STATE_LOCAL_OFFER;\n\n    /* When there is no active local SDP in state PJMEDIA_SDP_NEG_STATE_DONE,\n     * it means that the previous initial SDP nego must have been failed,\n     * so we'll just set the local SDP offer here.\n     */\n    if (!neg->active_local_sdp) {\n\tneg->initial_sdp_tmp = NULL;\n\tneg->initial_sdp = pjmedia_sdp_session_clone(pool, local);\n\tneg->neg_local_sdp = pjmedia_sdp_session_clone(pool, local);\n\n\treturn PJ_SUCCESS;\n    }\n\n    /* Init vars */\n    old_offer = neg->active_local_sdp;\n    new_offer = pjmedia_sdp_session_clone(pool, local);\n\n    /* RFC 3264 Section 8: When issuing an offer that modifies the session,\n     * the \"o=\" line of the new SDP MUST be identical to that in the", "project": "pjproject", "hash": 195009002714543189743300644043816489769, "size": 131, "commit_id": "97b3d7addbaa720b7ddb0af9bf6f3e443e664365", "message": "Merge pull request from GHSA-hvq6-f89p-frvp", "target": 0, "dataset": "other", "idx": 449653}
{"func": "\t\t\tprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\n\t\t\t       str);\n\t} else if (!strcmp(str, \"auto\")) {\n\t\tparport_nr[0] = LP_PARPORT_AUTO;\n\t} else if (!strcmp(str, \"none\")) {\n\t\tparport_nr[parport_ptr++] = LP_PARPORT_NONE;\n\t} else if (!strcmp(str, \"reset\")) {\n\t\treset = 1;\n\t}\n\treturn 1;\n}", "project": "linux", "hash": 245258957937587766339739480970569790552, "size": 29, "commit_id": "3e21f4af170bebf47c187c1ff8bf155583c9f3b1", "message": "char: lp: fix possible integer overflow in lp_setup()\n\nThe lp_setup() code doesn't apply any bounds checking when passing\n\"lp=none\", and only in this case, resulting in an overflow of the\nparport_nr[] array. All versions in Git history are affected.\n\nReported-By: Roee Hay <roee.hay@hcl.com>\nCc: Ben Hutchings <ben@decadent.org.uk>\nCc: stable@vger.kernel.org\nSigned-off-by: Willy Tarreau <w@1wt.eu>\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 1, "dataset": "other", "idx": 211709}
{"func": "\t\t\tprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\n\t\t\t       str);\n\t} else if (!strcmp(str, \"auto\")) {\n\t\tparport_nr[0] = LP_PARPORT_AUTO;\n\t} else if (!strcmp(str, \"none\")) {\n\t\tif (parport_ptr < LP_NO)\n\t\t\tparport_nr[parport_ptr++] = LP_PARPORT_NONE;\n\t\telse\n\t\t\tprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\n\t\t\t       str);\n\t} else if (!strcmp(str, \"reset\")) {\n\t\treset = 1;\n\t}\n\treturn 1;\n}", "project": "linux", "hash": 46004058772895788984844550774932256768, "size": 33, "commit_id": "3e21f4af170bebf47c187c1ff8bf155583c9f3b1", "message": "char: lp: fix possible integer overflow in lp_setup()\n\nThe lp_setup() code doesn't apply any bounds checking when passing\n\"lp=none\", and only in this case, resulting in an overflow of the\nparport_nr[] array. All versions in Git history are affected.\n\nReported-By: Roee Hay <roee.hay@hcl.com>\nCc: Ben Hutchings <ben@decadent.org.uk>\nCc: stable@vger.kernel.org\nSigned-off-by: Willy Tarreau <w@1wt.eu>\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 0, "dataset": "other", "idx": 450878}
{"func": "\tmap = (struct dx_map_entry *) (data2 + blocksize);\n\tcount = dx_make_map(dir, (struct ext4_dir_entry_2 *) data1,\n\t\t\t     blocksize, hinfo, map);\n\tmap -= count;\n\tdx_sort_map(map, count);\n\t/* Split the existing block in the middle, size-wise */\n\tsize = 0;\n\tmove = 0;\n\tfor (i = count-1; i >= 0; i--) {\n\t\t/* is more than half of this entry in 2nd half of the block? */\n\t\tif (size + map[i].size/2 > blocksize/2)\n\t\t\tbreak;\n\t\tsize += map[i].size;\n\t\tmove++;\n\t}\n\t/* map index at which we will split */\n\tsplit = count - move;\n\thash2 = map[split].hash;\n\tcontinued = hash2 == map[split - 1].hash;\n\tdxtrace(printk(KERN_INFO \"Split block %lu at %x, %i/%i\\n\",\n\t\t\t(unsigned long)dx_get_block(frame->at),\n\t\t\t\t\thash2, split, count-split));", "project": "linux", "hash": 303167763486678289730668385446929678121, "size": 105, "commit_id": "5872331b3d91820e14716632ebb56b1399b34fe1", "message": "ext4: fix potential negative array index in do_split()\n\nIf for any reason a directory passed to do_split() does not have enough\nactive entries to exceed half the size of the block, we can end up\niterating over all \"count\" entries without finding a split point.\n\nIn this case, count == move, and split will be zero, and we will\nattempt a negative index into map[].\n\nGuard against this by detecting this case, and falling back to\nsplit-to-half-of-count instead; in this case we will still have\nplenty of space (> half blocksize) in each split block.\n\nFixes: ef2b02d3e617 (\"ext34: ensure do_split leaves enough free space in both blocks\")\nSigned-off-by: Eric Sandeen <sandeen@redhat.com>\nReviewed-by: Andreas Dilger <adilger@dilger.ca>\nReviewed-by: Jan Kara <jack@suse.cz>\nLink: https://lore.kernel.org/r/f53e246b-647c-64bb-16ec-135383c70ad7@redhat.com\nSigned-off-by: Theodore Ts'o <tytso@mit.edu>", "target": 1, "dataset": "other", "idx": 211725}
{"func": "\tmap = (struct dx_map_entry *) (data2 + blocksize);\n\tcount = dx_make_map(dir, (struct ext4_dir_entry_2 *) data1,\n\t\t\t     blocksize, hinfo, map);\n\tmap -= count;\n\tdx_sort_map(map, count);\n\t/* Ensure that neither split block is over half full */\n\tsize = 0;\n\tmove = 0;\n\tfor (i = count-1; i >= 0; i--) {\n\t\t/* is more than half of this entry in 2nd half of the block? */\n\t\tif (size + map[i].size/2 > blocksize/2)\n\t\t\tbreak;\n\t\tsize += map[i].size;\n\t\tmove++;\n\t}\n\t/*\n\t * map index at which we will split\n\t *\n\t * If the sum of active entries didn't exceed half the block size, just\n\t * split it in half by count; each resulting block will have at least\n\t * half the space free.\n\t */\n\tif (i > 0)\n\t\tsplit = count - move;\n\telse\n\t\tsplit = count/2;\n\n\thash2 = map[split].hash;\n\tcontinued = hash2 == map[split - 1].hash;\n\tdxtrace(printk(KERN_INFO \"Split block %lu at %x, %i/%i\\n\",\n\t\t\t(unsigned long)dx_get_block(frame->at),\n\t\t\t\t\thash2, split, count-split));", "project": "linux", "hash": 129777832969130454101953936431569714950, "size": 115, "commit_id": "5872331b3d91820e14716632ebb56b1399b34fe1", "message": "ext4: fix potential negative array index in do_split()\n\nIf for any reason a directory passed to do_split() does not have enough\nactive entries to exceed half the size of the block, we can end up\niterating over all \"count\" entries without finding a split point.\n\nIn this case, count == move, and split will be zero, and we will\nattempt a negative index into map[].\n\nGuard against this by detecting this case, and falling back to\nsplit-to-half-of-count instead; in this case we will still have\nplenty of space (> half blocksize) in each split block.\n\nFixes: ef2b02d3e617 (\"ext34: ensure do_split leaves enough free space in both blocks\")\nSigned-off-by: Eric Sandeen <sandeen@redhat.com>\nReviewed-by: Andreas Dilger <adilger@dilger.ca>\nReviewed-by: Jan Kara <jack@suse.cz>\nLink: https://lore.kernel.org/r/f53e246b-647c-64bb-16ec-135383c70ad7@redhat.com\nSigned-off-by: Theodore Ts'o <tytso@mit.edu>", "target": 0, "dataset": "other", "idx": 451257}
{"func": "\tif (error)\n\t{\n\t\tfree_key_material(pkey);\n\t\treturn error;\n\t}\n\n\tcrypto_init();\n\n\t// Save the the key to the disk\n\tswitch (algorithm)\n\t{", "project": "SoftHSMv2", "hash": 327232896342706582422940253312057636815, "size": 176, "commit_id": "492447cd4a2be449e99fb9ad2519ea3277aaad28", "message": "SUPPORT-136: softhsm2-keyconv creates files with sensitive material in insecure way.", "target": 1, "dataset": "other", "idx": 211835}
{"func": "\t{\n\t\tfree_key_material(pkey);\n\t\treturn error;\n\t}\n\n\t// Create and set file permissions if the file does not exist.\n\tint fd = open(out_path, O_CREAT, S_IRUSR | S_IWUSR);\n\tif (fd == -1)\n\t{\n\t\tfprintf(stderr, \"ERROR: Could not open the output file: %s (errno %i)\\n\",\n\t\t\tout_path, errno);\n\t\tfree_key_material(pkey);\n\t\treturn 1;\n\t}\n\t::close(fd);\n\n\tcrypto_init();\n\n\t// Save the the key to the disk\n\tswitch (algorithm)\n\t{", "project": "SoftHSMv2", "hash": 230438268535972040487908663022608902484, "size": 187, "commit_id": "492447cd4a2be449e99fb9ad2519ea3277aaad28", "message": "SUPPORT-136: softhsm2-keyconv creates files with sensitive material in insecure way.", "target": 0, "dataset": "other", "idx": 452351}
{"func": "        TIFFGetField(tiff, TIFFTAG_TILEWIDTH, &tile_width);\n        TIFFGetField(tiff, TIFFTAG_TILELENGTH, &tile_length);\n\n        // We could use TIFFTileSize, but for YCbCr data it returns subsampled data size\n        row_byte_size = (tile_width * state->bits + 7) / 8;\n        state->bytes = row_byte_size * tile_length;\n\n        /* overflow check for malloc */\n        if (state->bytes > INT_MAX - 1) {\n            state->errcode = IMAGING_CODEC_MEMORY;\n            TIFFClose(tiff);\n            return -1;\n        }\n\n        /* realloc to fit whole tile */\n        new_data = realloc (state->buffer, state->bytes);\n        if (!new_data) {\n            state->errcode = IMAGING_CODEC_MEMORY;\n            TIFFClose(tiff);\n            return -1;\n            rows_per_strip = state->ysize;\n        }\n        TRACE((\"RowsPerStrip: %u \\n\", rows_per_strip));\n\n        // We could use TIFFStripSize, but for YCbCr data it returns subsampled data size\n        row_byte_size = (state->xsize * state->bits + 7) / 8;\n        state->bytes = rows_per_strip * row_byte_size;\n\n        TRACE((\"StripSize: %d \\n\", state->bytes));\n\n        /* realloc to fit whole strip */\n        new_data = realloc (state->buffer, state->bytes);\n        if (!new_data) {\n            state->errcode = IMAGING_CODEC_MEMORY;\n            TIFFClose(tiff);\n            return -1;", "project": "Pillow", "hash": 270190227381984561901599925314913105490, "size": 182, "commit_id": "4e2def2539ec13e53a82e06c4b3daf00454100c4", "message": "Overflow checks for realloc for tiff decoding", "target": 1, "dataset": "other", "idx": 211908}
{"func": "\n        TIFFGetField(tiff, TIFFTAG_TILEWIDTH, &tile_width);\n        TIFFGetField(tiff, TIFFTAG_TILELENGTH, &tile_length);\n\n        // We could use TIFFTileSize, but for YCbCr data it returns subsampled data size\n        row_byte_size = (tile_width * state->bits + 7) / 8;\n\n        /* overflow check for realloc */\n        if (INT_MAX / row_byte_size < tile_length) {\n            state->errcode = IMAGING_CODEC_MEMORY;\n            TIFFClose(tiff);\n            return -1;\n        }\n        \n        state->bytes = row_byte_size * tile_length;\n\n        /* realloc to fit whole tile */\n        /* malloc check above */\n        new_data = realloc (state->buffer, state->bytes);\n        if (!new_data) {\n            state->errcode = IMAGING_CODEC_MEMORY;\n            TIFFClose(tiff);\n            return -1;\n        }\n        TRACE((\"RowsPerStrip: %u \\n\", rows_per_strip));\n\n        // We could use TIFFStripSize, but for YCbCr data it returns subsampled data size\n        row_byte_size = (state->xsize * state->bits + 7) / 8;\n\n        /* overflow check for realloc */\n        if (INT_MAX / row_byte_size < rows_per_strip) {\n            state->errcode = IMAGING_CODEC_MEMORY;\n            TIFFClose(tiff);\n            return -1;\n        }\n        \n        state->bytes = rows_per_strip * row_byte_size;\n\n        TRACE((\"StripSize: %d \\n\", state->bytes));\n\n        /* realloc to fit whole strip */\n        /* malloc check above */\n        new_data = realloc (state->buffer, state->bytes);\n        if (!new_data) {\n            state->errcode = IMAGING_CODEC_MEMORY;\n            TIFFClose(tiff);\n            return -1;", "project": "Pillow", "hash": 88829763694371056964147542819562350572, "size": 193, "commit_id": "4e2def2539ec13e53a82e06c4b3daf00454100c4", "message": "Overflow checks for realloc for tiff decoding", "target": 0, "dataset": "other", "idx": 453246}
{"func": "\nstatic void bfq_idle_slice_timer_body(struct bfq_queue *bfqq)\n{\n\tstruct bfq_data *bfqd = bfqq->bfqd;\n\tenum bfqq_expiration reason;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\tbfq_clear_bfqq_wait_request(bfqq);\n\n\tif (bfqq != bfqd->in_service_queue) {\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\treturn;\n\t}\n\n\tif (bfq_bfqq_budget_timeout(bfqq))\n\t\t/*\n\t\t * Also here the queue can be safely expired\n\t\t * for budget timeout without wasting\n\t\t * guarantees\n\t\t */\n\t\treason = BFQQE_BUDGET_TIMEOUT;\n\telse if (bfqq->queued[0] == 0 && bfqq->queued[1] == 0)\n\t\t/*\n\t\t * The queue may not be empty upon timer expiration,\n\t\t * because we may not disable the timer when the\n\t\t * first request of the in-service queue arrives\n\t\t * during disk idling.\n\t\t */\n\t\treason = BFQQE_TOO_IDLE;\n\telse\n\t\tgoto schedule_dispatch;\n\n\tbfq_bfqq_expire(bfqd, bfqq, true, reason);\n\nschedule_dispatch:\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\tbfq_schedule_dispatch(bfqd);", "project": "linux", "hash": 128858888171698182771172635480199449190, "size": 38, "commit_id": "2f95fa5c955d0a9987ffdc3a095e2f4e62c5f2a9", "message": "block, bfq: fix use-after-free in bfq_idle_slice_timer_body\n\nIn bfq_idle_slice_timer func, bfqq = bfqd->in_service_queue is\nnot in bfqd-lock critical section. The bfqq, which is not\nequal to NULL in bfq_idle_slice_timer, may be freed after passing\nto bfq_idle_slice_timer_body. So we will access the freed memory.\n\nIn addition, considering the bfqq may be in race, we should\nfirstly check whether bfqq is in service before doing something\non it in bfq_idle_slice_timer_body func. If the bfqq in race is\nnot in service, it means the bfqq has been expired through\n__bfq_bfqq_expire func, and wait_request flags has been cleared in\n__bfq_bfqd_reset_in_service func. So we do not need to re-clear the\nwait_request of bfqq which is not in service.\n\nKASAN log is given as follows:\n[13058.354613] ==================================================================\n[13058.354640] BUG: KASAN: use-after-free in bfq_idle_slice_timer+0xac/0x290\n[13058.354644] Read of size 8 at addr ffffa02cf3e63f78 by task fork13/19767\n[13058.354646]\n[13058.354655] CPU: 96 PID: 19767 Comm: fork13\n[13058.354661] Call trace:\n[13058.354667]  dump_backtrace+0x0/0x310\n[13058.354672]  show_stack+0x28/0x38\n[13058.354681]  dump_stack+0xd8/0x108\n[13058.354687]  print_address_description+0x68/0x2d0\n[13058.354690]  kasan_report+0x124/0x2e0\n[13058.354697]  __asan_load8+0x88/0xb0\n[13058.354702]  bfq_idle_slice_timer+0xac/0x290\n[13058.354707]  __hrtimer_run_queues+0x298/0x8b8\n[13058.354710]  hrtimer_interrupt+0x1b8/0x678\n[13058.354716]  arch_timer_handler_phys+0x4c/0x78\n[13058.354722]  handle_percpu_devid_irq+0xf0/0x558\n[13058.354731]  generic_handle_irq+0x50/0x70\n[13058.354735]  __handle_domain_irq+0x94/0x110\n[13058.354739]  gic_handle_irq+0x8c/0x1b0\n[13058.354742]  el1_irq+0xb8/0x140\n[13058.354748]  do_wp_page+0x260/0xe28\n[13058.354752]  __handle_mm_fault+0x8ec/0x9b0\n[13058.354756]  handle_mm_fault+0x280/0x460\n[13058.354762]  do_page_fault+0x3ec/0x890\n[13058.354765]  do_mem_abort+0xc0/0x1b0\n[13058.354768]  el0_da+0x24/0x28\n[13058.354770]\n[13058.354773] Allocated by task 19731:\n[13058.354780]  kasan_kmalloc+0xe0/0x190\n[13058.354784]  kasan_slab_alloc+0x14/0x20\n[13058.354788]  kmem_cache_alloc_node+0x130/0x440\n[13058.354793]  bfq_get_queue+0x138/0x858\n[13058.354797]  bfq_get_bfqq_handle_split+0xd4/0x328\n[13058.354801]  bfq_init_rq+0x1f4/0x1180\n[13058.354806]  bfq_insert_requests+0x264/0x1c98\n[13058.354811]  blk_mq_sched_insert_requests+0x1c4/0x488\n[13058.354818]  blk_mq_flush_plug_list+0x2d4/0x6e0\n[13058.354826]  blk_flush_plug_list+0x230/0x548\n[13058.354830]  blk_finish_plug+0x60/0x80\n[13058.354838]  read_pages+0xec/0x2c0\n[13058.354842]  __do_page_cache_readahead+0x374/0x438\n[13058.354846]  ondemand_readahead+0x24c/0x6b0\n[13058.354851]  page_cache_sync_readahead+0x17c/0x2f8\n[13058.354858]  generic_file_buffered_read+0x588/0xc58\n[13058.354862]  generic_file_read_iter+0x1b4/0x278\n[13058.354965]  ext4_file_read_iter+0xa8/0x1d8 [ext4]\n[13058.354972]  __vfs_read+0x238/0x320\n[13058.354976]  vfs_read+0xbc/0x1c0\n[13058.354980]  ksys_read+0xdc/0x1b8\n[13058.354984]  __arm64_sys_read+0x50/0x60\n[13058.354990]  el0_svc_common+0xb4/0x1d8\n[13058.354994]  el0_svc_handler+0x50/0xa8\n[13058.354998]  el0_svc+0x8/0xc\n[13058.354999]\n[13058.355001] Freed by task 19731:\n[13058.355007]  __kasan_slab_free+0x120/0x228\n[13058.355010]  kasan_slab_free+0x10/0x18\n[13058.355014]  kmem_cache_free+0x288/0x3f0\n[13058.355018]  bfq_put_queue+0x134/0x208\n[13058.355022]  bfq_exit_icq_bfqq+0x164/0x348\n[13058.355026]  bfq_exit_icq+0x28/0x40\n[13058.355030]  ioc_exit_icq+0xa0/0x150\n[13058.355035]  put_io_context_active+0x250/0x438\n[13058.355038]  exit_io_context+0xd0/0x138\n[13058.355045]  do_exit+0x734/0xc58\n[13058.355050]  do_group_exit+0x78/0x220\n[13058.355054]  __wake_up_parent+0x0/0x50\n[13058.355058]  el0_svc_common+0xb4/0x1d8\n[13058.355062]  el0_svc_handler+0x50/0xa8\n[13058.355066]  el0_svc+0x8/0xc\n[13058.355067]\n[13058.355071] The buggy address belongs to the object at ffffa02cf3e63e70#012 which belongs to the cache bfq_queue of size 464\n[13058.355075] The buggy address is located 264 bytes inside of#012 464-byte region [ffffa02cf3e63e70, ffffa02cf3e64040)\n[13058.355077] The buggy address belongs to the page:\n[13058.355083] page:ffff7e80b3cf9800 count:1 mapcount:0 mapping:ffff802db5c90780 index:0xffffa02cf3e606f0 compound_mapcount: 0\n[13058.366175] flags: 0x2ffffe0000008100(slab|head)\n[13058.370781] raw: 2ffffe0000008100 ffff7e80b53b1408 ffffa02d730c1c90 ffff802db5c90780\n[13058.370787] raw: ffffa02cf3e606f0 0000000000370023 00000001ffffffff 0000000000000000\n[13058.370789] page dumped because: kasan: bad access detected\n[13058.370791]\n[13058.370792] Memory state around the buggy address:\n[13058.370797]  ffffa02cf3e63e00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fb fb\n[13058.370801]  ffffa02cf3e63e80: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[13058.370805] >ffffa02cf3e63f00: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[13058.370808]                                                                 ^\n[13058.370811]  ffffa02cf3e63f80: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[13058.370815]  ffffa02cf3e64000: fb fb fb fb fb fb fb fb fc fc fc fc fc fc fc fc\n[13058.370817] ==================================================================\n[13058.370820] Disabling lock debugging due to kernel taint\n\nHere, we directly pass the bfqd to bfq_idle_slice_timer_body func.\n--\nV2->V3: rewrite the comment as suggested by Paolo Valente\nV1->V2: add one comment, and add Fixes and Reported-by tag.\n\nFixes: aee69d78d (\"block, bfq: introduce the BFQ-v0 I/O scheduler as an extra scheduler\")\nAcked-by: Paolo Valente <paolo.valente@linaro.org>\nReported-by: Wang Wang <wangwang2@huawei.com>\nSigned-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>\nSigned-off-by: Feilong Lin <linfeilong@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>", "target": 1, "dataset": "other", "idx": 211918}
{"func": "static void\nbfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)\n{\n\tenum bfqq_expiration reason;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bfqd->lock, flags);\n\n\t/*\n\t * Considering that bfqq may be in race, we should firstly check\n\t * whether bfqq is in service before doing something on it. If\n\t * the bfqq in race is not in service, it has already been expired\n\t * through __bfq_bfqq_expire func and its wait_request flags has\n\t * been cleared in __bfq_bfqd_reset_in_service func.\n\t */\n\tif (bfqq != bfqd->in_service_queue) {\n\t\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\t\treturn;\n\t}\n\n\tbfq_clear_bfqq_wait_request(bfqq);\n\n\tif (bfq_bfqq_budget_timeout(bfqq))\n\t\t/*\n\t\t * Also here the queue can be safely expired\n\t\t * for budget timeout without wasting\n\t\t * guarantees\n\t\t */\n\t\treason = BFQQE_BUDGET_TIMEOUT;\n\telse if (bfqq->queued[0] == 0 && bfqq->queued[1] == 0)\n\t\t/*\n\t\t * The queue may not be empty upon timer expiration,\n\t\t * because we may not disable the timer when the\n\t\t * first request of the in-service queue arrives\n\t\t * during disk idling.\n\t\t */\n\t\treason = BFQQE_TOO_IDLE;\n\telse\n\t\tgoto schedule_dispatch;\n\n\tbfq_bfqq_expire(bfqd, bfqq, true, reason);\n\nschedule_dispatch:\n\tspin_unlock_irqrestore(&bfqd->lock, flags);\n\tbfq_schedule_dispatch(bfqd);", "project": "linux", "hash": 316977131305739620723653491038112566062, "size": 45, "commit_id": "2f95fa5c955d0a9987ffdc3a095e2f4e62c5f2a9", "message": "block, bfq: fix use-after-free in bfq_idle_slice_timer_body\n\nIn bfq_idle_slice_timer func, bfqq = bfqd->in_service_queue is\nnot in bfqd-lock critical section. The bfqq, which is not\nequal to NULL in bfq_idle_slice_timer, may be freed after passing\nto bfq_idle_slice_timer_body. So we will access the freed memory.\n\nIn addition, considering the bfqq may be in race, we should\nfirstly check whether bfqq is in service before doing something\non it in bfq_idle_slice_timer_body func. If the bfqq in race is\nnot in service, it means the bfqq has been expired through\n__bfq_bfqq_expire func, and wait_request flags has been cleared in\n__bfq_bfqd_reset_in_service func. So we do not need to re-clear the\nwait_request of bfqq which is not in service.\n\nKASAN log is given as follows:\n[13058.354613] ==================================================================\n[13058.354640] BUG: KASAN: use-after-free in bfq_idle_slice_timer+0xac/0x290\n[13058.354644] Read of size 8 at addr ffffa02cf3e63f78 by task fork13/19767\n[13058.354646]\n[13058.354655] CPU: 96 PID: 19767 Comm: fork13\n[13058.354661] Call trace:\n[13058.354667]  dump_backtrace+0x0/0x310\n[13058.354672]  show_stack+0x28/0x38\n[13058.354681]  dump_stack+0xd8/0x108\n[13058.354687]  print_address_description+0x68/0x2d0\n[13058.354690]  kasan_report+0x124/0x2e0\n[13058.354697]  __asan_load8+0x88/0xb0\n[13058.354702]  bfq_idle_slice_timer+0xac/0x290\n[13058.354707]  __hrtimer_run_queues+0x298/0x8b8\n[13058.354710]  hrtimer_interrupt+0x1b8/0x678\n[13058.354716]  arch_timer_handler_phys+0x4c/0x78\n[13058.354722]  handle_percpu_devid_irq+0xf0/0x558\n[13058.354731]  generic_handle_irq+0x50/0x70\n[13058.354735]  __handle_domain_irq+0x94/0x110\n[13058.354739]  gic_handle_irq+0x8c/0x1b0\n[13058.354742]  el1_irq+0xb8/0x140\n[13058.354748]  do_wp_page+0x260/0xe28\n[13058.354752]  __handle_mm_fault+0x8ec/0x9b0\n[13058.354756]  handle_mm_fault+0x280/0x460\n[13058.354762]  do_page_fault+0x3ec/0x890\n[13058.354765]  do_mem_abort+0xc0/0x1b0\n[13058.354768]  el0_da+0x24/0x28\n[13058.354770]\n[13058.354773] Allocated by task 19731:\n[13058.354780]  kasan_kmalloc+0xe0/0x190\n[13058.354784]  kasan_slab_alloc+0x14/0x20\n[13058.354788]  kmem_cache_alloc_node+0x130/0x440\n[13058.354793]  bfq_get_queue+0x138/0x858\n[13058.354797]  bfq_get_bfqq_handle_split+0xd4/0x328\n[13058.354801]  bfq_init_rq+0x1f4/0x1180\n[13058.354806]  bfq_insert_requests+0x264/0x1c98\n[13058.354811]  blk_mq_sched_insert_requests+0x1c4/0x488\n[13058.354818]  blk_mq_flush_plug_list+0x2d4/0x6e0\n[13058.354826]  blk_flush_plug_list+0x230/0x548\n[13058.354830]  blk_finish_plug+0x60/0x80\n[13058.354838]  read_pages+0xec/0x2c0\n[13058.354842]  __do_page_cache_readahead+0x374/0x438\n[13058.354846]  ondemand_readahead+0x24c/0x6b0\n[13058.354851]  page_cache_sync_readahead+0x17c/0x2f8\n[13058.354858]  generic_file_buffered_read+0x588/0xc58\n[13058.354862]  generic_file_read_iter+0x1b4/0x278\n[13058.354965]  ext4_file_read_iter+0xa8/0x1d8 [ext4]\n[13058.354972]  __vfs_read+0x238/0x320\n[13058.354976]  vfs_read+0xbc/0x1c0\n[13058.354980]  ksys_read+0xdc/0x1b8\n[13058.354984]  __arm64_sys_read+0x50/0x60\n[13058.354990]  el0_svc_common+0xb4/0x1d8\n[13058.354994]  el0_svc_handler+0x50/0xa8\n[13058.354998]  el0_svc+0x8/0xc\n[13058.354999]\n[13058.355001] Freed by task 19731:\n[13058.355007]  __kasan_slab_free+0x120/0x228\n[13058.355010]  kasan_slab_free+0x10/0x18\n[13058.355014]  kmem_cache_free+0x288/0x3f0\n[13058.355018]  bfq_put_queue+0x134/0x208\n[13058.355022]  bfq_exit_icq_bfqq+0x164/0x348\n[13058.355026]  bfq_exit_icq+0x28/0x40\n[13058.355030]  ioc_exit_icq+0xa0/0x150\n[13058.355035]  put_io_context_active+0x250/0x438\n[13058.355038]  exit_io_context+0xd0/0x138\n[13058.355045]  do_exit+0x734/0xc58\n[13058.355050]  do_group_exit+0x78/0x220\n[13058.355054]  __wake_up_parent+0x0/0x50\n[13058.355058]  el0_svc_common+0xb4/0x1d8\n[13058.355062]  el0_svc_handler+0x50/0xa8\n[13058.355066]  el0_svc+0x8/0xc\n[13058.355067]\n[13058.355071] The buggy address belongs to the object at ffffa02cf3e63e70#012 which belongs to the cache bfq_queue of size 464\n[13058.355075] The buggy address is located 264 bytes inside of#012 464-byte region [ffffa02cf3e63e70, ffffa02cf3e64040)\n[13058.355077] The buggy address belongs to the page:\n[13058.355083] page:ffff7e80b3cf9800 count:1 mapcount:0 mapping:ffff802db5c90780 index:0xffffa02cf3e606f0 compound_mapcount: 0\n[13058.366175] flags: 0x2ffffe0000008100(slab|head)\n[13058.370781] raw: 2ffffe0000008100 ffff7e80b53b1408 ffffa02d730c1c90 ffff802db5c90780\n[13058.370787] raw: ffffa02cf3e606f0 0000000000370023 00000001ffffffff 0000000000000000\n[13058.370789] page dumped because: kasan: bad access detected\n[13058.370791]\n[13058.370792] Memory state around the buggy address:\n[13058.370797]  ffffa02cf3e63e00: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fb fb\n[13058.370801]  ffffa02cf3e63e80: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[13058.370805] >ffffa02cf3e63f00: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[13058.370808]                                                                 ^\n[13058.370811]  ffffa02cf3e63f80: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb\n[13058.370815]  ffffa02cf3e64000: fb fb fb fb fb fb fb fb fc fc fc fc fc fc fc fc\n[13058.370817] ==================================================================\n[13058.370820] Disabling lock debugging due to kernel taint\n\nHere, we directly pass the bfqd to bfq_idle_slice_timer_body func.\n--\nV2->V3: rewrite the comment as suggested by Paolo Valente\nV1->V2: add one comment, and add Fixes and Reported-by tag.\n\nFixes: aee69d78d (\"block, bfq: introduce the BFQ-v0 I/O scheduler as an extra scheduler\")\nAcked-by: Paolo Valente <paolo.valente@linaro.org>\nReported-by: Wang Wang <wangwang2@huawei.com>\nSigned-off-by: Zhiqiang Liu <liuzhiqiang26@huawei.com>\nSigned-off-by: Feilong Lin <linfeilong@huawei.com>\nSigned-off-by: Jens Axboe <axboe@kernel.dk>", "target": 0, "dataset": "other", "idx": 453337}
{"func": "crypt_pw_cmp(const char *userpwd, const char *dbpwd)\n{\n    int rc;\n    char *cp;\n    struct crypt_data data;\n    data.initialized = 0;\n\n    /* we use salt (first 2 chars) of encoded password in call to crypt_r() */\n    cp = crypt_r(userpwd, dbpwd, &data);\n    if (cp) {\n        rc = slapi_ct_memcmp(dbpwd, cp, strlen(dbpwd));\n    } else {\n        rc = -1;\n    }\n    return rc;\n}", "project": "389-ds-base", "hash": 170054486704662206130926786349165718712, "size": 16, "commit_id": "aeb90eb0c41fc48541d983f323c627b2e6c328c7", "message": "Issue 4817 - BUG - locked crypt accounts on import may allow all passwords (#4819)\n\nBug Description: Due to mishanding of short dbpwd hashes, the\r\ncrypt_r algorithm was misused and was only comparing salts\r\nin some cases, rather than checking the actual content\r\nof the password.\r\n\r\nFix Description: Stricter checks on dbpwd lengths to ensure\r\nthat content passed to crypt_r has at least 2 salt bytes and\r\n1 hash byte, as well as stricter checks on ct_memcmp to ensure\r\nthat compared values are the same length, rather than potentially\r\nallowing overruns/short comparisons.\r\n\r\nfixes: https://github.com/389ds/389-ds-base/issues/4817\r\n\r\nAuthor: William Brown <william@blackhats.net.au>\r\n\r\nReview by: @mreynolds389", "target": 1, "dataset": "other", "idx": 211936}
{"func": "crypt_pw_cmp(const char *userpwd, const char *dbpwd)\n{\n    int rc = -1;\n    char *cp = NULL;\n    size_t dbpwd_len = strlen(dbpwd);\n    struct crypt_data data;\n    data.initialized = 0;\n\n    /*\n     * there MUST be at least 2 chars of salt and some pw bytes, else this is INVALID and will\n     * allow any password to bind as we then only compare SALTS.\n     */\n    if (dbpwd_len >= 3) {\n        /* we use salt (first 2 chars) of encoded password in call to crypt_r() */\n        cp = crypt_r(userpwd, dbpwd, &data);\n    }\n    /* If these are not the same length, we can not proceed safely with memcmp. */\n    if (cp && dbpwd_len == strlen(cp)) {\n        rc = slapi_ct_memcmp(dbpwd, cp, dbpwd_len);\n    } else {\n        rc = -1;\n    }\n    return rc;\n}", "project": "389-ds-base", "hash": 40922764562624925085830862501615542298, "size": 24, "commit_id": "aeb90eb0c41fc48541d983f323c627b2e6c328c7", "message": "Issue 4817 - BUG - locked crypt accounts on import may allow all passwords (#4819)\n\nBug Description: Due to mishanding of short dbpwd hashes, the\r\ncrypt_r algorithm was misused and was only comparing salts\r\nin some cases, rather than checking the actual content\r\nof the password.\r\n\r\nFix Description: Stricter checks on dbpwd lengths to ensure\r\nthat content passed to crypt_r has at least 2 salt bytes and\r\n1 hash byte, as well as stricter checks on ct_memcmp to ensure\r\nthat compared values are the same length, rather than potentially\r\nallowing overruns/short comparisons.\r\n\r\nfixes: https://github.com/389ds/389-ds-base/issues/4817\r\n\r\nAuthor: William Brown <william@blackhats.net.au>\r\n\r\nReview by: @mreynolds389", "target": 0, "dataset": "other", "idx": 453456}
{"func": "\tapdu.resplen = sizeof(rbuf);\n\tapdu.le = crgram_len;\n\n\tapdu.data = sbuf;\n\tapdu.lc = apdu.datalen = crgram_len+1;\n\tsbuf[0] = tcos3 ? 0x00 : ((data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1) ? 0x81 : 0x02);\n\tmemcpy(sbuf+1, crgram, crgram_len);\n\n\tr = sc_transmit_apdu(card, &apdu);\n\tLOG_TEST_RET(card->ctx, r, \"APDU transmit failed\");\n", "project": "OpenSC", "hash": 35692446162590344930620836498368260919, "size": 44, "commit_id": "9d294de90d1cc66956389856e60b6944b27b4817", "message": "prevent out of bounds write\n\nfixes https://oss-fuzz.com/testcase-detail/5226571123392512", "target": 1, "dataset": "other", "idx": 211948}
{"func": "\tapdu.le = crgram_len;\n\n\tapdu.data = sbuf;\n\tapdu.lc = apdu.datalen = crgram_len+1;\n\tsbuf[0] = tcos3 ? 0x00 : ((data->pad_flags & SC_ALGORITHM_RSA_PAD_PKCS1) ? 0x81 : 0x02);\n\tif (sizeof sbuf - 1 < crgram_len)\n\t\treturn SC_ERROR_INVALID_ARGUMENTS;\n\tmemcpy(sbuf+1, crgram, crgram_len);\n\n\tr = sc_transmit_apdu(card, &apdu);\n\tLOG_TEST_RET(card->ctx, r, \"APDU transmit failed\");\n", "project": "OpenSC", "hash": 135428929269373710791680828572609443287, "size": 46, "commit_id": "9d294de90d1cc66956389856e60b6944b27b4817", "message": "prevent out of bounds write\n\nfixes https://oss-fuzz.com/testcase-detail/5226571123392512", "target": 0, "dataset": "other", "idx": 453691}
{"func": "void luaD_callnoyield (lua_State *L, StkId func, int nResults) {\n  incXCcalls(L);\n  if (getCcalls(L) <= CSTACKERR)  /* possible stack overflow? */\n    luaE_freeCI(L);\n  luaD_call(L, func, nResults);\n  decXCcalls(L);\n}", "project": "lua", "hash": 156964113476265802309004545976693990252, "size": 7, "commit_id": "34affe7a63fc5d842580a9f23616d057e17dfe27", "message": "Fixed bug: 'luaD_callnoyield' called twice in a row\n\nIn luaD_callnoyield, when there is a possible stack overflow, it\nzeros the number of CallInfos to force a check when calling the\nfunction. However, if the \"function\" is not a function, the code will\nraise an error before checking the stack. Then, the error handling calls\nluaD_callnoyield again and nCcalls is decremented again, crossing the\nstack redzone without raising an error. (This loop can only happens\nonce, because the error handler must be a function.  But once is enough\nto cross the redzone.)", "target": 1, "dataset": "other", "idx": 211977}
{"func": "void luaD_callnoyield (lua_State *L, StkId func, int nResults) {\n  incXCcalls(L);\n  if (getCcalls(L) <= CSTACKERR) {  /* possible C stack overflow? */\n    luaE_exitCcall(L);  /* to compensate decrement in next call */\n    luaE_enterCcall(L);  /* check properly */\n  }\n  luaD_call(L, func, nResults);\n  decXCcalls(L);\n}", "project": "lua", "hash": 284813496960113122553300230525776419698, "size": 9, "commit_id": "34affe7a63fc5d842580a9f23616d057e17dfe27", "message": "Fixed bug: 'luaD_callnoyield' called twice in a row\n\nIn luaD_callnoyield, when there is a possible stack overflow, it\nzeros the number of CallInfos to force a check when calling the\nfunction. However, if the \"function\" is not a function, the code will\nraise an error before checking the stack. Then, the error handling calls\nluaD_callnoyield again and nCcalls is decremented again, crossing the\nstack redzone without raising an error. (This loop can only happens\nonce, because the error handler must be a function.  But once is enough\nto cross the redzone.)", "target": 0, "dataset": "other", "idx": 454326}
{"func": "\t\t\t\t}\n\t\t\t\tJAS_DBGLOG(10, (\"numnewpasses=%d \", numnewpasses));\n\t\t\t\tseg = cblk->curseg;\n\t\t\t\tsavenumnewpasses = numnewpasses;\n\t\t\t\tmycounter = 0;\n\t\t\t\tif (numnewpasses > 0) {\n\t\t\t\t\tif ((m = jpc_getcommacode(inb)) < 0) {\n\t\t\t\t\t\tjpc_bitstream_close(inb);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tcblk->numlenbits += m;", "project": "jasper", "hash": 116389874333112461743846737970773253172, "size": 266, "commit_id": "c6f9fb6ec7fc97a5c4213f9077faf8622685d160", "message": "jpc_t2dec: work around CVE-2016-9398 by limiting cblk->firstpassno\n\nThis replaces my commit 910c351ff5a80a373c5f0ec19f012e8d52b8b4c9 which\nwas reverted by fc03b57384225055847ec92659e50f95d9ea63f2\n\nCloses https://github.com/jasper-maint/jasper/issues/10", "target": 1, "dataset": "other", "idx": 212027}
{"func": "\t\t\t\tJAS_DBGLOG(10, (\"numnewpasses=%d \", numnewpasses));\n\t\t\t\tseg = cblk->curseg;\n\t\t\t\tsavenumnewpasses = numnewpasses;\n\t\t\t\tmycounter = 0;\n\t\t\t\tif (numnewpasses > 0) {\n\t\t\t\t\tif (cblk->firstpassno > 10000) {\n\t\t\t\t\t\t/* workaround for\n\t\t\t\t\t\t   CVE-2016-9398: this\n\t\t\t\t\t\t   large value would\n\t\t\t\t\t\t   make\n\t\t\t\t\t\t   JPC_SEGPASSCNT()\n\t\t\t\t\t\t   return a negative\n\t\t\t\t\t\t   value, causing an\n\t\t\t\t\t\t   assertion failure\n\t\t\t\t\t\t   in\n\t\t\t\t\t\t   jpc_floorlog2() */\n\t\t\t\t\t\tjpc_bitstream_close(inb);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tif ((m = jpc_getcommacode(inb)) < 0) {\n\t\t\t\t\t\tjpc_bitstream_close(inb);\n\t\t\t\t\t\treturn -1;\n\t\t\t\t\t}\n\t\t\t\t\tcblk->numlenbits += m;", "project": "jasper", "hash": 29552345775717961382863408064126086670, "size": 280, "commit_id": "c6f9fb6ec7fc97a5c4213f9077faf8622685d160", "message": "jpc_t2dec: work around CVE-2016-9398 by limiting cblk->firstpassno\n\nThis replaces my commit 910c351ff5a80a373c5f0ec19f012e8d52b8b4c9 which\nwas reverted by fc03b57384225055847ec92659e50f95d9ea63f2\n\nCloses https://github.com/jasper-maint/jasper/issues/10", "target": 0, "dataset": "other", "idx": 454553}
{"func": "wsrep_cb_status_t wsrep_sst_donate_cb (void* app_ctx, void* recv_ctx,\n                                       const void* msg, size_t msg_len,\n                                       const wsrep_gtid_t* current_gtid,\n                                       const char* state, size_t state_len,\n                                       bool bypass)\n{\n  /* This will be reset when sync callback is called.\n   * Should we set wsrep_ready to FALSE here too? */\n  local_status.set(WSREP_MEMBER_DONOR);\n\n  const char* method = (char*)msg;\n  size_t method_len  = strlen (method);\n  const char* data   = method + method_len + 1;\n\n  char uuid_str[37];\n  wsrep_uuid_print (&current_gtid->uuid, uuid_str, sizeof(uuid_str));\n\n  wsp::env env(NULL);\n  if (env.error())\n  {\n    WSREP_ERROR(\"wsrep_sst_donate_cb(): env var ctor failed: %d\", -env.error());", "project": "mysql-wsrep", "hash": 340163712951592193597010920393388964257, "size": 44, "commit_id": "4ea4b0c6a318209ac09b15aaa906c7b4a13b988c", "message": "codership/mysql-wsrep-bugs#758 Donor uses invalid SST methods", "target": 1, "dataset": "other", "idx": 212088}
{"func": "                                       const void* msg, size_t msg_len,\n                                       const wsrep_gtid_t* current_gtid,\n                                       const char* state, size_t state_len,\n                                       bool bypass)\n{\n  const char* method = (char*)msg;\n  size_t method_len  = strlen (method);\n\n  if (check_request_str(method, filename_char))\n  {\n    WSREP_ERROR(\"Bad SST method name. SST canceled.\");\n    return WSREP_CB_FAILURE;\n  }\n\n  const char* data   = method + method_len + 1;\n\n  if (check_request_str(data, address_char))\n  {\n    WSREP_ERROR(\"Bad SST address string. SST canceled.\");\n    return WSREP_CB_FAILURE;\n  }\n\n  char uuid_str[37];\n  wsrep_uuid_print (&current_gtid->uuid, uuid_str, sizeof(uuid_str));\n\n  /* This will be reset when sync callback is called.\n   * Should we set wsrep_ready to FALSE here too? */\n  local_status.set(WSREP_MEMBER_DONOR);\n\n  wsp::env env(NULL);\n  if (env.error())\n  {\n    WSREP_ERROR(\"wsrep_sst_donate_cb(): env var ctor failed: %d\", -env.error());", "project": "mysql-wsrep", "hash": 250096512772440836164701665139796929193, "size": 57, "commit_id": "4ea4b0c6a318209ac09b15aaa906c7b4a13b988c", "message": "codership/mysql-wsrep-bugs#758 Donor uses invalid SST methods", "target": 0, "dataset": "other", "idx": 454781}
{"func": "\t\t\tx = 0;\n\n\t\t\tcwords = br->consumed_words;\n\t\t\twords = br->words;\n\t\t\tucbits = FLAC__BITS_PER_WORD - br->consumed_bits;\n\t\t\tb = br->buffer[cwords] << br->consumed_bits;\n\t\t} while(cwords >= words && val < end);\n\t}\n\n\tif(ucbits == 0 && cwords < words) {\n\t\t/* don't leave the head word with no unconsumed bits */", "project": "flac", "hash": 173788847374942133534558930292688196794, "size": 138, "commit_id": "2e7931c27eb15e387da440a37f12437e35b22dd4", "message": "libFLAC/bitreader.c: Fix out-of-bounds read\n\nCredit: Oss-Fuzz\nIssue: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=17069\nTestcase: fuzzer_decoder-5670265022840832", "target": 1, "dataset": "other", "idx": 212139}
{"func": "\t\t\tx = 0;\n\n\t\t\tcwords = br->consumed_words;\n\t\t\twords = br->words;\n\t\t\tucbits = FLAC__BITS_PER_WORD - br->consumed_bits;\n\t\t\tb = cwords < br->capacity ? br->buffer[cwords] << br->consumed_bits : 0;\n\t\t} while(cwords >= words && val < end);\n\t}\n\n\tif(ucbits == 0 && cwords < words) {\n\t\t/* don't leave the head word with no unconsumed bits */", "project": "flac", "hash": 72624132530208831275896369357681622282, "size": 138, "commit_id": "2e7931c27eb15e387da440a37f12437e35b22dd4", "message": "libFLAC/bitreader.c: Fix out-of-bounds read\n\nCredit: Oss-Fuzz\nIssue: https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=17069\nTestcase: fuzzer_decoder-5670265022840832", "target": 0, "dataset": "other", "idx": 455064}
{"func": "  if (EFI_ERROR (Status)) {\n    //\n    // Old password exist, ask user for the old password\n    //\n    Status = ReadString (MenuOption, gPromptForPassword, StringPtr);\n    if (EFI_ERROR (Status)) {\n      FreePool (StringPtr);\n      return Status;\n    }\n\n    //\n        //\n        PasswordInvalid ();\n      } else {\n        Status = EFI_SUCCESS;\n      }\n\n      FreePool (StringPtr);\n      return Status;\n    }\n  }\n\n  Status = ReadString (MenuOption, gPromptForNewPassword, StringPtr);\n  if (EFI_ERROR (Status)) {\n    //\n    // Reset state machine for password\n    //\n    Question->PasswordCheck (gFormData, Question, NULL);\n    FreePool (StringPtr);\n    return Status;\n  }\n\n  //\n  Status = ReadString (MenuOption, gConfirmPassword, TempString);\n  if (EFI_ERROR (Status)) {\n    //\n    // Reset state machine for password\n    //\n    Question->PasswordCheck (gFormData, Question, NULL);\n    FreePool (StringPtr);\n    FreePool (TempString);\n    return Status;\n  }\n", "project": "edk2", "hash": 165118359344339716147785903103604284716, "size": 129, "commit_id": "f1d78c489a39971b5aac5d2fc8a39bfa925c3c5d", "message": "MdeModulePkg/DisplayEngine: Zero memory before free (CVE-2019-14558)\n\nREF: https://bugzilla.tianocore.org/show_bug.cgi?id=1611\n\nCc: Liming Gao <liming.gao@intel.com>\nCc: Eric Dong <eric.dong@intel.com>\nCc: Jian J Wang <jian.j.wang@intel.com>\nSigned-off-by: Dandan Bi <dandan.bi@intel.com>\nReviewed-by: Eric Dong <eric.dong@intel.com>\nReviewed-by: Jian J Wang <jian.j.wang@intel.com>", "target": 1, "dataset": "other", "idx": 212150}
{"func": "    //\n    // Old password exist, ask user for the old password\n    //\n    Status = ReadString (MenuOption, gPromptForPassword, StringPtr);\n    if (EFI_ERROR (Status)) {\n      ZeroMem (StringPtr, (Maximum + 1) * sizeof (CHAR16));\n      FreePool (StringPtr);\n      return Status;\n    }\n\n    //\n        //\n        PasswordInvalid ();\n      } else {\n        Status = EFI_SUCCESS;\n      }\n      ZeroMem (StringPtr, (Maximum + 1) * sizeof (CHAR16));\n      FreePool (StringPtr);\n      return Status;\n    }\n  }\n\n  if (EFI_ERROR (Status)) {\n    //\n    // Reset state machine for password\n    //\n    Question->PasswordCheck (gFormData, Question, NULL);\n    ZeroMem (StringPtr, (Maximum + 1) * sizeof (CHAR16));\n    FreePool (StringPtr);\n    return Status;\n  }\n\n  //\n  if (EFI_ERROR (Status)) {\n    //\n    // Reset state machine for password\n    //\n    Question->PasswordCheck (gFormData, Question, NULL);\n    ZeroMem (StringPtr, (Maximum + 1) * sizeof (CHAR16));\n    ZeroMem (TempString, (Maximum + 1) * sizeof (CHAR16));\n    FreePool (StringPtr);\n    FreePool (TempString);\n    return Status;\n  }\n", "project": "edk2", "hash": 248666332326382694389399302001879066678, "size": 133, "commit_id": "f1d78c489a39971b5aac5d2fc8a39bfa925c3c5d", "message": "MdeModulePkg/DisplayEngine: Zero memory before free (CVE-2019-14558)\n\nREF: https://bugzilla.tianocore.org/show_bug.cgi?id=1611\n\nCc: Liming Gao <liming.gao@intel.com>\nCc: Eric Dong <eric.dong@intel.com>\nCc: Jian J Wang <jian.j.wang@intel.com>\nSigned-off-by: Dandan Bi <dandan.bi@intel.com>\nReviewed-by: Eric Dong <eric.dong@intel.com>\nReviewed-by: Jian J Wang <jian.j.wang@intel.com>", "target": 0, "dataset": "other", "idx": 455272}
{"func": "    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {", "project": "redis", "hash": 176537741763154451900906839244155850569, "size": 211, "commit_id": "92e3b1802f72ca0c5b0bde97f01d9b57a758d85c", "message": "Fix integer overflow in STRALGO LCS (CVE-2021-29477)\n\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\nthe STRALGO LCS command to corrupt the heap and potentially result with remote\ncode execution.\n\n(cherry picked from commit f0c5f920d0f88bd8aa376a2c05af4902789d1ef9)", "target": 1, "dataset": "other", "idx": 212155}
{"func": "    uint32_t blen = sdslen(b);\n\n    /* Setup an uint32_t array to store at LCS[i,j] the length of the\n     * LCS A0..i-1, B0..j-1. Note that we have a linear array here, so\n     * we index it as LCS[j+(blen+1)*j] */\n    uint32_t *lcs = zmalloc((size_t)(alen+1)*(blen+1)*sizeof(uint32_t));\n    #define LCS(A,B) lcs[(B)+((A)*(blen+1))]\n\n    /* Start building the LCS table. */\n    for (uint32_t i = 0; i <= alen; i++) {\n        for (uint32_t j = 0; j <= blen; j++) {", "project": "redis", "hash": 315646448376140942479893639791006565820, "size": 211, "commit_id": "92e3b1802f72ca0c5b0bde97f01d9b57a758d85c", "message": "Fix integer overflow in STRALGO LCS (CVE-2021-29477)\n\nAn integer overflow bug in Redis version 6.0 or newer could be exploited using\nthe STRALGO LCS command to corrupt the heap and potentially result with remote\ncode execution.\n\n(cherry picked from commit f0c5f920d0f88bd8aa376a2c05af4902789d1ef9)", "target": 0, "dataset": "other", "idx": 455379}
{"func": "      if (exists) {\n\t/* reject unauthenticated response header manipulation, see\n\t * https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html */\n\tif (s->auth.identity->is_anonymous()) {\n\t  return -ERR_INVALID_REQUEST;\n\t}\n\tif (strcmp(p->param, \"response-content-type\") != 0) {\n\t  response_attrs[p->http_attr] = val;\n\t} else {\n\t  content_type_str = val;\n\t  content_type = content_type_str.c_str();", "project": "ceph", "hash": 107958417671493336880373196615590270707, "size": 207, "commit_id": "c7da604cb101cbe78a257a29498a98c69964e0a6", "message": "rgw: reject control characters in response-header actions\n\nS3 GetObject permits overriding response header values, but those inputs\nneed to be validated to insure only characters that are valid in an HTTP\nheader value are present.\n\nCredit: Initial vulnerability discovery by William Bowling (@wcbowling)\nCredit: Further vulnerability discovery by Robin H. Johnson <rjohnson@digitalocean.com>\nSigned-off-by: Robin H. Johnson <rjohnson@digitalocean.com>", "target": 1, "dataset": "other", "idx": 212167}
{"func": "\t/* reject unauthenticated response header manipulation, see\n\t * https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html */\n\tif (s->auth.identity->is_anonymous()) {\n\t  return -ERR_INVALID_REQUEST;\n\t}\n        /* HTTP specification says no control characters should be present in\n         * header values: https://tools.ietf.org/html/rfc7230#section-3.2\n         *      field-vchar    = VCHAR / obs-text\n         *\n         * Failure to validate this permits a CRLF injection in HTTP headers,\n         * whereas S3 GetObject only permits specific headers.\n         */\n        if(str_has_cntrl(val)) {\n          /* TODO: return a more distinct error in future;\n           * stating what the problem is */\n          return -ERR_INVALID_REQUEST;\n        }\n\n\tif (strcmp(p->param, \"response-content-type\") != 0) {\n\t  response_attrs[p->http_attr] = val;\n\t} else {\n\t  content_type_str = val;\n\t  content_type = content_type_str.c_str();", "project": "ceph", "hash": 201752665573529785548111322973661860146, "size": 220, "commit_id": "c7da604cb101cbe78a257a29498a98c69964e0a6", "message": "rgw: reject control characters in response-header actions\n\nS3 GetObject permits overriding response header values, but those inputs\nneed to be validated to insure only characters that are valid in an HTTP\nheader value are present.\n\nCredit: Initial vulnerability discovery by William Bowling (@wcbowling)\nCredit: Further vulnerability discovery by Robin H. Johnson <rjohnson@digitalocean.com>\nSigned-off-by: Robin H. Johnson <rjohnson@digitalocean.com>", "target": 0, "dataset": "other", "idx": 455531}
{"func": "    TIFFGetField(tif, TIFFTAG_TILEWIDTH, &tw);\n    TIFFGetField(tif, TIFFTAG_TILELENGTH, &th);\n\n    flip = setorientation(img);\n    if (flip & FLIP_VERTICALLY) {\n\t    y = h - 1;\n\t    toskew = -(int32)(tw + w);\n    }\n    else {\n\t    y = 0;\n\t    toskew = -(int32)(tw - w);\n    }\n     \n    /*\n     *\tLeftmost tile is clipped on left side if col_offset > 0.\n     */", "project": "libtiff", "hash": 969528565132778716942049224777711470, "size": 105, "commit_id": "c8d613ef497058fe653c467fc84c70a62a4a71b2", "message": "gtTileContig(): check Tile width for overflow\n\nfixes #211", "target": 1, "dataset": "other", "idx": 212214}
{"func": "    TIFFGetField(tif, TIFFTAG_TILEWIDTH, &tw);\n    TIFFGetField(tif, TIFFTAG_TILELENGTH, &th);\n\n    flip = setorientation(img);\n    if (flip & FLIP_VERTICALLY) {\n        if ((tw + w) > INT_MAX) {\n            TIFFErrorExt(tif->tif_clientdata, TIFFFileName(tif), \"%s\", \"unsupported tile size (too wide)\");\n            return (0);\n        }\n        y = h - 1;\n        toskew = -(int32)(tw + w);\n    }\n    else {\n        if (tw > (INT_MAX + w)) {\n            TIFFErrorExt(tif->tif_clientdata, TIFFFileName(tif), \"%s\", \"unsupported tile size (too wide)\");\n            return (0);\n        }\n        y = 0;\n        toskew = -(int32)(tw - w);\n    }\n     \n    /*\n     *\tLeftmost tile is clipped on left side if col_offset > 0.\n     */", "project": "libtiff", "hash": 105544395782509597916020347204964946551, "size": 113, "commit_id": "c8d613ef497058fe653c467fc84c70a62a4a71b2", "message": "gtTileContig(): check Tile width for overflow\n\nfixes #211", "target": 0, "dataset": "other", "idx": 456508}
{"func": "\tstruct io_ring_ctx *ctx = req->ctx;\n\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}", "project": "linux", "hash": 73404555622332263956206411713115350167, "size": 28, "commit_id": "6d816e088c359866f9867057e04f244c608c42fe", "message": "io_uring: hold 'ctx' reference around task_work queue + execute\n\nWe're holding the request reference, but we need to go one higher\nto ensure that the ctx remains valid after the request has finished.\nIf the ring is closed with pending task_work inflight, and the\ngiven io_kiocb finishes sync during issue, then we need a reference\nto the ring itself around the task_work execution cycle.\n\nCc: stable@vger.kernel.org # v5.7+\nReported-by: syzbot+9b260fc33297966f5a8e@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>", "target": 1, "dataset": "other", "idx": 212266}
{"func": "\n\ttrace_io_uring_task_run(req->ctx, req->opcode, req->user_data);\n\n\tif (io_poll_rewait(req, &apoll->poll)) {\n\t\tspin_unlock_irq(&ctx->completion_lock);\n\t\tpercpu_ref_put(&ctx->refs);\n\t\treturn;\n\t}\n\n\t/* If req is still hashed, it cannot have been canceled. Don't check. */\n\tif (hash_hashed(&req->hash_node))\n\tif (!READ_ONCE(apoll->poll.canceled))\n\t\t__io_req_task_submit(req);\n\telse\n\t\t__io_req_task_cancel(req, -ECANCELED);\n\n\tpercpu_ref_put(&ctx->refs);\n\tkfree(apoll->double_poll);\n\tkfree(apoll);\n}", "project": "linux", "hash": 269241368234990870928581246061985683801, "size": 30, "commit_id": "6d816e088c359866f9867057e04f244c608c42fe", "message": "io_uring: hold 'ctx' reference around task_work queue + execute\n\nWe're holding the request reference, but we need to go one higher\nto ensure that the ctx remains valid after the request has finished.\nIf the ring is closed with pending task_work inflight, and the\ngiven io_kiocb finishes sync during issue, then we need a reference\nto the ring itself around the task_work execution cycle.\n\nCc: stable@vger.kernel.org # v5.7+\nReported-by: syzbot+9b260fc33297966f5a8e@syzkaller.appspotmail.com\nSigned-off-by: Jens Axboe <axboe@kernel.dk>", "target": 0, "dataset": "other", "idx": 456954}
{"func": "\t  \tif ((ms->flags & (MAGIC_MIME|MAGIC_APPLE)) == 0 &&\n\t\t\t    file_printf(ms, m->desc, offset) == -1)\n\t\t\treturn -1;\n\t\t\tif (file_printf(ms, \"%s\", rbuf) == -1)\n\t\t\t\treturn -1;\n\t\t\tfree(rbuf);\n\t\t}\n\t\treturn rv;\n\n\tcase FILE_USE:\n\t\tif (nbytes < offset)", "project": "php-src", "hash": 20949309710087477528406375704484824069, "size": 636, "commit_id": "74555e7c26b2c61bb8e67b7d6a6f4d2b8eb3a5f3", "message": "Fixed bug #64830 mimetype detection segfaults on mp3 file", "target": 1, "dataset": "other", "idx": 212275}
{"func": "\t  \tif ((ms->flags & (MAGIC_MIME|MAGIC_APPLE)) == 0 &&\n\t\t\t    file_printf(ms, m->desc, offset) == -1)\n\t\t\treturn -1;\n\t\t\tif (file_printf(ms, \"%s\", rbuf) == -1)\n\t\t\t\treturn -1;\n\t\t\tefree(rbuf);\n\t\t}\n\t\treturn rv;\n\n\tcase FILE_USE:\n\t\tif (nbytes < offset)", "project": "php-src", "hash": 244766288179399097818062065157958913189, "size": 636, "commit_id": "74555e7c26b2c61bb8e67b7d6a6f4d2b8eb3a5f3", "message": "Fixed bug #64830 mimetype detection segfaults on mp3 file", "target": 0, "dataset": "other", "idx": 456966}
{"func": "                /* unknown chunk */\n                /* printf(\"unknown FLI/FLC chunk: %d\\n\", I16(ptr+4)); */\n                state->errcode = IMAGING_CODEC_UNKNOWN;\n                return -1;\n        }\n        advance = I32(ptr);\n        if (advance < 0 || advance > bytes) {\n            state->errcode = IMAGING_CODEC_OVERRUN;\n            return -1;\n        }\n        ptr += advance;", "project": "Pillow", "hash": 69735535805743995070076240386771278068, "size": 226, "commit_id": "bb6c11fb889e6c11b0ee122b828132ee763b5856", "message": "Fix FLI DOS -- CVE-2021-28676\n\n* FliDecode did not properly check that the block advance was\n  non-zero, potentally leading to an infinite loop on load.\n* This dates to the PIL Fork\n* Found with oss-fuzz", "target": 1, "dataset": "other", "idx": 212350}
{"func": "                /* printf(\"unknown FLI/FLC chunk: %d\\n\", I16(ptr+4)); */\n                state->errcode = IMAGING_CODEC_UNKNOWN;\n                return -1;\n        }\n        advance = I32(ptr);\n        if (advance == 0 ) {\n            // If there's no advance, we're in in infinite loop\n            state->errcode = IMAGING_CODEC_BROKEN;\n            return -1;\n        }\n        if (advance < 0 || advance > bytes) {\n            state->errcode = IMAGING_CODEC_OVERRUN;\n            return -1;\n        }\n        ptr += advance;", "project": "Pillow", "hash": 182488442401888661255544724253647090404, "size": 231, "commit_id": "bb6c11fb889e6c11b0ee122b828132ee763b5856", "message": "Fix FLI DOS -- CVE-2021-28676\n\n* FliDecode did not properly check that the block advance was\n  non-zero, potentally leading to an infinite loop on load.\n* This dates to the PIL Fork\n* Found with oss-fuzz", "target": 0, "dataset": "other", "idx": 458011}
{"func": "\t\t}\n\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tif (v.v_vlin)\n\t\t\t\tvc_cons[i].d->vc_scan_lines = v.v_vlin;\n\t\t\tif (v.v_clin)\n\t\t\t\tvc_cons[i].d->vc_font.height = v.v_clin;\n\t\t\tvc_cons[i].d->vc_resize_user = 1;\n\t\t\tvc_resize(vc_cons[i].d, v.v_cols, v.v_rows);\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n", "project": "linux", "hash": 215976747485693149279929143438303006406, "size": 696, "commit_id": "6cd1ed50efd88261298577cd92a14f2768eddeeb", "message": "vt: vt_ioctl: fix race in VT_RESIZEX\n\nWe need to make sure vc_cons[i].d is not NULL after grabbing\nconsole_lock(), or risk a crash.\n\ngeneral protection fault, probably for non-canonical address 0xdffffc0000000068: 0000 [#1] PREEMPT SMP KASAN\nKASAN: null-ptr-deref in range [0x0000000000000340-0x0000000000000347]\nCPU: 1 PID: 19462 Comm: syz-executor.5 Not tainted 5.5.0-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nRIP: 0010:vt_ioctl+0x1f96/0x26d0 drivers/tty/vt/vt_ioctl.c:883\nCode: 74 41 e8 bd a6 84 fd 48 89 d8 48 c1 e8 03 42 80 3c 28 00 0f 85 e4 04 00 00 48 8b 03 48 8d b8 40 03 00 00 48 89 fa 48 c1 ea 03 <42> 0f b6 14 2a 84 d2 74 09 80 fa 03 0f 8e b1 05 00 00 44 89 b8 40\nRSP: 0018:ffffc900086d7bb0 EFLAGS: 00010202\nRAX: 0000000000000000 RBX: ffffffff8c34ee88 RCX: ffffc9001415c000\nRDX: 0000000000000068 RSI: ffffffff83f0e6e3 RDI: 0000000000000340\nRBP: ffffc900086d7cd0 R08: ffff888054ce0100 R09: fffffbfff16a2f6d\nR10: ffff888054ce0998 R11: ffff888054ce0100 R12: 000000000000001d\nR13: dffffc0000000000 R14: 1ffff920010daf79 R15: 000000000000ff7f\nFS:  00007f7d13c12700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007ffd477e3c38 CR3: 0000000095d0a000 CR4: 00000000001406e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n tty_ioctl+0xa37/0x14f0 drivers/tty/tty_io.c:2660\n vfs_ioctl fs/ioctl.c:47 [inline]\n ksys_ioctl+0x123/0x180 fs/ioctl.c:763\n __do_sys_ioctl fs/ioctl.c:772 [inline]\n __se_sys_ioctl fs/ioctl.c:770 [inline]\n __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:770\n do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\nRIP: 0033:0x45b399\nCode: ad b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007f7d13c11c78 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\nRAX: ffffffffffffffda RBX: 00007f7d13c126d4 RCX: 000000000045b399\nRDX: 0000000020000080 RSI: 000000000000560a RDI: 0000000000000003\nRBP: 000000000075bf20 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 00000000ffffffff\nR13: 0000000000000666 R14: 00000000004c7f04 R15: 000000000075bf2c\nModules linked in:\n---[ end trace 80970faf7a67eb77 ]---\nRIP: 0010:vt_ioctl+0x1f96/0x26d0 drivers/tty/vt/vt_ioctl.c:883\nCode: 74 41 e8 bd a6 84 fd 48 89 d8 48 c1 e8 03 42 80 3c 28 00 0f 85 e4 04 00 00 48 8b 03 48 8d b8 40 03 00 00 48 89 fa 48 c1 ea 03 <42> 0f b6 14 2a 84 d2 74 09 80 fa 03 0f 8e b1 05 00 00 44 89 b8 40\nRSP: 0018:ffffc900086d7bb0 EFLAGS: 00010202\nRAX: 0000000000000000 RBX: ffffffff8c34ee88 RCX: ffffc9001415c000\nRDX: 0000000000000068 RSI: ffffffff83f0e6e3 RDI: 0000000000000340\nRBP: ffffc900086d7cd0 R08: ffff888054ce0100 R09: fffffbfff16a2f6d\nR10: ffff888054ce0998 R11: ffff888054ce0100 R12: 000000000000001d\nR13: dffffc0000000000 R14: 1ffff920010daf79 R15: 000000000000ff7f\nFS:  00007f7d13c12700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007ffd477e3c38 CR3: 0000000095d0a000 CR4: 00000000001406e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n\nFixes: 1da177e4c3f4 (\"Linux-2.6.12-rc2\")\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: stable <stable@vger.kernel.org>\nReported-by: syzbot <syzkaller@googlegroups.com>\nLink: https://lore.kernel.org/r/20200210190721.200418-1-edumazet@google.com\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 1, "dataset": "other", "idx": 212365}
{"func": "\n\t\tif (v.v_clin > 32)\n\t\t\treturn -EINVAL;\n\n\t\tfor (i = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\tstruct vc_data *vcp;\n\n\t\t\tif (!vc_cons[i].d)\n\t\t\t\tcontinue;\n\t\t\tconsole_lock();\n\t\t\tvcp = vc_cons[i].d;\n\t\t\tif (vcp) {\n\t\t\t\tif (v.v_vlin)\n\t\t\t\t\tvcp->vc_scan_lines = v.v_vlin;\n\t\t\t\tif (v.v_clin)\n\t\t\t\t\tvcp->vc_font.height = v.v_clin;\n\t\t\t\tvcp->vc_resize_user = 1;\n\t\t\t\tvc_resize(vcp, v.v_cols, v.v_rows);\n\t\t\t}\n\t\t\tconsole_unlock();\n\t\t}\n\t\tbreak;\n\t}\n", "project": "linux", "hash": 188901637298738474521041844492795398881, "size": 701, "commit_id": "6cd1ed50efd88261298577cd92a14f2768eddeeb", "message": "vt: vt_ioctl: fix race in VT_RESIZEX\n\nWe need to make sure vc_cons[i].d is not NULL after grabbing\nconsole_lock(), or risk a crash.\n\ngeneral protection fault, probably for non-canonical address 0xdffffc0000000068: 0000 [#1] PREEMPT SMP KASAN\nKASAN: null-ptr-deref in range [0x0000000000000340-0x0000000000000347]\nCPU: 1 PID: 19462 Comm: syz-executor.5 Not tainted 5.5.0-syzkaller #0\nHardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011\nRIP: 0010:vt_ioctl+0x1f96/0x26d0 drivers/tty/vt/vt_ioctl.c:883\nCode: 74 41 e8 bd a6 84 fd 48 89 d8 48 c1 e8 03 42 80 3c 28 00 0f 85 e4 04 00 00 48 8b 03 48 8d b8 40 03 00 00 48 89 fa 48 c1 ea 03 <42> 0f b6 14 2a 84 d2 74 09 80 fa 03 0f 8e b1 05 00 00 44 89 b8 40\nRSP: 0018:ffffc900086d7bb0 EFLAGS: 00010202\nRAX: 0000000000000000 RBX: ffffffff8c34ee88 RCX: ffffc9001415c000\nRDX: 0000000000000068 RSI: ffffffff83f0e6e3 RDI: 0000000000000340\nRBP: ffffc900086d7cd0 R08: ffff888054ce0100 R09: fffffbfff16a2f6d\nR10: ffff888054ce0998 R11: ffff888054ce0100 R12: 000000000000001d\nR13: dffffc0000000000 R14: 1ffff920010daf79 R15: 000000000000ff7f\nFS:  00007f7d13c12700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007ffd477e3c38 CR3: 0000000095d0a000 CR4: 00000000001406e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\nCall Trace:\n tty_ioctl+0xa37/0x14f0 drivers/tty/tty_io.c:2660\n vfs_ioctl fs/ioctl.c:47 [inline]\n ksys_ioctl+0x123/0x180 fs/ioctl.c:763\n __do_sys_ioctl fs/ioctl.c:772 [inline]\n __se_sys_ioctl fs/ioctl.c:770 [inline]\n __x64_sys_ioctl+0x73/0xb0 fs/ioctl.c:770\n do_syscall_64+0xfa/0x790 arch/x86/entry/common.c:294\n entry_SYSCALL_64_after_hwframe+0x49/0xbe\nRIP: 0033:0x45b399\nCode: ad b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00 00 66 90 48 89 f8 48 89 f7 48 89 d6 48 89 ca 4d 89 c2 4d 89 c8 4c 8b 4c 24 08 0f 05 <48> 3d 01 f0 ff ff 0f 83 7b b6 fb ff c3 66 2e 0f 1f 84 00 00 00 00\nRSP: 002b:00007f7d13c11c78 EFLAGS: 00000246 ORIG_RAX: 0000000000000010\nRAX: ffffffffffffffda RBX: 00007f7d13c126d4 RCX: 000000000045b399\nRDX: 0000000020000080 RSI: 000000000000560a RDI: 0000000000000003\nRBP: 000000000075bf20 R08: 0000000000000000 R09: 0000000000000000\nR10: 0000000000000000 R11: 0000000000000246 R12: 00000000ffffffff\nR13: 0000000000000666 R14: 00000000004c7f04 R15: 000000000075bf2c\nModules linked in:\n---[ end trace 80970faf7a67eb77 ]---\nRIP: 0010:vt_ioctl+0x1f96/0x26d0 drivers/tty/vt/vt_ioctl.c:883\nCode: 74 41 e8 bd a6 84 fd 48 89 d8 48 c1 e8 03 42 80 3c 28 00 0f 85 e4 04 00 00 48 8b 03 48 8d b8 40 03 00 00 48 89 fa 48 c1 ea 03 <42> 0f b6 14 2a 84 d2 74 09 80 fa 03 0f 8e b1 05 00 00 44 89 b8 40\nRSP: 0018:ffffc900086d7bb0 EFLAGS: 00010202\nRAX: 0000000000000000 RBX: ffffffff8c34ee88 RCX: ffffc9001415c000\nRDX: 0000000000000068 RSI: ffffffff83f0e6e3 RDI: 0000000000000340\nRBP: ffffc900086d7cd0 R08: ffff888054ce0100 R09: fffffbfff16a2f6d\nR10: ffff888054ce0998 R11: ffff888054ce0100 R12: 000000000000001d\nR13: dffffc0000000000 R14: 1ffff920010daf79 R15: 000000000000ff7f\nFS:  00007f7d13c12700(0000) GS:ffff8880ae900000(0000) knlGS:0000000000000000\nCS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033\nCR2: 00007ffd477e3c38 CR3: 0000000095d0a000 CR4: 00000000001406e0\nDR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000\nDR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400\n\nFixes: 1da177e4c3f4 (\"Linux-2.6.12-rc2\")\nSigned-off-by: Eric Dumazet <edumazet@google.com>\nCc: stable <stable@vger.kernel.org>\nReported-by: syzbot <syzkaller@googlegroups.com>\nLink: https://lore.kernel.org/r/20200210190721.200418-1-edumazet@google.com\nSigned-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>", "target": 0, "dataset": "other", "idx": 458189}
{"func": "    }\n    unsigned char ch = static_cast<unsigned char>((b[0] << 4) + b[1]);\n\n    QTC::TC(\"libtests\", \"Pl_ASCIIHexDecoder partial flush\",\n\t    (this->pos == 2) ? 0 : 1);\n    getNext()->write(&ch, 1);\n\n    this->pos = 0;\n    this->inbuf[0] = '0';\n    this->inbuf[1] = '0';\n    this->inbuf[2] = '\\0';\n}", "project": "qpdf", "hash": 203118129371621348619685394824494279281, "size": 30, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 1, "dataset": "other", "idx": 212373}
{"func": "    }\n    unsigned char ch = static_cast<unsigned char>((b[0] << 4) + b[1]);\n\n    QTC::TC(\"libtests\", \"Pl_ASCIIHexDecoder partial flush\",\n\t    (this->pos == 2) ? 0 : 1);\n    // Reset before calling getNext()->write in case that throws an\n    // exception.\n    this->pos = 0;\n    this->inbuf[0] = '0';\n    this->inbuf[1] = '0';\n    this->inbuf[2] = '\\0';\n\n    getNext()->write(&ch, 1);\n}", "project": "qpdf", "hash": 210240176858994773313725168075493405218, "size": 32, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 0, "dataset": "other", "idx": 458316}
{"func": "\t    if (strip)\n\t    {\n\t\tbytes -= last;\n\t    }\n\t}\n    }\n    getNext()->write(this->outbuf, bytes);\n    this->offset = 0;\n}", "project": "qpdf", "hash": 210723206961795046431763152490099955651, "size": 76, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 1, "dataset": "other", "idx": 212374}
{"func": "\t    {\n\t\tbytes -= last;\n\t    }\n\t}\n    }\n    this->offset = 0;\n    getNext()->write(this->outbuf, bytes);\n}", "project": "qpdf", "hash": 322713675875595696904824155170504505595, "size": 76, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 0, "dataset": "other", "idx": 458323}
{"func": "\tlval >>= 8;\n    }\n\n    QTC::TC(\"libtests\", \"Pl_ASCII85Decoder partial flush\",\n\t    (this->pos == 5) ? 0 : 1);\n    getNext()->write(outbuf, this->pos - 1);\n\n    this->pos = 0;\n    memset(this->inbuf, 117, 5);\n}", "project": "qpdf", "hash": 85528360671820919508396699545533821192, "size": 29, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 1, "dataset": "other", "idx": 212375}
{"func": "\tlval >>= 8;\n    }\n\n    QTC::TC(\"libtests\", \"Pl_ASCII85Decoder partial flush\",\n\t    (this->pos == 5) ? 0 : 1);\n    // Reset before calling getNext()->write in case that throws an\n    // exception.\n    auto t = this->pos - 1;\n    this->pos = 0;\n    memset(this->inbuf, 117, 5);\n\n    getNext()->write(outbuf, t);\n}", "project": "qpdf", "hash": 295062865177124626965605267616423090069, "size": 32, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 0, "dataset": "other", "idx": 458318}
{"func": "Pl_Count::write(unsigned char* buf, size_t len)\n{\n    if (len)\n    {\n\tthis->m->count += QIntC::to_offset(len);\n\tgetNext()->write(buf, len);\n\tthis->m->last_char = buf[len - 1];\n    }\n}", "project": "qpdf", "hash": 175837226178792908914233242113513690803, "size": 9, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 1, "dataset": "other", "idx": 212376}
{"func": "Pl_Count::write(unsigned char* buf, size_t len)\n{\n    if (len)\n    {\n\tthis->m->count += QIntC::to_offset(len);\n\tthis->m->last_char = buf[len - 1];\n\tgetNext()->write(buf, len);\n    }\n}", "project": "qpdf", "hash": 17982642016827745233059699776008762845, "size": 9, "commit_id": "dc92574c10f3e2516ec6445b88c5d584f40df4e5", "message": "Fix some pipelines to be safe if downstream write fails (fuzz issue 28262)", "target": 0, "dataset": "other", "idx": 458315}
{"func": "\t\t\tcode--;\n\n\t\tif (field->application == HID_GD_SYSTEM_MULTIAXIS)\n\t\t\tcode = BTN_0  + ((usage->hid - 1) & HID_USAGE);\n\n\t\thid_map_usage(hi, usage, bit, max, EV_KEY, code);\n\t\tinput_set_capability(hi->input, EV_KEY, code);\n\t\treturn 1;\n\n\tcase 0xff000000:\n\t\t/* we do not want to map these: no input-oriented meaning */", "project": "linux", "hash": 48282048289716496461900266037257497045, "size": 187, "commit_id": "35556bed836f8dc07ac55f69c8d17dce3e7f0e25", "message": "HID: core: Sanitize event code and type when mapping input\n\nWhen calling into hid_map_usage(), the passed event code is\nblindly stored as is, even if it doesn't fit in the associated bitmap.\n\nThis event code can come from a variety of sources, including devices\nmasquerading as input devices, only a bit more \"programmable\".\n\nInstead of taking the event code at face value, check that it actually\nfits the corresponding bitmap, and if it doesn't:\n- spit out a warning so that we know which device is acting up\n- NULLify the bitmap pointer so that we catch unexpected uses\n\nCode paths that can make use of untrusted inputs can now check\nthat the mapping was indeed correct and bail out if not.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Marc Zyngier <maz@kernel.org>\nSigned-off-by: Benjamin Tissoires <benjamin.tissoires@gmail.com>", "target": 1, "dataset": "other", "idx": 212377}
{"func": "\n\t\tif (field->application == HID_GD_SYSTEM_MULTIAXIS)\n\t\t\tcode = BTN_0  + ((usage->hid - 1) & HID_USAGE);\n\n\t\thid_map_usage(hi, usage, bit, max, EV_KEY, code);\n\t\tif (!*bit)\n\t\t\treturn -1;\n\t\tinput_set_capability(hi->input, EV_KEY, code);\n\t\treturn 1;\n\n\tcase 0xff000000:\n\t\t/* we do not want to map these: no input-oriented meaning */", "project": "linux", "hash": 237709689285808653462224634864316547202, "size": 189, "commit_id": "35556bed836f8dc07ac55f69c8d17dce3e7f0e25", "message": "HID: core: Sanitize event code and type when mapping input\n\nWhen calling into hid_map_usage(), the passed event code is\nblindly stored as is, even if it doesn't fit in the associated bitmap.\n\nThis event code can come from a variety of sources, including devices\nmasquerading as input devices, only a bit more \"programmable\".\n\nInstead of taking the event code at face value, check that it actually\nfits the corresponding bitmap, and if it doesn't:\n- spit out a warning so that we know which device is acting up\n- NULLify the bitmap pointer so that we catch unexpected uses\n\nCode paths that can make use of untrusted inputs can now check\nthat the mapping was indeed correct and bail out if not.\n\nCc: stable@vger.kernel.org\nSigned-off-by: Marc Zyngier <maz@kernel.org>\nSigned-off-by: Benjamin Tissoires <benjamin.tissoires@gmail.com>", "target": 0, "dataset": "other", "idx": 458382}
{"func": "    goto leave;\n\n  if (DBG_CIPHER)\n    log_printpnt (\"ecc_decrypt    kG\", &kG, NULL);\n\n  if (!(flags & PUBKEY_FLAG_DJB_TWEAK)\n      /* For X25519, by its definition, validation should not be done.  */\n      && !_gcry_mpi_ec_curve_point (&kG, ec))\n    {\n      rc = GPG_ERR_INV_DATA;\n      goto leave;\n    }\n", "project": "libgcrypt", "hash": 125664893693859396705351501621878215385, "size": 230, "commit_id": "bf76acbf0da6b0f245e491bec12c0f0a1b5be7c9", "message": "ecc: Add input validation for X25519.\n\n* cipher/ecc.c (ecc_decrypt_raw): Add input validation.\n* mpi/ec.c (ec_p_init): Use scratch buffer for bad points.\n(_gcry_mpi_ec_bad_point): New.\n\n--\n\nFollowing is the paper describing the attack:\n\n    May the Fourth Be With You: A Microarchitectural Side Channel Attack\n    on Real-World Applications of Curve25519\n    by Daniel Genkin, Luke Valenta, and Yuval Yarom\n\nIn the current implementation, we do output checking and it results an\nerror for those bad points.  However, when attacked, the computation\nwill done with leak of private key, even it will results errors.  To\nmitigate leak, we added input validation.\n\nNote that we only list bad points with MSB=0.  By X25519, MSB is\nalways cleared.\n\nIn future, we should implement constant-time field computation.  Then,\nthis input validation could be removed, if performance is important\nand we are sure for no leak.\n\nCVE-id: CVE-2017-0379\nSigned-off-by: NIIBE Yutaka <gniibe@fsij.org>", "target": 1, "dataset": "other", "idx": 212389}
{"func": "    goto leave;\n\n  if (DBG_CIPHER)\n    log_printpnt (\"ecc_decrypt    kG\", &kG, NULL);\n\n  if ((flags & PUBKEY_FLAG_DJB_TWEAK))\n    {\n      /* For X25519, by its definition, validation should not be done.  */\n      /* (Instead, we do output check.)\n       *\n       * However, to mitigate secret key leak from our implementation,\n       * we also do input validation here.  For constant-time\n       * implementation, we can remove this input validation.\n       */\n      if (_gcry_mpi_ec_bad_point (&kG, ec))\n        {\n          rc = GPG_ERR_INV_DATA;\n          goto leave;\n        }\n    }\n  else if (!_gcry_mpi_ec_curve_point (&kG, ec))\n    {\n      rc = GPG_ERR_INV_DATA;\n      goto leave;\n    }\n", "project": "libgcrypt", "hash": 221349693093505639736378931380558430882, "size": 243, "commit_id": "bf76acbf0da6b0f245e491bec12c0f0a1b5be7c9", "message": "ecc: Add input validation for X25519.\n\n* cipher/ecc.c (ecc_decrypt_raw): Add input validation.\n* mpi/ec.c (ec_p_init): Use scratch buffer for bad points.\n(_gcry_mpi_ec_bad_point): New.\n\n--\n\nFollowing is the paper describing the attack:\n\n    May the Fourth Be With You: A Microarchitectural Side Channel Attack\n    on Real-World Applications of Curve25519\n    by Daniel Genkin, Luke Valenta, and Yuval Yarom\n\nIn the current implementation, we do output checking and it results an\nerror for those bad points.  However, when attacked, the computation\nwill done with leak of private key, even it will results errors.  To\nmitigate leak, we added input validation.\n\nNote that we only list bad points with MSB=0.  By X25519, MSB is\nalways cleared.\n\nIn future, we should implement constant-time field computation.  Then,\nthis input validation could be removed, if performance is important\nand we are sure for no leak.\n\nCVE-id: CVE-2017-0379\nSigned-off-by: NIIBE Yutaka <gniibe@fsij.org>", "target": 0, "dataset": "other", "idx": 458698}
{"func": "      decode_options->ignore_transformations=1;\n    }\n  else\n    (void) SetImageProperty(image,\"exif:Orientation\",\"1\",exception);\n  error=heif_decode_image(image_handle,&heif_image,heif_colorspace_YCbCr,\n    heif_chroma_420,decode_options);\n  if (IsHeifSuccess(&error,image,exception) == MagickFalse)\n    {\n      heif_image_handle_release(image_handle);\n      return(MagickFalse);\n    }\n  if (decode_options != (struct heif_decoding_options *) NULL)\n    {\n      /*\n        Correct the width and height of the image.\n      */\n      image->columns=(size_t) heif_image_get_width(heif_image,heif_channel_Y);\n      image->rows=(size_t) heif_image_get_height(heif_image,heif_channel_Y);\n      status=SetImageExtent(image,image->columns,image->rows,exception);\n      heif_decoding_options_free(decode_options);\n      if (status == MagickFalse)\n        {\n          heif_image_release(heif_image);\n          heif_image_handle_release(image_handle);\n          return(MagickFalse);\n        }\n    }\n  p_y=heif_image_get_plane_readonly(heif_image,heif_channel_Y,&stride_y);\n  p_cb=heif_image_get_plane_readonly(heif_image,heif_channel_Cb,&stride_cb);\n  p_cr=heif_image_get_plane_readonly(heif_image,heif_channel_Cr,&stride_cr);\n  for (y=0; y < (ssize_t) image->rows; y++)", "project": "ImageMagick", "hash": 154557627643794820744368940487947565708, "size": 138, "commit_id": "868aad754ee599eb7153b84d610f2ecdf7b339f6", "message": "Always correct the width and height of the image (#1859).", "target": 1, "dataset": "other", "idx": 212425}
{"func": "    }\n  else\n    (void) SetImageProperty(image,\"exif:Orientation\",\"1\",exception);\n  error=heif_decode_image(image_handle,&heif_image,heif_colorspace_YCbCr,\n    heif_chroma_420,decode_options);\n  if (decode_options != (struct heif_decoding_options *) NULL)\n    heif_decoding_options_free(decode_options);\n  if (IsHeifSuccess(&error,image,exception) == MagickFalse)\n    {\n      heif_image_handle_release(image_handle);\n      return(MagickFalse);\n    }\n  /*\n    Correct the width and height of the image.\n  */\n  image->columns=(size_t) heif_image_get_width(heif_image,heif_channel_Y);\n  image->rows=(size_t) heif_image_get_height(heif_image,heif_channel_Y);\n  status=SetImageExtent(image,image->columns,image->rows,exception);\n  if (status == MagickFalse)\n    {\n      heif_image_release(heif_image);\n      heif_image_handle_release(image_handle);\n      return(MagickFalse);\n    }\n  p_y=heif_image_get_plane_readonly(heif_image,heif_channel_Y,&stride_y);\n  p_cb=heif_image_get_plane_readonly(heif_image,heif_channel_Cb,&stride_cb);\n  p_cr=heif_image_get_plane_readonly(heif_image,heif_channel_Cr,&stride_cr);\n  for (y=0; y < (ssize_t) image->rows; y++)", "project": "ImageMagick", "hash": 152426799834776983096599814137214229220, "size": 136, "commit_id": "868aad754ee599eb7153b84d610f2ecdf7b339f6", "message": "Always correct the width and height of the image (#1859).", "target": 0, "dataset": "other", "idx": 459319}
{"func": "    status=ReadProfile(image,\"8bim\",profile,(ssize_t) length,exception);\n#endif\n#if defined(TIFFTAG_RICHTIFFIPTC)\n  if ((TIFFGetField(tiff,TIFFTAG_RICHTIFFIPTC,&length,&profile) == 1) &&\n      (profile != (unsigned char *) NULL))\n    {\n      if (TIFFIsByteSwapped(tiff) != 0)\n        TIFFSwabArrayOfLong((uint32 *) profile,(size_t) length);\n      status=ReadProfile(image,\"iptc\",profile,4L*length,exception);\n    }\n#endif\n#if defined(TIFFTAG_XMLPACKET)\n  if ((TIFFGetField(tiff,TIFFTAG_XMLPACKET,&length,&profile) == 1) &&\n      (profile != (unsigned char *) NULL))", "project": "ImageMagick", "hash": 80273906805248286578189872570147097025, "size": 61, "commit_id": "824f344ceb823e156ad6e85314d79c087933c2a0", "message": "Check the type of the field before performing the multiplication (details in #2132)", "target": 1, "dataset": "other", "idx": 212435}
{"func": "#endif\n#if defined(TIFFTAG_RICHTIFFIPTC)\n  if ((TIFFGetField(tiff,TIFFTAG_RICHTIFFIPTC,&length,&profile) == 1) &&\n      (profile != (unsigned char *) NULL))\n    {\n      const TIFFField\n        *field;\n\n      if (TIFFIsByteSwapped(tiff) != 0)\n        TIFFSwabArrayOfLong((uint32 *) profile,(size_t) length);\n      field=TIFFFieldWithTag(tiff,TIFFTAG_RICHTIFFIPTC);\n      if (TIFFFieldDataType(field) == TIFF_LONG)\n        status=ReadProfile(image,\"iptc\",profile,4L*length,exception);\n      else\n        status=ReadProfile(image,\"iptc\",profile,length,exception);\n    }\n#endif\n#if defined(TIFFTAG_XMLPACKET)\n  if ((TIFFGetField(tiff,TIFFTAG_XMLPACKET,&length,&profile) == 1) &&\n      (profile != (unsigned char *) NULL))", "project": "ImageMagick", "hash": 13944005594410488667034111483743253825, "size": 68, "commit_id": "824f344ceb823e156ad6e85314d79c087933c2a0", "message": "Check the type of the field before performing the multiplication (details in #2132)", "target": 0, "dataset": "other", "idx": 459504}
{"func": "#endif\n#ifdef UNIX\n\t/* Set swap file protection bits after creating it. */\n\tif (swap_mode > 0 && curbuf->b_ml.ml_mfp != NULL\n\t\t\t  && curbuf->b_ml.ml_mfp->mf_fname != NULL)\n\t    (void)mch_setperm(curbuf->b_ml.ml_mfp->mf_fname, (long)swap_mode);\n#endif\n    }\n\n#if defined(HAS_SWAP_EXISTS_ACTION)\n    /* If \"Quit\" selected at ATTENTION dialog, don't load the file */", "project": "vim", "hash": 249564375416889539424239000884906838896, "size": 2516, "commit_id": "5a73e0ca54c77e067c3b12ea6f35e3e8681e8cf8", "message": "patch 8.0.1263: others can read the swap file if a user is careless\n\nProblem:    Others can read the swap file if a user is careless with his\n            primary group.\nSolution:   If the group permission allows for reading but the world\n            permissions doesn't, make sure the group is right.", "target": 1, "dataset": "other", "idx": 212695}
{"func": "#endif\n#ifdef UNIX\n\t/* Set swap file protection bits after creating it. */\n\tif (swap_mode > 0 && curbuf->b_ml.ml_mfp != NULL\n\t\t\t  && curbuf->b_ml.ml_mfp->mf_fname != NULL)\n\t{\n\t    char_u *swap_fname = curbuf->b_ml.ml_mfp->mf_fname;\n\n\t    /*\n\t     * If the group-read bit is set but not the world-read bit, then\n\t     * the group must be equal to the group of the original file.  If\n\t     * we can't make that happen then reset the group-read bit.  This\n\t     * avoids making the swap file readable to more users when the\n\t     * primary group of the user is too permissive.\n\t     */\n\t    if ((swap_mode & 044) == 040)\n\t    {\n\t\tstat_T\tswap_st;\n\n\t\tif (mch_stat((char *)swap_fname, &swap_st) >= 0\n\t\t\t&& st.st_gid != swap_st.st_gid\n\t\t\t&& fchown(curbuf->b_ml.ml_mfp->mf_fd, -1, st.st_gid)\n\t\t\t\t\t\t\t\t\t == -1)\n\t\t    swap_mode &= 0600;\n\t    }\n\n\t    (void)mch_setperm(swap_fname, (long)swap_mode);\n\t}\n#endif\n    }\n\n#if defined(HAS_SWAP_EXISTS_ACTION)\n    /* If \"Quit\" selected at ATTENTION dialog, don't load the file */", "project": "vim", "hash": 253046681285963764072266830667800150363, "size": 2538, "commit_id": "5a73e0ca54c77e067c3b12ea6f35e3e8681e8cf8", "message": "patch 8.0.1263: others can read the swap file if a user is careless\n\nProblem:    Others can read the swap file if a user is careless with his\n            primary group.\nSolution:   If the group permission allows for reading but the world\n            permissions doesn't, make sure the group is right.", "target": 0, "dataset": "other", "idx": 460737}
{"func": "\t\t\tpr_err(\"bpf_jit: fatal insn size error\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (image) {\n\t\t\tif (unlikely(proglen + ilen > oldproglen)) {\n\t\t\t\tpr_err(\"bpf_jit: fatal error\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tmemcpy(image + proglen, temp, ilen);\n\t\t}", "project": "linux", "hash": 176161434758084402194530235503098784254, "size": 814, "commit_id": "e4d4d456436bfb2fe412ee2cd489f7658449b098", "message": "bpf, x86: Validate computation of branch displacements for x86-64\n\nThe branch displacement logic in the BPF JIT compilers for x86 assumes\nthat, for any generated branch instruction, the distance cannot\nincrease between optimization passes.\n\nBut this assumption can be violated due to how the distances are\ncomputed. Specifically, whenever a backward branch is processed in\ndo_jit(), the distance is computed by subtracting the positions in the\nmachine code from different optimization passes. This is because part\nof addrs[] is already updated for the current optimization pass, before\nthe branch instruction is visited.\n\nAnd so the optimizer can expand blocks of machine code in some cases.\n\nThis can confuse the optimizer logic, where it assumes that a fixed\npoint has been reached for all machine code blocks once the total\nprogram size stops changing. And then the JIT compiler can output\nabnormal machine code containing incorrect branch displacements.\n\nTo mitigate this issue, we assert that a fixed point is reached while\npopulating the output image. This rejects any problematic programs.\nThe issue affects both x86-32 and x86-64. We mitigate separately to\nease backporting.\n\nSigned-off-by: Piotr Krysiuk <piotras@gmail.com>\nReviewed-by: Daniel Borkmann <daniel@iogearbox.net>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>", "target": 1, "dataset": "other", "idx": 212767}
{"func": "\t\t\tpr_err(\"bpf_jit: fatal insn size error\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (image) {\n\t\t\t/*\n\t\t\t * When populating the image, assert that:\n\t\t\t *\n\t\t\t *  i) We do not write beyond the allocated space, and\n\t\t\t * ii) addrs[i] did not change from the prior run, in order\n\t\t\t *     to validate assumptions made for computing branch\n\t\t\t *     displacements.\n\t\t\t */\n\t\t\tif (unlikely(proglen + ilen > oldproglen ||\n\t\t\t\t     proglen + ilen != addrs[i])) {\n\t\t\t\tpr_err(\"bpf_jit: fatal error\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tmemcpy(image + proglen, temp, ilen);\n\t\t}", "project": "linux", "hash": 188508326208408747512908522230061011678, "size": 823, "commit_id": "e4d4d456436bfb2fe412ee2cd489f7658449b098", "message": "bpf, x86: Validate computation of branch displacements for x86-64\n\nThe branch displacement logic in the BPF JIT compilers for x86 assumes\nthat, for any generated branch instruction, the distance cannot\nincrease between optimization passes.\n\nBut this assumption can be violated due to how the distances are\ncomputed. Specifically, whenever a backward branch is processed in\ndo_jit(), the distance is computed by subtracting the positions in the\nmachine code from different optimization passes. This is because part\nof addrs[] is already updated for the current optimization pass, before\nthe branch instruction is visited.\n\nAnd so the optimizer can expand blocks of machine code in some cases.\n\nThis can confuse the optimizer logic, where it assumes that a fixed\npoint has been reached for all machine code blocks once the total\nprogram size stops changing. And then the JIT compiler can output\nabnormal machine code containing incorrect branch displacements.\n\nTo mitigate this issue, we assert that a fixed point is reached while\npopulating the output image. This rejects any problematic programs.\nThe issue affects both x86-32 and x86-64. We mitigate separately to\nease backporting.\n\nSigned-off-by: Piotr Krysiuk <piotras@gmail.com>\nReviewed-by: Daniel Borkmann <daniel@iogearbox.net>\nSigned-off-by: Daniel Borkmann <daniel@iogearbox.net>", "target": 0, "dataset": "other", "idx": 461582}
{"func": "\tenum nl80211_band band;\n\n\tif (hw) {\n\t\tieee80211_stop_queues(hw);\n\t\tieee80211_unregister_hw(hw);\n\t\tieee80211_free_hw(hw);\n\t}\n\n\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\t\t\t&adapter->sbands[band];", "project": "linux", "hash": 236883142955445427225930806435433027313, "size": 23, "commit_id": "abd39c6ded9db53aa44c2540092bdd5fb6590fa8", "message": "rsi: add fix for crash during assertions\n\nObserved crash in some scenarios when assertion has occurred,\nthis is because hw structure is freed and is tried to get\naccessed in some functions where null check is already\npresent. So, avoided the crash by making the hw to NULL after\nfreeing.\n\nSigned-off-by: Sanjay Konduri <sanjay.konduri@redpinesignals.com>\nSigned-off-by: Sushant Kumar Mishra <sushant.mishra@redpinesignals.com>\nSigned-off-by: Kalle Valo <kvalo@codeaurora.org>", "target": 1, "dataset": "other", "idx": 212771}
{"func": "\n\tif (hw) {\n\t\tieee80211_stop_queues(hw);\n\t\tieee80211_unregister_hw(hw);\n\t\tieee80211_free_hw(hw);\n\t\tadapter->hw = NULL;\n\t}\n\n\tfor (band = 0; band < NUM_NL80211_BANDS; band++) {\n\t\tstruct ieee80211_supported_band *sband =\n\t\t\t\t\t&adapter->sbands[band];", "project": "linux", "hash": 10029051295079005700052093461301549026, "size": 24, "commit_id": "abd39c6ded9db53aa44c2540092bdd5fb6590fa8", "message": "rsi: add fix for crash during assertions\n\nObserved crash in some scenarios when assertion has occurred,\nthis is because hw structure is freed and is tried to get\naccessed in some functions where null check is already\npresent. So, avoided the crash by making the hw to NULL after\nfreeing.\n\nSigned-off-by: Sanjay Konduri <sanjay.konduri@redpinesignals.com>\nSigned-off-by: Sushant Kumar Mishra <sushant.mishra@redpinesignals.com>\nSigned-off-by: Kalle Valo <kvalo@codeaurora.org>", "target": 0, "dataset": "other", "idx": 461627}
{"func": "\t\tintl_error_set( NULL, U_ILLEGAL_ARGUMENT_ERROR,\n\t\t\t\"msgfmt_parse_message: unable to parse input params\", 0 );\n\n\t\tRETURN_FALSE;\n\t}\n\n\tmemset(mfo, 0, sizeof(*mfo));\n\tmsgformat_data_init(&mfo->mf_data);\n\n\tif(pattern && pattern_len) {\n\t\tintl_convert_utf8_to_utf16(&spattern, &spattern_len, pattern, pattern_len, &INTL_DATA_ERROR_CODE(mfo));", "project": "php-src", "hash": 6806426083657384091418405807666534217, "size": 63, "commit_id": "95c4564f939c916538579ef63602a3cd31941c51", "message": "Fixed bug #73473: Stack Buffer Overflow in msgfmt_parse_message", "target": 1, "dataset": "other", "idx": 212820}
{"func": "\t\t\t\"msgfmt_parse_message: unable to parse input params\", 0 );\n\n\t\tRETURN_FALSE;\n\t}\n\n\tINTL_CHECK_LOCALE_LEN(slocale_len);\n\tmemset(mfo, 0, sizeof(*mfo));\n\tmsgformat_data_init(&mfo->mf_data);\n\n\tif(pattern && pattern_len) {\n\t\tintl_convert_utf8_to_utf16(&spattern, &spattern_len, pattern, pattern_len, &INTL_DATA_ERROR_CODE(mfo));", "project": "php-src", "hash": 91908387484897166366489060678294896561, "size": 64, "commit_id": "95c4564f939c916538579ef63602a3cd31941c51", "message": "Fixed bug #73473: Stack Buffer Overflow in msgfmt_parse_message", "target": 0, "dataset": "other", "idx": 462287}
{"func": "writepid (const char* pidfile, pid_t pid)\n{\n\tFILE* f;\n\n\tif ((f = fopen(pidfile, \"w\")) ==  NULL ) {\n\t\tlog_err(\"cannot open pidfile %s: %s\", \n\t\t\tpidfile, strerror(errno));\n\t\treturn;\n\t}\n\tif(fprintf(f, \"%lu\\n\", (unsigned long)pid) < 0) {\n\t\tlog_err(\"cannot write to pidfile %s: %s\", \n\t\t\tpidfile, strerror(errno));\n\t}\n\tfclose(f);\n}", "project": "unbound", "hash": 81315826788429169272585727489229965045, "size": 15, "commit_id": "ad387832979b6ce4c93f64fe706301cd7d034e87", "message": "- Fix for #303 CVE-2020-28935 : Fix that symlink does not interfere\n  with chown of pidfile.", "target": 1, "dataset": "other", "idx": 212821}
{"func": "writepid (const char* pidfile, pid_t pid)\n{\n\tint fd;\n\tchar pidbuf[32];\n\tsize_t count = 0;\n\tsnprintf(pidbuf, sizeof(pidbuf), \"%lu\\n\", (unsigned long)pid);\n\n\tif((fd = open(pidfile, O_WRONLY | O_CREAT | O_TRUNC\n#ifdef O_NOFOLLOW\n\t\t| O_NOFOLLOW\n#endif\n\t\t, 0644)) == -1) {\n\t\tlog_err(\"cannot open pidfile %s: %s\", \n\t\t\tpidfile, strerror(errno));\n\t\treturn;\n\t}\n\twhile(count < strlen(pidbuf)) {\n\t\tssize_t r = write(fd, pidbuf+count, strlen(pidbuf)-count);\n\t\tif(r == -1) {\n\t\t\tif(errno == EAGAIN || errno == EINTR)\n\t\t\t\tcontinue;\n\t\t\tlog_err(\"cannot write to pidfile %s: %s\",\n\t\t\t\tpidfile, strerror(errno));\n\t\t\tbreak;\n\t\t}\n\t\tcount += r;\n\t}\n\tclose(fd);\n}", "project": "unbound", "hash": 307992336681835740844480617769066838283, "size": 29, "commit_id": "ad387832979b6ce4c93f64fe706301cd7d034e87", "message": "- Fix for #303 CVE-2020-28935 : Fix that symlink does not interfere\n  with chown of pidfile.", "target": 0, "dataset": "other", "idx": 462294}
{"func": "static int dw_spi_transfer_one(struct spi_controller *master,\n\t\tstruct spi_device *spi, struct spi_transfer *transfer)\n{\n\tstruct dw_spi *dws = spi_controller_get_devdata(master);\n\tstruct chip_data *chip = spi_get_ctldata(spi);\n\tu8 imask = 0;\n\tu16 txlevel = 0;\n\tu32 cr0;\n\tint ret;\n\n\tdws->dma_mapped = 0;\n\n\tdws->tx = (void *)transfer->tx_buf;\n\tdws->tx_end = dws->tx + transfer->len;\n\tdws->rx = transfer->rx_buf;\n\tdws->rx_end = dws->rx + transfer->len;\n\tdws->len = transfer->len;\n\n\tspi_enable_chip(dws, 0);\n\n\t/* Handle per transfer options for bpw and speed */\n\tif (transfer->speed_hz != dws->current_freq) {", "project": "linux", "hash": 191752239953234250352277066792734233491, "size": 101, "commit_id": "19b61392c5a852b4e8a0bf35aecb969983c5932d", "message": "spi: spi-dw: Add lock protect dw_spi rx/tx to prevent concurrent calls\n\ndw_spi_irq() and dw_spi_transfer_one concurrent calls.\n\nI find a panic in dw_writer(): txw = *(u8 *)(dws->tx), when dw->tx==null,\ndw->len==4, and dw->tx_end==1.\n\nWhen tpm driver's message overtime dw_spi_irq() and dw_spi_transfer_one\nmay concurrent visit dw_spi, so I think dw_spi structure lack of protection.\n\nOtherwise dw_spi_transfer_one set dw rx/tx buffer and then open irq,\nstore dw rx/tx instructions and other cores handle irq load dw rx/tx\ninstructions may out of order.\n\n\t[ 1025.321302] Call trace:\n\t...\n\t[ 1025.321319]  __crash_kexec+0x98/0x148\n\t[ 1025.321323]  panic+0x17c/0x314\n\t[ 1025.321329]  die+0x29c/0x2e8\n\t[ 1025.321334]  die_kernel_fault+0x68/0x78\n\t[ 1025.321337]  __do_kernel_fault+0x90/0xb0\n\t[ 1025.321346]  do_page_fault+0x88/0x500\n\t[ 1025.321347]  do_translation_fault+0xa8/0xb8\n\t[ 1025.321349]  do_mem_abort+0x68/0x118\n\t[ 1025.321351]  el1_da+0x20/0x8c\n\t[ 1025.321362]  dw_writer+0xc8/0xd0\n\t[ 1025.321364]  interrupt_transfer+0x60/0x110\n\t[ 1025.321365]  dw_spi_irq+0x48/0x70\n\t...\n\nSigned-off-by: wuxu.wu <wuxu.wu@huawei.com>\nLink: https://lore.kernel.org/r/1577849981-31489-1-git-send-email-wuxu.wu@huawei.com\nSigned-off-by: Mark Brown <broonie@kernel.org>", "target": 1, "dataset": "other", "idx": 212837}
{"func": "static int dw_spi_transfer_one(struct spi_controller *master,\n\t\tstruct spi_device *spi, struct spi_transfer *transfer)\n{\n\tstruct dw_spi *dws = spi_controller_get_devdata(master);\n\tstruct chip_data *chip = spi_get_ctldata(spi);\n\tunsigned long flags;\n\tu8 imask = 0;\n\tu16 txlevel = 0;\n\tu32 cr0;\n\tint ret;\n\n\tdws->dma_mapped = 0;\n\tspin_lock_irqsave(&dws->buf_lock, flags);\n\tdws->tx = (void *)transfer->tx_buf;\n\tdws->tx_end = dws->tx + transfer->len;\n\tdws->rx = transfer->rx_buf;\n\tdws->rx_end = dws->rx + transfer->len;\n\tdws->len = transfer->len;\n\tspin_unlock_irqrestore(&dws->buf_lock, flags);\n\n\tspi_enable_chip(dws, 0);\n\n\t/* Handle per transfer options for bpw and speed */\n\tif (transfer->speed_hz != dws->current_freq) {", "project": "linux", "hash": 323360310926930360690361021045187060996, "size": 103, "commit_id": "19b61392c5a852b4e8a0bf35aecb969983c5932d", "message": "spi: spi-dw: Add lock protect dw_spi rx/tx to prevent concurrent calls\n\ndw_spi_irq() and dw_spi_transfer_one concurrent calls.\n\nI find a panic in dw_writer(): txw = *(u8 *)(dws->tx), when dw->tx==null,\ndw->len==4, and dw->tx_end==1.\n\nWhen tpm driver's message overtime dw_spi_irq() and dw_spi_transfer_one\nmay concurrent visit dw_spi, so I think dw_spi structure lack of protection.\n\nOtherwise dw_spi_transfer_one set dw rx/tx buffer and then open irq,\nstore dw rx/tx instructions and other cores handle irq load dw rx/tx\ninstructions may out of order.\n\n\t[ 1025.321302] Call trace:\n\t...\n\t[ 1025.321319]  __crash_kexec+0x98/0x148\n\t[ 1025.321323]  panic+0x17c/0x314\n\t[ 1025.321329]  die+0x29c/0x2e8\n\t[ 1025.321334]  die_kernel_fault+0x68/0x78\n\t[ 1025.321337]  __do_kernel_fault+0x90/0xb0\n\t[ 1025.321346]  do_page_fault+0x88/0x500\n\t[ 1025.321347]  do_translation_fault+0xa8/0xb8\n\t[ 1025.321349]  do_mem_abort+0x68/0x118\n\t[ 1025.321351]  el1_da+0x20/0x8c\n\t[ 1025.321362]  dw_writer+0xc8/0xd0\n\t[ 1025.321364]  interrupt_transfer+0x60/0x110\n\t[ 1025.321365]  dw_spi_irq+0x48/0x70\n\t...\n\nSigned-off-by: wuxu.wu <wuxu.wu@huawei.com>\nLink: https://lore.kernel.org/r/1577849981-31489-1-git-send-email-wuxu.wu@huawei.com\nSigned-off-by: Mark Brown <broonie@kernel.org>", "target": 0, "dataset": "other", "idx": 462461}
{"func": "INLINE void gdi_RectToCRgn(const HGDI_RECT rect, INT32* x, INT32* y, INT32* w, INT32* h)\n{\n\t*x = rect->left;\n\t*y = rect->top;\n\t*w = rect->right - rect->left + 1;\n\t*h = rect->bottom - rect->top + 1;\n}", "project": "FreeRDP", "hash": 15598217798169870623186526285570205870, "size": 7, "commit_id": "ce21b9d7ecd967e0bc98ed31a6b3757848aa6c9e", "message": "Fix CVE-2020-11523: clamp invalid rectangles to size 0\n\nThanks to Sunglin and HuanGMz from Knownsec 404", "target": 1, "dataset": "other", "idx": 212839}
{"func": "INLINE void gdi_RectToRgn(HGDI_RECT rect, HGDI_RGN rgn)\n{\n\trgn->x = rect->left;\n\trgn->y = rect->top;\n\trgn->w = rect->right - rect->left + 1;\n\trgn->h = rect->bottom - rect->top + 1;\n}", "project": "FreeRDP", "hash": 282322436390722434711482192909845070013, "size": 7, "commit_id": "ce21b9d7ecd967e0bc98ed31a6b3757848aa6c9e", "message": "Fix CVE-2020-11523: clamp invalid rectangles to size 0\n\nThanks to Sunglin and HuanGMz from Knownsec 404", "target": 0, "dataset": "other", "idx": 462482}
{"func": "\n\t\tsize = str->size;\n\t\tret = data2hex(value, value_size, str->data, &size);\n\t\tif (ret < 0) {\n\t\t\tgnutls_assert();\n\t\t\tgnutls_free(str->data);\n\t\t\treturn ret;\n\t\t}\n\t\tstr->size = size;\n\t\treturn 0;\n\t}", "project": "gnutls", "hash": 264692986559165841563862292253464619100, "size": 57, "commit_id": "272854367efc130fbd4f1a51840d80c630214e12", "message": "Reset the output value on error in _gnutls_x509_dn_to_string()\n\nReported by Kurt Roeckx.", "target": 1, "dataset": "other", "idx": 212884}
{"func": "\t\tsize = str->size;\n\t\tret = data2hex(value, value_size, str->data, &size);\n\t\tif (ret < 0) {\n\t\t\tgnutls_assert();\n\t\t\tgnutls_free(str->data);\n\t\t\tstr->data = NULL;\n\t\t\treturn ret;\n\t\t}\n\t\tstr->size = size;\n\t\treturn 0;\n\t}", "project": "gnutls", "hash": 163417149612341670448441181123820076841, "size": 58, "commit_id": "272854367efc130fbd4f1a51840d80c630214e12", "message": "Reset the output value on error in _gnutls_x509_dn_to_string()\n\nReported by Kurt Roeckx.", "target": 0, "dataset": "other", "idx": 462706}
{"func": "static const char *GetMagickPropertyLetter(const ImageInfo *image_info,\n  Image *image,const char letter)\n{\n  char\n    value[MaxTextExtent];\n\n  const char\n    *string;\n    }\n    case 'o':\n    {\n      /*\n        Output Filename - for delegate use only\n      */\n      string=image_info->filename;\n      break;\n    }\n    case 'p':\n    {\n    }\n    case 's':\n    {\n      /*\n        Image scene number.\n      */\n      if (image_info->number_scenes != 0)\n        (void) FormatLocaleString(value,MaxTextExtent,\"%.20g\",(double)\n          image_info->scene);\n      else\n        (void) FormatLocaleString(value,MaxTextExtent,\"%.20g\",(double)\n    }\n    case 'u':\n    {\n      /*\n        Unique filename.\n      */\n      string=image_info->unique;\n      break;\n    }\n    case 'w':\n    {\n    }\n    case 'S':\n    {\n      /*\n        Image scenes.\n      */\n      if (image_info->number_scenes == 0)\n        string=\"2147483647\";\n      else\n        (void) FormatLocaleString(value,MaxTextExtent,\"%.20g\",(double)\n          image_info->scene+image_info->number_scenes);\n    }\n    case 'Z':\n    {\n      /*\n        Zero filename.\n      */\n      string=image_info->zero;\n      break;\n    }\n    case '@':\n    {", "project": "ImageMagick6", "hash": 81789685423307343026163762196655100181, "size": 463, "commit_id": "5bf7ff59c8ada957d6a681a0a2cc29f3813ad4bc", "message": "https://github.com/ImageMagick/ImageMagick/issues/1225", "target": 1, "dataset": "other", "idx": 212909}
{"func": "static const char *GetMagickPropertyLetter(const ImageInfo *image_info,\n  Image *image,const char letter)\n{\n#define WarnNoImageInfoReturn(format,arg) \\\n  if (image_info == (ImageInfo *) NULL ) { \\\n    (void) ThrowMagickException(&image->exception,GetMagickModule(), \\\n      OptionWarning,\"NoImageInfoForProperty\",format,arg); \\\n    return((const char *) NULL); \\\n  }\n\n  char\n    value[MaxTextExtent];\n\n  const char\n    *string;\n    case 'o':\n    {\n      /*\n        Output Filename - for delegate use only\n      */\n      WarnNoImageInfoReturn(\"\\\"%%%c\\\"\",letter);\n      string=image_info->filename;\n      break;\n    }\n    case 'p':\n    {\n    case 's':\n    {\n      /*\n        Image scene number.\n      */\n      WarnNoImageInfoReturn(\"\\\"%%%c\\\"\",letter);\n      if (image_info->number_scenes != 0)\n        (void) FormatLocaleString(value,MaxTextExtent,\"%.20g\",(double)\n          image_info->scene);\n      else\n        (void) FormatLocaleString(value,MaxTextExtent,\"%.20g\",(double)\n    case 'u':\n    {\n      /*\n        Unique filename.\n      */\n      WarnNoImageInfoReturn(\"\\\"%%%c\\\"\",letter);\n      string=image_info->unique;\n      break;\n    }\n    case 'w':\n    {\n    case 'S':\n    {\n      /*\n        Image scenes.\n      */\n      WarnNoImageInfoReturn(\"\\\"%%%c\\\"\",letter);\n      if (image_info->number_scenes == 0)\n        string=\"2147483647\";\n      else\n        (void) FormatLocaleString(value,MaxTextExtent,\"%.20g\",(double)\n          image_info->scene+image_info->number_scenes);\n    case 'Z':\n    {\n      /*\n        Zero filename.\n      */\n      WarnNoImageInfoReturn(\"\\\"%%%c\\\"\",letter);\n      string=image_info->zero;\n      break;\n    }\n    case '@':\n    {", "project": "ImageMagick6", "hash": 207668242379766768614404100976312119040, "size": 475, "commit_id": "5bf7ff59c8ada957d6a681a0a2cc29f3813ad4bc", "message": "https://github.com/ImageMagick/ImageMagick/issues/1225", "target": 0, "dataset": "other", "idx": 462960}
{"func": "\tepos = a->end.pos - 1;\n    }\n    switch (form->type) {\n    case FORM_INPUT_CHECKBOX:\n    case FORM_INPUT_RADIO:\n\tif (spos >= buf->currentLine->len || spos < 0)\n\t    break;\n\tif (form->checked)\n\t    buf->currentLine->lineBuf[spos] = '*';\n\telse\n\t    buf->currentLine->lineBuf[spos] = ' ';", "project": "w3m", "hash": 15956644405726083189492084201505214654, "size": 97, "commit_id": "e2c7ecec6f9b730ad3c9bf8c8df9212970f183d7", "message": "Prevent dereference near-null pointer in formUpdateBuffer\n\nBug-Debian: https://github.com/tats/w3m/issues/35", "target": 1, "dataset": "other", "idx": 212915}
{"func": "\tepos = a->end.pos - 1;\n    }\n    switch (form->type) {\n    case FORM_INPUT_CHECKBOX:\n    case FORM_INPUT_RADIO:\n\tif (buf->currentLine == NULL ||\n\t    spos >= buf->currentLine->len || spos < 0)\n\t    break;\n\tif (form->checked)\n\t    buf->currentLine->lineBuf[spos] = '*';\n\telse\n\t    buf->currentLine->lineBuf[spos] = ' ';", "project": "w3m", "hash": 45577909683508718942973794972642241617, "size": 98, "commit_id": "e2c7ecec6f9b730ad3c9bf8c8df9212970f183d7", "message": "Prevent dereference near-null pointer in formUpdateBuffer\n\nBug-Debian: https://github.com/tats/w3m/issues/35", "target": 0, "dataset": "other", "idx": 463013}
{"func": "\t * then lets unpin all the registered memory.\n\t */\n\tif (!list_empty(head)) {\n\t\tlist_for_each_safe(pos, q, head) {\n\t\t\t__unregister_enc_region_locked(kvm,\n\t\t\t\tlist_entry(pos, struct enc_region, list));\n\t\t}\n\t}\n\n\tmutex_unlock(&kvm->lock);\n", "project": "linux", "hash": 59854663405289045336965850461540245489, "size": 34, "commit_id": "7be74942f184fdfba34ddd19a0d995deb34d4a03", "message": "KVM: SVM: Periodically schedule when unregistering regions on destroy\n\nThere may be many encrypted regions that need to be unregistered when a\nSEV VM is destroyed.  This can lead to soft lockups.  For example, on a\nhost running 4.15:\n\nwatchdog: BUG: soft lockup - CPU#206 stuck for 11s! [t_virtual_machi:194348]\nCPU: 206 PID: 194348 Comm: t_virtual_machi\nRIP: 0010:free_unref_page_list+0x105/0x170\n...\nCall Trace:\n [<0>] release_pages+0x159/0x3d0\n [<0>] sev_unpin_memory+0x2c/0x50 [kvm_amd]\n [<0>] __unregister_enc_region_locked+0x2f/0x70 [kvm_amd]\n [<0>] svm_vm_destroy+0xa9/0x200 [kvm_amd]\n [<0>] kvm_arch_destroy_vm+0x47/0x200\n [<0>] kvm_put_kvm+0x1a8/0x2f0\n [<0>] kvm_vm_release+0x25/0x30\n [<0>] do_exit+0x335/0xc10\n [<0>] do_group_exit+0x3f/0xa0\n [<0>] get_signal+0x1bc/0x670\n [<0>] do_signal+0x31/0x130\n\nAlthough the CLFLUSH is no longer issued on every encrypted region to be\nunregistered, there are no other changes that can prevent soft lockups for\nvery large SEV VMs in the latest kernel.\n\nPeriodically schedule if necessary.  This still holds kvm->lock across the\nresched, but since this only happens when the VM is destroyed this is\nassumed to be acceptable.\n\nSigned-off-by: David Rientjes <rientjes@google.com>\nMessage-Id: <alpine.DEB.2.23.453.2008251255240.2987727@chino.kir.corp.google.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 1, "dataset": "other", "idx": 212916}
{"func": "\t */\n\tif (!list_empty(head)) {\n\t\tlist_for_each_safe(pos, q, head) {\n\t\t\t__unregister_enc_region_locked(kvm,\n\t\t\t\tlist_entry(pos, struct enc_region, list));\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tmutex_unlock(&kvm->lock);\n", "project": "linux", "hash": 32944969904859958026118495600871042845, "size": 35, "commit_id": "7be74942f184fdfba34ddd19a0d995deb34d4a03", "message": "KVM: SVM: Periodically schedule when unregistering regions on destroy\n\nThere may be many encrypted regions that need to be unregistered when a\nSEV VM is destroyed.  This can lead to soft lockups.  For example, on a\nhost running 4.15:\n\nwatchdog: BUG: soft lockup - CPU#206 stuck for 11s! [t_virtual_machi:194348]\nCPU: 206 PID: 194348 Comm: t_virtual_machi\nRIP: 0010:free_unref_page_list+0x105/0x170\n...\nCall Trace:\n [<0>] release_pages+0x159/0x3d0\n [<0>] sev_unpin_memory+0x2c/0x50 [kvm_amd]\n [<0>] __unregister_enc_region_locked+0x2f/0x70 [kvm_amd]\n [<0>] svm_vm_destroy+0xa9/0x200 [kvm_amd]\n [<0>] kvm_arch_destroy_vm+0x47/0x200\n [<0>] kvm_put_kvm+0x1a8/0x2f0\n [<0>] kvm_vm_release+0x25/0x30\n [<0>] do_exit+0x335/0xc10\n [<0>] do_group_exit+0x3f/0xa0\n [<0>] get_signal+0x1bc/0x670\n [<0>] do_signal+0x31/0x130\n\nAlthough the CLFLUSH is no longer issued on every encrypted region to be\nunregistered, there are no other changes that can prevent soft lockups for\nvery large SEV VMs in the latest kernel.\n\nPeriodically schedule if necessary.  This still holds kvm->lock across the\nresched, but since this only happens when the VM is destroyed this is\nassumed to be acceptable.\n\nSigned-off-by: David Rientjes <rientjes@google.com>\nMessage-Id: <alpine.DEB.2.23.453.2008251255240.2987727@chino.kir.corp.google.com>\nSigned-off-by: Paolo Bonzini <pbonzini@redhat.com>", "target": 0, "dataset": "other", "idx": 463016}
{"func": "\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tif (!(flags & TTM_PAGE_FLAG_DMA32) &&\n\t\t\t    (npages - i) >= HPAGE_PMD_NR) {\n\t\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\t\tif (p++ != pages[i + j])\n\t\t\t\t\t    break;\n\n\t\t\t\tif (j == HPAGE_PMD_NR)\n\t\t\t\t\torder = HPAGE_PMD_ORDER;\n\t\t\t}\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tif (p++ != pages[i + j])\n\t\t\t\t    break;\n\n\t\t\tif (j != HPAGE_PMD_NR)\n\t\t\t\tbreak;\n", "project": "linux", "hash": 193657400577846816147392268052783548278, "size": 113, "commit_id": "453393369dc9806d2455151e329c599684762428", "message": "drm/ttm: fix incrementing the page pointer for huge pages\n\nWhen we increment the counter we need to increment the pointer as well.\n\nSigned-off-by: Christian K\u00f6nig <christian.koenig@amd.com>\nFixes: e16858a7e6e7 drm/ttm: fix start page for huge page check in ttm_put_pages()\nReviewed-by: Michel D\u00e4nzer <michel.daenzer@amd.com>\nAcked-by: Huang Rui <ray.huang@amd.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 212944}
{"func": "\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\t\t\tif (!(flags & TTM_PAGE_FLAG_DMA32) &&\n\t\t\t    (npages - i) >= HPAGE_PMD_NR) {\n\t\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\t\tif (++p != pages[i + j])\n\t\t\t\t\t    break;\n\n\t\t\t\tif (j == HPAGE_PMD_NR)\n\t\t\t\t\torder = HPAGE_PMD_ORDER;\n\t\t\t}\n\n\t\t\tif (!p)\n\t\t\t\tbreak;\n\n\t\t\tfor (j = 1; j < HPAGE_PMD_NR; ++j)\n\t\t\t\tif (++p != pages[i + j])\n\t\t\t\t    break;\n\n\t\t\tif (j != HPAGE_PMD_NR)\n\t\t\t\tbreak;\n", "project": "linux", "hash": 326634532465872425758167027077494247468, "size": 113, "commit_id": "453393369dc9806d2455151e329c599684762428", "message": "drm/ttm: fix incrementing the page pointer for huge pages\n\nWhen we increment the counter we need to increment the pointer as well.\n\nSigned-off-by: Christian K\u00f6nig <christian.koenig@amd.com>\nFixes: e16858a7e6e7 drm/ttm: fix start page for huge page check in ttm_put_pages()\nReviewed-by: Michel D\u00e4nzer <michel.daenzer@amd.com>\nAcked-by: Huang Rui <ray.huang@amd.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 463460}
{"func": "             ciphertext->data, ciphertext->size)) < 0)\n        return gnutls_assert_val(ret);\n\n      break;\n    case CIPHER_BLOCK:\n      if (ciphertext->size < MAX(blocksize, tag_size) || (ciphertext->size % blocksize != 0))\n        return gnutls_assert_val(GNUTLS_E_UNEXPECTED_PACKET_LENGTH);\n\n      /* ignore the IV in TLS 1.1+\n       */\n      if (explicit_iv)\n        {\n          _gnutls_auth_cipher_setiv(&params->read.cipher_state,\n            ciphertext->data, blocksize);\n\n          ciphertext->size -= blocksize;\n          ciphertext->data += blocksize;\n\n          if (ciphertext->size == 0)\n            {\n              gnutls_assert ();\n              return GNUTLS_E_DECRYPTION_FAILED;\n            }\n        }\n\n      /* we don't use the auth_cipher interface here, since\n       * TLS with block ciphers is impossible to be used under such\n       * an API. (the length of plaintext is required to calculate\n       * auth_data, but it is not available before decryption).\n      if ((ret =\n           _gnutls_cipher_decrypt (&params->read.cipher_state.cipher,\n             ciphertext->data, ciphertext->size)) < 0)\n        return gnutls_assert_val(ret);\n\n      pad = ciphertext->data[ciphertext->size - 1] + 1;   /* pad */\n\n      if ((int) pad > (int) ciphertext->size - tag_size)\n        {\n          gnutls_assert ();\n          _gnutls_record_log", "project": "gnutls", "hash": 259478820528968398998523761592348550013, "size": 184, "commit_id": "b495740f2ff66550ca9395b3fda3ea32c3acb185", "message": "changes in packet parsing.", "target": 1, "dataset": "other", "idx": 213025}
{"func": "             ciphertext->data, ciphertext->size)) < 0)\n        return gnutls_assert_val(ret);\n\n      break;\n    case CIPHER_BLOCK:\n      if (ciphertext->size < blocksize || (ciphertext->size % blocksize != 0))\n        return gnutls_assert_val(GNUTLS_E_UNEXPECTED_PACKET_LENGTH);\n\n      /* ignore the IV in TLS 1.1+\n       */\n      if (explicit_iv)\n          _gnutls_auth_cipher_setiv(&params->read.cipher_state,\n            ciphertext->data, blocksize);\n\n          ciphertext->size -= blocksize;\n          ciphertext->data += blocksize;\n        }\n\n      if (ciphertext->size < tag_size)\n        return gnutls_assert_val(GNUTLS_E_DECRYPTION_FAILED);\n\n      /* we don't use the auth_cipher interface here, since\n       * TLS with block ciphers is impossible to be used under such\n       * an API. (the length of plaintext is required to calculate\n       * auth_data, but it is not available before decryption).\n           _gnutls_cipher_decrypt (&params->read.cipher_state.cipher,\n             ciphertext->data, ciphertext->size)) < 0)\n        return gnutls_assert_val(ret);\n\n      pad = ciphertext->data[ciphertext->size - 1] + 1;   /* pad */\n\n\n      if ((int) pad > (int) ciphertext->size - tag_size)\n        {\n          gnutls_assert ();\n          _gnutls_record_log", "project": "gnutls", "hash": 154589990996518600118997836491749802345, "size": 182, "commit_id": "b495740f2ff66550ca9395b3fda3ea32c3acb185", "message": "changes in packet parsing.", "target": 0, "dataset": "other", "idx": 464938}
{"func": "table_regex_match(const char *string, const char *pattern)\n{\n\tregex_t preg;\n\tint\tcflags = REG_EXTENDED|REG_NOSUB;\n\n\tif (strncmp(pattern, \"(?i)\", 4) == 0) {\n\t\tcflags |= REG_ICASE;\n\t\tpattern += 4;\n\t}\n\n\tif (regcomp(&preg, pattern, cflags) != 0)\n\t\treturn (0);\n\n\tif (regexec(&preg, string, 0, NULL, 0) != 0)\n\t\treturn (0);\n\n\treturn (1);\n}", "project": "src", "hash": 134955981984600634024647053081479880826, "size": 18, "commit_id": "79a034b4aed29e965f45a13409268290c9910043", "message": "Use regfree after we're done with preg.\n\nFrom gilles@", "target": 1, "dataset": "other", "idx": 213469}
{"func": "table_regex_match(const char *string, const char *pattern)\n{\n\tregex_t preg;\n\tint\tcflags = REG_EXTENDED|REG_NOSUB;\n\tint ret;\n\n\tif (strncmp(pattern, \"(?i)\", 4) == 0) {\n\t\tcflags |= REG_ICASE;\n\t\tpattern += 4;\n\t}\n\n\tif (regcomp(&preg, pattern, cflags) != 0)\n\t\treturn (0);\n\n\tret = regexec(&preg, string, 0, NULL, 0);\n\n\tregfree(&preg);\n\n\tif (ret != 0)\n\t\treturn (0);\n\n\treturn (1);\n}", "project": "src", "hash": 27915400491158450836907057889102967473, "size": 23, "commit_id": "79a034b4aed29e965f45a13409268290c9910043", "message": "Use regfree after we're done with preg.\n\nFrom gilles@", "target": 0, "dataset": "other", "idx": 468895}
{"func": "\t    !(flags & MSG_MORE) &&\n\t    !exthdrlen)\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tcork->length += length;\n\tif ((((length + (skb ? skb->len : fragheaderlen)) > mtu) ||\n\t     (skb && skb_is_gso(skb))) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx) {\n\t\terr = ip_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t hh_len, fragheaderlen, transhdrlen,\n\t\t\t\t\t maxfraglen, flags);\n\t\tif (err)\n\t\t\tgoto error;", "project": "net", "hash": 322529707948405632291501247024639555798, "size": 253, "commit_id": "85f1bd9a7b5a79d5baa8bf44af19658f7bf77bfa", "message": "udp: consistently apply ufo or fragmentation\n\nWhen iteratively building a UDP datagram with MSG_MORE and that\ndatagram exceeds MTU, consistently choose UFO or fragmentation.\n\nOnce skb_is_gso, always apply ufo. Conversely, once a datagram is\nsplit across multiple skbs, do not consider ufo.\n\nSendpage already maintains the first invariant, only add the second.\nIPv6 does not have a sendpage implementation to modify.\n\nA gso skb must have a partial checksum, do not follow sk_no_check_tx\nin udp_send_skb.\n\nFound by syzkaller.\n\nFixes: e89e9cf539a2 (\"[IPv4/IPv6]: UFO Scatter-gather approach\")\nReported-by: Andrey Konovalov <andreyknvl@google.com>\nSigned-off-by: Willem de Bruijn <willemb@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 213470}
{"func": "\t    !(flags & MSG_MORE) &&\n\t    !exthdrlen)\n\t\tcsummode = CHECKSUM_PARTIAL;\n\n\tcork->length += length;\n\tif ((skb && skb_is_gso(skb)) ||\n\t    (((length + (skb ? skb->len : fragheaderlen)) > mtu) &&\n\t    (skb_queue_len(queue) <= 1) &&\n\t    (sk->sk_protocol == IPPROTO_UDP) &&\n\t    (rt->dst.dev->features & NETIF_F_UFO) && !dst_xfrm(&rt->dst) &&\n\t    (sk->sk_type == SOCK_DGRAM) && !sk->sk_no_check_tx)) {\n\t\terr = ip_ufo_append_data(sk, queue, getfrag, from, length,\n\t\t\t\t\t hh_len, fragheaderlen, transhdrlen,\n\t\t\t\t\t maxfraglen, flags);\n\t\tif (err)\n\t\t\tgoto error;", "project": "net", "hash": 333052664581020657981105773969472049388, "size": 254, "commit_id": "85f1bd9a7b5a79d5baa8bf44af19658f7bf77bfa", "message": "udp: consistently apply ufo or fragmentation\n\nWhen iteratively building a UDP datagram with MSG_MORE and that\ndatagram exceeds MTU, consistently choose UFO or fragmentation.\n\nOnce skb_is_gso, always apply ufo. Conversely, once a datagram is\nsplit across multiple skbs, do not consider ufo.\n\nSendpage already maintains the first invariant, only add the second.\nIPv6 does not have a sendpage implementation to modify.\n\nA gso skb must have a partial checksum, do not follow sk_no_check_tx\nin udp_send_skb.\n\nFound by syzkaller.\n\nFixes: e89e9cf539a2 (\"[IPv4/IPv6]: UFO Scatter-gather approach\")\nReported-by: Andrey Konovalov <andreyknvl@google.com>\nSigned-off-by: Willem de Bruijn <willemb@google.com>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 468988}
{"func": "static int crypto_rsa_common(const BYTE* input, int length, UINT32 key_length, const BYTE* modulus,\n                             const BYTE* exponent, int exponent_size, BYTE* output)\n{\n\tBN_CTX* ctx;\n\tint output_length = -1;\n\tBYTE* input_reverse;\n\tBYTE* modulus_reverse;\n\tBYTE* exponent_reverse;\n\tBIGNUM *mod, *exp, *x, *y;\n\tinput_reverse = (BYTE*)malloc(2 * key_length + exponent_size);\n\n\tif (!input_reverse)\n\t\treturn -1;\n\n\tmodulus_reverse = input_reverse + key_length;\n\t\tgoto fail_bn_x;\n\n\tif (!(y = BN_new()))\n\t\tgoto fail_bn_y;\n\n\tBN_bin2bn(modulus_reverse, key_length, mod);\n\tBN_bin2bn(exponent_reverse, exponent_size, exp);\n\tBN_bin2bn(input_reverse, length, x);\n\tBN_mod_exp(y, x, exp, mod, ctx);\n\toutput_length = BN_bn2bin(y, output);\n\tcrypto_reverse(output, output_length);\n\n\tif (output_length < (int)key_length)\n\t\tmemset(output + output_length, 0, key_length - output_length);\n\n\tBN_free(y);\nfail_bn_y:\n\tBN_clear_free(x);\nfail_bn_x:\n\tBN_free(exp);", "project": "FreeRDP", "hash": 248112898649019039113029495278291277796, "size": 61, "commit_id": "8305349a943c68b1bc8c158f431dc607655aadea", "message": "Fixed  GHSL-2020-102 heap overflow\n\n(cherry picked from commit 197b16cc15a12813c2e4fa2d6ae9cd9c4a57e581)", "target": 1, "dataset": "other", "idx": 213662}
{"func": "static int crypto_rsa_common(const BYTE* input, int length, UINT32 key_length, const BYTE* modulus,\n                             const BYTE* exponent, int exponent_size, BYTE* output)\n{\n\tBN_CTX* ctx = NULL;\n\tint output_length = -1;\n\tBYTE* input_reverse = NULL;\n\tBYTE* modulus_reverse = NULL;\n\tBYTE* exponent_reverse = NULL;\n\tBIGNUM* mod = NULL;\n\tBIGNUM* exp = NULL;\n\tBIGNUM* x = NULL;\n\tBIGNUM* y = NULL;\n\tsize_t bufferSize = 2 * key_length + exponent_size;\n\n\tif (!input || (length < 0) || (exponent_size < 0) || !modulus || !exponent || !output)\n\t\treturn -1;\n\n\tif (length > bufferSize)\n\t\tbufferSize = length;\n\n\tinput_reverse = (BYTE*)calloc(bufferSize, 1);\n\n\tif (!input_reverse)\n\t\treturn -1;\n\n\tmodulus_reverse = input_reverse + key_length;\n\t\tgoto fail_bn_x;\n\n\tif (!(y = BN_new()))\n\t\tgoto fail_bn_y;\n\n\tif (!BN_bin2bn(modulus_reverse, key_length, mod))\n\t\tgoto fail;\n\n\tif (!BN_bin2bn(exponent_reverse, exponent_size, exp))\n\t\tgoto fail;\n\tif (!BN_bin2bn(input_reverse, length, x))\n\t\tgoto fail;\n\tif (BN_mod_exp(y, x, exp, mod, ctx) != 1)\n\t\tgoto fail;\n\toutput_length = BN_bn2bin(y, output);\n\tif (output_length < 0)\n\t\tgoto fail;\n\tcrypto_reverse(output, output_length);\n\n\tif (output_length < key_length)\n\t\tmemset(output + output_length, 0, key_length - output_length);\n\nfail:\n\tBN_free(y);\nfail_bn_y:\n\tBN_clear_free(x);\nfail_bn_x:\n\tBN_free(exp);", "project": "FreeRDP", "hash": 241821229484338014163593582174308767879, "size": 80, "commit_id": "8305349a943c68b1bc8c158f431dc607655aadea", "message": "Fixed  GHSL-2020-102 heap overflow\n\n(cherry picked from commit 197b16cc15a12813c2e4fa2d6ae9cd9c4a57e581)", "target": 0, "dataset": "other", "idx": 473441}
{"func": "{\n  SSL_CTX *ssl_ctx = NULL;\n  SSL *ssl = NULL;\n  int n, finished = 0;\n  X509_VERIFY_PARAM *param;\n  uint8_t verify_crls = cred->x509Credential.x509CrlVerifyMode;\n\n  if (!(ssl_ctx = SSL_CTX_new(SSLv23_client_method())))\n  {\n    rfbClientLog(\"Could not create new SSL context.\\n\");\n    return NULL;\n\n  param = X509_VERIFY_PARAM_new();\n\n  /* Setup verification if not anonymous */\n  if (!anonTLS)\n  {\n    if (cred->x509Credential.x509CACertFile)\n    {\n      if (!SSL_CTX_load_verify_locations(ssl_ctx, cred->x509Credential.x509CACertFile, NULL))\n      {\n        rfbClientLog(\"Failed to load CA certificate from %s.\\n\",", "project": "libvncserver", "hash": 117013703740746928818923290774033041402, "size": 118, "commit_id": "33441d90a506d5f3ae9388f2752901227e430553", "message": "libvncclient/tls_openssl: do not deref a NULL pointer\n\nHappens in anonTLS mode where cred is NULL.\n\nre #347", "target": 1, "dataset": "other", "idx": 214025}
{"func": "{\n  SSL_CTX *ssl_ctx = NULL;\n  SSL *ssl = NULL;\n  int n, finished = 0;\n  X509_VERIFY_PARAM *param;\n  uint8_t verify_crls;\n\n  if (!(ssl_ctx = SSL_CTX_new(SSLv23_client_method())))\n  {\n    rfbClientLog(\"Could not create new SSL context.\\n\");\n    return NULL;\n  param = X509_VERIFY_PARAM_new();\n\n  /* Setup verification if not anonymous */\n  if (!anonTLS)\n  {\n    verify_crls = cred->x509Credential.x509CrlVerifyMode;\n    if (cred->x509Credential.x509CACertFile)\n    {\n      if (!SSL_CTX_load_verify_locations(ssl_ctx, cred->x509Credential.x509CACertFile, NULL))\n      {\n        rfbClientLog(\"Failed to load CA certificate from %s.\\n\",", "project": "libvncserver", "hash": 65290517691015150407412891236689283742, "size": 119, "commit_id": "33441d90a506d5f3ae9388f2752901227e430553", "message": "libvncclient/tls_openssl: do not deref a NULL pointer\n\nHappens in anonTLS mode where cred is NULL.\n\nre #347", "target": 0, "dataset": "other", "idx": 474542}
{"func": "                    new_off += sub_dissected;\n\n                    if ((sub_dissected <= bb_data_len) && (sub_dissected >= DVB_S2_GSE_MINSIZE)) {\n                        bb_data_len -= sub_dissected;\n                        if (bb_data_len < DVB_S2_GSE_MINSIZE)\n                            bb_data_len = 0;\n                    }\n                }\n            }\n        } else {\n            proto_tree_add_item(dvb_s2_bb_tree, hf_dvb_s2_bb_df, tvb, cur_off + new_off, bb_data_len, ENC_NA);", "project": "wireshark", "hash": 308318766763033510949212779797367389554, "size": 147, "commit_id": "0d8be1fb797b3d65f1c2c204da76af8e8de6d3cc", "message": "DVB-S2-BB: Prevent infinite loop\n\nCommit 4bf4ee88f0544727e7f89f3f288c6afd2f650a4c removed an else\nstatement that broke out of the BBFrame processing loop. Without\nit, infinite loops might be possible if the GSE frames have bit errors\nin the length field.\n\n\n(cherry picked from commit 0137c24d60934f131b25506a88c9464e4dc827de)", "target": 1, "dataset": "other", "idx": 214056}
{"func": "\n                    if ((sub_dissected <= bb_data_len) && (sub_dissected >= DVB_S2_GSE_MINSIZE)) {\n                        bb_data_len -= sub_dissected;\n                        if (bb_data_len < DVB_S2_GSE_MINSIZE)\n                            bb_data_len = 0;\n                    } else {\n                        bb_data_len = 0;\n                    }\n                }\n            }\n        } else {\n            proto_tree_add_item(dvb_s2_bb_tree, hf_dvb_s2_bb_df, tvb, cur_off + new_off, bb_data_len, ENC_NA);", "project": "wireshark", "hash": 292878332668751229125261027041642584721, "size": 149, "commit_id": "0d8be1fb797b3d65f1c2c204da76af8e8de6d3cc", "message": "DVB-S2-BB: Prevent infinite loop\n\nCommit 4bf4ee88f0544727e7f89f3f288c6afd2f650a4c removed an else\nstatement that broke out of the BBFrame processing loop. Without\nit, infinite loops might be possible if the GSE frames have bit errors\nin the length field.\n\n\n(cherry picked from commit 0137c24d60934f131b25506a88c9464e4dc827de)", "target": 0, "dataset": "other", "idx": 475261}
{"func": "  Yp=(Jzazbz_g*Y-X*(Jzazbz_g-1));\n  Zp=Z;\n  L=0.41478972*Xp+0.579999*Yp+0.0146480*Zp;\n  M=(-0.2015100)*Xp+1.120649*Yp+0.0531008*Zp;\n  S=(-0.0166008)*Xp+0.264800*Yp+0.6684799*Zp;\n  gamma=pow(L/white_luminance,Jzazbz_n);\n  Lp=pow((Jzazbz_c1+Jzazbz_c2*gamma)/(1.0+Jzazbz_c3*gamma),Jzazbz_p);\n  gamma=pow(M/white_luminance,Jzazbz_n);\n  Mp=pow((Jzazbz_c1+Jzazbz_c2*gamma)/(1.0+Jzazbz_c3*gamma),Jzazbz_p);\n  gamma=pow(S/white_luminance,Jzazbz_n);\n  Sp=pow((Jzazbz_c1+Jzazbz_c2*gamma)/(1.0+Jzazbz_c3*gamma),Jzazbz_p);\n  Iz=0.5*Lp+0.5*Mp;\n  *az=3.52400*Lp-4.066708*Mp+0.542708*Sp+0.5;\n  *bz=0.199076*Lp+1.096799*Mp-1.295875*Sp+0.5;\n  *Jz=((Jzazbz_d+1.0)*Iz)/(Jzazbz_d*Iz+1.0)-Jzazbz_d0;", "project": "ImageMagick", "hash": 238788498874258350151289869670329565824, "size": 43, "commit_id": "75f6f5032690077cae3eaeda3c0165cc765eaeb5", "message": "https://github.com/ImageMagick/ImageMagick/issues/3295", "target": 1, "dataset": "other", "idx": 214120}
{"func": "  Yp=(Jzazbz_g*Y-X*(Jzazbz_g-1));\n  Zp=Z;\n  L=0.41478972*Xp+0.579999*Yp+0.0146480*Zp;\n  M=(-0.2015100)*Xp+1.120649*Yp+0.0531008*Zp;\n  S=(-0.0166008)*Xp+0.264800*Yp+0.6684799*Zp;\n  gamma=pow(L*PerceptibleReciprocal(white_luminance),Jzazbz_n);\n  Lp=pow((Jzazbz_c1+Jzazbz_c2*gamma)/(1.0+Jzazbz_c3*gamma),Jzazbz_p);\n  gamma=pow(M*PerceptibleReciprocal(white_luminance),Jzazbz_n);\n  Mp=pow((Jzazbz_c1+Jzazbz_c2*gamma)/(1.0+Jzazbz_c3*gamma),Jzazbz_p);\n  gamma=pow(S*PerceptibleReciprocal(white_luminance),Jzazbz_n);\n  Sp=pow((Jzazbz_c1+Jzazbz_c2*gamma)/(1.0+Jzazbz_c3*gamma),Jzazbz_p);\n  Iz=0.5*Lp+0.5*Mp;\n  *az=3.52400*Lp-4.066708*Mp+0.542708*Sp+0.5;\n  *bz=0.199076*Lp+1.096799*Mp-1.295875*Sp+0.5;\n  *Jz=((Jzazbz_d+1.0)*Iz)/(Jzazbz_d*Iz+1.0)-Jzazbz_d0;", "project": "ImageMagick", "hash": 198227054044668048777169440025596538351, "size": 43, "commit_id": "75f6f5032690077cae3eaeda3c0165cc765eaeb5", "message": "https://github.com/ImageMagick/ImageMagick/issues/3295", "target": 0, "dataset": "other", "idx": 475970}
{"func": "        if (s->regs[SONIC_RCR] & (SONIC_RCR_LB1 | SONIC_RCR_LB0)) {\n            /* Loopback */\n            s->regs[SONIC_TCR] |= SONIC_TCR_CRSL;\n            if (nc->info->can_receive(nc)) {\n                s->loopback_packet = 1;\n                nc->info->receive(nc, s->tx_buffer, tx_len);\n            }\n        } else {\n            /* Transmit packet */\n            qemu_send_packet(nc, s->tx_buffer, tx_len);\n        }", "project": "qemu", "hash": 316061785017432960037477437719244038033, "size": 113, "commit_id": "331d2ac9ea307c990dc86e6493e8f0c48d14bb33", "message": "dp8393x: switch to use qemu_receive_packet() for loopback packet\n\nThis patch switches to use qemu_receive_packet() which can detect\nreentrancy and return early.\n\nThis is intended to address CVE-2021-3416.\n\nCc: Prasad J Pandit <ppandit@redhat.com>\nCc: qemu-stable@nongnu.org\nReviewed-by: Philippe Mathieu-Daud\u00e9 <philmd@redhat.com\nSigned-off-by: Jason Wang <jasowang@redhat.com>", "target": 1, "dataset": "other", "idx": 214193}
{"func": "        if (s->regs[SONIC_RCR] & (SONIC_RCR_LB1 | SONIC_RCR_LB0)) {\n            /* Loopback */\n            s->regs[SONIC_TCR] |= SONIC_TCR_CRSL;\n            if (nc->info->can_receive(nc)) {\n                s->loopback_packet = 1;\n                qemu_receive_packet(nc, s->tx_buffer, tx_len);\n            }\n        } else {\n            /* Transmit packet */\n            qemu_send_packet(nc, s->tx_buffer, tx_len);\n        }", "project": "qemu", "hash": 258794860106692447895072609340176223318, "size": 113, "commit_id": "331d2ac9ea307c990dc86e6493e8f0c48d14bb33", "message": "dp8393x: switch to use qemu_receive_packet() for loopback packet\n\nThis patch switches to use qemu_receive_packet() which can detect\nreentrancy and return early.\n\nThis is intended to address CVE-2021-3416.\n\nCc: Prasad J Pandit <ppandit@redhat.com>\nCc: qemu-stable@nongnu.org\nReviewed-by: Philippe Mathieu-Daud\u00e9 <philmd@redhat.com\nSigned-off-by: Jason Wang <jasowang@redhat.com>", "target": 0, "dataset": "other", "idx": 476446}
{"func": "#endif\n    \n    test_arm();\n    printf(\"==========================\\n\");\n    test_thumb();\n\n    // dynamically free shared library\n#ifdef DYNLOAD\n    uc_dyn_free();\n#endif\n    ", "project": "unicorn", "hash": 185725400238744388569454420547208561164, "size": 24, "commit_id": "bf1713d9e011b55ca1f502a6779fc4722b4bb077", "message": "Add arm ite blocks samples from #853 (#1381)", "target": 1, "dataset": "other", "idx": 214241}
{"func": "#endif\n    \n    test_arm();\n    printf(\"==========================\\n\");\n    test_thumb();\n    printf(\"==========================\\n\");\n    test_thumb_ite();\n    // dynamically free shared library\n#ifdef DYNLOAD\n    uc_dyn_free();\n#endif\n    ", "project": "unicorn", "hash": 168497872221435357783029447196663285853, "size": 25, "commit_id": "bf1713d9e011b55ca1f502a6779fc4722b4bb077", "message": "Add arm ite blocks samples from #853 (#1381)", "target": 0, "dataset": "other", "idx": 476899}
{"func": "\twhile ((match[end] != 0) && (match[end] != '|')) {\n\t    if (match[end] == '[') {\n\t        end = skipPredicate(match, end);\n\t\tif (end <= 0) {\n\t\t    xsltTransformError(NULL, style, inst,\n\t\t                       \"key pattern is malformed: %s\",\n\t\t\t\t       key->match);\n\t\t    if (style != NULL) style->errors++;\n\t\t    goto error;\n\t\t}\n\t    } else\n\t\tend++;\n\t}\n\tif (current == end) {\n\t    xsltTransformError(NULL, style, inst,\n\t\t\t       \"key pattern is empty\\n\");\n\t    if (style != NULL) style->errors++;\n\t    goto error;\n\t}\n\tif (match[start] != '/') {\n\t    pattern = xmlStrcat(pattern, (xmlChar *)\"//\");\n\tif (match[end] == '|') {\n\t    pattern = xmlStrcat(pattern, (xmlChar *)\"|\");\n\t    end++;\n\t}\n\tcurrent = end;\n    }\n#ifdef WITH_XSLT_DEBUG_KEYS\n    xsltGenericDebug(xsltGenericDebugContext,\n\t\"   resulting pattern %s\\n\", pattern);\n#endif\n    /*\n#else\n    key->comp = xsltXPathCompile(style, pattern);\n#endif\n    if (key->comp == NULL) {\n\txsltTransformError(NULL, style, inst,\n\t\t\"xsl:key : XPath pattern compilation failed '%s'\\n\",\n\t\t         pattern);\n\tif (style != NULL) style->errors++;\n    }\n#ifdef XML_XPATH_NOVAR\n    key->usecomp = xsltXPathCompileFlags(style, use, XML_XPATH_NOVAR);\n#else\n    key->usecomp = xsltXPathCompile(style, use);\n#endif\n    if (key->usecomp == NULL) {\n\txsltTransformError(NULL, style, inst,\n\t\t\"xsl:key : XPath pattern compilation failed '%s'\\n\",\n\t\t         use);\n\tif (style != NULL) style->errors++;\n    }\n\n    /*", "project": "libxslt", "hash": 70118853582158048870420962848114929649, "size": 130, "commit_id": "dc11b6b379a882418093ecc8adf11f6166682e8d", "message": "Fix crash with empty xsl:key/@match attribute\n\nSee https://bugzilla.gnome.org/show_bug.cgi?id=685328\n\nAlso improve some xsl:key error messages.", "target": 1, "dataset": "other", "idx": 214313}
{"func": "\twhile ((match[end] != 0) && (match[end] != '|')) {\n\t    if (match[end] == '[') {\n\t        end = skipPredicate(match, end);\n\t\tif (end <= 0) {\n\t\t    xsltTransformError(NULL, style, inst,\n\t\t        \"xsl:key : 'match' pattern is malformed: %s\",\n\t\t        key->match);\n\t\t    if (style != NULL) style->errors++;\n\t\t    goto error;\n\t\t}\n\t    } else\n\t\tend++;\n\t}\n\tif (current == end) {\n\t    xsltTransformError(NULL, style, inst,\n\t\t\t       \"xsl:key : 'match' pattern is empty\\n\");\n\t    if (style != NULL) style->errors++;\n\t    goto error;\n\t}\n\tif (match[start] != '/') {\n\t    pattern = xmlStrcat(pattern, (xmlChar *)\"//\");\n\t    pattern = xmlStrcat(pattern, (xmlChar *)\"|\");\n\t    end++;\n\t}\n\tcurrent = end;\n    }\n    if (pattern == NULL) {\n        xsltTransformError(NULL, style, inst,\n                           \"xsl:key : 'match' pattern is empty\\n\");\n        if (style != NULL) style->errors++;\n        goto error;\n    }\n#ifdef WITH_XSLT_DEBUG_KEYS\n    xsltGenericDebug(xsltGenericDebugContext,\n\t\"   resulting pattern %s\\n\", pattern);\n#endif\n    /*\n#else\n    key->comp = xsltXPathCompile(style, pattern);\n#endif\n    if (key->comp == NULL) {\n\txsltTransformError(NULL, style, inst,\n\t\t\"xsl:key : 'match' pattern compilation failed '%s'\\n\",\n\t\t         pattern);\n\tif (style != NULL) style->errors++;\n    }\n#ifdef XML_XPATH_NOVAR\n    key->usecomp = xsltXPathCompileFlags(style, use, XML_XPATH_NOVAR);\n#else\n    key->usecomp = xsltXPathCompile(style, use);\n#endif\n    if (key->usecomp == NULL) {\n\txsltTransformError(NULL, style, inst,\n\t\t\"xsl:key : 'use' expression compilation failed '%s'\\n\",\n\t\t         use);\n\tif (style != NULL) style->errors++;\n    }\n\n    /*", "project": "libxslt", "hash": 54032911179374397987084442578242302169, "size": 136, "commit_id": "dc11b6b379a882418093ecc8adf11f6166682e8d", "message": "Fix crash with empty xsl:key/@match attribute\n\nSee https://bugzilla.gnome.org/show_bug.cgi?id=685328\n\nAlso improve some xsl:key error messages.", "target": 0, "dataset": "other", "idx": 477535}
{"func": "\t * This check might also be called for unencrypted connections\n\t * that have no key size requirements. Ensure that the link is\n\t * actually encrypted before enforcing a key size.\n\t */\n\treturn (!test_bit(HCI_CONN_ENCRYPT, &hcon->flags) ||\n\t\thcon->enc_key_size > HCI_MIN_ENC_KEY_SIZE);\n}", "project": "linux", "hash": 212269271340075918698506153323878239453, "size": 14, "commit_id": "eca94432934fe5f141d084f2e36ee2c0e614cc04", "message": "Bluetooth: Fix faulty expression for minimum encryption key size check\n\nFix minimum encryption key size check so that HCI_MIN_ENC_KEY_SIZE is\nalso allowed as stated in the comment.\n\nThis bug caused connection problems with devices having maximum\nencryption key size of 7 octets (56-bit).\n\nFixes: 693cd8ce3f88 (\"Bluetooth: Fix regression with minimum encryption key size alignment\")\nBugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=203997\nSigned-off-by: Matias Karhumaa <matias.karhumaa@gmail.com>\nCc: stable@vger.kernel.org\nSigned-off-by: Marcel Holtmann <marcel@holtmann.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 1, "dataset": "other", "idx": 214365}
{"func": "\t * This check might also be called for unencrypted connections\n\t * that have no key size requirements. Ensure that the link is\n\t * actually encrypted before enforcing a key size.\n\t */\n\treturn (!test_bit(HCI_CONN_ENCRYPT, &hcon->flags) ||\n\t\thcon->enc_key_size >= HCI_MIN_ENC_KEY_SIZE);\n}", "project": "linux", "hash": 319380273204740789729762662276611703691, "size": 14, "commit_id": "eca94432934fe5f141d084f2e36ee2c0e614cc04", "message": "Bluetooth: Fix faulty expression for minimum encryption key size check\n\nFix minimum encryption key size check so that HCI_MIN_ENC_KEY_SIZE is\nalso allowed as stated in the comment.\n\nThis bug caused connection problems with devices having maximum\nencryption key size of 7 octets (56-bit).\n\nFixes: 693cd8ce3f88 (\"Bluetooth: Fix regression with minimum encryption key size alignment\")\nBugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=203997\nSigned-off-by: Matias Karhumaa <matias.karhumaa@gmail.com>\nCc: stable@vger.kernel.org\nSigned-off-by: Marcel Holtmann <marcel@holtmann.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "target": 0, "dataset": "other", "idx": 477978}
{"func": "\n    /* The tree size was computed when writing the file, so that we can\n     * allocate it as one long block. <nodecount> */\n    len = get4c(fd);\n    if (len < 0)\n\treturn SP_TRUNCERROR;\n    if (len > 0)\n    {\n\t/* Allocate the byte array. */\n\tbp = lalloc((long_u)len, TRUE);\n\tif (bp == NULL)", "project": "vim", "hash": 299704516071114356319960105952359505097, "size": 38, "commit_id": "399c297aa93afe2c0a39e2a1b3f972aebba44c9d", "message": "patch 8.0.0322: possible overflow with corrupted spell file\n\nProblem:    Possible overflow with spell file where the tree length is\n            corrupted.\nSolution:   Check for an invalid length (suggested by shqking)", "target": 1, "dataset": "other", "idx": 214366}
{"func": "    /* The tree size was computed when writing the file, so that we can\n     * allocate it as one long block. <nodecount> */\n    len = get4c(fd);\n    if (len < 0)\n\treturn SP_TRUNCERROR;\n    if (len >= 0x3ffffff)\n\t/* Invalid length, multiply with sizeof(int) would overflow. */\n\treturn SP_FORMERROR;\n    if (len > 0)\n    {\n\t/* Allocate the byte array. */\n\tbp = lalloc((long_u)len, TRUE);\n\tif (bp == NULL)", "project": "vim", "hash": 297151306078472778214889535736355052234, "size": 41, "commit_id": "399c297aa93afe2c0a39e2a1b3f972aebba44c9d", "message": "patch 8.0.0322: possible overflow with corrupted spell file\n\nProblem:    Possible overflow with spell file where the tree length is\n            corrupted.\nSolution:   Check for an invalid length (suggested by shqking)", "target": 0, "dataset": "other", "idx": 478041}
{"func": "    {\n      addinfostatus = ares__parse_into_addrinfo(abuf, alen, hquery->ai);\n    }\n  else if (status == ARES_EDESTRUCTION)\n    {\n      end_hquery(hquery, status);\n    }\n\n  if (!hquery->remaining)\n    {\n      if (addinfostatus != ARES_SUCCESS)", "project": "c-ares", "hash": 338428520293637615587127648198461687166, "size": 41, "commit_id": "1cc7e83c3bdfaafbc5919c95025592d8de3a170e", "message": "Prevent possible double-free in ares_getaddrinfo() if ares_destroy() is called\n\nIn the event that ares_destroy() is called prior to ares_getaddrinfo() completing,\nit would result in an invalid read and double-free due to calling end_hquery() twice.\n\nReported By: Jann Horn @ Google Project Zero", "target": 1, "dataset": "other", "idx": 214405}
{"func": "      addinfostatus = ares__parse_into_addrinfo(abuf, alen, hquery->ai);\n    }\n  else if (status == ARES_EDESTRUCTION)\n    {\n      end_hquery(hquery, status);\n      return;\n    }\n\n  if (!hquery->remaining)\n    {\n      if (addinfostatus != ARES_SUCCESS)", "project": "c-ares", "hash": 65112642692504141750909294658666282062, "size": 42, "commit_id": "1cc7e83c3bdfaafbc5919c95025592d8de3a170e", "message": "Prevent possible double-free in ares_getaddrinfo() if ares_destroy() is called\n\nIn the event that ares_destroy() is called prior to ares_getaddrinfo() completing,\nit would result in an invalid read and double-free due to calling end_hquery() twice.\n\nReported By: Jann Horn @ Google Project Zero", "target": 0, "dataset": "other", "idx": 478396}
{"func": "        goto fin;\n    }\n    image->x0 = (OPJ_UINT32)params->image_offset_x0;\n    image->y0 = (OPJ_UINT32)params->image_offset_y0;\n    image->x1 = (OPJ_UINT32)(image->x0 + (width  - 1) * (OPJ_UINT32)\n                             params->subsampling_dx + 1 + image->x0);\n    image->y1 = (OPJ_UINT32)(image->y0 + (height - 1) * (OPJ_UINT32)\n                             params->subsampling_dy + 1 + image->y0);\n\n    row32s = (OPJ_INT32 *)malloc((size_t)width * nr_comp * sizeof(OPJ_INT32));\n    if (row32s == NULL) {\n        goto fin;\n    }", "project": "openjpeg", "hash": 316530901664567822353293166039183109395, "size": 202, "commit_id": "b2072402b7e14d22bba6fb8cde2a1e9996e9a919", "message": "pngtoimage(): fix wrong computation of x1,y1 if -d option is used, that would result in a heap buffer overflow (fixes #1284)", "target": 1, "dataset": "other", "idx": 214409}
{"func": "        goto fin;\n    }\n    image->x0 = (OPJ_UINT32)params->image_offset_x0;\n    image->y0 = (OPJ_UINT32)params->image_offset_y0;\n    image->x1 = (OPJ_UINT32)(image->x0 + (width  - 1) * (OPJ_UINT32)\n                             params->subsampling_dx + 1);\n    image->y1 = (OPJ_UINT32)(image->y0 + (height - 1) * (OPJ_UINT32)\n                             params->subsampling_dy + 1);\n\n    row32s = (OPJ_INT32 *)malloc((size_t)width * nr_comp * sizeof(OPJ_INT32));\n    if (row32s == NULL) {\n        goto fin;\n    }", "project": "openjpeg", "hash": 111527323522954682670053156159153319598, "size": 202, "commit_id": "b2072402b7e14d22bba6fb8cde2a1e9996e9a919", "message": "pngtoimage(): fix wrong computation of x1,y1 if -d option is used, that would result in a heap buffer overflow (fixes #1284)", "target": 0, "dataset": "other", "idx": 478420}
{"func": "\tstruct can_frame cf;\n\tint i, tmp;\n\tu32 tmpid;\n\tchar *cmd = sl->rbuff;\n\n\tcf.can_id = 0;\n\n\tswitch (*cmd) {\n\tcase 'r':\n\t\tcf.can_id = CAN_RTR_FLAG;\n\t\t/* fallthrough */\n\tif (cf.can_dlc >= '0' && cf.can_dlc < '9')\n\t\tcf.can_dlc -= '0';\n\telse\n\t\treturn;\n\n\t*(u64 *) (&cf.data) = 0; /* clear payload */\n\n\t/* RTR frames may have a dlc > 0 but they never have any data bytes */\n\tif (!(cf.can_id & CAN_RTR_FLAG)) {\n\t\tfor (i = 0; i < cf.can_dlc; i++) {\n\t\t\ttmp = hex_to_bin(*cmd++);\n\t\t\tif (tmp < 0)", "project": "linux", "hash": 21373888120245067949221535188537720172, "size": 83, "commit_id": "b9258a2cece4ec1f020715fe3554bc2e360f6264", "message": "slcan: Don't transmit uninitialized stack data in padding\n\nstruct can_frame contains some padding which is not explicitly zeroed in\nslc_bump. This uninitialized data will then be transmitted if the stack\ninitialization hardening feature is not enabled (CONFIG_INIT_STACK_ALL).\n\nThis commit just zeroes the whole struct including the padding.\n\nSigned-off-by: Richard Palethorpe <rpalethorpe@suse.com>\nFixes: a1044e36e457 (\"can: add slcan driver for serial/USB-serial CAN adapters\")\nReviewed-by: Kees Cook <keescook@chromium.org>\nCc: linux-can@vger.kernel.org\nCc: netdev@vger.kernel.org\nCc: security@kernel.org\nCc: wg@grandegger.com\nCc: mkl@pengutronix.de\nCc: davem@davemloft.net\nAcked-by: Marc Kleine-Budde <mkl@pengutronix.de>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 1, "dataset": "other", "idx": 214425}
{"func": "\tstruct can_frame cf;\n\tint i, tmp;\n\tu32 tmpid;\n\tchar *cmd = sl->rbuff;\n\n\tmemset(&cf, 0, sizeof(cf));\n\n\tswitch (*cmd) {\n\tcase 'r':\n\t\tcf.can_id = CAN_RTR_FLAG;\n\t\t/* fallthrough */\n\t/* get can_dlc from sanitized ASCII value */\n\tif (cf.can_dlc >= '0' && cf.can_dlc < '9')\n\t\tcf.can_dlc -= '0';\n\telse\n\t\treturn;\n\n\t/* RTR frames may have a dlc > 0 but they never have any data bytes */\n\tif (!(cf.can_id & CAN_RTR_FLAG)) {\n\t\tfor (i = 0; i < cf.can_dlc; i++) {\n\t\t\ttmp = hex_to_bin(*cmd++);\n\t\t\tif (tmp < 0)", "project": "linux", "hash": 325057014109399130748004599861744385783, "size": 81, "commit_id": "b9258a2cece4ec1f020715fe3554bc2e360f6264", "message": "slcan: Don't transmit uninitialized stack data in padding\n\nstruct can_frame contains some padding which is not explicitly zeroed in\nslc_bump. This uninitialized data will then be transmitted if the stack\ninitialization hardening feature is not enabled (CONFIG_INIT_STACK_ALL).\n\nThis commit just zeroes the whole struct including the padding.\n\nSigned-off-by: Richard Palethorpe <rpalethorpe@suse.com>\nFixes: a1044e36e457 (\"can: add slcan driver for serial/USB-serial CAN adapters\")\nReviewed-by: Kees Cook <keescook@chromium.org>\nCc: linux-can@vger.kernel.org\nCc: netdev@vger.kernel.org\nCc: security@kernel.org\nCc: wg@grandegger.com\nCc: mkl@pengutronix.de\nCc: davem@davemloft.net\nAcked-by: Marc Kleine-Budde <mkl@pengutronix.de>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "target": 0, "dataset": "other", "idx": 478530}
{"func": "\tunsigned int pgoff;\n\tunsigned int pgend;\n\tvoid *kaddr;\n\n\tmaxlen = xdr->buf->page_len;\n\tif (base >= maxlen) {\n\t\tbase = maxlen;\n\t\tmaxlen = 0;\n\t} else\n\t\tmaxlen -= base;\n\tif (len > maxlen)\n\t\tlen = maxlen;\n\n\txdr_stream_page_set_pos(xdr, base);", "project": "linux", "hash": 274422321753562622361315728373118052641, "size": 35, "commit_id": "6d1c0f3d28f98ea2736128ed3e46821496dc3a8c", "message": "sunrpc: Avoid a KASAN slab-out-of-bounds bug in xdr_set_page_base()\n\nThis seems to happen fairly easily during READ_PLUS testing on NFS v4.2.\nI found that we could end up accessing xdr->buf->pages[pgnr] with a pgnr\ngreater than the number of pages in the array. So let's just return\nearly if we're setting base to a point at the end of the page data and\nlet xdr_set_tail_base() handle setting up the buffer pointers instead.\n\nSigned-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>\nFixes: 8d86e373b0ef (\"SUNRPC: Clean up helpers xdr_set_iov() and xdr_set_page_base()\")\nSigned-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>", "target": 1, "dataset": "other", "idx": 214890}
{"func": "\tunsigned int pgoff;\n\tunsigned int pgend;\n\tvoid *kaddr;\n\n\tmaxlen = xdr->buf->page_len;\n\tif (base >= maxlen)\n\t\treturn 0;\n\telse\n\t\tmaxlen -= base;\n\tif (len > maxlen)\n\t\tlen = maxlen;\n\n\txdr_stream_page_set_pos(xdr, base);", "project": "linux", "hash": 272280587655938095471660267479071957383, "size": 34, "commit_id": "6d1c0f3d28f98ea2736128ed3e46821496dc3a8c", "message": "sunrpc: Avoid a KASAN slab-out-of-bounds bug in xdr_set_page_base()\n\nThis seems to happen fairly easily during READ_PLUS testing on NFS v4.2.\nI found that we could end up accessing xdr->buf->pages[pgnr] with a pgnr\ngreater than the number of pages in the array. So let's just return\nearly if we're setting base to a point at the end of the page data and\nlet xdr_set_tail_base() handle setting up the buffer pointers instead.\n\nSigned-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>\nFixes: 8d86e373b0ef (\"SUNRPC: Clean up helpers xdr_set_iov() and xdr_set_page_base()\")\nSigned-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>", "target": 0, "dataset": "other", "idx": 481051}
{"func": "\n      for(v = tn->telnet_vars; v; v = v->next) {\n        size_t tmplen = (strlen(v->data) + 1);\n        /* Add the variable only if it fits */\n        if(len + tmplen < (int)sizeof(temp)-6) {\n          if(sscanf(v->data, \"%127[^,],%127s\", varname, varval)) {\n            msnprintf((char *)&temp[len], sizeof(temp) - len,\n                      \"%c%s%c%s\", CURL_NEW_ENV_VAR, varname,\n                      CURL_NEW_ENV_VALUE, varval);\n            len += tmplen;\n          }", "project": "curl", "hash": 294290589936532991240501967275180184556, "size": 69, "commit_id": "39ce47f219b09c380b81f89fe54ac586c8db6bde", "message": "telnet: check sscanf() for correct number of matches\n\nCVE-2021-22898\n\nBug: https://curl.se/docs/CVE-2021-22898.html", "target": 1, "dataset": "other", "idx": 214926}
{"func": "\n      for(v = tn->telnet_vars; v; v = v->next) {\n        size_t tmplen = (strlen(v->data) + 1);\n        /* Add the variable only if it fits */\n        if(len + tmplen < (int)sizeof(temp)-6) {\n          if(sscanf(v->data, \"%127[^,],%127s\", varname, varval) == 2) {\n            msnprintf((char *)&temp[len], sizeof(temp) - len,\n                      \"%c%s%c%s\", CURL_NEW_ENV_VAR, varname,\n                      CURL_NEW_ENV_VALUE, varval);\n            len += tmplen;\n          }", "project": "curl", "hash": 162263181786713465891376018168428301129, "size": 69, "commit_id": "39ce47f219b09c380b81f89fe54ac586c8db6bde", "message": "telnet: check sscanf() for correct number of matches\n\nCVE-2021-22898\n\nBug: https://curl.se/docs/CVE-2021-22898.html", "target": 0, "dataset": "other", "idx": 481492}
{"func": "\t\t\t\t\t\tFORCC for (int j = 0; j < 3; j++) cm[c][j] =\n\t\t\t\t\t\t\ttiff_ifd[sidx].dng_color[colidx].colormatrix[c][j];\n\n\t\t\t\t\t\tif (calidx[colidx] == sidx)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tfor (int i = 0; i < colors; i++)\n\t\t\t\t\t\t\t\tFORCC\n\t\t\t\t\t\t\t\tcc[i][c] = tiff_ifd[sidx].dng_color[colidx].calibration[i][c];\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (abidx == sidx)\n\t\t\t\t\t\t\tfor (int i = 0; i < colors; i++)\n\t\t\t\t\t\t\t\tFORCC cc[i][c] *= tiff_ifd[sidx].dng_levels.analogbalance[i];\n\t\t\t\t\t\tint j;\n\t\t\t\t\t\tFORCC for (int i = 0; i < 3; i++) for (cam_xyz[c][i] = j = 0;\n\t\t\t\t\t\t\tj < colors; j++)\n\t\t\t\t\t\t\tcam_xyz[c][i] +=\n\t\t\t\t\t\t\tcc[c][j] * cm[j][i]; // add AsShotXY later * xyz[i];\n\t\t\t\t\t\tcam_xyz_coeff(cmatrix, cam_xyz);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Special case, per_channel blacks in RepeatDim, average for per-channel */\n\t\t\tint csum[4] = { 0,0,0,0 }, ccount[4] = { 0,0,0,0 };\n\t\t\tint i = 6;\n\t\t\tfor (unsigned row = 0; row < imgdata.color.dng_levels.dng_cblack[4]; row++)\n\t\t\t\tfor (unsigned col = 0; col < imgdata.color.dng_levels.dng_cblack[5]; col++)\n\t\t\t\t\tfor (unsigned c = 0; c < tiff_samples; c++)\n\t\t\t\t\t{\n\t\t\t\t\t\tcsum[c] += imgdata.color.dng_levels.dng_cblack[i];\n\t\t\t\t\t\tccount[c]++;\n\t\t\t\t\t\ti++;\n\t\t\t\t\t}", "project": "LibRaw", "hash": 208600525321410359156531031353922710019, "size": 344, "commit_id": "4feaed4dea636cee4fee010f615881ccf76a096d", "message": "limit loops to MIN(colors,4) in dng fields parser", "target": 1, "dataset": "other", "idx": 214990}
{"func": "\t\t\t\t\t\tFORCC for (int j = 0; j < 3; j++) cm[c][j] =\n\t\t\t\t\t\t\ttiff_ifd[sidx].dng_color[colidx].colormatrix[c][j];\n\n\t\t\t\t\t\tif (calidx[colidx] == sidx)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tfor (int i = 0; i < colors && i < 4; i++)\n\t\t\t\t\t\t\t\tFORCC\n\t\t\t\t\t\t\t\tcc[i][c] = tiff_ifd[sidx].dng_color[colidx].calibration[i][c];\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (abidx == sidx)\n\t\t\t\t\t\t\tfor (int i = 0; i < colors && i < 4; i++)\n\t\t\t\t\t\t\t\tFORCC cc[i][c] *= tiff_ifd[sidx].dng_levels.analogbalance[i];\n\t\t\t\t\t\tint j;\n\t\t\t\t\t\tFORCC for (int i = 0; i < 3; i++) \n                            for (cam_xyz[c][i] = j = 0; j < colors && j < 4; j++)\n\t\t\t\t\t\t\t    cam_xyz[c][i] +=\n\t\t\t\t\t\t\t        cc[c][j] * cm[j][i]; // add AsShotXY later * xyz[i];\n\t\t\t\t\t\tcam_xyz_coeff(cmatrix, cam_xyz);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* Special case, per_channel blacks in RepeatDim, average for per-channel */\n\t\t\tint csum[4] = { 0,0,0,0 }, ccount[4] = { 0,0,0,0 };\n\t\t\tint i = 6;\n\t\t\tfor (unsigned row = 0; row < imgdata.color.dng_levels.dng_cblack[4]; row++)\n\t\t\t\tfor (unsigned col = 0; col < imgdata.color.dng_levels.dng_cblack[5]; col++)\n\t\t\t\t\tfor (unsigned c = 0; c < tiff_samples && c < 4; c++)\n\t\t\t\t\t{\n\t\t\t\t\t\tcsum[c] += imgdata.color.dng_levels.dng_cblack[i];\n\t\t\t\t\t\tccount[c]++;\n\t\t\t\t\t\ti++;\n\t\t\t\t\t}", "project": "LibRaw", "hash": 37186369853422378844569840658555284518, "size": 344, "commit_id": "4feaed4dea636cee4fee010f615881ccf76a096d", "message": "limit loops to MIN(colors,4) in dng fields parser", "target": 0, "dataset": "other", "idx": 482359}
{"func": "      *hue=2.0+(QuantumScale*blue-QuantumScale*red)/c;\n    else\n      *hue=4.0+(QuantumScale*red-QuantumScale*green)/c;\n  *hue*=60.0/360.0;\n  if (*lightness <= 0.5)\n    *saturation=c/(2.0*(*lightness));\n  else\n    *saturation=c/(2.0-2.0*(*lightness));\n}", "project": "ImageMagick6", "hash": 64316640286512378958089004373682516754, "size": 43, "commit_id": "64c0cc234280544dabacc2b28017521851deebde", "message": "https://github.com/ImageMagick/ImageMagick/issues/3321", "target": 1, "dataset": "other", "idx": 215048}
{"func": "      *hue=2.0+(QuantumScale*blue-QuantumScale*red)/c;\n    else\n      *hue=4.0+(QuantumScale*red-QuantumScale*green)/c;\n  *hue*=60.0/360.0;\n  if (*lightness <= 0.5)\n    *saturation=c*PerceptibleReciprocal(2.0*(*lightness));\n  else\n    *saturation=c*PerceptibleReciprocal(2.0-2.0*(*lightness));\n}", "project": "ImageMagick6", "hash": 224060726924583917833900427488966825442, "size": 43, "commit_id": "64c0cc234280544dabacc2b28017521851deebde", "message": "https://github.com/ImageMagick/ImageMagick/issues/3321", "target": 0, "dataset": "other", "idx": 482921}
{"func": "\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 162375462985061810672821165302100186277, "size": 22, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 215059}
{"func": "\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 12504623577116996886674834703313256811, "size": 23, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 482955}
{"func": "struct clock_source *dce80_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 47031023642369501827563085012832170420, "size": 22, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 215060}
{"func": "struct clock_source *dce100_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 12504623577116996886674834703313256811, "size": 23, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 482955}
{"func": "struct clock_source *dcn10_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 271763125377938091679854459980248910171, "size": 22, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 215061}
{"func": "struct clock_source *dce100_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 12504623577116996886674834703313256811, "size": 23, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 482955}
{"func": "struct clock_source *dce112_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 199589559251523108029584750650448513021, "size": 22, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 215062}
{"func": "struct clock_source *dce100_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 12504623577116996886674834703313256811, "size": 23, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 482955}
{"func": "struct clock_source *dce110_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 31654476780755587973778973098883413139, "size": 22, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 215063}
{"func": "struct clock_source *dce100_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce110_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 12504623577116996886674834703313256811, "size": 23, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 482955}
{"func": "struct clock_source *dcn20_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dcn20_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 177672847907118008764814032612740810929, "size": 22, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 215064}
{"func": "struct clock_source *dcn10_clock_source_create(\n\tstruct dc_context *ctx,\n\tstruct dc_bios *bios,\n\tenum clock_source_id id,\n\tconst struct dce110_clk_src_regs *regs,\n\tbool dp_clk_src)\n{\n\tstruct dce110_clk_src *clk_src =\n\t\tkzalloc(sizeof(struct dce110_clk_src), GFP_KERNEL);\n\n\tif (!clk_src)\n\t\treturn NULL;\n\n\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\tregs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 139266790086854436831409090331754192145, "size": 23, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 483006}
{"func": "\tif (dce112_clk_src_construct(clk_src, ctx, bios, id,\n\t\t\t\t     regs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 288594546535569243400673837265126927733, "size": 22, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 1, "dataset": "other", "idx": 215065}
{"func": "\t\t\t\t     regs, &cs_shift, &cs_mask)) {\n\t\tclk_src->base.dp_clk_src = dp_clk_src;\n\t\treturn &clk_src->base;\n\t}\n\n\tkfree(clk_src);\n\tBREAK_TO_DEBUGGER();\n\treturn NULL;\n}", "project": "linux", "hash": 326037735483413902171736333958270598485, "size": 23, "commit_id": "055e547478a11a6360c7ce05e2afc3e366968a12", "message": "drm/amd/display: memory leak\n\nIn dcn*_clock_source_create when dcn20_clk_src_construct fails allocated\nclk_src needs release.\n\nSigned-off-by: Navid Emamdoost <navid.emamdoost@gmail.com>\nSigned-off-by: Alex Deucher <alexander.deucher@amd.com>", "target": 0, "dataset": "other", "idx": 483005}
{"func": "\t\tover_bar = 0;\n\t\tif (over_time < 0) {\n\t\t\terror(1, s, \"Erroneous end of voice overlap\");\n\t\t\treturn;\n\t\t}\n\t\tif (p_voice->time != over_mxtime)\n\t\t\terror(1, s, tx_wrong_dur);\n\t\tcurvoice = &voice_tb[over_voice];\n\t\tover_mxtime = 0;\n\t\tover_voice = -1;\n\t\tover_time = -1;\n\t\treturn;\n\t}", "project": "abcm2ps", "hash": 89478143332361880739890507095670885956, "size": 133, "commit_id": "2f56e1179cab6affeb8afa9d6c324008fe40d8e3", "message": "fix: array overflow when wrong duration in voice overlay\n\nIssue #83,", "target": 1, "dataset": "other", "idx": 215165}
{"func": "\t\tover_bar = 0;\n\t\tif (over_time < 0) {\n\t\t\terror(1, s, \"Erroneous end of voice overlap\");\n\t\t\treturn;\n\t\t}\n\t\tcurvoice = &voice_tb[over_voice];\n\t\tif (p_voice->time != over_mxtime) {\n\t\t\terror(1, s, tx_wrong_dur);\n\t\t\tif (p_voice->time > over_mxtime)\n\t\t\t\tcurvoice->time = p_voice->time;\n\t\t\telse\n\t\t\t\tp_voice->time = curvoice->time;\n\t\t}\n\t\tover_mxtime = 0;\n\t\tover_voice = -1;\n\t\tover_time = -1;\n\t\treturn;\n\t}", "project": "abcm2ps", "hash": 106849796852152645656287252191337327464, "size": 138, "commit_id": "2f56e1179cab6affeb8afa9d6c324008fe40d8e3", "message": "fix: array overflow when wrong duration in voice overlay\n\nIssue #83,", "target": 0, "dataset": "other", "idx": 484351}
{"func": "static int exif_process_IFD_in_MAKERNOTE(image_info_type *ImageInfo, char * value_ptr, int value_len, char *offset_base, size_t IFDlength, size_t displacement TSRMLS_DC)\n{\n\tint de, i=0, section_index = SECTION_MAKERNOTE;\n\tint NumDirEntries, old_motorola_intel, offset_diff;\n\tconst maker_note_type *maker_note;\n\tchar *dir_start;\n\n\tfor (i=0; i<=sizeof(maker_note_array)/sizeof(maker_note_type); i++) {\n\t\tif (i==sizeof(maker_note_array)/sizeof(maker_note_type)) {\n#ifdef EXIF_DEBUG\n\t\t\texif_error_docref(NULL EXIFERR_CC, ImageInfo, E_NOTICE, \"No maker note data found. Detected maker: %s (length = %d)\", ImageInfo->make, strlen(ImageInfo->make));\n\n\tNumDirEntries = php_ifd_get16u(dir_start, ImageInfo->motorola_intel);\n\n\tswitch (maker_note->offset_mode) {\n\t\tcase MN_OFFSET_MAKER:\n\t\t\toffset_base = value_ptr;\n\t\t\tbreak;\n\t\tcase MN_OFFSET_GUESS:\n\t\t\tif (maker_note->offset + 10 + 4 >= value_len) {\n\t\t\t\t/* Can not read dir_start+10 since it's beyond value end */\n\t\t\t\texif_error_docref(\"exif_read_data#error_ifd\" EXIFERR_CC, ImageInfo, E_WARNING, \"IFD data too short: 0x%04X\", value_len);\n#endif\n\t\t\tif (offset_diff < 0 || offset_diff >= value_len ) {\n\t\t\t\texif_error_docref(\"exif_read_data#error_ifd\" EXIFERR_CC, ImageInfo, E_WARNING, \"IFD data bad offset: 0x%04X length 0x%04X\", offset_diff, value_len);\n\t\t\t\treturn FALSE;\n\t\t\t}\n\t\t\toffset_base = value_ptr + offset_diff;\n\t\t\tbreak;\n\t\tdefault:\n\t\tcase MN_OFFSET_NORMAL:\n\t\t\tbreak;\n\t}\n\t\treturn FALSE;\n\t}\n\n\tfor (de=0;de<NumDirEntries;de++) {\n\t\tif (!exif_process_IFD_TAG(ImageInfo, dir_start + 2 + 12 * de,\n\t\t\t\t\t\t\t\t  offset_base, IFDlength, displacement, section_index, 0, maker_note->tag_table TSRMLS_CC)) {\n\t\t\treturn FALSE;\n\t\t}\n\t}\n\tImageInfo->motorola_intel = old_motorola_intel;\n/*\tNextDirOffset (must be NULL) = php_ifd_get32u(dir_start+2+12*de, ImageInfo->motorola_intel);*/", "project": "php-src", "hash": 22189568364832264784102840174267992879, "size": 100, "commit_id": "3462efa386f26d343062094514af604c29e3edce", "message": "Fix bug #76557: heap-buffer-overflow (READ of size 48) while reading exif data\n\nUse MAKERNOTE length as data size.", "target": 1, "dataset": "other", "idx": 215213}
{"func": "{\n\tint de, i=0, section_index = SECTION_MAKERNOTE;\n\tint NumDirEntries, old_motorola_intel, offset_diff;\n\tconst maker_note_type *maker_note;\n\tchar *dir_start;\n\tint data_len;\n\n\tfor (i=0; i<=sizeof(maker_note_array)/sizeof(maker_note_type); i++) {\n\t\tif (i==sizeof(maker_note_array)/sizeof(maker_note_type)) {\n#ifdef EXIF_DEBUG\n\t\t\texif_error_docref(NULL EXIFERR_CC, ImageInfo, E_NOTICE, \"No maker note data found. Detected maker: %s (length = %d)\", ImageInfo->make, strlen(ImageInfo->make));\n\tNumDirEntries = php_ifd_get16u(dir_start, ImageInfo->motorola_intel);\n\n\tswitch (maker_note->offset_mode) {\n\t\tcase MN_OFFSET_MAKER:\n\t\t\toffset_base = value_ptr;\n\t\t\tdata_len = value_len;\n\t\t\tbreak;\n\t\tcase MN_OFFSET_GUESS:\n\t\t\tif (maker_note->offset + 10 + 4 >= value_len) {\n\t\t\t\t/* Can not read dir_start+10 since it's beyond value end */\n\t\t\t\texif_error_docref(\"exif_read_data#error_ifd\" EXIFERR_CC, ImageInfo, E_WARNING, \"IFD data too short: 0x%04X\", value_len);\n\t\t\tif (offset_diff < 0 || offset_diff >= value_len ) {\n\t\t\t\texif_error_docref(\"exif_read_data#error_ifd\" EXIFERR_CC, ImageInfo, E_WARNING, \"IFD data bad offset: 0x%04X length 0x%04X\", offset_diff, value_len);\n\t\t\t\treturn FALSE;\n\t\t\t}\n\t\t\toffset_base = value_ptr + offset_diff;\n\t\t\tdata_len = value_len - offset_diff;\n\t\t\tbreak;\n\t\tdefault:\n\t\tcase MN_OFFSET_NORMAL:\n\t\t\tbreak;\n\t}\n\t\treturn FALSE;\n\t}\n\n\tfor (de=0;de<NumDirEntries;de++) {\n\t\tif (!exif_process_IFD_TAG(ImageInfo, dir_start + 2 + 12 * de,\n\t\t\t\t\t\t\t\t  offset_base, data_len, displacement, section_index, 0, maker_note->tag_table TSRMLS_CC)) {\n\t\t\treturn FALSE;\n\t\t}\n\t}\n\tImageInfo->motorola_intel = old_motorola_intel;\n/*\tNextDirOffset (must be NULL) = php_ifd_get32u(dir_start+2+12*de, ImageInfo->motorola_intel);*/", "project": "php-src", "hash": 179703936053344348653074411508239418076, "size": 103, "commit_id": "3462efa386f26d343062094514af604c29e3edce", "message": "Fix bug #76557: heap-buffer-overflow (READ of size 48) while reading exif data\n\nUse MAKERNOTE length as data size.", "target": 0, "dataset": "other", "idx": 485283}
{"func": "gdImagePtr gdImageCrop(gdImagePtr src, const gdRectPtr crop)\n{\n\tgdImagePtr dst;\n\tint y;\n\n\t/* check size */\n\tif (crop->width<=0 || crop->height<=0) {\n\t\treturn NULL;\n\t}\n\n\t/* allocate the requested size (could be only partially filled) */\n\tif (src->trueColor) {\n\t\tdst = gdImageCreateTrueColor(crop->width, crop->height);\n\t\tgdImageSaveAlpha(dst, 1);\n\t} else {\n\t\tdst = gdImageCreate(crop->width, crop->height);\n\t\tgdImagePaletteCopy(dst, src);\n\t}\n\tif (dst == NULL) {\n\t\treturn NULL;\n\t}\n\tdst->transparent = src->transparent;\n\n\t/* check position in the src image */\n\tif (crop->x < 0 || crop->x>=src->sx || crop->y<0 || crop->y>=src->sy) {", "project": "php-src", "hash": 121046021861658525511228885017249088421, "size": 56, "commit_id": "af09d8b96a8aacdd7d738fec81b695c1c58368f7", "message": "Fixed Bug #66815 imagecrop(): insufficient fix for NULL defer CVE-2013-7327\n\nThis amends commit 8f4a537, which aimed to correct NULL dereference because of\nmissing check of gdImageCreateTrueColor() / gdImageCreate() return value.  That\ncommit checks for negative crop rectangle width and height, but\ngdImageCreate*() can also return NULL when width * height overflows.  Hence\nNULL deref is still possible, as gdImageSaveAlpha() and gdImagePaletteCopy()\nis called before dst == NULL check.\n\nThis moves NULL check to happen right after gdImageCreate*().  It also removes\nwidth and height check before gdImageCreate*(), as the same check is done by\nimage create functions (with an extra warning).\n\nFrom thoger redhat com", "target": 1, "dataset": "other", "idx": 215215}
{"func": "gdImagePtr gdImageCrop(gdImagePtr src, const gdRectPtr crop)\n{\n\tgdImagePtr dst;\n\tint y;\n\n\t/* allocate the requested size (could be only partially filled) */\n\tif (src->trueColor) {\n\t\tdst = gdImageCreateTrueColor(crop->width, crop->height);\n\t\tif (dst == NULL) {\n\t\t\treturn NULL;\n\t\t}\n\t\tgdImageSaveAlpha(dst, 1);\n\t} else {\n\t\tdst = gdImageCreate(crop->width, crop->height);\n\t\tif (dst == NULL) {\n\t\t\treturn NULL;\n\t\t}\n\t\tgdImagePaletteCopy(dst, src);\n\t}\n\tdst->transparent = src->transparent;\n\n\t/* check position in the src image */\n\tif (crop->x < 0 || crop->x>=src->sx || crop->y<0 || crop->y>=src->sy) {", "project": "php-src", "hash": 211902895967623924199736087663142577675, "size": 54, "commit_id": "af09d8b96a8aacdd7d738fec81b695c1c58368f7", "message": "Fixed Bug #66815 imagecrop(): insufficient fix for NULL defer CVE-2013-7327\n\nThis amends commit 8f4a537, which aimed to correct NULL dereference because of\nmissing check of gdImageCreateTrueColor() / gdImageCreate() return value.  That\ncommit checks for negative crop rectangle width and height, but\ngdImageCreate*() can also return NULL when width * height overflows.  Hence\nNULL deref is still possible, as gdImageSaveAlpha() and gdImagePaletteCopy()\nis called before dst == NULL check.\n\nThis moves NULL check to happen right after gdImageCreate*().  It also removes\nwidth and height check before gdImageCreate*(), as the same check is done by\nimage create functions (with an extra warning).\n\nFrom thoger redhat com", "target": 0, "dataset": "other", "idx": 485284}
{"func": "\t\t\t\tisdn_free_channel(lp->pre_device, lp->pre_channel, ISDN_USAGE_NET);\n\t\t\t\tdrvidx = -1;\n\t\t\t\tchidx = -1;\n\t\t\t}\n\t\t}\n\t\tstrcpy(lp->msn, cfg->eaz);\n\t\tlp->pre_device = drvidx;\n\t\tlp->pre_channel = chidx;\n\t\tlp->onhtime = cfg->onhtime;\n\t\tlp->charge = cfg->charge;\n\t\tlp->l2_proto = cfg->l2_proto;", "target": 1, "cwe": ["CWE-119"], "project": "linux-2.6", "commit_id": "0f13864e5b24d9cbe18d125d41bfa4b726a82e40", "hash": 80796293720356790057810511838637183896, "size": 215, "message": "isdn: avoid copying overly-long strings\n\nAddresses http://bugzilla.kernel.org/show_bug.cgi?id=9416\n\nSigned-off-by: Karsten Keil <kkeil@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 215300}
{"func": "\t\t\t\tisdn_free_channel(lp->pre_device, lp->pre_channel, ISDN_USAGE_NET);\n\t\t\t\tdrvidx = -1;\n\t\t\t\tchidx = -1;\n\t\t\t}\n\t\t}\n\t\tstrlcpy(lp->msn, cfg->eaz, sizeof(lp->msn));\n\t\tlp->pre_device = drvidx;\n\t\tlp->pre_channel = chidx;\n\t\tlp->onhtime = cfg->onhtime;\n\t\tlp->charge = cfg->charge;\n\t\tlp->l2_proto = cfg->l2_proto;", "target": 0, "cwe": ["CWE-119"], "project": "linux-2.6", "commit_id": "0f13864e5b24d9cbe18d125d41bfa4b726a82e40", "hash": 155181909137210519302296943237798383854, "size": 215, "message": "isdn: avoid copying overly-long strings\n\nAddresses http://bugzilla.kernel.org/show_bug.cgi?id=9416\n\nSigned-off-by: Karsten Keil <kkeil@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 487418}
{"func": "\tstruct dccp_so_feat opt;\n\tu8 *val;\n\tint rc;\n\n\tif (copy_from_user(&opt, optval, sizeof(opt)))\n\t\treturn -EFAULT;\n\n\tval = kmalloc(opt.dccpsf_len, GFP_KERNEL);\n\tif (!val)\n\t\treturn -ENOMEM;\n", "target": 1, "cwe": ["CWE-189"], "project": "linux-2.6", "commit_id": "3e8a0a559c66ee9e7468195691a56fefc3589740", "hash": 235127973499082598612098670784403396457, "size": 31, "message": "dccp: change L/R must have at least one byte in the dccpsf_val field\n    \nThanks to Eugene Teo for reporting this problem.\n    \nSigned-off-by: Eugene Teo <eugenete@kernel.sg>\nSigned-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>\nSigned-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "dataset": "other", "idx": 215361}
{"func": "\tu8 *val;\n\tint rc;\n\n\tif (copy_from_user(&opt, optval, sizeof(opt)))\n\t\treturn -EFAULT;\n\t/*\n\t * rfc4340: 6.1. Change Options\n\t */\n\tif (opt.dccpsf_len < 1)\n\t\treturn -EINVAL;\n\n\tval = kmalloc(opt.dccpsf_len, GFP_KERNEL);\n\tif (!val)\n\t\treturn -ENOMEM;\n", "target": 0, "cwe": ["CWE-189"], "project": "linux-2.6", "commit_id": "3e8a0a559c66ee9e7468195691a56fefc3589740", "hash": 78779612884963753154791824378443907712, "size": 36, "message": "dccp: change L/R must have at least one byte in the dccpsf_val field\n    \nThanks to Eugene Teo for reporting this problem.\n    \nSigned-off-by: Eugene Teo <eugenete@kernel.sg>\nSigned-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>\nSigned-off-by: Gerrit Renker <gerrit@erg.abdn.ac.uk>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "dataset": "other", "idx": 488804}
{"func": "\t\t\t\t\tsizeof(struct sbni_in_stats) ))\n\t\t\terror = -EFAULT;\n\t\tbreak;\n\n\tcase  SIOCDEVRESINSTATS :\n\t\tif( current->euid != 0 )\t/* root only */\n\t\t\treturn  -EPERM;\n\t\tmemset( &nl->in_stats, 0, sizeof(struct sbni_in_stats) );\n\t\tbreak;\n\n\tcase  SIOCDEVGHWSTATE :\n\t\tif (copy_to_user( ifr->ifr_data, &flags, sizeof flags ))\n\t\t\terror = -EFAULT;\n\t\tbreak;\n\n\tcase  SIOCDEVSHWSTATE :\n\t\tif( current->euid != 0 )\t/* root only */\n\t\t\treturn  -EPERM;\n\n\t\tspin_lock( &nl->lock );\n\t\tflags = *(struct sbni_flags*) &ifr->ifr_ifru;\n\t\tif( flags.fixed_rxl )\n\t\tbreak;\n\n#ifdef CONFIG_SBNI_MULTILINE\n\n\tcase  SIOCDEVENSLAVE :\n\t\tif( current->euid != 0 )\t/* root only */\n\t\t\treturn  -EPERM;\n\n\t\tif (copy_from_user( slave_name, ifr->ifr_data, sizeof slave_name ))\n\t\t\treturn -EFAULT;\n\t\tslave_dev = dev_get_by_name(&init_net, slave_name );\n\t\t}\n\n\t\treturn  enslave( dev, slave_dev );\n\n\tcase  SIOCDEVEMANSIPATE :\n\t\tif( current->euid != 0 )\t/* root only */\n\t\t\treturn  -EPERM;\n\n\t\treturn  emancipate( dev );\n\n#endif\t/* CONFIG_SBNI_MULTILINE */", "target": 1, "cwe": ["CWE-264"], "project": "linux-2.6", "commit_id": "f2455eb176ac87081bbfc9a44b21c7cd2bc1967e", "hash": 334362285214514794810840380680596988050, "size": 85, "message": "wan: Missing capability checks in sbni_ioctl()\n\nThere are missing capability checks in the following code:\n\n1300 static int\n1301 sbni_ioctl( struct net_device  *dev,  struct ifreq  *ifr,  int  cmd)\n1302 {\n[...]\n1319     case  SIOCDEVRESINSTATS :\n1320         if( current->euid != 0 )    /* root only */\n1321             return  -EPERM;\n[...]\n1336     case  SIOCDEVSHWSTATE :\n1337         if( current->euid != 0 )    /* root only */\n1338             return  -EPERM;\n[...]\n1357     case  SIOCDEVENSLAVE :\n1358         if( current->euid != 0 )    /* root only */\n1359             return  -EPERM;\n[...]\n1372     case  SIOCDEVEMANSIPATE :\n1373         if( current->euid != 0 )    /* root only */\n1374             return  -EPERM;\n\nHere's my proposed fix:\n\nMissing capability checks.\n\nSigned-off-by: Eugene Teo <eugeneteo@kernel.sg>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "dataset": "other", "idx": 215379}
{"func": "\t\t\t\t\tsizeof(struct sbni_in_stats) ))\n\t\t\terror = -EFAULT;\n\t\tbreak;\n\n\tcase  SIOCDEVRESINSTATS :\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn  -EPERM;\n\t\tmemset( &nl->in_stats, 0, sizeof(struct sbni_in_stats) );\n\t\tbreak;\n\n\tcase  SIOCDEVGHWSTATE :\n\t\tif (copy_to_user( ifr->ifr_data, &flags, sizeof flags ))\n\t\t\terror = -EFAULT;\n\t\tbreak;\n\n\tcase  SIOCDEVSHWSTATE :\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn  -EPERM;\n\n\t\tspin_lock( &nl->lock );\n\t\tflags = *(struct sbni_flags*) &ifr->ifr_ifru;\n\t\tif( flags.fixed_rxl )\n\t\tbreak;\n\n#ifdef CONFIG_SBNI_MULTILINE\n\n\tcase  SIOCDEVENSLAVE :\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn  -EPERM;\n\n\t\tif (copy_from_user( slave_name, ifr->ifr_data, sizeof slave_name ))\n\t\t\treturn -EFAULT;\n\t\tslave_dev = dev_get_by_name(&init_net, slave_name );\n\t\t}\n\n\t\treturn  enslave( dev, slave_dev );\n\n\tcase  SIOCDEVEMANSIPATE :\n\t\tif (!capable(CAP_NET_ADMIN))\n\t\t\treturn  -EPERM;\n\n\t\treturn  emancipate( dev );\n\n#endif\t/* CONFIG_SBNI_MULTILINE */", "target": 0, "cwe": ["CWE-264"], "project": "linux-2.6", "commit_id": "f2455eb176ac87081bbfc9a44b21c7cd2bc1967e", "hash": 232324628272873095618685062531940041066, "size": 85, "message": "wan: Missing capability checks in sbni_ioctl()\n\nThere are missing capability checks in the following code:\n\n1300 static int\n1301 sbni_ioctl( struct net_device  *dev,  struct ifreq  *ifr,  int  cmd)\n1302 {\n[...]\n1319     case  SIOCDEVRESINSTATS :\n1320         if( current->euid != 0 )    /* root only */\n1321             return  -EPERM;\n[...]\n1336     case  SIOCDEVSHWSTATE :\n1337         if( current->euid != 0 )    /* root only */\n1338             return  -EPERM;\n[...]\n1357     case  SIOCDEVENSLAVE :\n1358         if( current->euid != 0 )    /* root only */\n1359             return  -EPERM;\n[...]\n1372     case  SIOCDEVEMANSIPATE :\n1373         if( current->euid != 0 )    /* root only */\n1374             return  -EPERM;\n\nHere's my proposed fix:\n\nMissing capability checks.\n\nSigned-off-by: Eugene Teo <eugeneteo@kernel.sg>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "dataset": "other", "idx": 488945}
{"func": "\t/* let server handle listen on unbound sockets */\n\tif (test_bit(ATM_VF_SESSION,&vcc->flags)) {\n\t\terror = -EINVAL;\n\t\tgoto out;\n\t}\n\tvcc_insert_socket(sk);\n\tset_bit(ATM_VF_WAITING, &vcc->flags);\n\tprepare_to_wait(sk->sk_sleep, &wait, TASK_UNINTERRUPTIBLE);\n\tsigd_enq(vcc,as_listen,NULL,NULL,&vcc->local);\n\twhile (test_bit(ATM_VF_WAITING, &vcc->flags) && sigd) {\n\t\tschedule();\n\tfinish_wait(sk->sk_sleep, &wait);\n\tif (!sigd) {\n\t\terror = -EUNATCH;\n\t\tgoto out;\n\t}\n\tset_bit(ATM_VF_LISTEN,&vcc->flags);\n\tsk->sk_max_ack_backlog = backlog > 0 ? backlog : ATM_BACKLOG_DEFAULT;\n\terror = -sk->sk_err;\nout:\n\trelease_sock(sk);\n\treturn error;", "target": 1, "cwe": ["CWE-399"], "project": "linux-2.6", "commit_id": "17b24b3c97498935a2ef9777370b1151dfed3f6f", "hash": 319036396244354828165290676039011878041, "size": 34, "message": "ATM: CVE-2008-5079: duplicate listen() on socket corrupts the vcc table\n\nAs reported by Hugo Dias that it is possible to cause a local denial\nof service attack by calling the svc_listen function twice on the same\nsocket and reading /proc/net/atm/*vc\n\nSigned-off-by: Chas Williams <chas@cmf.nrl.navy.mil>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "dataset": "other", "idx": 215446}
{"func": "\t/* let server handle listen on unbound sockets */\n\tif (test_bit(ATM_VF_SESSION,&vcc->flags)) {\n\t\terror = -EINVAL;\n\t\tgoto out;\n\t}\n\tif (test_bit(ATM_VF_LISTEN, &vcc->flags)) {\n\t\terror = -EADDRINUSE;\n\t\tgoto out;\n        }\n\tset_bit(ATM_VF_WAITING, &vcc->flags);\n\tprepare_to_wait(sk->sk_sleep, &wait, TASK_UNINTERRUPTIBLE);\n\tsigd_enq(vcc,as_listen,NULL,NULL,&vcc->local);\n\twhile (test_bit(ATM_VF_WAITING, &vcc->flags) && sigd) {\n\t\tschedule();\n\tif (!sigd) {\n\t\terror = -EUNATCH;\n\t\tgoto out;\n\t}\n\tset_bit(ATM_VF_LISTEN,&vcc->flags);\n\tvcc_insert_socket(sk);\n\tsk->sk_max_ack_backlog = backlog > 0 ? backlog : ATM_BACKLOG_DEFAULT;\n\terror = -sk->sk_err;\nout:\n\trelease_sock(sk);\n\treturn error;", "target": 0, "cwe": ["CWE-399"], "project": "linux-2.6", "commit_id": "17b24b3c97498935a2ef9777370b1151dfed3f6f", "hash": 124880737917680262812022112240721930476, "size": 38, "message": "ATM: CVE-2008-5079: duplicate listen() on socket corrupts the vcc table\n\nAs reported by Hugo Dias that it is possible to cause a local denial\nof service attack by calling the svc_listen function twice on the same\nsocket and reading /proc/net/atm/*vc\n\nSigned-off-by: Chas Williams <chas@cmf.nrl.navy.mil>\nSigned-off-by: David S. Miller <davem@davemloft.net>", "dataset": "other", "idx": 489598}
{"func": "\n\tnum_grp_locked = ext4_mb_get_buddy_cache_lock(sb, input->group);\n\t/* Update group descriptor block for new group */\n\tgdp = (struct ext4_group_desc *)((char *)primary->b_data +\n\t\t\t\t\t gdb_off * EXT4_DESC_SIZE(sb));\n\n\text4_block_bitmap_set(sb, gdp, input->block_bitmap); /* LV FIXME */\n\text4_inode_bitmap_set(sb, gdp, input->inode_bitmap); /* LV FIXME */\n\text4_inode_table_set(sb, gdp, input->inode_table); /* LV FIXME */\n\text4_free_blks_set(sb, gdp, input->free_blocks_count);\n\text4_free_inodes_set(sb, gdp, EXT4_INODES_PER_GROUP(sb));\n\tgdp->bg_flags |= cpu_to_le16(EXT4_BG_INODE_ZEROED);\n\tgdp->bg_checksum = ext4_group_desc_csum(sbi, input->group, gdp);\n\n\t/*\n\t * We can allocate memory for mb_alloc based on the new group\n\t * descriptor", "target": 1, "cwe": ["CWE-20"], "project": "linux-2.6", "commit_id": "fdff73f094e7220602cc3f8959c7230517976412", "hash": 281245565929730846006190005126700543134, "size": 225, "message": "ext4: Initialize the new group descriptor when resizing the filesystem\n\nMake sure all of the fields of the group descriptor are properly\ninitialized.  Previously, we allowed bg_flags field to be contain\nrandom garbage, which could trigger non-deterministic behavior,\nincluding a kernel OOPS.\n\nhttp://bugzilla.kernel.org/show_bug.cgi?id=12433\n\nSigned-off-by: \"Theodore Ts'o\" <tytso@mit.edu>\nCc: stable@kernel.org", "dataset": "other", "idx": 215458}
{"func": "\tnum_grp_locked = ext4_mb_get_buddy_cache_lock(sb, input->group);\n\t/* Update group descriptor block for new group */\n\tgdp = (struct ext4_group_desc *)((char *)primary->b_data +\n\t\t\t\t\t gdb_off * EXT4_DESC_SIZE(sb));\n\n\tmemset(gdp, 0, EXT4_DESC_SIZE(sb));\n\text4_block_bitmap_set(sb, gdp, input->block_bitmap); /* LV FIXME */\n\text4_inode_bitmap_set(sb, gdp, input->inode_bitmap); /* LV FIXME */\n\text4_inode_table_set(sb, gdp, input->inode_table); /* LV FIXME */\n\text4_free_blks_set(sb, gdp, input->free_blocks_count);\n\text4_free_inodes_set(sb, gdp, EXT4_INODES_PER_GROUP(sb));\n\tgdp->bg_flags = cpu_to_le16(EXT4_BG_INODE_ZEROED);\n\tgdp->bg_checksum = ext4_group_desc_csum(sbi, input->group, gdp);\n\n\t/*\n\t * We can allocate memory for mb_alloc based on the new group\n\t * descriptor", "target": 0, "cwe": ["CWE-20"], "project": "linux-2.6", "commit_id": "fdff73f094e7220602cc3f8959c7230517976412", "hash": 193067835456666808284941587516352105575, "size": 226, "message": "ext4: Initialize the new group descriptor when resizing the filesystem\n\nMake sure all of the fields of the group descriptor are properly\ninitialized.  Previously, we allowed bg_flags field to be contain\nrandom garbage, which could trigger non-deterministic behavior,\nincluding a kernel OOPS.\n\nhttp://bugzilla.kernel.org/show_bug.cgi?id=12433\n\nSigned-off-by: \"Theodore Ts'o\" <tytso@mit.edu>\nCc: stable@kernel.org", "dataset": "other", "idx": 489876}
{"func": "ecryptfs_write_metadata_to_contents(struct ecryptfs_crypt_stat *crypt_stat,\n\t\t\t\t    struct dentry *ecryptfs_dentry,\n\t\t\t\t    char *virt)\n{\n\tint rc;\n\n\trc = ecryptfs_write_lower(ecryptfs_dentry->d_inode, virt,\n\t\t\t\t  0, crypt_stat->num_header_bytes_at_front);\n\tif (rc)\n\t\tprintk(KERN_ERR \"%s: Error attempting to write header \"\n\t\t       \"information to lower file; rc = [%d]\\n\", __func__,\n\t\t       rc);\n\treturn rc;", "target": 1, "cwe": ["CWE-189"], "project": "linux-2.6", "commit_id": "8faece5f906725c10e7a1f6caf84452abadbdc7b", "hash": 51541081489523491690380425150429530165, "size": 14, "message": "eCryptfs: Allocate a variable number of pages for file headers\n\nWhen allocating the memory used to store the eCryptfs header contents, a\nsingle, zeroed page was being allocated with get_zeroed_page().\nHowever, the size of an eCryptfs header is either PAGE_CACHE_SIZE or\nECRYPTFS_MINIMUM_HEADER_EXTENT_SIZE (8192), whichever is larger, and is\nstored in the file's private_data->crypt_stat->num_header_bytes_at_front\nfield.\n\necryptfs_write_metadata_to_contents() was using\nnum_header_bytes_at_front to decide how many bytes should be written to\nthe lower filesystem for the file header.  Unfortunately, at least 8K\nwas being written from the page, despite the chance of the single,\nzeroed page being smaller than 8K.  This resulted in random areas of\nkernel memory being written between the 0x1000 and 0x1FFF bytes offsets\nin the eCryptfs file headers if PAGE_SIZE was 4K.\n\nThis patch allocates a variable number of pages, calculated with\nnum_header_bytes_at_front, and passes the number of allocated pages\nalong to ecryptfs_write_metadata_to_contents().\n\nThanks to Florian Streibelt for reporting the data leak and working with\nme to find the problem.  2.6.28 is the only kernel release with this\nvulnerability.  Corresponds to CVE-2009-0787\n\nSigned-off-by: Tyler Hicks <tyhicks@linux.vnet.ibm.com>\nAcked-by: Dustin Kirkland <kirkland@canonical.com>\nReviewed-by: Eric Sandeen <sandeen@redhat.com>\nReviewed-by: Eugene Teo <eugeneteo@kernel.sg>\nCc: Greg KH <greg@kroah.com>\nCc: dann frazier <dannf@dannf.org>\nCc: Serge E. Hallyn <serue@us.ibm.com>\nCc: Florian Streibelt <florian@f-streibelt.de>\nCc: stable@kernel.org\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 215467}
{"func": "ecryptfs_write_metadata_to_contents(struct dentry *ecryptfs_dentry,\n\t\t\t\t    char *virt, size_t virt_len)\n{\n\tint rc;\n\n\trc = ecryptfs_write_lower(ecryptfs_dentry->d_inode, virt,\n\t\t\t\t  0, virt_len);\n\tif (rc)\n\t\tprintk(KERN_ERR \"%s: Error attempting to write header \"\n\t\t       \"information to lower file; rc = [%d]\\n\", __func__,\n\t\t       rc);\n\treturn rc;", "target": 0, "cwe": ["CWE-189"], "project": "linux-2.6", "commit_id": "8faece5f906725c10e7a1f6caf84452abadbdc7b", "hash": 39007406608263614536820566580045545541, "size": 13, "message": "eCryptfs: Allocate a variable number of pages for file headers\n\nWhen allocating the memory used to store the eCryptfs header contents, a\nsingle, zeroed page was being allocated with get_zeroed_page().\nHowever, the size of an eCryptfs header is either PAGE_CACHE_SIZE or\nECRYPTFS_MINIMUM_HEADER_EXTENT_SIZE (8192), whichever is larger, and is\nstored in the file's private_data->crypt_stat->num_header_bytes_at_front\nfield.\n\necryptfs_write_metadata_to_contents() was using\nnum_header_bytes_at_front to decide how many bytes should be written to\nthe lower filesystem for the file header.  Unfortunately, at least 8K\nwas being written from the page, despite the chance of the single,\nzeroed page being smaller than 8K.  This resulted in random areas of\nkernel memory being written between the 0x1000 and 0x1FFF bytes offsets\nin the eCryptfs file headers if PAGE_SIZE was 4K.\n\nThis patch allocates a variable number of pages, calculated with\nnum_header_bytes_at_front, and passes the number of allocated pages\nalong to ecryptfs_write_metadata_to_contents().\n\nThanks to Florian Streibelt for reporting the data leak and working with\nme to find the problem.  2.6.28 is the only kernel release with this\nvulnerability.  Corresponds to CVE-2009-0787\n\nSigned-off-by: Tyler Hicks <tyhicks@linux.vnet.ibm.com>\nAcked-by: Dustin Kirkland <kirkland@canonical.com>\nReviewed-by: Eric Sandeen <sandeen@redhat.com>\nReviewed-by: Eugene Teo <eugeneteo@kernel.sg>\nCc: Greg KH <greg@kroah.com>\nCc: dann frazier <dannf@dannf.org>\nCc: Serge E. Hallyn <serue@us.ibm.com>\nCc: Florian Streibelt <florian@f-streibelt.de>\nCc: stable@kernel.org\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 490193}
{"func": "size_t util_path_encode(char *s, size_t len)\n{\n\tchar t[(len * 3)+1];\n\tsize_t i, j;\n\n\tfor (i = 0, j = 0; s[i] != '\\0'; i++) {\n\t\tif (s[i] == '/') {\n\t\t\tmemcpy(&t[j], \"\\\\x2f\", 4);", "target": 1, "cwe": ["CWE-120"], "project": "udev", "commit_id": "662c3110803bd8c1aedacc36788e6fd028944314", "hash": 255890177997629526431555852871596671591, "size": 24, "message": "path_encode: fix max length calculation\n\nSebastian Krahmer wrote:\n> it should reserve 4 times not 3 times len :)", "dataset": "other", "idx": 215481}
{"func": "size_t util_path_encode(char *s, size_t len)\n{\n\tchar t[(len * 4)+1];\n\tsize_t i, j;\n\n\tfor (i = 0, j = 0; s[i] != '\\0'; i++) {\n\t\tif (s[i] == '/') {\n\t\t\tmemcpy(&t[j], \"\\\\x2f\", 4);", "target": 0, "cwe": ["CWE-120"], "project": "udev", "commit_id": "662c3110803bd8c1aedacc36788e6fd028944314", "hash": 108373855040332074515926363142152806355, "size": 24, "message": "path_encode: fix max length calculation\n\nSebastian Krahmer wrote:\n> it should reserve 4 times not 3 times len :)", "dataset": "other", "idx": 490328}
{"func": "\t\trt_mutex_unlock(&q.pi_state->pi_mutex);\n\n\t/* Unqueue and drop the lock */\n\tunqueue_me_pi(&q);\n\n\tgoto out;\n\nout_unlock_put_key:\n\tqueue_unlock(&q, hb);\n\nout_put_key:", "target": 1, "cwe": [], "project": "linux-2.6", "commit_id": "5ecb01cfdf96c5f465192bdb2a4fd4a61a24c6cc", "hash": 229495631557006192714814605323704821133, "size": 119, "message": "futex_lock_pi() key refcnt fix\n\nThis fixes a futex key reference count bug in futex_lock_pi(),\nwhere a key's reference count is incremented twice but decremented\nonly once, causing the backing object to not be released.\n\nIf the futex is created in a temporary file in an ext3 file system,\nthis bug causes the file's inode to become an \"undead\" orphan,\nwhich causes an oops from a BUG_ON() in ext3_put_super() when the\nfile system is unmounted. glibc's test suite is known to trigger this,\nsee <http://bugzilla.kernel.org/show_bug.cgi?id=14256>.\n\nThe bug is a regression from 2.6.28-git3, namely Peter Zijlstra's\n38d47c1b7075bd7ec3881141bb3629da58f88dab \"[PATCH] futex: rely on\nget_user_pages() for shared futexes\". That commit made get_futex_key()\nalso increment the reference count of the futex key, and updated its\ncallers to decrement the key's reference count before returning.\nUnfortunately the normal exit path in futex_lock_pi() wasn't corrected:\nthe reference count is incremented by get_futex_key() and queue_lock(),\nbut the normal exit path only decrements once, via unqueue_me_pi().\nThe fix is to put_futex_key() after unqueue_me_pi(), since 2.6.31\nthis is easily done by 'goto out_put_key' rather than 'goto out'.\n\nSigned-off-by: Mikael Pettersson <mikpe@it.uu.se>\nAcked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>\nAcked-by: Darren Hart <dvhltc@us.ibm.com>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nCc: <stable@kernel.org>", "dataset": "other", "idx": 215572}
{"func": "\t\trt_mutex_unlock(&q.pi_state->pi_mutex);\n\n\t/* Unqueue and drop the lock */\n\tunqueue_me_pi(&q);\n\n\tgoto out_put_key;\n\nout_unlock_put_key:\n\tqueue_unlock(&q, hb);\n\nout_put_key:", "target": 0, "cwe": [], "project": "linux-2.6", "commit_id": "5ecb01cfdf96c5f465192bdb2a4fd4a61a24c6cc", "hash": 243045543915221577830247168819573698971, "size": 119, "message": "futex_lock_pi() key refcnt fix\n\nThis fixes a futex key reference count bug in futex_lock_pi(),\nwhere a key's reference count is incremented twice but decremented\nonly once, causing the backing object to not be released.\n\nIf the futex is created in a temporary file in an ext3 file system,\nthis bug causes the file's inode to become an \"undead\" orphan,\nwhich causes an oops from a BUG_ON() in ext3_put_super() when the\nfile system is unmounted. glibc's test suite is known to trigger this,\nsee <http://bugzilla.kernel.org/show_bug.cgi?id=14256>.\n\nThe bug is a regression from 2.6.28-git3, namely Peter Zijlstra's\n38d47c1b7075bd7ec3881141bb3629da58f88dab \"[PATCH] futex: rely on\nget_user_pages() for shared futexes\". That commit made get_futex_key()\nalso increment the reference count of the futex key, and updated its\ncallers to decrement the key's reference count before returning.\nUnfortunately the normal exit path in futex_lock_pi() wasn't corrected:\nthe reference count is incremented by get_futex_key() and queue_lock(),\nbut the normal exit path only decrements once, via unqueue_me_pi().\nThe fix is to put_futex_key() after unqueue_me_pi(), since 2.6.31\nthis is easily done by 'goto out_put_key' rather than 'goto out'.\n\nSigned-off-by: Mikael Pettersson <mikpe@it.uu.se>\nAcked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>\nAcked-by: Darren Hart <dvhltc@us.ibm.com>\nSigned-off-by: Thomas Gleixner <tglx@linutronix.de>\nCc: <stable@kernel.org>", "dataset": "other", "idx": 492307}
{"func": "\t    pcred->egid\t!= mycred->egid\t||\n\t    pcred->sgid\t!= mycred->egid)\n\t\tgoto not_permitted;\n\n\t/* the keyrings must have the same UID */\n\tif (pcred->tgcred->session_keyring->uid != mycred->euid ||\n\t    mycred->tgcred->session_keyring->uid != mycred->euid)\n\t\tgoto not_permitted;\n\n\t/* if there's an already pending keyring replacement, then we replace\n\t * that */", "target": 1, "cwe": ["CWE-476"], "project": "linux-2.6", "commit_id": "3d96406c7da1ed5811ea52a3b0905f4f0e295376", "hash": 32494453156495132678173766772202466555, "size": 99, "message": "KEYS: Fix bug in keyctl_session_to_parent() if parent has no session keyring\n\nFix a bug in keyctl_session_to_parent() whereby it tries to check the ownership\nof the parent process's session keyring whether or not the parent has a session\nkeyring [CVE-2010-2960].\n\nThis results in the following oops:\n\n  BUG: unable to handle kernel NULL pointer dereference at 00000000000000a0\n  IP: [<ffffffff811ae4dd>] keyctl_session_to_parent+0x251/0x443\n  ...\n  Call Trace:\n   [<ffffffff811ae2f3>] ? keyctl_session_to_parent+0x67/0x443\n   [<ffffffff8109d286>] ? __do_fault+0x24b/0x3d0\n   [<ffffffff811af98c>] sys_keyctl+0xb4/0xb8\n   [<ffffffff81001eab>] system_call_fastpath+0x16/0x1b\n\nif the parent process has no session keyring.\n\nIf the system is using pam_keyinit then it mostly protected against this as all\nprocesses derived from a login will have inherited the session keyring created\nby pam_keyinit during the log in procedure.\n\nTo test this, pam_keyinit calls need to be commented out in /etc/pam.d/.\n\nReported-by: Tavis Ormandy <taviso@cmpxchg8b.com>\nSigned-off-by: David Howells <dhowells@redhat.com>\nAcked-by: Tavis Ormandy <taviso@cmpxchg8b.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 215590}
{"func": "\t    pcred->egid\t!= mycred->egid\t||\n\t    pcred->sgid\t!= mycred->egid)\n\t\tgoto not_permitted;\n\n\t/* the keyrings must have the same UID */\n\tif ((pcred->tgcred->session_keyring &&\n\t     pcred->tgcred->session_keyring->uid != mycred->euid) ||\n\t    mycred->tgcred->session_keyring->uid != mycred->euid)\n\t\tgoto not_permitted;\n\n\t/* if there's an already pending keyring replacement, then we replace\n\t * that */", "target": 0, "cwe": ["CWE-476"], "project": "linux-2.6", "commit_id": "3d96406c7da1ed5811ea52a3b0905f4f0e295376", "hash": 215543301000421930365201907714146972341, "size": 100, "message": "KEYS: Fix bug in keyctl_session_to_parent() if parent has no session keyring\n\nFix a bug in keyctl_session_to_parent() whereby it tries to check the ownership\nof the parent process's session keyring whether or not the parent has a session\nkeyring [CVE-2010-2960].\n\nThis results in the following oops:\n\n  BUG: unable to handle kernel NULL pointer dereference at 00000000000000a0\n  IP: [<ffffffff811ae4dd>] keyctl_session_to_parent+0x251/0x443\n  ...\n  Call Trace:\n   [<ffffffff811ae2f3>] ? keyctl_session_to_parent+0x67/0x443\n   [<ffffffff8109d286>] ? __do_fault+0x24b/0x3d0\n   [<ffffffff811af98c>] sys_keyctl+0xb4/0xb8\n   [<ffffffff81001eab>] system_call_fastpath+0x16/0x1b\n\nif the parent process has no session keyring.\n\nIf the system is using pam_keyinit then it mostly protected against this as all\nprocesses derived from a login will have inherited the session keyring created\nby pam_keyinit during the log in procedure.\n\nTo test this, pam_keyinit calls need to be commented out in /etc/pam.d/.\n\nReported-by: Tavis Ormandy <taviso@cmpxchg8b.com>\nSigned-off-by: David Howells <dhowells@redhat.com>\nAcked-by: Tavis Ormandy <taviso@cmpxchg8b.com>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 492806}
{"func": "grep (int fd, char const *file, struct stats *stats)\n{\n  int nlines, i;\n  int not_text;\n  size_t residue, save;\n  char oldc;\n  char *beg;\n  char *lim;", "target": 1, "cwe": ["CWE-189"], "project": "grep", "commit_id": "8fcf61523644df42e1905c81bed26838e0b04f91", "hash": 127493158851729294951078837033344135674, "size": 127, "message": "grep: fix integer-overflow issues in main program\n\n* NEWS: Document this.\n* bootstrap.conf (gnulib_modules): Add inttypes, xstrtoimax.\nRemove xstrtoumax.\n* src/main.c: Include <inttypes.h>, for INTMAX_MAX, PRIdMAX.\n(context_length_arg, prtext, grepbuf, grep, grepfile)\n(get_nondigit_option, main):\nUse intmax_t, not int, for line counts.\n(context_length_arg, main): Silently ceiling line counts\nto maximum value, since there's no practical difference between\ndoing that and using infinite-precision arithmetic.\n(out_before, out_after, pending): Now intmax_t, not int.\n(max_count, outleft): Now intmax_t, not off_t.\n(prepend_args, prepend_default_options, main):\nUse size_t, not int, for sizes.\n(prepend_default_options): Check for int and size_t overflow.", "dataset": "other", "idx": 215830}
{"func": "grep (int fd, char const *file, struct stats *stats)\n{\n  intmax_t nlines, i;\n  int not_text;\n  size_t residue, save;\n  char oldc;\n  char *beg;\n  char *lim;", "target": 0, "cwe": ["CWE-189"], "project": "grep", "commit_id": "8fcf61523644df42e1905c81bed26838e0b04f91", "hash": 271423593932472745351308763697044508813, "size": 127, "message": "grep: fix integer-overflow issues in main program\n\n* NEWS: Document this.\n* bootstrap.conf (gnulib_modules): Add inttypes, xstrtoimax.\nRemove xstrtoumax.\n* src/main.c: Include <inttypes.h>, for INTMAX_MAX, PRIdMAX.\n(context_length_arg, prtext, grepbuf, grep, grepfile)\n(get_nondigit_option, main):\nUse intmax_t, not int, for line counts.\n(context_length_arg, main): Silently ceiling line counts\nto maximum value, since there's no practical difference between\ndoing that and using infinite-precision arithmetic.\n(out_before, out_after, pending): Now intmax_t, not int.\n(max_count, outleft): Now intmax_t, not off_t.\n(prepend_args, prepend_default_options, main):\nUse size_t, not int, for sizes.\n(prepend_default_options): Check for int and size_t overflow.", "dataset": "other", "idx": 496454}
{"func": "        {\n          XIDeviceInfo *info;\n          int n_devices;\n\n          CLUTTER_NOTE (EVENT, \"Hierarchy event: device enabled\");\n\n          info = XIQueryDevice (backend_x11->xdpy,\n                                ev->info[i].deviceid,\n                                &n_devices);\n          add_device (manager_xi2, backend_x11, &info[0], FALSE);\n        }\n      else if (ev->info[i].flags & XIDeviceDisabled)\n        {\n          CLUTTER_NOTE (EVENT, \"Hierarchy event: device disabled\");\n\n              send_changed = TRUE;\n            }\n\n          /* and attach the slave to the new master if needed */\n          if (ev->info[i].flags & XISlaveAttached)\n            {\n              info = XIQueryDevice (backend_x11->xdpy,\n                                    ev->info[i].deviceid,\n                                    &n_devices);\n              master = g_hash_table_lookup (manager_xi2->devices_by_id,\n                                            GINT_TO_POINTER (info->attachment));\n              _clutter_input_device_set_associated_device (slave, master);\n              _clutter_input_device_add_slave (master, slave);\n\n              send_changed = TRUE;\n              XIFreeDeviceInfo (info);\n            }\n\n          if (send_changed)\n            {\n              ClutterStage *stage = _clutter_input_device_get_stage (master);", "target": 1, "cwe": ["CWE-264"], "project": "clutter", "commit_id": "e310c68d7b38d521e341f4e8a36f54303079d74e", "hash": 20403663631467032878668258860120289189, "size": 78, "message": "x11: trap errors when calling XIQueryDevice\n\nDevices can disappear at any time, causing XIQueryDevice\nto throw an error. At the same time, plug a memory leak.\n\nhttps://bugzilla.gnome.org/show_bug.cgi?id=701974", "dataset": "other", "idx": 215902}
{"func": "          XIDeviceInfo *info;\n          int n_devices;\n\n          CLUTTER_NOTE (EVENT, \"Hierarchy event: device enabled\");\n\n          clutter_x11_trap_x_errors ();\n          info = XIQueryDevice (backend_x11->xdpy,\n                                ev->info[i].deviceid,\n                                &n_devices);\n          clutter_x11_untrap_x_errors ();\n          if (info != NULL)\n            {\n              add_device (manager_xi2, backend_x11, &info[0], FALSE);\n              XIFreeDeviceInfo (info);\n            }\n        }\n      else if (ev->info[i].flags & XIDeviceDisabled)\n        {\n          CLUTTER_NOTE (EVENT, \"Hierarchy event: device disabled\");\n\n            }\n\n          /* and attach the slave to the new master if needed */\n          if (ev->info[i].flags & XISlaveAttached)\n            {\n              clutter_x11_trap_x_errors ();\n              info = XIQueryDevice (backend_x11->xdpy,\n                                    ev->info[i].deviceid,\n                                    &n_devices);\n              clutter_x11_untrap_x_errors ();\n              if (info != NULL)\n                {\n                  master = g_hash_table_lookup (manager_xi2->devices_by_id,\n                                                GINT_TO_POINTER (info->attachment));\n                  if (master != NULL)\n                    {\n                      _clutter_input_device_set_associated_device (slave, master);\n                      _clutter_input_device_add_slave (master, slave);\n\n                      send_changed = TRUE;\n                    }\n                  XIFreeDeviceInfo (info);\n                }\n            }\n\n          if (send_changed)\n            {\n              ClutterStage *stage = _clutter_input_device_get_stage (master);", "target": 0, "cwe": ["CWE-264"], "project": "clutter", "commit_id": "e310c68d7b38d521e341f4e8a36f54303079d74e", "hash": 270833825451384971935526577161445293974, "size": 92, "message": "x11: trap errors when calling XIQueryDevice\n\nDevices can disappear at any time, causing XIQueryDevice\nto throw an error. At the same time, plug a memory leak.\n\nhttps://bugzilla.gnome.org/show_bug.cgi?id=701974", "dataset": "other", "idx": 497483}
{"func": "\n\t\t/* Setup front stub */\n\t\tint len = q->length - (res - q->data);\n\t\tint counter = 0;\n\n\t\td (1, g_printerr (\"MERGE needed (%d) which is >= %d + %d;\\n\",\n\t\t\t      num_bytes, offset, state->end_offset););\n\n\t\tdo {\n\t\t\td (1, g_printerr (\"record %d) add %d bytes;\\n\", ++counter, len););\n\t\t\t/* copy necessary portion of current record */\n\t\t\tmemcpy (tmp, res, len);\n\t\t\ttmp += len;\n\n\t\t\t/* Get next record */\n\t\t\tif (q->opcode != BIFF_MS_O_DRAWING &&\n\t\t\t    q->opcode != BIFF_MS_O_DRAWING_GROUP &&\n\t\t\t    q->opcode != BIFF_MS_O_DRAWING_SELECTION &&\n\t\t\t    q->opcode != BIFF_CHART_gelframe &&\n\t\t\t    q->opcode != BIFF_CONTINUE) {\n\t\t\t  g_warning (\"Unexpected record type 0x%x @ 0x%lx;\", q->opcode, (long)q->streamPos);\n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\tstate->start_offset = state->end_offset;\n\t\t\tstate->end_offset += q->length;", "target": 1, "cwe": ["CWE-119"], "project": "gnumeric", "commit_id": "b5480b69345b3c6d56ee0ed9c9e9880bb2a08cdc", "hash": 257840832045771375406916626167501889085, "size": 94, "message": "xls: fuzzed file crash.", "dataset": "other", "idx": 215903}
{"func": "\n\t\t/* Setup front stub */\n\t\tint len = q->length - (res - q->data);\n\t\tint counter = 0;\n\n\t\td (1, g_printerr (\"MERGE needed (%d) which is >= -%d + %d;\\n\",\n\t\t\t      num_bytes, offset, state->end_offset););\n\n\t\tdo {\n\t\t\tint maxlen = (buffer + num_bytes) - tmp;\n\t\t\tlen = MIN (len, maxlen);\n\t\t\td (1, g_printerr (\"record %d) add %d bytes;\\n\", ++counter, len););\n\n\t\t\t/* copy necessary portion of current record */\n\t\t\tmemcpy (tmp, res, len);\n\t\t\ttmp += len;\n\n\t\t\t/* Get next record */\n\t\t\tif (q->opcode != BIFF_MS_O_DRAWING &&\n\t\t\t    q->opcode != BIFF_MS_O_DRAWING_GROUP &&\n\t\t\t    q->opcode != BIFF_MS_O_DRAWING_SELECTION &&\n\t\t\t    q->opcode != BIFF_CHART_gelframe &&\n\t\t\t    q->opcode != BIFF_CONTINUE) {\n\t\t\t\tg_warning (\"Unexpected record type 0x%x @ 0x%lx;\", q->opcode, (long)q->streamPos);\n\t\t\t\tg_free (buffer);\n\t\t\t\treturn NULL;\n\t\t\t}\n\n\t\t\tstate->start_offset = state->end_offset;\n\t\t\tstate->end_offset += q->length;", "target": 0, "cwe": ["CWE-119"], "project": "gnumeric", "commit_id": "b5480b69345b3c6d56ee0ed9c9e9880bb2a08cdc", "hash": 70824620653332871173225127729054253170, "size": 98, "message": "xls: fuzzed file crash.", "dataset": "other", "idx": 497531}
{"func": "        session->srv.rsa_key = ssh_key_dup(sshbind->rsa);\n        if (session->srv.rsa_key == NULL) {\n          ssh_set_error_oom(sshbind);\n          return SSH_ERROR;\n        }\n    }\n    return SSH_OK;\n}", "target": 1, "cwe": ["CWE-310"], "project": "libssh", "commit_id": "e99246246b4061f7e71463f8806b9dcad65affa0", "hash": 155342663606614956692829213292082664878, "size": 79, "message": "security: fix for vulnerability CVE-2014-0017\n\nWhen accepting a new connection, a forking server based on libssh forks\nand the child process handles the request. The RAND_bytes() function of\nopenssl doesn't reset its state after the fork, but simply adds the\ncurrent process id (getpid) to the PRNG state, which is not guaranteed\nto be unique.\nThis can cause several children to end up with same PRNG state which is\na security issue.", "dataset": "other", "idx": 215904}
{"func": "        if (session->srv.rsa_key == NULL) {\n          ssh_set_error_oom(sshbind);\n          return SSH_ERROR;\n        }\n    }\n    /* force PRNG to change state in case we fork after ssh_bind_accept */\n    ssh_reseed();\n    return SSH_OK;\n}", "target": 0, "cwe": ["CWE-310"], "project": "libssh", "commit_id": "e99246246b4061f7e71463f8806b9dcad65affa0", "hash": 50685877395540749769671536212771341726, "size": 81, "message": "security: fix for vulnerability CVE-2014-0017\n\nWhen accepting a new connection, a forking server based on libssh forks\nand the child process handles the request. The RAND_bytes() function of\nopenssl doesn't reset its state after the fork, but simply adds the\ncurrent process id (getpid) to the PRNG state, which is not guaranteed\nto be unique.\nThis can cause several children to end up with same PRNG state which is\na security issue.", "dataset": "other", "idx": 497609}
{"func": "\n  /* Make a final check that the path length is acceptable? */\n  /* TODO: check fnres.base for path length problem */\n\n  xfree (temp_fnres.base);\n\n  /* Check the cases in which the unique extensions are not used:\n     1) Clobbering is turned off (-nc).\n     2) Retrieval with regetting.\n     3) Timestamping is used.\n     4) Hierarchy is built.", "target": 1, "cwe": [], "project": "wget", "commit_id": "59b920874daa565a1323ffa1e756e80493190686", "hash": 248269352987592620345400062055884756082, "size": 171, "message": "Support non-ASCII URLs\n\n* src/url.c [HAVE_ICONV]: Include iconv.h and langinfo.h.\n(convert_fname): New function.\n[HAVE_ICONV]: Convert file name from remote encoding to local\nencoding.\n(url_file_name): Call convert_fname.\n(filechr_table): Don't consider bytes in 128..159 as control\ncharacters.\n\n* tests/Test-ftp-iri.px: Fix the expected file name to match the\nnew file-name recoding.  State the remote encoding explicitly on\nthe Wget command line.\n\n* NEWS: Mention the URI recoding when built with libiconv.", "dataset": "other", "idx": 215957}
{"func": "  /* Make a final check that the path length is acceptable? */\n  /* TODO: check fnres.base for path length problem */\n\n  xfree (temp_fnres.base);\n\n  fname = convert_fname (fname);\n\n  /* Check the cases in which the unique extensions are not used:\n     1) Clobbering is turned off (-nc).\n     2) Retrieval with regetting.\n     3) Timestamping is used.\n     4) Hierarchy is built.", "target": 0, "cwe": [], "project": "wget", "commit_id": "59b920874daa565a1323ffa1e756e80493190686", "hash": 328521672192198944507160659111401695202, "size": 173, "message": "Support non-ASCII URLs\n\n* src/url.c [HAVE_ICONV]: Include iconv.h and langinfo.h.\n(convert_fname): New function.\n[HAVE_ICONV]: Convert file name from remote encoding to local\nencoding.\n(url_file_name): Call convert_fname.\n(filechr_table): Don't consider bytes in 128..159 as control\ncharacters.\n\n* tests/Test-ftp-iri.px: Fix the expected file name to match the\nnew file-name recoding.  State the remote encoding explicitly on\nthe Wget command line.\n\n* NEWS: Mention the URI recoding when built with libiconv.", "dataset": "other", "idx": 498301}
{"func": "      case TGA_TYPE_COLOR:\n        if ((info.bpp != 15 && info.bpp != 16 &&\n             info.bpp != 24 && info.bpp != 32)      ||\n            ((info.bpp == 15 || info.bpp == 24) &&\n             info.alphaBits != 0)                   ||\n            (info.bpp == 16 && info.alphaBits != 1) ||\n            (info.bpp == 32 && info.alphaBits != 8))\n          {\n            g_message (\"Unhandled sub-format in '%s' (type = %u, bpp = %u, alpha = %u)\",\n                       gimp_filename_to_utf8 (filename),\n                       info.imageType, info.bpp, info.alphaBits);", "target": 1, "cwe": ["CWE-125"], "project": "GIMP", "commit_id": "22e2571c25425f225abdb11a566cc281fca6f366", "hash": 210802702046896555195187298959116285766, "size": 208, "message": "plug-ins: TGA 16-bit RGB (without alpha bit) is also valid.\n\nAccording to some spec on the web, 16-bit RGB is also valid. In this\ncase, the last bit is simply ignored (at least that's how it is\nimplemented right now).\n\n(cherry picked from commit 8ea316667c8a3296bce2832b3986b58d0fdfc077)", "dataset": "other", "idx": 215994}
{"func": "      case TGA_TYPE_COLOR:\n        if ((info.bpp != 15 && info.bpp != 16 &&\n             info.bpp != 24 && info.bpp != 32)      ||\n            ((info.bpp == 15 || info.bpp == 24) &&\n             info.alphaBits != 0)                   ||\n            (info.bpp == 16 && info.alphaBits != 1 &&\n             info.alphaBits != 0)                   ||\n            (info.bpp == 32 && info.alphaBits != 8))\n          {\n            g_message (\"Unhandled sub-format in '%s' (type = %u, bpp = %u, alpha = %u)\",\n                       gimp_filename_to_utf8 (filename),\n                       info.imageType, info.bpp, info.alphaBits);", "target": 0, "cwe": ["CWE-125"], "project": "GIMP", "commit_id": "22e2571c25425f225abdb11a566cc281fca6f366", "hash": 112786509503859538202403338615754577633, "size": 209, "message": "plug-ins: TGA 16-bit RGB (without alpha bit) is also valid.\n\nAccording to some spec on the web, 16-bit RGB is also valid. In this\ncase, the last bit is simply ignored (at least that's how it is\nimplemented right now).\n\n(cherry picked from commit 8ea316667c8a3296bce2832b3986b58d0fdfc077)", "dataset": "other", "idx": 498639}
{"func": "\tif (inerrno != ENOENT)\n\t  {\n\t    *outname_needs_removal = true;\n\t    copy_file (inname, outname, 0, exclusive, instat.st_mode, true);\n\t  }\n\tsprintf (buf, \"%s %s%s\", editor_program,\n\t\t verbosity == VERBOSE ? \"\" : \"- \",\n\t\t outname);\n\tfflush (stdout);\n\n\tpid = fork();\n\tif (pid == -1)\n\t  pfatal (\"Can't fork\");\n\telse if (pid == 0)\n\t  {\n\t    dup2 (tmpfd, 0);\n\t    execl (\"/bin/sh\", \"sh\", \"-c\", buf, (char *) 0);\n\t    _exit (2);\n\t  }\n\telse\n\t  {\n\t    int wstatus;", "target": 1, "cwe": ["CWE-78"], "project": "patch", "commit_id": "3fcd042d26d70856e826a42b5f93dc4854d80bf0", "hash": 338700816905051362992541803818419670330, "size": 115, "message": "Invoke ed directly instead of using the shell\n\n* src/pch.c (do_ed_script): Invoke ed directly instead of using a shell\ncommand to avoid quoting vulnerabilities.", "dataset": "other", "idx": 216036}
{"func": "\t*outname_needs_removal = true;\n\tif (inerrno != ENOENT)\n\t  {\n\t    *outname_needs_removal = true;\n\t    copy_file (inname, outname, 0, exclusive, instat.st_mode, true);\n\t  }\n\tfflush (stdout);\n\n\tpid = fork();\n\tif (pid == -1)\n\t  pfatal (\"Can't fork\");\n\telse if (pid == 0)\n\t  {\n\t    dup2 (tmpfd, 0);\n\t    assert (outname[0] != '!' && outname[0] != '-');\n\t    execlp (editor_program, editor_program, \"-\", outname, (char  *) NULL);\n\t    _exit (2);\n\t  }\n\telse\n\t  {\n\t    int wstatus;", "target": 0, "cwe": ["CWE-78"], "project": "patch", "commit_id": "3fcd042d26d70856e826a42b5f93dc4854d80bf0", "hash": 164336965162787411663255462273976593275, "size": 113, "message": "Invoke ed directly instead of using the shell\n\n* src/pch.c (do_ed_script): Invoke ed directly instead of using a shell\ncommand to avoid quoting vulnerabilities.", "dataset": "other", "idx": 498970}
{"func": "_eddsa_hash (const struct ecc_modulo *m,\n\t     mp_limb_t *rp, size_t digest_size, const uint8_t *digest)\n{\n  mp_size_t nlimbs = (8*digest_size + GMP_NUMB_BITS - 1) / GMP_NUMB_BITS;\n\n  mpn_set_base256_le (rp, nlimbs, digest, digest_size);\n\n  if (nlimbs > 2*m->size)\n    {\n      hi = mpn_addmul_1 (rp + m->size, m->B, m->size, hi);\n      assert (hi <= 1);\n      hi = mpn_cnd_add_n (hi, rp + m->size, rp + m->size, m->B, m->size);\n      assert (hi == 0);\n    }\n  m->mod (m, rp, rp);\n}", "target": 1, "cwe": ["CWE-787"], "project": "nettle", "commit_id": "ae3801a0e5cce276c270973214385c86048d5f7b", "hash": 268891952040565861726467544094713063054, "size": 29, "message": "Similar fix for eddsa.\n\n* eddsa-hash.c (_eddsa_hash): Ensure result is canonically\nreduced. Two of the three call sites need that.\n\n(cherry picked from commit d9b564e4b3b3a5691afb9328c7342b3f7ca64288)", "dataset": "other", "idx": 216084}
{"func": "_eddsa_hash (const struct ecc_modulo *m,\n\t     mp_limb_t *rp, size_t digest_size, const uint8_t *digest)\n{\n  mp_size_t nlimbs = (8*digest_size + GMP_NUMB_BITS - 1) / GMP_NUMB_BITS;\n  mp_limb_t cy;\n\n  mpn_set_base256_le (rp, nlimbs, digest, digest_size);\n\n  if (nlimbs > 2*m->size)\n    {\n      hi = mpn_addmul_1 (rp + m->size, m->B, m->size, hi);\n      assert (hi <= 1);\n      hi = mpn_cnd_add_n (hi, rp + m->size, rp + m->size, m->B, m->size);\n      assert (hi == 0);\n    }\n  m->mod (m, rp + m->size , rp);\n  /* Ensure canonical reduction. */\n  cy = mpn_sub_n (rp, rp + m->size, m->m, m->size);\n  cnd_copy (cy, rp, rp + m->size, m->size);\n}", "target": 0, "cwe": ["CWE-787"], "project": "nettle", "commit_id": "ae3801a0e5cce276c270973214385c86048d5f7b", "hash": 184589664896563874284916719738565724397, "size": 33, "message": "Similar fix for eddsa.\n\n* eddsa-hash.c (_eddsa_hash): Ensure result is canonically\nreduced. Two of the three call sites need that.\n\n(cherry picked from commit d9b564e4b3b3a5691afb9328c7342b3f7ca64288)", "dataset": "other", "idx": 499623}
{"func": "ds_fgetstr (FILE *f, dynamic_string *s, char eos)\n{\n  int insize;\t\t\t/* Amount needed for line.  */\n  int strsize;\t\t\t/* Amount allocated for S.  */\n  int next_ch;\n\n  /* Initialize.  */\n  insize = 0;\n  strsize = s->ds_length;\n\n  /* Read the input string.  */\n  next_ch = getc (f);\n  while (next_ch != eos && next_ch != EOF)\n    {\n      if (insize >= strsize - 1)\n\t{\n\t  ds_resize (s, strsize * 2 + 2);\n\t  strsize = s->ds_length;\n\t}\n      s->ds_string[insize++] = next_ch;\n      next_ch = getc (f);\n    }\n  s->ds_string[insize++] = '\\0';\n\n  if (insize == 1 && next_ch == EOF)\n    return NULL;\n  else\n    return s->ds_string;\n}", "target": 1, "cwe": ["CWE-190"], "project": "cpio", "commit_id": "dd96882877721703e19272fe25034560b794061b", "hash": 89622374372923890041979167530041250799, "size": 29, "message": "Rewrite dynamic string support.\n\n* src/dstring.c (ds_init): Take a single argument.\n(ds_free): New function.\n(ds_resize): Take a single argument.  Use x2nrealloc to expand\nthe storage.\n(ds_reset,ds_append,ds_concat,ds_endswith): New function.\n(ds_fgetstr): Rewrite.  In particular, this fixes integer overflow.\n* src/dstring.h (dynamic_string): Keep both the allocated length\n(ds_size) and index of the next free byte in the string (ds_idx).\n(ds_init,ds_resize): Change signature.\n(ds_len): New macro.\n(ds_free,ds_reset,ds_append,ds_concat,ds_endswith): New protos.\n* src/copyin.c: Use new ds_ functions.\n* src/copyout.c: Likewise.\n* src/copypass.c: Likewise.\n* src/util.c: Likewise.", "dataset": "other", "idx": 216101}
{"func": "ds_fgetstr (FILE *f, dynamic_string *s, char eos)\n{\n  int next_ch;\n\n  /* Initialize.  */\n  s->ds_idx = 0;\n\n  /* Read the input string.  */\n  while ((next_ch = getc (f)) != eos && next_ch != EOF)\n    {\n      ds_resize (s);\n      s->ds_string[s->ds_idx++] = next_ch;\n    }\n  ds_resize (s);\n  s->ds_string[s->ds_idx] = '\\0';\n\n  if (s->ds_idx == 0 && next_ch == EOF)\n    return NULL;\n  else\n    return s->ds_string;\n}", "target": 0, "cwe": ["CWE-190"], "project": "cpio", "commit_id": "dd96882877721703e19272fe25034560b794061b", "hash": 38898591507161366429804741213308730595, "size": 21, "message": "Rewrite dynamic string support.\n\n* src/dstring.c (ds_init): Take a single argument.\n(ds_free): New function.\n(ds_resize): Take a single argument.  Use x2nrealloc to expand\nthe storage.\n(ds_reset,ds_append,ds_concat,ds_endswith): New function.\n(ds_fgetstr): Rewrite.  In particular, this fixes integer overflow.\n* src/dstring.h (dynamic_string): Keep both the allocated length\n(ds_size) and index of the next free byte in the string (ds_idx).\n(ds_init,ds_resize): Change signature.\n(ds_len): New macro.\n(ds_free,ds_reset,ds_append,ds_concat,ds_endswith): New protos.\n* src/copyin.c: Use new ds_ functions.\n* src/copyout.c: Likewise.\n* src/copypass.c: Likewise.\n* src/util.c: Likewise.", "dataset": "other", "idx": 499640}
{"func": "unsigned int get_random_int(void)\n{\n\t/*\n\t * Use IP's RNG. It suits our purpose perfectly: it re-keys itself\n\t * every second, from the entropy pool (and thus creates a limited\n\t * drain on it), and uses halfMD4Transform within the second. We\n\t * also mix it with jiffies and the PID:\n\t */\n\treturn secure_ip_id((__force __be32)(current->pid + jiffies));\n}", "target": 1, "cwe": ["CWE-310"], "project": "linux-2.6", "commit_id": "8a0a9bd4db63bc45e3017bedeafbd88d0eb84d02", "hash": 291941620665843470215173682867258940568, "size": 10, "message": "random: make get_random_int() more random\n\nIt's a really simple patch that basically just open-codes the current\n\"secure_ip_id()\" call, but when open-coding it we now use a _static_\nhashing area, so that it gets updated every time.\n\nAnd to make sure somebody can't just start from the same original seed of\nall-zeroes, and then do the \"half_md4_transform()\" over and over until\nthey get the same sequence as the kernel has, each iteration also mixes in\nthe same old \"current->pid + jiffies\" we used - so we should now have a\nregular strong pseudo-number generator, but we also have one that doesn't\nhave a single seed.\n\nNote: the \"pid + jiffies\" is just meant to be a tiny tiny bit of noise. It\nhas no real meaning. It could be anything. I just picked the previous\nseed, it's just that now we keep the state in between calls and that will\nfeed into the next result, and that should make all the difference.\n\nI made that hash be a per-cpu data just to avoid cache-line ping-pong:\nhaving multiple CPU's write to the same data would be fine for randomness,\nand add yet another layer of chaos to it, but since get_random_int() is\nsupposed to be a fast interface I did it that way instead. I considered\nusing \"__raw_get_cpu_var()\" to avoid any preemption overhead while still\ngetting the hash be _mostly_ ping-pong free, but in the end good taste won\nout.\n\nSigned-off-by: Ingo Molnar <mingo@elte.hu>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 216119}
{"func": "unsigned int get_random_int(void)\n{\n\tstruct keydata *keyptr;\n\t__u32 *hash = get_cpu_var(get_random_int_hash);\n\tint ret;\n\n\tkeyptr = get_keyptr();\n\thash[0] += current->pid + jiffies + get_cycles() + (int)(long)&ret;\n\n\tret = half_md4_transform(hash, keyptr->secret);\n\tput_cpu_var(get_random_int_hash);\n\n\treturn ret;\n}", "target": 0, "cwe": ["CWE-310"], "project": "linux-2.6", "commit_id": "8a0a9bd4db63bc45e3017bedeafbd88d0eb84d02", "hash": 134018116402125477535352207466632733436, "size": 14, "message": "random: make get_random_int() more random\n\nIt's a really simple patch that basically just open-codes the current\n\"secure_ip_id()\" call, but when open-coding it we now use a _static_\nhashing area, so that it gets updated every time.\n\nAnd to make sure somebody can't just start from the same original seed of\nall-zeroes, and then do the \"half_md4_transform()\" over and over until\nthey get the same sequence as the kernel has, each iteration also mixes in\nthe same old \"current->pid + jiffies\" we used - so we should now have a\nregular strong pseudo-number generator, but we also have one that doesn't\nhave a single seed.\n\nNote: the \"pid + jiffies\" is just meant to be a tiny tiny bit of noise. It\nhas no real meaning. It could be anything. I just picked the previous\nseed, it's just that now we keep the state in between calls and that will\nfeed into the next result, and that should make all the difference.\n\nI made that hash be a per-cpu data just to avoid cache-line ping-pong:\nhaving multiple CPU's write to the same data would be fine for randomness,\nand add yet another layer of chaos to it, but since get_random_int() is\nsupposed to be a fast interface I did it that way instead. I considered\nusing \"__raw_get_cpu_var()\" to avoid any preemption overhead while still\ngetting the hash be _mostly_ ping-pong free, but in the end good taste won\nout.\n\nSigned-off-by: Ingo Molnar <mingo@elte.hu>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>", "dataset": "other", "idx": 499883}
{"func": "static int __init acpi_parse_hpet(unsigned long phys, unsigned long size)\n{\n\tstruct acpi_table_hpet *hpet_tbl;\n\n\tif (!phys || !size)\n\t\treturn -EINVAL;\n\n\thpet_tbl = (struct acpi_table_hpet *)__acpi_map_table(phys, size);\n\n\tif (hpet_tbl->addr.space_id != ACPI_SPACE_MEM) {\n\t\tprintk(KERN_WARNING PREFIX \"HPET timers must be located in \"\n\t\t       \"memory.\\n\");\n\t\treturn -1;\n\t}\n#ifdef\tCONFIG_X86_64\n\tvxtime.hpet_address = hpet_tbl->addr.addrl |\n\t    ((long)hpet_tbl->addr.addrh << 32);\n\n\tprintk(KERN_INFO PREFIX \"HPET id: %#x base: %#lx\\n\",\n\t       hpet_tbl->id, vxtime.hpet_address);\n#else\t\t\t\t/* X86 */\n\t{\n\t\textern unsigned long hpet_address;\n\n\t\thpet_address = hpet_tbl->addr.addrl;\n\t\tprintk(KERN_INFO PREFIX \"HPET id: %#x base: %#lx\\n\",\n\t\t       hpet_tbl->id, hpet_address);\n\t}\n#endif\t\t\t\t/* X86 */\n\n\treturn 0;\n}", "target": 1, "cwe": [], "project": "linux-2.6", "commit_id": "f0f4c3432e5e1087b3a8c0e6bd4113d3c37497ff", "hash": 62909509703384276763101755339705010618, "size": 36, "message": "[PATCH] i386: add HPET(s) into resource map\n\nAdd HPET(s) into resource map. This will allow for the HPET(s) to be\nvisibile within /proc/iomem.\n\nSigned-off-by: Aaron Durbin <adurbin@google.com>\nSigned-off-by: Andi Kleen <ak@suse.de>", "dataset": "other", "idx": 216124}
{"func": "static int __init acpi_parse_hpet(unsigned long phys, unsigned long size)\n{\n\tstruct acpi_table_hpet *hpet_tbl;\n\tstruct resource *hpet_res;\n\tresource_size_t res_start;\n\n\tif (!phys || !size)\n\t\treturn -EINVAL;\n\n\thpet_tbl = (struct acpi_table_hpet *)__acpi_map_table(phys, size);\n\tif (hpet_tbl->addr.space_id != ACPI_SPACE_MEM) {\n\t\tprintk(KERN_WARNING PREFIX \"HPET timers must be located in \"\n\t\t       \"memory.\\n\");\n\t\treturn -1;\n\t}\n\n#define HPET_RESOURCE_NAME_SIZE 9\n\thpet_res = alloc_bootmem(sizeof(*hpet_res) + HPET_RESOURCE_NAME_SIZE);\n\tif (hpet_res) {\n\t\tmemset(hpet_res, 0, sizeof(*hpet_res));\n\t\thpet_res->name = (void *)&hpet_res[1];\n\t\thpet_res->flags = IORESOURCE_MEM | IORESOURCE_BUSY;\n\t\tsnprintf((char *)hpet_res->name, HPET_RESOURCE_NAME_SIZE,\n\t\t\t \"HPET %u\", hpet_tbl->number);\n\t\thpet_res->end = (1 * 1024) - 1;\n\t}\n\n#ifdef\tCONFIG_X86_64\n\tvxtime.hpet_address = hpet_tbl->addr.addrl |\n\t    ((long)hpet_tbl->addr.addrh << 32);\n\n\tprintk(KERN_INFO PREFIX \"HPET id: %#x base: %#lx\\n\",\n\t       hpet_tbl->id, vxtime.hpet_address);\n\n\tres_start = vxtime.hpet_address;\n#else\t\t\t\t/* X86 */\n\t{\n\t\textern unsigned long hpet_address;\n\n\t\thpet_address = hpet_tbl->addr.addrl;\n\t\tprintk(KERN_INFO PREFIX \"HPET id: %#x base: %#lx\\n\",\n\t\t       hpet_tbl->id, hpet_address);\n\n\t\tres_start = hpet_address;\n\t}\n#endif\t\t\t\t/* X86 */\n\n\tif (hpet_res) {\n\t\thpet_res->start = res_start;\n\t\thpet_res->end += res_start;\n\t\tinsert_resource(&iomem_resource, hpet_res);\n\t}\n\n\treturn 0;\n}", "target": 0, "cwe": [], "project": "linux-2.6", "commit_id": "f0f4c3432e5e1087b3a8c0e6bd4113d3c37497ff", "hash": 73361527373490632346304437063543698062, "size": 60, "message": "[PATCH] i386: add HPET(s) into resource map\n\nAdd HPET(s) into resource map. This will allow for the HPET(s) to be\nvisibile within /proc/iomem.\n\nSigned-off-by: Aaron Durbin <adurbin@google.com>\nSigned-off-by: Andi Kleen <ak@suse.de>", "dataset": "other", "idx": 500008}
{"func": "\t\telse if ((l & SSL_kECDH) || (l & SSL_kECDHE))\n\t\t\t{\n\t\t\tconst EC_GROUP *srvr_group = NULL;\n\t\t\tEC_KEY *tkey;\n\t\t\tint ecdh_clnt_cert = 0;\n\t\t\tint field_size = 0;\n\n\t\t\t/* Did we send out the client's\n\t\t\t * ECDH share for use in premaster\n\t\t\t * computation as part of client certificate?\n\t\t\t * If so, set ecdh_clnt_cert to 1.", "target": 1, "cwe": ["CWE-476"], "project": "openssl", "commit_id": "141a5482fdd1944804cc342c1c443362eed8501b", "hash": 77229545539009014294689173951026139347, "size": 502, "message": "Fix CVE-2014-3470\n\nCheck session_cert is not NULL before dereferencing it.", "dataset": "other", "idx": 216368}
{"func": "\t\t\t{\n\t\t\tconst EC_GROUP *srvr_group = NULL;\n\t\t\tEC_KEY *tkey;\n\t\t\tint ecdh_clnt_cert = 0;\n\t\t\tint field_size = 0;\n\n\t\t\tif (s->session->sess_cert == NULL) \n\t\t\t\t{\n\t\t\t\tssl3_send_alert(s,SSL3_AL_FATAL,SSL_AD_UNEXPECTED_MESSAGE);\n\t\t\t\tSSLerr(SSL_F_SSL3_SEND_CLIENT_KEY_EXCHANGE,SSL_R_UNEXPECTED_MESSAGE);\n\t\t\t\tgoto err;\n\t\t\t\t}\n\n\t\t\t/* Did we send out the client's\n\t\t\t * ECDH share for use in premaster\n\t\t\t * computation as part of client certificate?\n\t\t\t * If so, set ecdh_clnt_cert to 1.", "target": 0, "cwe": ["CWE-476"], "project": "openssl", "commit_id": "141a5482fdd1944804cc342c1c443362eed8501b", "hash": 226060406209456150787342782468227641903, "size": 509, "message": "Fix CVE-2014-3470\n\nCheck session_cert is not NULL before dereferencing it.", "dataset": "other", "idx": 501557}
{"func": "\n    i = OBJ_obj2nid(p7->type);\n    p7->state = PKCS7_S_HEADER;\n\n    switch (i) {\n    case NID_pkcs7_signed:\n        data_body = PKCS7_get_octet_string(p7->d.sign->contents);\n        if (!PKCS7_is_detached(p7) && data_body == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_INVALID_SIGNED_DATA_TYPE);\n            goto err;\n        }\n        md_sk = p7->d.sign->md_algs;\n        break;\n    case NID_pkcs7_signedAndEnveloped:\n        rsk = p7->d.signed_and_enveloped->recipientinfo;\n        md_sk = p7->d.signed_and_enveloped->md_algs;\n        data_body = p7->d.signed_and_enveloped->enc_data->enc_data;\n        enc_alg = p7->d.signed_and_enveloped->enc_data->algorithm;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n            goto err;\n        }\n        break;\n    case NID_pkcs7_enveloped:\n        rsk = p7->d.enveloped->recipientinfo;\n        enc_alg = p7->d.enveloped->enc_data->algorithm;\n        data_body = p7->d.enveloped->enc_data->enc_data;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_UNSUPPORTED_CIPHER_TYPE);\n            goto err;\n        }\n        break;\n    default:\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_UNSUPPORTED_CONTENT_TYPE);\n        goto err;\n    }\n\n    /* We will be checking the signature */\n    if (md_sk != NULL) {\n        else\n            BIO_push(out, etmp);\n        etmp = NULL;\n    }\n#if 1\n    if (PKCS7_is_detached(p7) || (in_bio != NULL)) {\n        bio = in_bio;\n    } else {\n# if 0\n        bio = BIO_new(BIO_s_mem());\n        /*", "target": 1, "cwe": [], "project": "openssl", "commit_id": "5fbc59cac60db4d7c3172152b8bdafe0c675fabd", "hash": 28643171687418372612575244617991130930, "size": 259, "message": "PKCS#7: Fix NULL dereference with missing EncryptedContent.\n\nCVE-2015-1790\n\nReviewed-by: Rich Salz <rsalz@openssl.org>", "dataset": "other", "idx": 216512}
{"func": "    i = OBJ_obj2nid(p7->type);\n    p7->state = PKCS7_S_HEADER;\n\n    switch (i) {\n    case NID_pkcs7_signed:\n        /*\n         * p7->d.sign->contents is a PKCS7 structure consisting of a contentType\n         * field and optional content.\n         * data_body is NULL if that structure has no (=detached) content\n         * or if the contentType is wrong (i.e., not \"data\").\n         */\n        data_body = PKCS7_get_octet_string(p7->d.sign->contents);\n        if (!PKCS7_is_detached(p7) && data_body == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_INVALID_SIGNED_DATA_TYPE);\n            goto err;\n        md_sk = p7->d.sign->md_algs;\n        break;\n    case NID_pkcs7_signedAndEnveloped:\n        rsk = p7->d.signed_and_enveloped->recipientinfo;\n        md_sk = p7->d.signed_and_enveloped->md_algs;\n        /* data_body is NULL if the optional EncryptedContent is missing. */\n        data_body = p7->d.signed_and_enveloped->enc_data->enc_data;\n        enc_alg = p7->d.signed_and_enveloped->enc_data->algorithm;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n        }\n        break;\n    case NID_pkcs7_enveloped:\n        rsk = p7->d.enveloped->recipientinfo;\n        enc_alg = p7->d.enveloped->enc_data->algorithm;\n        /* data_body is NULL if the optional EncryptedContent is missing. */\n        data_body = p7->d.enveloped->enc_data->enc_data;\n        evp_cipher = EVP_get_cipherbyobj(enc_alg->algorithm);\n        if (evp_cipher == NULL) {\n            PKCS7err(PKCS7_F_PKCS7_DATADECODE,\n                     PKCS7_R_UNSUPPORTED_CIPHER_TYPE);\n            goto err;\n        }\n        break;\n    default:\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_UNSUPPORTED_CONTENT_TYPE);\n        goto err;\n    }\n\n    /* Detached content must be supplied via in_bio instead. */\n    if (data_body == NULL && in_bio == NULL) {\n        PKCS7err(PKCS7_F_PKCS7_DATADECODE, PKCS7_R_NO_CONTENT);\n        goto err;\n    }\n\n    /* We will be checking the signature */\n    if (md_sk != NULL) {\n        else\n            BIO_push(out, etmp);\n        etmp = NULL;\n    }\n#if 1\n    if (in_bio != NULL) {\n        bio = in_bio;\n    } else {\n# if 0\n        bio = BIO_new(BIO_s_mem());\n        /*", "target": 0, "cwe": [], "project": "openssl", "commit_id": "5fbc59cac60db4d7c3172152b8bdafe0c675fabd", "hash": 162832984232169353223754051711762414456, "size": 273, "message": "PKCS#7: Fix NULL dereference with missing EncryptedContent.\n\nCVE-2015-1790\n\nReviewed-by: Rich Salz <rsalz@openssl.org>", "dataset": "other", "idx": 502658}
{"func": "         * If we haven't got a least one certificate from our store then check\n         * if there is an alternative chain that could be used.  We only do this\n         * if the user hasn't switched off alternate chain checking\n         */\n        retry = 0;\n        if (j == ctx->last_untrusted &&\n            !(ctx->param->flags & X509_V_FLAG_NO_ALT_CHAINS)) {\n            while (j-- > 1) {\n                xtmp2 = sk_X509_value(ctx->chain, j - 1);\n                ok = ctx->get_issuer(&xtmp, ctx, xtmp2);\n                if (ok < 0)", "target": 1, "cwe": ["CWE-254"], "project": "openssl", "commit_id": "cb22d2ae5a5b6069dbf66dbcce07223ac15a16de", "hash": 157510682266880135227270063780618680419, "size": 285, "message": "Fix alt chains bug\n\nThis is a follow up to the alternate chains certificate forgery issue\n(CVE-2015-1793). That issue is exacerbated in 1.0.1 by a related bug which\nmeans that we *always* check for an alternative chain, even if we have\nalready found a chain. The code is supposed to stop as soon as it has found\none (and does do in master and 1.0.2).\n\nReviewed-by: Stephen Henson <steve@openssl.org>", "dataset": "other", "idx": 216521}
{"func": "         * If we haven't got a least one certificate from our store then check\n         * if there is an alternative chain that could be used.  We only do this\n         * if the user hasn't switched off alternate chain checking\n         */\n        retry = 0;\n        if (num == ctx->last_untrusted &&\n            !(ctx->param->flags & X509_V_FLAG_NO_ALT_CHAINS)) {\n            while (j-- > 1) {\n                xtmp2 = sk_X509_value(ctx->chain, j - 1);\n                ok = ctx->get_issuer(&xtmp, ctx, xtmp2);\n                if (ok < 0)", "target": 0, "cwe": ["CWE-254"], "project": "openssl", "commit_id": "cb22d2ae5a5b6069dbf66dbcce07223ac15a16de", "hash": 142820662815600028556509579005056804595, "size": 285, "message": "Fix alt chains bug\n\nThis is a follow up to the alternate chains certificate forgery issue\n(CVE-2015-1793). That issue is exacerbated in 1.0.1 by a related bug which\nmeans that we *always* check for an alternative chain, even if we have\nalready found a chain. The code is supposed to stop as soon as it has found\none (and does do in master and 1.0.2).\n\nReviewed-by: Stephen Henson <steve@openssl.org>", "dataset": "other", "idx": 502765}
{"func": "            else\n                n = (unsigned int)len - read_bytes;\n\n            memcpy(buf, &(rr->data[rr->off]), n);\n            buf += n;\n            if (!peek) {\n                SSL3_RECORD_sub_length(rr, n);\n                SSL3_RECORD_add_off(rr, n);\n                if (SSL3_RECORD_get_length(rr) == 0) {\n                    s->rlayer.rstate = SSL_ST_READ_HEADER;\n                    SSL3_RECORD_set_off(rr, 0);", "target": 1, "cwe": ["CWE-20"], "project": "openssl", "commit_id": "63658103d4441924f8dbfc517b99bb54758a98b9", "hash": 229842592254180110350134928519396587226, "size": 532, "message": "Fix a hang with SSL_peek()\n\nIf while calling SSL_peek() we read an empty record then we go into an\ninfinite loop, continually trying to read data from the empty record and\nnever making any progress. This could be exploited by a malicious peer in\na Denial Of Service attack.\n\nCVE-2016-6305\n\nGitHub Issue #1563\n\nReviewed-by: Rich Salz <rsalz@openssl.org>", "dataset": "other", "idx": 216634}
{"func": "            else\n                n = (unsigned int)len - read_bytes;\n\n            memcpy(buf, &(rr->data[rr->off]), n);\n            buf += n;\n            if (peek) {\n                /* Mark any zero length record as consumed CVE-2016-6305 */\n                if (SSL3_RECORD_get_length(rr) == 0)\n                    SSL3_RECORD_set_read(rr);\n            } else {\n                SSL3_RECORD_sub_length(rr, n);\n                SSL3_RECORD_add_off(rr, n);\n                if (SSL3_RECORD_get_length(rr) == 0) {\n                    s->rlayer.rstate = SSL_ST_READ_HEADER;\n                    SSL3_RECORD_set_off(rr, 0);", "target": 0, "cwe": ["CWE-20"], "project": "openssl", "commit_id": "63658103d4441924f8dbfc517b99bb54758a98b9", "hash": 282966599163418320840049525998042662202, "size": 536, "message": "Fix a hang with SSL_peek()\n\nIf while calling SSL_peek() we read an empty record then we go into an\ninfinite loop, continually trying to read data from the empty record and\nnever making any progress. This could be exploited by a malicious peer in\na Denial Of Service attack.\n\nCVE-2016-6305\n\nGitHub Issue #1563\n\nReviewed-by: Rich Salz <rsalz@openssl.org>", "dataset": "other", "idx": 503768}
{"func": "\tstruct auth_client_request *request = *_request;\n\n\t*_request = NULL;\n\n\tauth_client_send_cancel(request->conn->client, request->id);\n\tcall_callback(request, AUTH_REQUEST_STATUS_ABORT, NULL, NULL);\n\tpool_unref(&request->pool);\n}", "target": 1, "cwe": [], "project": "core", "commit_id": "a9b135760aea6d1790d447d351c56b78889dac22", "hash": 314867317538556373212284150622992900603, "size": 10, "message": "lib-auth: Remove request after abort\n\nOtherwise the request will still stay in hash table\nand get dereferenced when all requests are aborted\ncausing an attempt to access free'd memory.\n\nFound by Apollon Oikonomopoulos <apoikos@debian.org>\n\nBroken in 1a29ed2f96da1be22fa5a4d96c7583aa81b8b060", "dataset": "other", "idx": 216710}
{"func": "\n\t*_request = NULL;\n\n\tauth_client_send_cancel(request->conn->client, request->id);\n\tcall_callback(request, AUTH_REQUEST_STATUS_ABORT, NULL, NULL);\n\t/* remove the request */\n\tauth_server_connection_remove_request(request->conn, request->id);\n\tpool_unref(&request->pool);\n}", "target": 0, "cwe": [], "project": "core", "commit_id": "a9b135760aea6d1790d447d351c56b78889dac22", "hash": 93121729489284854446543863404294639172, "size": 12, "message": "lib-auth: Remove request after abort\n\nOtherwise the request will still stay in hash table\nand get dereferenced when all requests are aborted\ncausing an attempt to access free'd memory.\n\nFound by Apollon Oikonomopoulos <apoikos@debian.org>\n\nBroken in 1a29ed2f96da1be22fa5a4d96c7583aa81b8b060", "dataset": "other", "idx": 505209}
{"func": "ECDSA_SIG *ossl_ecdsa_sign_sig(const unsigned char *dgst, int dgst_len,\n                               const BIGNUM *in_kinv, const BIGNUM *in_r,\n                               EC_KEY *eckey)\n{\n    int ok = 0, i;\n    BIGNUM *kinv = NULL, *s, *m = NULL, *tmp = NULL;\n    const BIGNUM *order, *ckinv;\n    BN_CTX *ctx = NULL;\n    const EC_GROUP *group;\n    ECDSA_SIG *ret;\n    const BIGNUM *priv_key;\n        ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n    s = ret->s;\n\n    if ((ctx = BN_CTX_new()) == NULL ||\n        (tmp = BN_new()) == NULL || (m = BN_new()) == NULL) {\n        ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n\n    order = EC_GROUP_get0_order(group);\n                ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_MALLOC_FAILURE);\n                goto err;\n            }\n        }\n\n        if (!BN_mod_mul(tmp, priv_key, ret->r, order, ctx)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n        if (!BN_mod_add_quick(s, tmp, m, order)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n        if (!BN_mod_mul(s, s, ckinv, order, ctx)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n        if (BN_is_zero(s)) {\n            /*\n             * if kinv and r have been supplied by the caller don't to\n             * generate new kinv and r values\n             */\n    ok = 1;\n err:\n    if (!ok) {\n        ECDSA_SIG_free(ret);\n        ret = NULL;\n    }\n    BN_CTX_free(ctx);\n    BN_clear_free(m);\n    BN_clear_free(tmp);\n    BN_clear_free(kinv);\n    return ret;\n}", "target": 1, "cwe": ["CWE-203"], "project": "openssl", "commit_id": "0c27d793745c7837b13646302b6890a556b7017a", "hash": 151477642914054661196425866678434310834, "size": 118, "message": "Add blinding to an ECDSA signature\n\nKeegan Ryan (NCC Group) has demonstrated a side channel attack on an\nECDSA signature operation. During signing the signer calculates:\n\ns:= k^-1 * (m + r * priv_key) mod order\n\nThe addition operation above provides a sufficient signal for a\nflush+reload attack to derive the private key given sufficient signature\noperations.\n\nAs a mitigation (based on a suggestion from Keegan) we add blinding to\nthe operation so that:\n\ns := k^-1 * blind^-1 (blind * m + blind * r * priv_key) mod order\n\nSince this attack is a localhost side channel only no CVE is assigned.\n\nReviewed-by: Rich Salz <rsalz@openssl.org>", "dataset": "other", "idx": 216718}
{"func": "ECDSA_SIG *ossl_ecdsa_sign_sig(const unsigned char *dgst, int dgst_len,\n                               const BIGNUM *in_kinv, const BIGNUM *in_r,\n                               EC_KEY *eckey)\n{\n    int ok = 0, i;\n    BIGNUM *kinv = NULL, *s, *m = NULL, *tmp = NULL, *blind = NULL;\n    BIGNUM *blindm = NULL;\n    const BIGNUM *order, *ckinv;\n    BN_CTX *ctx = NULL;\n    const EC_GROUP *group;\n    ECDSA_SIG *ret;\n    const BIGNUM *priv_key;\n        ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n    s = ret->s;\n\n    ctx = BN_CTX_secure_new();\n    if (ctx == NULL) {\n        ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n\n    BN_CTX_start(ctx);\n    tmp = BN_CTX_get(ctx);\n    m = BN_CTX_get(ctx);\n    blind = BN_CTX_get(ctx);\n    blindm = BN_CTX_get(ctx);\n    if (blindm == NULL) {\n        ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_MALLOC_FAILURE);\n        goto err;\n    }\n\n    order = EC_GROUP_get0_order(group);\n                ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_MALLOC_FAILURE);\n                goto err;\n            }\n        }\n\n        /*\n         * The normal signature calculation is:\n         *\n         *   s := k^-1 * (m + r * priv_key) mod order\n         *\n         * We will blind this to protect against side channel attacks\n         *\n         *   s := k^-1 * blind^-1 * (blind * m + blind * r * priv_key) mod order\n         */\n\n        /* Generate a blinding value */\n        do {\n            if (!BN_rand(blind, BN_num_bits(order) - 1, BN_RAND_TOP_ANY,\n                         BN_RAND_BOTTOM_ANY))\n                goto err;\n        } while (BN_is_zero(blind));\n        BN_set_flags(blind, BN_FLG_CONSTTIME);\n        BN_set_flags(blindm, BN_FLG_CONSTTIME);\n        BN_set_flags(tmp, BN_FLG_CONSTTIME);\n\n        /* tmp := blind * priv_key * r mod order */\n        if (!BN_mod_mul(tmp, blind, priv_key, order, ctx)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n        if (!BN_mod_mul(tmp, tmp, ret->r, order, ctx)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n\n        /* blindm := blind * m mod order */\n        if (!BN_mod_mul(blindm, blind, m, order, ctx)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n\n        /* s : = (blind * priv_key * r) + (blind * m) mod order */\n        if (!BN_mod_add_quick(s, tmp, blindm, order)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n\n        /* s:= s * blind^-1 mod order */\n        if (BN_mod_inverse(blind, blind, order, ctx) == NULL) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n        if (!BN_mod_mul(s, s, blind, order, ctx)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n\n        /* s := s * k^-1 mod order */\n        if (!BN_mod_mul(s, s, ckinv, order, ctx)) {\n            ECerr(EC_F_OSSL_ECDSA_SIGN_SIG, ERR_R_BN_LIB);\n            goto err;\n        }\n\n        if (BN_is_zero(s)) {\n            /*\n             * if kinv and r have been supplied by the caller don't to\n             * generate new kinv and r values\n             */\n err:\n    if (!ok) {\n        ECDSA_SIG_free(ret);\n        ret = NULL;\n    }\n    BN_CTX_end(ctx);\n    BN_CTX_free(ctx);\n    BN_clear_free(kinv);\n    return ret;\n}", "target": 0, "cwe": ["CWE-203"], "project": "openssl", "commit_id": "0c27d793745c7837b13646302b6890a556b7017a", "hash": 100110287515814318888302066096494027558, "size": 174, "message": "Add blinding to an ECDSA signature\n\nKeegan Ryan (NCC Group) has demonstrated a side channel attack on an\nECDSA signature operation. During signing the signer calculates:\n\ns:= k^-1 * (m + r * priv_key) mod order\n\nThe addition operation above provides a sufficient signal for a\nflush+reload attack to derive the private key given sufficient signature\noperations.\n\nAs a mitigation (based on a suggestion from Keegan) we add blinding to\nthe operation so that:\n\ns := k^-1 * blind^-1 (blind * m + blind * r * priv_key) mod order\n\nSince this attack is a localhost side channel only no CVE is assigned.\n\nReviewed-by: Rich Salz <rsalz@openssl.org>", "dataset": "other", "idx": 505371}
{"func": "struct cgit_cmd *cgit_get_cmd(struct cgit_context *ctx)\n{\n\tstatic struct cgit_cmd cmds[] = {\n\t\tdef_cmd(about, 0, 1),\n\t\tdef_cmd(blob, 1, 0),\n\t\tdef_cmd(commit, 1, 1),\n\t\tdef_cmd(diff, 1, 1),\n\t\tdef_cmd(log, 1, 1),\n\t\tdef_cmd(ls_cache, 0, 0),\n\t\tdef_cmd(patch, 1, 0),\n\t\tdef_cmd(refs, 1, 1),\n\t\tdef_cmd(repolist, 0, 0),\n\t\tdef_cmd(snapshot, 1, 0),\n\t\tdef_cmd(summary, 1, 1),", "target": 1, "cwe": [], "project": "cgit", "commit_id": "02a545e63454530c1639014d3239c14ced2022c6", "hash": 93798479963557747596875102576095851163, "size": 31, "message": "Add support for cloning over http\n\nThis patch implements basic support for cloning over http, based on the\nwork on git-http-backend by Shawn O. Pearce.\n\nSigned-off-by: Lars Hjemli <hjemli@gmail.com>", "dataset": "other", "idx": 216720}
{"func": "struct cgit_cmd *cgit_get_cmd(struct cgit_context *ctx)\n{\n\tstatic struct cgit_cmd cmds[] = {\n\t\tdef_cmd(HEAD, 1, 0),\n\t\tdef_cmd(about, 0, 1),\n\t\tdef_cmd(blob, 1, 0),\n\t\tdef_cmd(commit, 1, 1),\n\t\tdef_cmd(diff, 1, 1),\n\t\tdef_cmd(info, 1, 0),\n\t\tdef_cmd(log, 1, 1),\n\t\tdef_cmd(ls_cache, 0, 0),\n\t\tdef_cmd(objects, 1, 0),\n\t\tdef_cmd(patch, 1, 0),\n\t\tdef_cmd(refs, 1, 1),\n\t\tdef_cmd(repolist, 0, 0),\n\t\tdef_cmd(snapshot, 1, 0),\n\t\tdef_cmd(summary, 1, 1),", "target": 0, "cwe": [], "project": "cgit", "commit_id": "02a545e63454530c1639014d3239c14ced2022c6", "hash": 156010204768308273094220846969433284739, "size": 34, "message": "Add support for cloning over http\n\nThis patch implements basic support for cloning over http, based on the\nwork on git-http-backend by Shawn O. Pearce.\n\nSigned-off-by: Lars Hjemli <hjemli@gmail.com>", "dataset": "other", "idx": 505388}
{"func": "Agraph_t *agroot(void* obj)\n{\n    switch (AGTYPE(obj)) {\n    case AGINEDGE:\n    case AGOUTEDGE:\n\treturn ((Agedge_t *) obj)->node->root;\n    case AGNODE:\n\treturn ((Agnode_t *) obj)->root;\n    case AGRAPH:\n\treturn ((Agraph_t *) obj)->root;\n    default:\t\t\t/* actually can't occur if only 2 bit tags */\n\tagerr(AGERR, \"agroot of a bad object\");\n\treturn NILgraph;\n    }\n}", "target": 1, "cwe": ["CWE-476"], "project": "graphviz", "commit_id": "839085f8026afd6f6920a0c31ad2a9d880d97932", "hash": 285926376776107496641309642772781622865, "size": 15, "message": "attempted fix for null pointer deference on malformed input", "dataset": "other", "idx": 216728}
{"func": "Agraph_t *agraphof(void *obj)\n{\n    switch (AGTYPE(obj)) {\n    case AGINEDGE:\n    case AGOUTEDGE:\n\treturn ((Agedge_t *) obj)->node->root;\n    case AGNODE:\n\treturn ((Agnode_t *) obj)->root;\n    case AGRAPH:\n\treturn (Agraph_t *) obj;\n    default:\t\t\t/* actually can't occur if only 2 bit tags */\n\tagerr(AGERR, \"agraphof a bad object\");\n\treturn NILgraph;\n    }\n}", "target": 0, "cwe": ["CWE-476"], "project": "graphviz", "commit_id": "839085f8026afd6f6920a0c31ad2a9d880d97932", "hash": 248563598394800902846024511939663049757, "size": 15, "message": "attempted fix for null pointer deference on malformed input", "dataset": "other", "idx": 505492}
{"func": "\tuint16_t length = read_le16(&buffer->length);\n\tuint16_t space = read_le16(&buffer->space);\n\n\t/* Empty buffer is ok */\n\tif (length == 0 && space == 0)\n\t\treturn TRUE;\n\n\tif (offset >= data_size) {\n\t\t*error = \"buffer offset out of bounds\";\n\t\treturn FALSE;\n\t}", "target": 1, "cwe": ["CWE-125"], "project": "core", "commit_id": "fb246611e62ad8c5a95b0ca180a63f17aa34b0d8", "hash": 44503422073269898902321827906643231697, "size": 23, "message": "lib-ntlm: Check buffer length on responses\n\nAdd missing check for buffer length.\n\nIf this is not checked, it is possible to send message which\ncauses read past buffer bug.\n\nBroken in c7480644202e5451fbed448508ea29a25cffc99c", "dataset": "other", "idx": 216799}
{"func": "\tuint16_t space = read_le16(&buffer->space);\n\n\t/* Empty buffer is ok */\n\tif (length == 0 && space == 0)\n\t\treturn TRUE;\n\n\tif (length > data_size) {\n\t\t*error = \"buffer length out of bounds\";\n\t\treturn FALSE;\n\t}\n\n\tif (offset >= data_size) {\n\t\t*error = \"buffer offset out of bounds\";\n\t\treturn FALSE;\n\t}", "target": 0, "cwe": ["CWE-125"], "project": "core", "commit_id": "fb246611e62ad8c5a95b0ca180a63f17aa34b0d8", "hash": 207567383368999279475384375671028455850, "size": 28, "message": "lib-ntlm: Check buffer length on responses\n\nAdd missing check for buffer length.\n\nIf this is not checked, it is possible to send message which\ncauses read past buffer bug.\n\nBroken in c7480644202e5451fbed448508ea29a25cffc99c", "dataset": "other", "idx": 506421}
{"func": "\t    if (0 > pclose(print_out))\n\t\tperror(print_out_name);\n\t} else\n#endif\n\t    if (0 > fclose(print_out))\n\t\tperror(print_out_name);\n    }\n\n    free(print_out_name);\n    print_out_name = NULL;\n    print_out_var = NULL;", "target": 1, "cwe": ["CWE-415"], "project": "gnuplot", "commit_id": "052cbd17c3cbbc602ee080b2617d32a8417d7563", "hash": 335395554038302725624523074852189909378, "size": 60, "message": "successive failures of \"set print <foo>\" could cause double-free\nBug #2312", "dataset": "other", "idx": 216803}
{"func": "\t\tperror(print_out_name);\n\t} else\n#endif\n\t    if (0 > fclose(print_out))\n\t\tperror(print_out_name);\n\tprint_out = stderr;\n    }\n\n    free(print_out_name);\n    print_out_name = NULL;\n    print_out_var = NULL;", "target": 0, "cwe": ["CWE-415"], "project": "gnuplot", "commit_id": "052cbd17c3cbbc602ee080b2617d32a8417d7563", "hash": 335994289631102428001579590913177625446, "size": 61, "message": "successive failures of \"set print <foo>\" could cause double-free\nBug #2312", "dataset": "other", "idx": 506550}
{"func": "\t\t\t++p;\n\t\t\twhile (*p != '\\0' && *p != '}' && *p != *start_of_fontname)\n\t\t\t    ++p;\n\t\t\tif (*p != *start_of_fontname) {\n\t\t\t    int_warn(NO_CARET, \"cannot interpret font name %s\", start_of_fontname);\n\t\t\t    p = start_of_fontname;\n\t\t\t}\n\t\t\tstart_of_fontname++;\n\t\t\tend_of_fontname = p++;\n\t\t\tch = *p;\n\t\t    } else {", "target": 1, "cwe": ["CWE-787"], "project": "gnuplot", "commit_id": "963c7df3e0c5266efff260d0dff757dfe03d3632", "hash": 274740808820572089301785844084339721669, "size": 391, "message": "Better error handling for faulty font syntax\n\nA missing close-quote in an enhanced text font specification could\ncause a segfault.\nBug #2303", "dataset": "other", "idx": 216804}
{"func": "\t\t\t++p;\n\t\t\twhile (*p != '\\0' && *p != '}' && *p != *start_of_fontname)\n\t\t\t    ++p;\n\t\t\tif (*p != *start_of_fontname) {\n\t\t\t    int_warn(NO_CARET, \"cannot interpret font name %s\", start_of_fontname);\n\t\t\t    p = start_of_fontname + 1;\n\t\t\t}\n\t\t\tstart_of_fontname++;\n\t\t\tend_of_fontname = p++;\n\t\t\tch = *p;\n\t\t    } else {", "target": 0, "cwe": ["CWE-787"], "project": "gnuplot", "commit_id": "963c7df3e0c5266efff260d0dff757dfe03d3632", "hash": 149342336659596954781928859318497749578, "size": 391, "message": "Better error handling for faulty font syntax\n\nA missing close-quote in an enhanced text font specification could\ncause a segfault.\nBug #2303", "dataset": "other", "idx": 506600}
{"func": "{\n  int err= 0;\n  bool free_join= 1;\n  DBUG_ENTER(\"mysql_select\");\n\n  select_lex->context.resolve_in_select_list= TRUE;\n  JOIN *join;\n  if (select_lex->join != 0)\n  {\n    join= select_lex->join;\n    /*", "target": 1, "cwe": [], "project": "server", "commit_id": "ff77a09bda884fe6bf3917eb29b9d3a2f53f919b", "hash": 228957657691958004226213087331589913552, "size": 101, "message": "MDEV-22464 Server crash on UPDATE with nested subquery\n\nUninitialized ref_pointer_array[] because setup_fields() got empty\nfields list.  mysql_multi_update() for some reason does that by\nsubstituting the fields list with empty total_list for the\nmysql_select() call (looks like wrong merge since total_list is not\nused anywhere else and is always empty). The fix would be to return\nback the original fields list. But this fails update_use_source.test\ncase:\n\n  --error ER_BAD_FIELD_ERROR\n  update v1 set t1c1=2 order by 1;\n\nActually not failing the above seems to be ok.\n\nThe other fix would be to keep resolve_in_select_list false (and that\nkeeps outer context from being resolved in\nItem_ref::fix_fields()). This fix is more consistent with how SELECT\nbehaves:\n\n  --error ER_SUBQUERY_NO_1_ROW\n  select a from t1 where a= (select 2 from t1 having (a = 3));\n\nSo this patch implements this fix.", "dataset": "other", "idx": 216900}
{"func": "{\n  int err= 0;\n  bool free_join= 1;\n  DBUG_ENTER(\"mysql_select\");\n\n  if (!fields.is_empty())\n    select_lex->context.resolve_in_select_list= true;\n  JOIN *join;\n  if (select_lex->join != 0)\n  {\n    join= select_lex->join;\n    /*", "target": 0, "cwe": [], "project": "server", "commit_id": "ff77a09bda884fe6bf3917eb29b9d3a2f53f919b", "hash": 220114340611820363716888880887583014373, "size": 102, "message": "MDEV-22464 Server crash on UPDATE with nested subquery\n\nUninitialized ref_pointer_array[] because setup_fields() got empty\nfields list.  mysql_multi_update() for some reason does that by\nsubstituting the fields list with empty total_list for the\nmysql_select() call (looks like wrong merge since total_list is not\nused anywhere else and is always empty). The fix would be to return\nback the original fields list. But this fails update_use_source.test\ncase:\n\n  --error ER_BAD_FIELD_ERROR\n  update v1 set t1c1=2 order by 1;\n\nActually not failing the above seems to be ok.\n\nThe other fix would be to keep resolve_in_select_list false (and that\nkeeps outer context from being resolved in\nItem_ref::fix_fields()). This fix is more consistent with how SELECT\nbehaves:\n\n  --error ER_SUBQUERY_NO_1_ROW\n  select a from t1 where a= (select 2 from t1 having (a = 3));\n\nSo this patch implements this fix.", "dataset": "other", "idx": 508727}
{"func": "  bool const_item() const { return used_tables() == 0; }", "target": 1, "cwe": ["CWE-617"], "project": "server", "commit_id": "2e7891080667c59ac80f788eef4d59d447595772", "hash": 176129634520084965565043652739444849021, "size": 1, "message": "MDEV-25635 Assertion failure when pushing from HAVING into WHERE of view\n\nThis bug could manifest itself after pushing a where condition over a\nmergeable derived table / view / CTE DT into a grouping view / derived\ntable / CTE V whose item list contained set functions with constant\narguments such as MIN(2), SUM(1) etc. In such cases the field references\nused in the condition pushed into the view V that correspond set functions\nare wrapped into Item_direct_view_ref wrappers. Due to a wrong implementation\nof the virtual method const_item() for the class Item_direct_view_ref the\nwrapped set functions with constant arguments could be erroneously taken\nfor constant items. This could lead to a wrong result set returned by the\nmain select query in 10.2. In 10.4 where a possibility of pushing condition\nfrom HAVING into WHERE had been added this could cause a crash.\n\nApproved by Sergey Petrunya <sergey.petrunya@mariadb.com>", "dataset": "other", "idx": 216904}
{"func": "  bool const_item() const { return true; }", "target": 0, "cwe": ["CWE-617"], "project": "server", "commit_id": "2e7891080667c59ac80f788eef4d59d447595772", "hash": 159584463450720417609161999255773197240, "size": 1, "message": "MDEV-25635 Assertion failure when pushing from HAVING into WHERE of view\n\nThis bug could manifest itself after pushing a where condition over a\nmergeable derived table / view / CTE DT into a grouping view / derived\ntable / CTE V whose item list contained set functions with constant\narguments such as MIN(2), SUM(1) etc. In such cases the field references\nused in the condition pushed into the view V that correspond set functions\nare wrapped into Item_direct_view_ref wrappers. Due to a wrong implementation\nof the virtual method const_item() for the class Item_direct_view_ref the\nwrapped set functions with constant arguments could be erroneously taken\nfor constant items. This could lead to a wrong result set returned by the\nmain select query in 10.2. In 10.4 where a possibility of pushing condition\nfrom HAVING into WHERE had been added this could cause a crash.\n\nApproved by Sergey Petrunya <sergey.petrunya@mariadb.com>", "dataset": "other", "idx": 509429}
{"func": "    (see condition before prepare_check_option() call)\n  */\n  bool it_is_update= (select_lex == thd->lex->first_select_lex()) &&\n    thd->lex->which_check_option_applicable();\n  bool save_is_item_list_lookup= select_lex->is_item_list_lookup;\n  TABLE_LIST *derived= select_lex->master_unit()->derived;\n  DBUG_ENTER(\"setup_conds\");\n\n  select_lex->is_item_list_lookup= 0;\n\n  thd->column_usage= MARK_COLUMNS_READ;\n  DBUG_PRINT(\"info\", (\"thd->column_usage: %d\", thd->column_usage));\n  select_lex->cond_count= 0;\n  select_lex->between_count= 0;\n\n      We do this ON -> WHERE transformation only once per PS/SP statement.\n    */\n    select_lex->where= *conds;\n  }\n  thd->lex->current_select->is_item_list_lookup= save_is_item_list_lookup;\n  DBUG_RETURN(thd->is_error());\n\nerr_no_arena:\n  select_lex->is_item_list_lookup= save_is_item_list_lookup;\n  DBUG_RETURN(1);", "target": 1, "cwe": ["CWE-416"], "project": "server", "commit_id": "0beed9b5e933f0ff79b3bb346524f7a451d14e38", "hash": 262702369848133778206065023423367520417, "size": 78, "message": "MDEV-28097 use-after-free when WHERE has subquery with an outer reference in HAVING\n\nwhen resolving WHERE and ON clauses, do not look in\nSELECT list/aliases.", "dataset": "other", "idx": 216967}
{"func": "  */\n  bool it_is_update= (select_lex == thd->lex->first_select_lex()) &&\n    thd->lex->which_check_option_applicable();\n  bool save_is_item_list_lookup= select_lex->is_item_list_lookup;\n  TABLE_LIST *derived= select_lex->master_unit()->derived;\n  bool save_resolve_in_select_list= select_lex->context.resolve_in_select_list;\n  DBUG_ENTER(\"setup_conds\");\n\n  select_lex->is_item_list_lookup= 0;\n  select_lex->context.resolve_in_select_list= false;\n\n  thd->column_usage= MARK_COLUMNS_READ;\n  DBUG_PRINT(\"info\", (\"thd->column_usage: %d\", thd->column_usage));\n  select_lex->cond_count= 0;\n  select_lex->between_count= 0;\n      We do this ON -> WHERE transformation only once per PS/SP statement.\n    */\n    select_lex->where= *conds;\n  }\n  thd->lex->current_select->is_item_list_lookup= save_is_item_list_lookup;\n  select_lex->context.resolve_in_select_list= save_resolve_in_select_list;\n  DBUG_RETURN(thd->is_error());\n\nerr_no_arena:\n  select_lex->is_item_list_lookup= save_is_item_list_lookup;\n  DBUG_RETURN(1);", "target": 0, "cwe": ["CWE-416"], "project": "server", "commit_id": "0beed9b5e933f0ff79b3bb346524f7a451d14e38", "hash": 194559671954579966469337004526733694851, "size": 81, "message": "MDEV-28097 use-after-free when WHERE has subquery with an outer reference in HAVING\n\nwhen resolving WHERE and ON clauses, do not look in\nSELECT list/aliases.", "dataset": "other", "idx": 514568}
{"func": "address_space_translate_for_iotlb(CPUState *cpu, int asidx, hwaddr addr,\n                                  hwaddr *xlat, hwaddr *plen,\n                                  MemTxAttrs attrs, int *prot)\n{\n    MemoryRegionSection *section;\n    IOMMUMemoryRegion *iommu_mr;\n    IOMMUMemoryRegionClass *imrc;\n    IOMMUTLBEntry iotlb;\n    int iommu_idx;\n    AddressSpaceDispatch *d =\n        qatomic_rcu_read(&cpu->cpu_ases[asidx].memory_dispatch);\n\n    for (;;) {\n        section = address_space_translate_internal(d, addr, &addr, plen, false);\n\n    assert(!memory_region_is_iommu(section->mr));\n    *xlat = addr;\n    return section;\n\ntranslate_fail:\n    return &d->map.sections[PHYS_SECTION_UNASSIGNED];\n}", "target": 1, "cwe": ["CWE-908"], "project": "qemu", "commit_id": "418ade7849ce7641c0f7333718caf5091a02fd4c", "hash": 270213508919707612636527664610402006659, "size": 55, "message": "softmmu: Always initialize xlat in address_space_translate_for_iotlb\n\nThe bug is an uninitialized memory read, along the translate_fail\npath, which results in garbage being read from iotlb_to_section,\nwhich can lead to a crash in io_readx/io_writex.\n\nThe bug may be fixed by writing any value with zero\nin ~TARGET_PAGE_MASK, so that the call to iotlb_to_section using\nthe xlat'ed address returns io_mem_unassigned, as desired by the\ntranslate_fail path.\n\nIt is most useful to record the original physical page address,\nwhich will eventually be logged by memory_region_access_valid\nwhen the access is rejected by unassigned_mem_accepts.\n\nResolves: https://gitlab.com/qemu-project/qemu/-/issues/1065\nSigned-off-by: Richard Henderson <richard.henderson@linaro.org>\nReviewed-by: Peter Maydell <peter.maydell@linaro.org>\nMessage-Id: <20220621153829.366423-1-richard.henderson@linaro.org>", "dataset": "other", "idx": 216983}
{"func": "address_space_translate_for_iotlb(CPUState *cpu, int asidx, hwaddr orig_addr,\n                                  hwaddr *xlat, hwaddr *plen,\n                                  MemTxAttrs attrs, int *prot)\n{\n    MemoryRegionSection *section;\n    IOMMUMemoryRegion *iommu_mr;\n    IOMMUMemoryRegionClass *imrc;\n    IOMMUTLBEntry iotlb;\n    int iommu_idx;\n    hwaddr addr = orig_addr;\n    AddressSpaceDispatch *d =\n        qatomic_rcu_read(&cpu->cpu_ases[asidx].memory_dispatch);\n\n    for (;;) {\n        section = address_space_translate_internal(d, addr, &addr, plen, false);\n    assert(!memory_region_is_iommu(section->mr));\n    *xlat = addr;\n    return section;\n\ntranslate_fail:\n    /*\n     * We should be given a page-aligned address -- certainly\n     * tlb_set_page_with_attrs() does so.  The page offset of xlat\n     * is used to index sections[], and PHYS_SECTION_UNASSIGNED = 0.\n     * The page portion of xlat will be logged by memory_region_access_valid()\n     * when this memory access is rejected, so use the original untranslated\n     * physical address.\n     */\n    assert((orig_addr & ~TARGET_PAGE_MASK) == 0);\n    *xlat = orig_addr;\n    return &d->map.sections[PHYS_SECTION_UNASSIGNED];\n}", "target": 0, "cwe": ["CWE-908"], "project": "qemu", "commit_id": "418ade7849ce7641c0f7333718caf5091a02fd4c", "hash": 321441136289267261236517804510082881725, "size": 66, "message": "softmmu: Always initialize xlat in address_space_translate_for_iotlb\n\nThe bug is an uninitialized memory read, along the translate_fail\npath, which results in garbage being read from iotlb_to_section,\nwhich can lead to a crash in io_readx/io_writex.\n\nThe bug may be fixed by writing any value with zero\nin ~TARGET_PAGE_MASK, so that the call to iotlb_to_section using\nthe xlat'ed address returns io_mem_unassigned, as desired by the\ntranslate_fail path.\n\nIt is most useful to record the original physical page address,\nwhich will eventually be logged by memory_region_access_valid\nwhen the access is rejected by unassigned_mem_accepts.\n\nResolves: https://gitlab.com/qemu-project/qemu/-/issues/1065\nSigned-off-by: Richard Henderson <richard.henderson@linaro.org>\nReviewed-by: Peter Maydell <peter.maydell@linaro.org>\nMessage-Id: <20220621153829.366423-1-richard.henderson@linaro.org>", "dataset": "other", "idx": 514752}
{"project": "bootstrap-dht", "commit_id": "e809ea80e3527e32c40756eddd8b2ae44bc3af1a", "target": 1, "func": "\t\t\t\t\tbdecode_errors::error_code_enum e = bdecode_errors::no_error;\n\t\t\t\t\tstart = parse_int(start, end, ':', len, e);\n\t\t\t\t\tif (e)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(e);\n\n\t\t\t\t\tif (start + len + 1 > end)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::unexpected_eof);\n\n\t\t\t\t\tif (len < 0)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::overflow);\n\n\t\t\t\t\tboost::int64_t len = t - '0';\n\t\t\t\t\tbdecode_errors::error_code_enum e = bdecode_errors::no_error;\n\t\t\t\t\tstart = parse_int(start, end, ':', len, e);\n\t\t\t\t\tif (e)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(e);\n\t\t\t\t\tif (start + len + 1 > end)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::unexpected_eof);\n\t\t\t\t\tif (len < 0)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::overflow);\n\n\t\t\t\t\t++start;\n\t\t\t\t\ttop->construct_string(start, int(len));\n\t\t\t\t\tstack.pop_back();\n\t\t\t\t\tstart += len;\n\t\t\t\t\tcontinue;\n\t\t\t\t}", "idx": 217129, "cwe": "CWE-20", "hash": 303897306844529998268848892134869917191, "dataset": "other"}
{"project": "bootstrap-dht", "commit_id": "e809ea80e3527e32c40756eddd8b2ae44bc3af1a", "target": 0, "func": "\t\t\t\t\tbdecode_errors::error_code_enum e = bdecode_errors::no_error;\n\t\t\t\t\tstart = parse_int(start, end, ':', len, e);\n\t\t\t\t\tif (e)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(e);\n\n\t\t\t\t\t// remaining buffer size excluding ':'\n\t\t\t\t\tconst ptrdiff_t buff_size = end - start - 1;\n\t\t\t\t\tif (len > buff_size)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::unexpected_eof);\n\n\t\t\t\t\tif (len < 0)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::overflow);\n\n\t\t\t\t\tboost::int64_t len = t - '0';\n\t\t\t\t\tbdecode_errors::error_code_enum e = bdecode_errors::no_error;\n\t\t\t\t\tstart = parse_int(start, end, ':', len, e);\n\t\t\t\t\tif (e)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(e);\n\n\t\t\t\t\t// remaining buffer size excluding ':'\n\t\t\t\t\tconst ptrdiff_t buff_size = end - start - 1;\n\t\t\t\t\tif (len > buff_size)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::unexpected_eof);\n\t\t\t\t\tif (len < 0)\n\t\t\t\t\t\tTORRENT_FAIL_BDECODE(bdecode_errors::overflow);\n\n\t\t\t\t\t++start;\n\t\t\t\t\tif (start == end) TORRENT_FAIL_BDECODE(bdecode_errors::unexpected_eof);\n\t\t\t\t\ttop->construct_string(start, int(len));\n\t\t\t\t\tstack.pop_back();\n\t\t\t\t\tstart += len;\n\t\t\t\t\tcontinue;\n\t\t\t\t}", "idx": 516864, "cwe": "CWE-20", "hash": 299748660269779445607709495916524481939, "dataset": "other"}
{"project": "gd-libgd", "commit_id": "47eb44b2e90ca88a08dca9f9a1aa9041e9587f43", "target": 1, "func": "\t\tscd->lastbit = (2 + count) * 8;\n\t}\n\n\tret = 0;\n\tfor (i = scd->curbit, j = 0; j < code_size; ++i, ++j) {\n\t\tret |= ((scd->buf[i / 8] & (1 << (i % 8))) != 0) << j;\n\t}\n\n\tscd->curbit += code_size;\n\n\treturn ret;", "idx": 217131, "cwe": "CWE-119", "hash": 8231978434120641265466209385096992968, "dataset": "other"}
{"project": "gd-libgd", "commit_id": "47eb44b2e90ca88a08dca9f9a1aa9041e9587f43", "target": 0, "func": "\t\tscd->lastbit = (2 + count) * 8;\n\t}\n\n\tret = 0;\n\tfor (i = scd->curbit, j = 0; j < code_size; ++i, ++j) {\n\t\tif (i < CSD_BUF_SIZE * 8) {\n\t\t\tret |= ((scd->buf[i / 8] & (1 << (i % 8))) != 0) << j;\n\t\t} else {\n\t\t\tret = -1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tscd->curbit += code_size;\n\n\treturn ret;", "idx": 516911, "cwe": "CWE-119", "hash": 206733771084438888735004024525182153352, "dataset": "other"}
{"project": "opaque", "commit_id": "5ddda15d89f5ac82f4416208c5319ace4aecdc36", "target": 1, "func": "void ocall_malloc(size_t size, uint8_t **ret) {\n  *ret = static_cast<uint8_t *>(malloc(size));\n}", "idx": 217231, "cwe": "CWE-787", "hash": 130259907659980781908050896256972879576, "dataset": "other"}
{"project": "opaque", "commit_id": "5ddda15d89f5ac82f4416208c5319ace4aecdc36", "target": 0, "func": "void unsafe_ocall_malloc(size_t size, uint8_t **ret) {\n  *ret = static_cast<uint8_t *>(malloc(size));\n}", "idx": 519071, "cwe": "CWE-787", "hash": 130685230068749250584832344066735474045, "dataset": "other"}
{"project": "univention-corporate-server", "commit_id": "a28053045bd2e778c50ed1acaf4e52e1e34f6e34", "target": 1, "func": "\t\t\t}\n\n\t\t\tp+=strlen(network_line);\n\n\n\t\t} else if ( !strncmp(network_line, \"GET_DN \", strlen(\"GET_DN \")) && msg_id != UINT32_MAX && network_client_get_version(fd) > 0) {\n\n\t\t\tunivention_debug(UV_DEBUG_TRANSFILE, UV_DEBUG_ALL, \"RECV: GET_DN\");\n\n\t\t\tid=strtoul(&(network_line[strlen(\"GET_DN \")]), NULL, 10);\n", "idx": 217234, "cwe": "CWE-200", "hash": 188430389350604684179918044848604196033, "dataset": "other"}
{"project": "univention-corporate-server", "commit_id": "a28053045bd2e778c50ed1acaf4e52e1e34f6e34", "target": 0, "func": "\t\t\t}\n\n\t\t\tp+=strlen(network_line);\n\n\n\t\t} else if ( !strncmp(network_line, \"GET_DN \", strlen(\"GET_DN \")) && msg_id != UINT32_MAX && version > PROTOCOL_UNKNOWN && version < PROTOCOL_3) {\n\n\t\t\tunivention_debug(UV_DEBUG_TRANSFILE, UV_DEBUG_ALL, \"RECV: GET_DN\");\n\n\t\t\tid=strtoul(&(network_line[strlen(\"GET_DN \")]), NULL, 10);\n", "idx": 519112, "cwe": "CWE-200", "hash": 191505813336538609500405503383750297649, "dataset": "other"}
{"project": "serenity", "commit_id": "48fbf6a88d4822a1e5470cf08f29464511bd72c1", "target": 1, "func": "    if (r == 0) {\n        // n - 1 is odd, so n was even. But there is only one even prime:\n        return n == 2;\n    }\n\n    for (auto a : tests) {\n        // Technically: ASSERT(2 <= a && a <= n - 2)\n        ASSERT(a < n);\n        auto x = ModularPower(a, d, n);\n        if (x == 1 || x == predecessor)\n            continue;", "idx": 217238, "cwe": "CWE-120", "hash": 179391134827512981723637669244439395484, "dataset": "other"}
{"project": "serenity", "commit_id": "48fbf6a88d4822a1e5470cf08f29464511bd72c1", "target": 0, "func": "    if (r == 0) {\n        // n - 1 is odd, so n was even. But there is only one even prime:\n        return n == 2;\n    }\n\n    for (auto& a : tests) {\n        // Technically: ASSERT(2 <= a && a <= n - 2)\n        ASSERT(a < n);\n        auto x = ModularPower(a, d, n);\n        if (x == 1 || x == predecessor)\n            continue;", "idx": 519155, "cwe": "CWE-120", "hash": 295059986924029830016313633716285228943, "dataset": "other"}
{"project": "serenity", "commit_id": "c9f25bca048443e317f1994ba9b106f2386688c3", "target": 1, "func": "String UTF16BEDecoder::to_utf8(const StringView& input)\n{\n    StringBuilder builder(input.length() / 2);\n    for (size_t i = 0; i < input.length(); i += 2) {\n        u16 code_point = (input[i] << 8) | input[i + 1];\n        builder.append_code_point(code_point);\n    }\n    return builder.to_string();\n}", "idx": 217239, "cwe": "CWE-120", "hash": 267515062059261157996667906553021221826, "dataset": "other"}
{"project": "serenity", "commit_id": "c9f25bca048443e317f1994ba9b106f2386688c3", "target": 0, "func": "String UTF16BEDecoder::to_utf8(const StringView& input)\n{\n    StringBuilder builder(input.length() / 2);\n    size_t utf16_length = input.length() - (input.length() % 2);\n    for (size_t i = 0; i < utf16_length; i += 2) {\n        u16 code_point = (input[i] << 8) | input[i + 1];\n        builder.append_code_point(code_point);\n    }\n    return builder.to_string();\n}", "idx": 519174, "cwe": "CWE-120", "hash": 241191260041407383507436626312122244727, "dataset": "other"}
{"project": "serenity", "commit_id": "4317db7498eaa5a37068052bb0310fbc6a5f78e4", "target": 1, "func": "    bool read(ReadonlyBytes buffer)\n    {\n        auto fields_size = sizeof(CentralDirectoryRecord) - (sizeof(u8*) * 3);\n        if (buffer.size() < fields_size)\n            return false;\n        if (memcmp(buffer.data(), central_directory_record_signature, sizeof(central_directory_record_signature)) != 0)\n            return false;\n        memcpy(reinterpret_cast<void*>(&made_by_version), buffer.data() + sizeof(central_directory_record_signature), fields_size);\n        name = buffer.data() + sizeof(central_directory_record_signature) + fields_size;\n        extra_data = name + name_length;\n        comment = extra_data + extra_data_length;\n        return true;\n    }", "idx": 217240, "cwe": "CWE-120", "hash": 69369911002037312040550057926154543506, "dataset": "other"}
{"project": "serenity", "commit_id": "4317db7498eaa5a37068052bb0310fbc6a5f78e4", "target": 0, "func": "    bool read(ReadonlyBytes buffer)\n    {\n        auto fields_size = sizeof(CentralDirectoryRecord) - (sizeof(u8*) * 3);\n        if (buffer.size() < sizeof(central_directory_record_signature) + fields_size)\n            return false;\n        if (memcmp(buffer.data(), central_directory_record_signature, sizeof(central_directory_record_signature)) != 0)\n            return false;\n        memcpy(reinterpret_cast<void*>(&made_by_version), buffer.data() + sizeof(central_directory_record_signature), fields_size);\n        if (buffer.size() < sizeof(end_of_central_directory_signature) + fields_size + comment_length + name_length + extra_data_length)\n            return false;\n        name = buffer.data() + sizeof(central_directory_record_signature) + fields_size;\n        extra_data = name + name_length;\n        comment = extra_data + extra_data_length;\n        return true;\n    }", "idx": 519180, "cwe": "CWE-120", "hash": 215987508473597737335837343307564688788, "dataset": "other"}
{"project": "serenity", "commit_id": "4317db7498eaa5a37068052bb0310fbc6a5f78e4", "target": 1, "func": "    bool read(ReadonlyBytes buffer)\n    {\n        auto fields_size = sizeof(LocalFileHeader) - (sizeof(u8*) * 3);\n        if (buffer.size() < fields_size)\n            return false;\n        if (memcmp(buffer.data(), local_file_header_signature, sizeof(local_file_header_signature)) != 0)\n            return false;\n        memcpy(reinterpret_cast<void*>(&minimum_version), buffer.data() + sizeof(local_file_header_signature), fields_size);\n        name = buffer.data() + sizeof(local_file_header_signature) + fields_size;\n        extra_data = name + name_length;\n        compressed_data = extra_data + extra_data_length;\n        return true;\n    }", "idx": 217241, "cwe": "CWE-120", "hash": 319771508886292667317399837063760564198, "dataset": "other"}
{"project": "serenity", "commit_id": "4317db7498eaa5a37068052bb0310fbc6a5f78e4", "target": 0, "func": "    bool read(ReadonlyBytes buffer)\n    {\n        auto fields_size = sizeof(LocalFileHeader) - (sizeof(u8*) * 3);\n        if (buffer.size() < sizeof(local_file_header_signature) + fields_size)\n            return false;\n        if (memcmp(buffer.data(), local_file_header_signature, sizeof(local_file_header_signature)) != 0)\n            return false;\n        memcpy(reinterpret_cast<void*>(&minimum_version), buffer.data() + sizeof(local_file_header_signature), fields_size);\n        if (buffer.size() < sizeof(end_of_central_directory_signature) + fields_size + name_length + extra_data_length + compressed_size)\n            return false;\n        name = buffer.data() + sizeof(local_file_header_signature) + fields_size;\n        extra_data = name + name_length;\n        compressed_data = extra_data + extra_data_length;\n        return true;\n    }", "idx": 519176, "cwe": "CWE-120", "hash": 170061068009080567685608343570245159652, "dataset": "other"}
{"project": "serenity", "commit_id": "4317db7498eaa5a37068052bb0310fbc6a5f78e4", "target": 1, "func": "    bool read(ReadonlyBytes buffer)\n    {\n        auto fields_size = sizeof(EndOfCentralDirectory) - sizeof(u8*);\n        if (buffer.size() < fields_size)\n            return false;\n        if (memcmp(buffer.data(), end_of_central_directory_signature, sizeof(end_of_central_directory_signature)) != 0)\n            return false;\n        memcpy(reinterpret_cast<void*>(&disk_number), buffer.data() + sizeof(end_of_central_directory_signature), fields_size);\n        comment = buffer.data() + sizeof(end_of_central_directory_signature) + fields_size;\n        return true;\n    }", "idx": 217242, "cwe": "CWE-120", "hash": 192941992425069693221994624340355163369, "dataset": "other"}
{"project": "serenity", "commit_id": "4317db7498eaa5a37068052bb0310fbc6a5f78e4", "target": 0, "func": "    bool read(ReadonlyBytes buffer)\n    {\n        auto fields_size = sizeof(EndOfCentralDirectory) - sizeof(u8*);\n        if (buffer.size() < sizeof(end_of_central_directory_signature) + fields_size)\n            return false;\n        if (memcmp(buffer.data(), end_of_central_directory_signature, sizeof(end_of_central_directory_signature)) != 0)\n            return false;\n        memcpy(reinterpret_cast<void*>(&disk_number), buffer.data() + sizeof(end_of_central_directory_signature), fields_size);\n        if (buffer.size() < sizeof(end_of_central_directory_signature) + fields_size + comment_length)\n            return false;\n        comment = buffer.data() + sizeof(end_of_central_directory_signature) + fields_size;\n        return true;\n    }", "idx": 519179, "cwe": "CWE-120", "hash": 113107175078160338642110725818946808617, "dataset": "other"}
{"project": "mbed-coap", "commit_id": "4647a68e364401e81dbd370728127d844f221d93", "target": 1, "func": "                                                &message_left);\n        if (option_parse_result != 0) {\n            return -1;\n        }\n        /* Add previous option to option delta and get option number */\n        option_number += previous_option_number;\n\n        /* Add possible option length extension to resolve full length of the option */\n        option_parse_result = parse_ext_option(&option_len,\n                                                packet_data_pptr,\n                                                packet_data_start_ptr,\n                    return -1;\n                }\n                message_left = sn_coap_parser_move_packet_ptr(packet_data_pptr, packet_data_start_ptr, packet_len, option_len);\n                break;\n\n            case COAP_OPTION_ETAG:\n                /* This is managed independently because User gives this option in one character table */\n                ret_status = sn_coap_parser_options_parse_multiple_options(handle, packet_data_pptr,\n                             message_left,\n                             &dst_coap_msg_ptr->options_list_ptr->etag_ptr,\n                             (uint16_t *)&dst_coap_msg_ptr->options_list_ptr->etag_len,\n                    return -1;\n                }\n                dst_coap_msg_ptr->options_list_ptr->uri_port = sn_coap_parser_options_parse_uint(packet_data_pptr, option_len);\n                break;\n\n            case COAP_OPTION_LOCATION_QUERY:\n                ret_status = sn_coap_parser_options_parse_multiple_options(handle, packet_data_pptr, message_left,\n                             &dst_coap_msg_ptr->options_list_ptr->location_query_ptr, &dst_coap_msg_ptr->options_list_ptr->location_query_len,\n                             COAP_OPTION_LOCATION_QUERY, option_len);\n                if (ret_status < 0) {\n                    tr_error(\"sn_coap_parser_options_parse - COAP_OPTION_LOCATION_QUERY not valid!\");\n                    return -1;\n                }\n\n                break;\n\n            case COAP_OPTION_URI_PATH:\n                ret_status = sn_coap_parser_options_parse_multiple_options(handle, packet_data_pptr, message_left,\n                             &dst_coap_msg_ptr->uri_path_ptr, &dst_coap_msg_ptr->uri_path_len,\n                             COAP_OPTION_URI_PATH, option_len);\n                if (ret_status < 0) {\n                    tr_error(\"sn_coap_parser_options_parse - COAP_OPTION_URI_PATH not valid!\");", "idx": 217244, "cwe": "CWE-401", "hash": 214117041663586615268817958198922051959, "dataset": "other"}
{"project": "mbed-coap", "commit_id": "4647a68e364401e81dbd370728127d844f221d93", "target": 0, "func": "                                                &message_left);\n        if (option_parse_result != 0) {\n            return -1;\n        }\n        /* Add previous option to option delta and get option number */\n        if(sn_coap_parser_add_u16_limit(option_number, previous_option_number, &option_number) != 0)\n        {\n            return -1;\n        }\n\n        /* Add possible option length extension to resolve full length of the option */\n        option_parse_result = parse_ext_option(&option_len,\n                                                packet_data_pptr,\n                                                packet_data_start_ptr,\n                }\n                message_left = sn_coap_parser_move_packet_ptr(packet_data_pptr, packet_data_start_ptr, packet_len, option_len);\n                break;\n\n            case COAP_OPTION_ETAG:\n                if (dst_coap_msg_ptr->options_list_ptr->etag_ptr)\n                {\n                    tr_error(\"sn_coap_parser_options_parse - COAP_OPTION_ETAG exists!\");\n                    return -1;\n                }\n                /* This is managed independently because User gives this option in one character table */\n                ret_status = sn_coap_parser_options_parse_multiple_options(handle, packet_data_pptr,\n                             message_left,\n                             &dst_coap_msg_ptr->options_list_ptr->etag_ptr,\n                             (uint16_t *)&dst_coap_msg_ptr->options_list_ptr->etag_len,\n                }\n                dst_coap_msg_ptr->options_list_ptr->uri_port = sn_coap_parser_options_parse_uint(packet_data_pptr, option_len);\n                break;\n\n            case COAP_OPTION_LOCATION_QUERY:\n                if (dst_coap_msg_ptr->options_list_ptr->location_query_ptr)\n                {\n                    tr_error(\"sn_coap_parser_options_parse - COAP_OPTION_LOCATION_QUERY exists!\");\n                    return -1;\n                }\n                ret_status = sn_coap_parser_options_parse_multiple_options(handle, packet_data_pptr, message_left,\n                             &dst_coap_msg_ptr->options_list_ptr->location_query_ptr, &dst_coap_msg_ptr->options_list_ptr->location_query_len,\n                             COAP_OPTION_LOCATION_QUERY, option_len);\n                if (ret_status < 0) {\n                    tr_error(\"sn_coap_parser_options_parse - COAP_OPTION_LOCATION_QUERY not valid!\");\n                }\n\n                break;\n\n            case COAP_OPTION_URI_PATH:\n                if (dst_coap_msg_ptr->uri_path_ptr)\n                {\n                    tr_error(\"sn_coap_parser_options_parse - COAP_OPTION_URI_PATH exists!\");\n                    return -1;\n                }\n                ret_status = sn_coap_parser_options_parse_multiple_options(handle, packet_data_pptr, message_left,\n                             &dst_coap_msg_ptr->uri_path_ptr, &dst_coap_msg_ptr->uri_path_len,\n                             COAP_OPTION_URI_PATH, option_len);\n                if (ret_status < 0) {\n                    tr_error(\"sn_coap_parser_options_parse - COAP_OPTION_URI_PATH not valid!\");", "idx": 519481, "cwe": "CWE-401", "hash": 204700212510397032757265218058647314504, "dataset": "other"}
{"project": "phosphor-host-ipmid", "commit_id": "b265455a2518ece7c004b43c144199ec980fc620", "target": 1, "func": "        close(fd);\n        log<level::DEBUG>(\"Error creating temp file\");\n        return -EIO;\n    }\n\n    // Set the file mode as of actual ipmi-pass file.\n    if (fchmod(fileno((temp)()), st.st_mode) < 0)\n    {\n        log<level::DEBUG>(\"Error setting fchmod for temp file\");\n        return -EIO;\n    }\n", "idx": 217248, "cwe": "CWE-276", "hash": 221959307217368580668393288338011666999, "dataset": "other"}
{"project": "phosphor-host-ipmid", "commit_id": "b265455a2518ece7c004b43c144199ec980fc620", "target": 0, "func": "        close(fd);\n        log<level::DEBUG>(\"Error creating temp file\");\n        return -EIO;\n    }\n\n    // Set the file mode as read-write for owner only\n    if (fchmod(fileno((temp)()), S_IRUSR | S_IWUSR) < 0)\n    {\n        log<level::DEBUG>(\"Error setting fchmod for temp file\");\n        return -EIO;\n    }\n", "idx": 519579, "cwe": "CWE-276", "hash": 122115830430551595209272746493260850260, "dataset": "other"}
{"project": "bsdiff4", "commit_id": "49a4cee2feef7deaf9d89e5e793a8824930284d7", "target": 1, "func": "        }\n        x = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 0));\n        y = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 1));\n        z = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 2));\n        if (newpos + x > newDataLength ||\n                diffPtr + x > diffBlock + diffBlockLength ||\n                extraPtr + y > extraBlock + extraBlockLength) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_ValueError, \"corrupt patch (overflow)\");\n            return NULL;\n        }\n        memcpy(newData + newpos, diffPtr, x);\n        diffPtr += x;\n        for (j = 0; j < x; j++)\n            if ((oldpos + j >= 0) && (oldpos + j < origDataLength))\n                newData[newpos + j] += origData[oldpos + j];\n        newpos += x;\n        oldpos += x;\n        memcpy(newData + newpos, extraPtr, y);\n        extraPtr += y;\n        newpos += y;\n        oldpos += z;\n    }", "idx": 217249, "cwe": "CWE-787", "hash": 263309723372044428910155475489612911037, "dataset": "other"}
{"project": "bsdiff4", "commit_id": "49a4cee2feef7deaf9d89e5e793a8824930284d7", "target": 0, "func": "        }\n        x = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 0));\n        y = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 1));\n        z = PyLong_AsLong(PyTuple_GET_ITEM(tuple, 2));\n        if (newpos + x > newDataLength ||\n                diffPtr + x > diffBlock + diffBlockLength) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_ValueError, \"corrupt patch (overflow)\");\n            return NULL;\n        }\n        memcpy(newData + newpos, diffPtr, x);\n        for (j = 0; j < x; j++)\n            if ((oldpos + j >= 0) && (oldpos + j < origDataLength))\n                newData[newpos + j] += origData[oldpos + j];\n        newpos += x;\n        oldpos += x;\n        if (newpos + y > newDataLength ||\n                extraPtr + y > extraBlock + extraBlockLength) {\n            PyMem_Free(newData);\n            PyErr_SetString(PyExc_ValueError, \"corrupt patch (overflow)\");\n            return NULL;\n        }\n        memcpy(newData + newpos, extraPtr, y);\n        extraPtr += y;\n        newpos += y;\n        oldpos += z;\n    }", "idx": 519593, "cwe": "CWE-787", "hash": 28471137080724427079748331216528499874, "dataset": "other"}
{"project": "gilcc", "commit_id": "803969389ca9c06237075a7f8eeb1a19e6651759", "target": 1, "func": "                case '\\n':\n                    if (pbuf.tmp_indx &&\n                            (PBUF_TMP_PREV_CHAR(pbuf) == ' ' || PBUF_TMP_PREV_CHAR(pbuf) == '\\t' ||\n                             PBUF_TMP_PREV_CHAR(pbuf) == '\\n')) {\n                        pbuf.f_indx++;\n                    } else if (pbuf.tmp_indx && \n                            (PBUF_TMP_PREV_CHAR(pbuf) == '\\\\')) {\n                        pbuf.tmp_indx--;\n                        pbuf.f_indx++;\n                    } else {\n                        p_buf_push_tmp_char(&pbuf, '\\n');\n                    }\n\n                    continue;\n\n                case '\\\\':\n                    p_buf_push_tmp_char(&pbuf, '\\\\');\n                    continue;\n\n                case '/':\n                    p_buf_push_tmp_char(&pbuf, '/');\n                    continue;\n\n                case '*':\n                    if (pbuf.tmp_indx &&", "idx": 217253, "cwe": "CWE-120", "hash": 156243165944298433475865161512344109547, "dataset": "other"}
{"project": "gilcc", "commit_id": "803969389ca9c06237075a7f8eeb1a19e6651759", "target": 0, "func": "                case '\\n':\n                    if (pbuf.tmp_indx &&\n                            (PBUF_TMP_PREV_CHAR(pbuf) == ' ' || PBUF_TMP_PREV_CHAR(pbuf) == '\\t' ||\n                             PBUF_TMP_PREV_CHAR(pbuf) == '\\n')) {\n                        pbuf.f_indx++;\n                    } else if (pbuf.tmp_indx &&\n                            (PBUF_TMP_PREV_CHAR(pbuf) == '\\\\')) {\n                        pbuf.tmp_indx--;\n                        pbuf.f_indx++;\n                    } else {\n                        p_buf_push_tmp_char(&pbuf, '\\n');\n                    }\n\n                    continue;\n\n                case '\\\\':\n                    p_buf_write_tmp(&pbuf, tmp_fd);\n                    p_buf_push_tmp_char(&pbuf, '\\\\');\n                    continue;\n\n                case '/':\n                    p_buf_write_tmp(&pbuf, tmp_fd);\n                    p_buf_push_tmp_char(&pbuf, '/');\n                    continue;\n\n                case '*':\n                    if (pbuf.tmp_indx &&", "idx": 519639, "cwe": "CWE-120", "hash": 243330918601381518712115173837645650891, "dataset": "other"}
{"project": "ardour", "commit_id": "96daa4036a425ff3f23a7dfcba57bfb0f942bec6", "target": 1, "func": "static XMLSharedNodeList* find_impl(xmlXPathContext* ctxt, const string& xpath)\n{\n\txmlXPathObject* result = xmlXPathEval((const xmlChar*)xpath.c_str(), ctxt);\n\n\tif (!result) {\n\t\txmlXPathFreeContext(ctxt);\n\t\txmlFreeDoc(ctxt->doc);\n\n\t\tthrow XMLException(\"Invalid XPath: \" + xpath);\n\t}\n\n\tif (result->type != XPATH_NODESET) {\n\t\txmlXPathFreeObject(result);\n\t\txmlXPathFreeContext(ctxt);\n\t\txmlFreeDoc(ctxt->doc);\n\n\t\tthrow XMLException(\"Only nodeset result types are supported.\");\n\t}\n\n\txmlNodeSet* nodeset = result->nodesetval;", "idx": 217254, "cwe": "CWE-416", "hash": 54268186819182218721269174810414224706, "dataset": "other"}
{"project": "ardour", "commit_id": "96daa4036a425ff3f23a7dfcba57bfb0f942bec6", "target": 0, "func": "static XMLSharedNodeList* find_impl(xmlXPathContext* ctxt, const string& xpath)\n{\n\txmlXPathObject* result = xmlXPathEval((const xmlChar*)xpath.c_str(), ctxt);\n\n\tif (!result) {\n\t\txmlFreeDoc(ctxt->doc);\n\t\txmlXPathFreeContext(ctxt);\n\n\t\tthrow XMLException(\"Invalid XPath: \" + xpath);\n\t}\n\n\tif (result->type != XPATH_NODESET) {\n\t\txmlXPathFreeObject(result);\n\t\txmlFreeDoc(ctxt->doc);\n\t\txmlXPathFreeContext(ctxt);\n\n\t\tthrow XMLException(\"Only nodeset result types are supported.\");\n\t}\n\n\txmlNodeSet* nodeset = result->nodesetval;", "idx": 519644, "cwe": "CWE-416", "hash": 16148318620035569612064845448555816656, "dataset": "other"}
{"project": "jsish", "commit_id": "858da537bde4de9d8c92466d5a866505310bc328", "target": 1, "func": "int Jsi_ObjArraySizer(Jsi_Interp *interp, Jsi_Obj *obj, uint len)\n{\n    int nsiz = len + 1, mod = ALLOC_MOD_SIZE;\n    assert(obj->isarrlist);\n    if (mod>1)\n        nsiz = nsiz + ((mod-1) - (nsiz + mod - 1)%mod);\n    if (nsiz > MAX_ARRAY_LIST) {\n        Jsi_LogError(\"array size too large\");\n        return 0;\n    }\n    if (len >= obj->arrMaxSize) {\n        int oldsz = (nsiz-obj->arrMaxSize);\n        obj->arr = (Jsi_Value**)Jsi_Realloc(obj->arr, nsiz*sizeof(Jsi_Value*));", "idx": 217321, "cwe": "CWE-190", "hash": 172155516843930203788657327116633597249, "dataset": "other"}
{"project": "jsish", "commit_id": "858da537bde4de9d8c92466d5a866505310bc328", "target": 0, "func": "int Jsi_ObjArraySizer(Jsi_Interp *interp, Jsi_Obj *obj, uint len)\n{\n    uint nsiz = len + 1, mod = ALLOC_MOD_SIZE;\n    assert(obj->isarrlist);\n    if (mod>1)\n        nsiz = nsiz + ((mod-1) - (nsiz + mod - 1)%mod);\n    if (len >= interp->maxArrayList || nsiz > interp->maxArrayList) {\n        Jsi_LogError(\"array size too big: %u >= %u\", len, interp->maxArrayList);\n        return 0;\n    }\n    if (len >= obj->arrMaxSize) {\n        int oldsz = (nsiz-obj->arrMaxSize);\n        obj->arr = (Jsi_Value**)Jsi_Realloc(obj->arr, nsiz*sizeof(Jsi_Value*));", "idx": 520950, "cwe": "CWE-190", "hash": 185362943759358555372661991746152891573, "dataset": "other"}
{"project": "spnego-http-auth-nginx-module", "commit_id": "a06f9efca373e25328b1c53639a48decd0854570", "target": 1, "func": "        if (NGX_OK == ret) {\n            spnego_debug0(\"Basic auth credentials supplied by client\");\n            /* If basic auth is enabled and basic creds are supplied\n             * attempt basic auth.  If we attempt basic auth, we do\n             * not fall through to real SPNEGO */\n            if (NGX_DECLINED == ngx_http_auth_spnego_basic(r, ctx, alcf)) {\n                spnego_debug0(\"Basic auth failed\");\n                if (NGX_ERROR == ngx_http_auth_spnego_headers_basic_only(r, ctx, alcf)) {\n                    spnego_debug0(\"Error setting headers\");\n                    return (ctx->ret = NGX_HTTP_INTERNAL_SERVER_ERROR);\n                }", "idx": 217457, "cwe": "CWE-287", "hash": 86248875192199300747266955157361751740, "dataset": "other"}
{"project": "spnego-http-auth-nginx-module", "commit_id": "a06f9efca373e25328b1c53639a48decd0854570", "target": 0, "func": "        if (NGX_OK == ret) {\n            spnego_debug0(\"Basic auth credentials supplied by client\");\n            /* If basic auth is enabled and basic creds are supplied\n             * attempt basic auth.  If we attempt basic auth, we do\n             * not fall through to real SPNEGO */\n            if (NGX_OK != ngx_http_auth_spnego_basic(r, ctx, alcf)) {\n                spnego_debug0(\"Basic auth failed\");\n                if (NGX_ERROR == ngx_http_auth_spnego_headers_basic_only(r, ctx, alcf)) {\n                    spnego_debug0(\"Error setting headers\");\n                    return (ctx->ret = NGX_HTTP_INTERNAL_SERVER_ERROR);\n                }", "idx": 521446, "cwe": "CWE-287", "hash": 103708724397830618393148159210467547382, "dataset": "other"}
{"project": "skiboot", "commit_id": "5be38b672c1410e2f10acd3ad2eecfdc81d5daf7", "target": 1, "func": "static uint64_t unpack_timestamp(const struct efi_time *timestamp)\n{\n\tuint64_t val = 0;\n\tuint16_t year = le32_to_cpu(timestamp->year);\n\n\t/* pad1, nanosecond, timezone, daylight and pad2 are meant to be zero */\n\tval |= ((uint64_t) timestamp->pad1 & 0xFF) << 0;\n\tval |= ((uint64_t) timestamp->second & 0xFF) << (1*8);\n\tval |= ((uint64_t) timestamp->minute & 0xFF) << (2*8);", "idx": 217514, "cwe": "CWE-681", "hash": 261903108962534180969470598132431142070, "dataset": "other"}
{"project": "skiboot", "commit_id": "5be38b672c1410e2f10acd3ad2eecfdc81d5daf7", "target": 0, "func": "static uint64_t unpack_timestamp(const struct efi_time *timestamp)\n{\n\tuint64_t val = 0;\n\tuint16_t year = le16_to_cpu(timestamp->year);\n\n\t/* pad1, nanosecond, timezone, daylight and pad2 are meant to be zero */\n\tval |= ((uint64_t) timestamp->pad1 & 0xFF) << 0;\n\tval |= ((uint64_t) timestamp->second & 0xFF) << (1*8);\n\tval |= ((uint64_t) timestamp->minute & 0xFF) << (2*8);", "idx": 521649, "cwe": "CWE-681", "hash": 337463125729458282018478225518421551092, "dataset": "other"}
